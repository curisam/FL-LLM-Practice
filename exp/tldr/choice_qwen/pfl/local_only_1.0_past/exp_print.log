2025-10-02 06:44:19 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/local_only_1.0/exp_print.log
2025-10-02 06:44:19 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/local_only_1.0
2025-10-02 06:44:42 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-10-02 06:45:24 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-10-02 06:45:25 (federatedscope.core.configs.config:256) INFO: the used configs are: 
adapter:
  use: False
aggregator:
  BFT_args:
    
  byzantine_node_num: 0
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
  robust_rule: fedavg
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.9637]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.1592]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: True
  drop_last: False
  file_path: 
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  load_splits: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  save_splits: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.9, 0.09, 0.01]
  splits_path: ./final_data_splits
  splitter: meta
  splitter_args: []
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: reddit-tldr-comparison-choice@llm
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 2
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 0
eval:
  baseline_before_ft: True
  best_res_update_round_wise_key: val_loss
  count_flops: False
  every_n_train_steps: 10
  freq: 1
  local_only: True
  metrics: ['loss', 'acc']
  monitoring: []
  outdir: exp/tldr/choice_qwen/pfl/local_only_1.0/raw
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['val', 'test']
expname: 
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_idx_for_local_train: 0
  client_num: 53
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  master_addr: 127.0.0.1
  master_port: 29500
  merge_test_data: False
  merge_val_data: False
  method: FedAvg
  mode: standalone
  online_aggr: False
  process_num: 1
  resource_info_file: 
  restore_from: 
  sample_client_num: 53
  sample_client_rate: -1.0
  sampler: uniform
  save_client_model: False
  save_freq: -1
  save_to: 
  share_local_model: True
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
fedswa:
  use: False
finetune:
  batch_or_epoch: epoch
  before_eval: False
  epoch_linear: 10
  freeze_param: 
  local_param: []
  local_update_steps: 1
  lr_linear: 0.005
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
  simple_tuning: False
  weight_decay: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  fts:
    M: 100
    M_target: 200
    allow_load_existing_info: True
    diff: False
    fed_bo_max_iter: 50
    g_var: 1e-06
    gp_opt_schedule: 1
    local_bo_epochs: 50
    local_bo_max_iter: 50
    ls: 1.0
    obs_noise: 1e-06
    ss: 
    target_clients: []
    use: False
    v_kernel: 1.0
    var: 0.1
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  pfedhpo:
    discrete: False
    ss: 
    target_fl_total_round: 1000
    train_anchor: False
    train_fl: False
    use: False
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  trial_index: 0
  working_folder: hpo
llm:
  accelerator:
    config: 
    use: True
  adapter:
    args: [{'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}]
    balance: True
    count: 1
    grouping:
      round: 50
      use: False
    local_only: False
    mv_to_cpu: False
    use: True
    warmup:
      round: 10
      use: False
  cache:
    model: 
  chat:
    max_history_len: 10
    max_len: 1024
  deepspeed:
    ds_config: 
    use: False
  fedrlhf:
    config_file: 
    frequency: 100
    pretrained: False
    train:
      batch_or_epoch: batch
      local_update_steps: 10
    use: False
  grad_accum_step: 2
  max_new_token: 60
  num_completions: 2
  offsite_tuning:
    emu_align:
      data:
        root: data
        splits: [0.8, 0.1, 0.1]
        type: alpaca@llm
      exit_after_align: False
      init_enable_ground_truth: False
      initial_only: True
      kl_divergence: raw
      layerwise_distill: False
      restore_from: 
      save_to: 
      sim_loss: l2
      train:
        batch_or_epoch: batch
        enable_ground_truth: False
        initial_update_rounds: 50
        kd_loss_weight: 0.9
        lm_loss_weight: 0.1
        local_update_steps: 10
        optimizer:
          lr: 0.01
          type: SGD
      use: False
    emu_l: 1
    emu_r: 10
    eval_type: emu
    kwargs: [{}]
    llm_generated:
      ratio: 0.1
      use: False
    save_full_model: False
    strategy: drop_layer
    use: False
  retry_on_nan_loss: False
  reward_coeff: 0.1
  rlhf: False
  tok_len: 1024
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.5
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 256
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  llm_kwargs: [{}]
  llm_type: CausalLM
  load_from_local_pretrained_fs_config: 
  load_from_local_pretrained_model_path: 
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 1
  pretrain_tasks: []
  stage: 
  task: node
  type: Qwen/Qwen2-0.5B@huggingface_llm
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/tldr/choice_qwen/pfl/local_only_1.0
personalization:
  K: 5
  beta: 1.0
  epoch_feature: 1
  epoch_linear: 2
  local_param: []
  local_update_steps: 800
  lr: 1e-05
  lr_feature: 0.1
  lr_linear: 0.1
  regular_weight: 0.1
  share_non_trainable_para: False
  weight_decay: 0.0
print_decimal_digits: 6
quantization:
  method: none
  nbits: 8
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  data_para_dids: []
  is_enable_half: True
  local_update_steps: 800
  optimizer:
    betas: (0.9, 0.95)
    lr: 1e-05
    type: AdamW
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  choices: ['A', 'B']
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.0001
    gamma: 0.03
    inc_factor: 1.0
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: llmrewardchoicetrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2025-10-02 06:45:26 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-10-02 06:45:26 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-10-02 06:45:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=139961813524544 | out_emb=(Linear) num=151646 ptr=139961813524544 | lora_ptr=None
2025-10-02 06:45:39 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-10-02 06:45:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:45:42 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-10-02 06:45:42 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:45:45 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-10-02 06:45:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:45:47 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-10-02 06:45:47 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:45:49 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-10-02 06:45:50 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:45:52 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-10-02 06:45:52 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:45:54 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-10-02 06:45:54 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:45:56 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-10-02 06:45:57 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:45:59 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-10-02 06:45:59 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:02 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-10-02 06:46:02 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:04 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-10-02 06:46:04 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:06 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-10-02 06:46:07 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:09 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-10-02 06:46:09 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:11 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-10-02 06:46:11 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:14 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-10-02 06:46:14 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:16 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-10-02 06:46:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:18 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-10-02 06:46:19 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:21 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-10-02 06:46:21 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:23 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-10-02 06:46:23 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:26 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-10-02 06:46:26 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:28 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-10-02 06:46:29 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:31 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-10-02 06:46:31 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:33 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-10-02 06:46:33 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:36 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-10-02 06:46:36 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:38 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-10-02 06:46:38 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:41 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-10-02 06:46:41 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:43 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-10-02 06:46:43 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:46 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-10-02 06:46:46 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:48 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-10-02 06:46:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:51 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-10-02 06:46:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:53 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-10-02 06:46:53 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:56 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-10-02 06:46:56 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:46:58 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-10-02 06:46:58 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:01 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-10-02 06:47:01 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:03 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-10-02 06:47:03 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:06 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-10-02 06:47:06 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:08 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-10-02 06:47:08 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:11 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-10-02 06:47:11 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:13 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-10-02 06:47:13 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:16 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-10-02 06:47:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:18 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-10-02 06:47:19 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:21 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-10-02 06:47:21 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:23 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-10-02 06:47:23 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:26 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-10-02 06:47:26 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:28 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-10-02 06:47:28 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:30 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-10-02 06:47:31 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:33 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-10-02 06:47:33 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:35 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-10-02 06:47:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:37 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-10-02 06:47:38 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:41 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-10-02 06:47:41 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:44 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-10-02 06:47:44 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:46 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-10-02 06:47:46 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:48 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-10-02 06:47:49 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-02 06:47:51 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-10-02 06:47:51 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-10-02 06:47:51 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 336.
2025-10-02 06:47:51 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 626.
2025-10-02 06:47:51 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 336. 
Preserved para names in local update: {'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight'}.
2025-10-02 06:47:51 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-10-02 06:47:51 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-10-02 06:47:51 (federatedscope.core.workers.server:970) INFO: Waited all clients join, start now...
2025-10-02 06:47:51 (federatedscope.core.workers.server:979) INFO: ----------- Starting training (Round #0) -------------
2025-10-02 06:47:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 06:47:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 06:47:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:47:55 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 06:47:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:47:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 06:47:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:47:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:47:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:48:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=149.848358, avg_loss=0.749242, seen=200, correct=96, accuracy=0.480000
2025-10-02 06:48:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:48:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1768MB allocated=1751MB
2025-10-02 06:48:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:48:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:48:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:48:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.491446, avg_loss=0.787286, seen=40, correct=13, accuracy=0.325000
2025-10-02 06:48:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:48:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:48:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1768MB allocated=1751MB
2025-10-02 06:48:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.325000
2025-10-02 06:48:08 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_044.ckpt
2025-10-02 06:48:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 06:48:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-02 06:48:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 06:48:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:48:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 06:48:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 06:48:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 06:48:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 06:48:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 06:48:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:48:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:48:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.729034, avg_loss=0.723645, seen=200, correct=91, accuracy=0.455000
2025-10-02 06:48:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:48:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:48:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:48:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:48:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:48:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.360783, avg_loss=0.709020, seen=40, correct=20, accuracy=0.500000
2025-10-02 06:48:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:48:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:48:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:48:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 06:48:27 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_044.ckpt
2025-10-02 06:48:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 06:48:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 06:48:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 06:48:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:48:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:48:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:48:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.459503, avg_loss=0.722298, seen=200, correct=94, accuracy=0.470000
2025-10-02 06:48:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:48:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:48:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:48:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:48:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:48:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.705322, avg_loss=0.692633, seen=40, correct=21, accuracy=0.525000
2025-10-02 06:48:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:48:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:48:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 06:48:42 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_044.ckpt
2025-10-02 06:48:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 06:48:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 06:48:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 06:48:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:48:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:48:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.226318, avg_loss=0.716132, seen=200, correct=99, accuracy=0.495000
2025-10-02 06:48:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:48:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:48:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:48:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:48:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:48:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:48:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.788910, avg_loss=0.719723, seen=40, correct=21, accuracy=0.525000
2025-10-02 06:48:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:48:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:48:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:48:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:48:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.525000
2025-10-02 06:49:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 06:49:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 06:49:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 06:49:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:49:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:49:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.453888, avg_loss=0.717269, seen=200, correct=95, accuracy=0.475000
2025-10-02 06:49:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:49:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:49:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:49:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:49:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:49:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.782053, avg_loss=0.694551, seen=40, correct=22, accuracy=0.550000
2025-10-02 06:49:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:49:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:49:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:49:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 06:49:16 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_044.ckpt
2025-10-02 06:49:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 06:49:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 06:49:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 06:49:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:49:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:49:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:49:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.152130, avg_loss=0.725761, seen=200, correct=99, accuracy=0.495000
2025-10-02 06:49:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:49:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:49:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:49:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:49:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:49:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:49:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.157272, avg_loss=0.678932, seen=40, correct=21, accuracy=0.525000
2025-10-02 06:49:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:49:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:49:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:49:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.525000
2025-10-02 06:49:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 06:49:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 06:49:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 06:49:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:49:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:49:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:49:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.765961, avg_loss=0.728830, seen=200, correct=101, accuracy=0.505000
2025-10-02 06:49:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:49:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:49:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:49:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:49:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:49:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.796824, avg_loss=0.669921, seen=40, correct=24, accuracy=0.600000
2025-10-02 06:49:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:49:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:49:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:49:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 06:49:50 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_044.ckpt
2025-10-02 06:49:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 06:49:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 06:49:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 06:49:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:49:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:50:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:50:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.982391, avg_loss=0.714912, seen=200, correct=100, accuracy=0.500000
2025-10-02 06:50:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:50:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:50:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:50:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:50:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:50:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.451069, avg_loss=0.686277, seen=40, correct=20, accuracy=0.500000
2025-10-02 06:50:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:50:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:50:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.500000
2025-10-02 06:50:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 06:50:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 06:50:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 06:50:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:50:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:50:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:50:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.520798, avg_loss=0.712604, seen=200, correct=95, accuracy=0.475000
2025-10-02 06:50:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:50:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:50:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:50:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:50:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:50:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:50:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.977098, avg_loss=0.699427, seen=40, correct=21, accuracy=0.525000
2025-10-02 06:50:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:50:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:50:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:50:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.525000
2025-10-02 06:50:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 06:50:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 06:50:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 06:50:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:50:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:50:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:50:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.516785, avg_loss=0.712584, seen=200, correct=92, accuracy=0.460000
2025-10-02 06:50:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:50:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:50:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:50:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:50:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:50:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.435585, avg_loss=0.710890, seen=40, correct=20, accuracy=0.500000
2025-10-02 06:50:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:50:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:50:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.500000
2025-10-02 06:50:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 06:50:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 06:50:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 06:50:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:50:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:50:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:50:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.631805, avg_loss=0.708159, seen=200, correct=95, accuracy=0.475000
2025-10-02 06:50:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:50:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:50:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:50:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:50:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:50:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:50:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.912600, avg_loss=0.697815, seen=40, correct=20, accuracy=0.500000
2025-10-02 06:50:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:50:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:50:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:50:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.500000
2025-10-02 06:51:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 06:51:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 06:51:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 06:51:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:51:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:51:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:51:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.995667, avg_loss=0.709978, seen=200, correct=92, accuracy=0.460000
2025-10-02 06:51:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:51:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:51:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:51:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:51:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:51:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:51:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.344307, avg_loss=0.708608, seen=40, correct=20, accuracy=0.500000
2025-10-02 06:51:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:51:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:51:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.500000
2025-10-02 06:51:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 06:51:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 06:51:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 06:51:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:51:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:51:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.464218, avg_loss=0.707321, seen=200, correct=91, accuracy=0.455000
2025-10-02 06:51:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:51:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:51:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:51:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:51:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:51:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:51:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.789131, avg_loss=0.694728, seen=40, correct=20, accuracy=0.500000
2025-10-02 06:51:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:51:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:51:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.500000
2025-10-02 06:51:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 06:51:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 06:51:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 06:51:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:51:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:51:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:51:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.271271, avg_loss=0.711356, seen=200, correct=94, accuracy=0.470000
2025-10-02 06:51:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:51:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:51:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:51:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:51:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:51:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.885010, avg_loss=0.722125, seen=40, correct=15, accuracy=0.375000
2025-10-02 06:51:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:51:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:51:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.375000
2025-10-02 06:51:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 06:51:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 06:51:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 06:51:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:51:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:51:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:52:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.754791, avg_loss=0.723774, seen=200, correct=97, accuracy=0.485000
2025-10-02 06:52:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:52:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:52:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:52:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:52:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.283489, avg_loss=0.757087, seen=40, correct=15, accuracy=0.375000
2025-10-02 06:52:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:52:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:52:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:52:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.375000
2025-10-02 06:52:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 06:52:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 06:52:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 06:52:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:52:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:52:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.697815, avg_loss=0.723489, seen=200, correct=101, accuracy=0.505000
2025-10-02 06:52:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:52:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:52:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:52:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:52:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.790848, avg_loss=0.744771, seen=40, correct=15, accuracy=0.375000
2025-10-02 06:52:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:52:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:52:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.375000
2025-10-02 06:52:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 06:52:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 06:52:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 06:52:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 06:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 06:52:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.008301, avg_loss=0.705042, seen=200, correct=93, accuracy=0.465000
2025-10-02 06:52:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:52:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:52:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:52:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:52:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:52:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.415863, avg_loss=0.685397, seen=40, correct=22, accuracy=0.550000
2025-10-02 06:52:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:52:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:52:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.600000, curr=0.550000
2025-10-02 06:52:43 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 06:52:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 06:52:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 06:52:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:52:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1793MB
2025-10-02 06:52:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 06:52:44 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {}}
2025-10-02 06:52:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 06:52:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 06:52:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=705, num_train_batch_last_epoch=95, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:44 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 06:52:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 06:52:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:52:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=705, num_train_batch_last_epoch=95, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:52:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.011772, avg_loss=0.702862, seen=74, correct=40, accuracy=0.540541
2025-10-02 06:52:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:52:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:52:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1808MB allocated=1776MB
2025-10-02 06:52:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:52:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=705, num_train_batch_last_epoch=95, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:52:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.557457, avg_loss=0.763936, seen=40, correct=18, accuracy=0.450000
2025-10-02 06:52:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:52:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1808MB allocated=1776MB
2025-10-02 06:52:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-02 06:52:52 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_033.ckpt
2025-10-02 06:52:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 06:52:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-02 06:52:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:52:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 06:52:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=705, num_train_batch_last_epoch=95, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:52:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 06:52:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 06:53:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 06:53:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 06:53:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 06:53:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:53:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:53:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:53:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.553440, avg_loss=0.710182, seen=74, correct=39, accuracy=0.527027
2025-10-02 06:53:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:53:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:53:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:53:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:53:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:53:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:53:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.995060, avg_loss=0.749876, seen=40, correct=19, accuracy=0.475000
2025-10-02 06:53:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:53:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:53:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:53:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-02 06:53:07 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_033.ckpt
2025-10-02 06:53:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 06:53:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 06:53:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 06:53:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:53:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:53:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:53:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=51.928841, avg_loss=0.701741, seen=74, correct=36, accuracy=0.486486
2025-10-02 06:53:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:53:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:53:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:53:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:53:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:53:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.194923, avg_loss=0.729873, seen=40, correct=17, accuracy=0.425000
2025-10-02 06:53:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:53:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:53:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:53:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.475000, curr=0.425000
2025-10-02 06:53:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 06:53:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 06:53:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 06:53:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:53:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:53:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:53:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.198261, avg_loss=0.705382, seen=74, correct=39, accuracy=0.527027
2025-10-02 06:53:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:53:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:53:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:53:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:53:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:53:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.772068, avg_loss=0.719302, seen=40, correct=14, accuracy=0.350000
2025-10-02 06:53:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:53:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:53:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:53:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.475000, curr=0.350000
2025-10-02 06:53:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 06:53:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 06:53:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 06:53:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:53:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:53:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.229694, avg_loss=0.705807, seen=74, correct=36, accuracy=0.486486
2025-10-02 06:53:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:53:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:53:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:53:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:53:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:53:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:53:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.321297, avg_loss=0.733032, seen=40, correct=17, accuracy=0.425000
2025-10-02 06:53:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:53:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:53:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:53:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:53:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.475000, curr=0.425000
2025-10-02 06:54:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 06:54:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 06:54:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 06:54:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:54:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:54:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.788601, avg_loss=0.713359, seen=74, correct=38, accuracy=0.513514
2025-10-02 06:54:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:54:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:54:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:54:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:54:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:54:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.948782, avg_loss=0.723720, seen=40, correct=15, accuracy=0.375000
2025-10-02 06:54:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:54:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:54:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:54:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.475000, curr=0.375000
2025-10-02 06:54:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 06:54:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 06:54:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 06:54:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:54:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:54:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:54:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.354424, avg_loss=0.721006, seen=74, correct=37, accuracy=0.500000
2025-10-02 06:54:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:54:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:54:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:54:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:54:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.579792, avg_loss=0.714495, seen=40, correct=16, accuracy=0.400000
2025-10-02 06:54:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:54:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:54:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:54:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.475000, curr=0.400000
2025-10-02 06:54:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 06:54:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 06:54:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 06:54:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:54:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:54:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:54:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.625015, avg_loss=0.711149, seen=74, correct=36, accuracy=0.486486
2025-10-02 06:54:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:54:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:54:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:54:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:54:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:54:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.552322, avg_loss=0.713808, seen=40, correct=17, accuracy=0.425000
2025-10-02 06:54:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:54:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:54:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.475000, curr=0.425000
2025-10-02 06:54:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 06:54:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 06:54:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 06:54:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:54:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:54:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:54:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.463467, avg_loss=0.708966, seen=74, correct=37, accuracy=0.500000
2025-10-02 06:54:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:54:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:54:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:54:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:54:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:54:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:54:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.449963, avg_loss=0.711249, seen=40, correct=21, accuracy=0.525000
2025-10-02 06:54:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:54:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:54:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:55:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:55:00 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 06:55:00 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_033.ckpt
2025-10-02 06:55:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 06:55:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 06:55:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 06:55:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:55:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:55:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.883686, avg_loss=0.714644, seen=74, correct=36, accuracy=0.486486
2025-10-02 06:55:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:55:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:55:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:55:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:55:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:55:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.497414, avg_loss=0.712435, seen=40, correct=18, accuracy=0.450000
2025-10-02 06:55:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:55:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:55:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:55:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.450000
2025-10-02 06:55:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 06:55:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 06:55:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 06:55:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:55:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:55:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:55:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.410236, avg_loss=0.721760, seen=74, correct=36, accuracy=0.486486
2025-10-02 06:55:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:55:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:55:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:55:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:55:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:55:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.163164, avg_loss=0.704079, seen=40, correct=19, accuracy=0.475000
2025-10-02 06:55:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:55:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:55:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:55:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.525000, curr=0.475000
2025-10-02 06:55:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 06:55:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 06:55:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 06:55:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:55:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:55:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:55:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.288059, avg_loss=0.720109, seen=74, correct=37, accuracy=0.500000
2025-10-02 06:55:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:55:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:55:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:55:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:55:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:55:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:55:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.108482, avg_loss=0.702712, seen=40, correct=18, accuracy=0.450000
2025-10-02 06:55:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:55:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:55:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:55:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.525000, curr=0.450000
2025-10-02 06:55:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 06:55:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 06:55:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 06:55:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:55:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:55:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:55:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:55:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.560284, avg_loss=0.723788, seen=74, correct=40, accuracy=0.540541
2025-10-02 06:55:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:56:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:56:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:56:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:56:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:56:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.580580, avg_loss=0.714514, seen=40, correct=18, accuracy=0.450000
2025-10-02 06:56:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:56:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:56:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.525000, curr=0.450000
2025-10-02 06:56:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 06:56:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 06:56:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 06:56:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:56:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:56:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.475964, avg_loss=0.722648, seen=74, correct=41, accuracy=0.554054
2025-10-02 06:56:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:56:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:56:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:56:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:56:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:56:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.438669, avg_loss=0.710967, seen=40, correct=19, accuracy=0.475000
2025-10-02 06:56:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:56:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:56:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.525000, curr=0.475000
2025-10-02 06:56:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 06:56:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 06:56:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 06:56:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:56:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:56:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:56:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.877758, avg_loss=0.728078, seen=74, correct=31, accuracy=0.418919
2025-10-02 06:56:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:56:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:56:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:56:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:56:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:56:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:56:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.951221, avg_loss=0.698781, seen=40, correct=21, accuracy=0.525000
2025-10-02 06:56:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:56:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:56:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:56:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.525000, curr=0.525000
2025-10-02 06:56:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 06:56:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 06:56:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 06:56:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:56:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:56:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=54.050247, avg_loss=0.730409, seen=74, correct=27, accuracy=0.364865
2025-10-02 06:56:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:56:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:56:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:56:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:56:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:56:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:56:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.617512, avg_loss=0.690438, seen=40, correct=19, accuracy=0.475000
2025-10-02 06:56:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:56:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:56:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:56:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.525000, curr=0.475000
2025-10-02 06:56:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 06:56:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 06:56:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 06:56:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:56:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:56:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:56:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:56:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.467331, avg_loss=0.722531, seen=74, correct=40, accuracy=0.540541
2025-10-02 06:56:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:56:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:57:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:57:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:57:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.995405, avg_loss=0.699885, seen=40, correct=19, accuracy=0.475000
2025-10-02 06:57:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:57:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:57:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.525000, curr=0.475000
2025-10-02 06:57:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 06:57:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 06:57:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 06:57:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:57:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=52.963760, avg_loss=0.715726, seen=74, correct=41, accuracy=0.554054
2025-10-02 06:57:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:57:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:57:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:57:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.045856, avg_loss=0.701146, seen=40, correct=19, accuracy=0.475000
2025-10-02 06:57:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:57:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:57:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.525000, curr=0.475000
2025-10-02 06:57:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 06:57:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 06:57:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 06:57:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 06:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 06:57:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.387978, avg_loss=0.721459, seen=74, correct=36, accuracy=0.486486
2025-10-02 06:57:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:57:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:57:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:57:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.862471, avg_loss=0.696562, seen=40, correct=18, accuracy=0.450000
2025-10-02 06:57:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:57:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:57:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.525000, curr=0.450000
2025-10-02 06:57:33 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 06:57:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 06:57:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 06:57:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1810MB
2025-10-02 06:57:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 06:57:34 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {}}
2025-10-02 06:57:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 06:57:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 06:57:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=797, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:35 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 06:57:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:57:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 06:57:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=797, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:57:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=63.525070, avg_loss=0.765362, seen=83, correct=41, accuracy=0.493976
2025-10-02 06:57:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:57:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1830MB allocated=1793MB
2025-10-02 06:57:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=797, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:57:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.291281, avg_loss=0.732282, seen=40, correct=22, accuracy=0.550000
2025-10-02 06:57:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:57:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1830MB allocated=1793MB
2025-10-02 06:57:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 06:57:42 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_039.ckpt
2025-10-02 06:57:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 06:57:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-02 06:57:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 06:57:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=797, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 06:57:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 06:57:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 06:57:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 06:57:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 06:57:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:57:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.673859, avg_loss=0.718962, seen=83, correct=39, accuracy=0.469880
2025-10-02 06:57:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:57:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:57:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:57:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:57:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.731497, avg_loss=0.718287, seen=40, correct=19, accuracy=0.475000
2025-10-02 06:57:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:57:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:57:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:57:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:57:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.475000
2025-10-02 06:58:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 06:58:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 06:58:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 06:58:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:58:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:58:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.885483, avg_loss=0.721512, seen=83, correct=42, accuracy=0.506024
2025-10-02 06:58:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:58:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:58:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:58:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:58:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:58:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:58:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.965496, avg_loss=0.749137, seen=40, correct=18, accuracy=0.450000
2025-10-02 06:58:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:58:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:58:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:58:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.450000
2025-10-02 06:58:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 06:58:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 06:58:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 06:58:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:58:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:58:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.396423, avg_loss=0.715620, seen=83, correct=41, accuracy=0.493976
2025-10-02 06:58:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:58:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:58:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:58:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:58:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:58:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:58:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.455889, avg_loss=0.711397, seen=40, correct=21, accuracy=0.525000
2025-10-02 06:58:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:58:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:58:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.525000
2025-10-02 06:58:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 06:58:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 06:58:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 06:58:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:58:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:58:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:58:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.740654, avg_loss=0.719767, seen=83, correct=41, accuracy=0.493976
2025-10-02 06:58:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:58:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:58:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:58:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:58:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:58:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:58:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.557446, avg_loss=0.713936, seen=40, correct=19, accuracy=0.475000
2025-10-02 06:58:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:58:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:58:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:58:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.475000
2025-10-02 06:58:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 06:58:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 06:58:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 06:58:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:58:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:58:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:58:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.387909, avg_loss=0.715517, seen=83, correct=39, accuracy=0.469880
2025-10-02 06:58:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:58:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:58:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:58:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:58:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.450966, avg_loss=0.711274, seen=40, correct=20, accuracy=0.500000
2025-10-02 06:58:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:58:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:58:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:58:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:58:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.500000
2025-10-02 06:59:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 06:59:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 06:59:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 06:59:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:59:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:59:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.484425, avg_loss=0.704632, seen=83, correct=39, accuracy=0.469880
2025-10-02 06:59:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:59:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:59:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:59:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:59:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:59:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.465355, avg_loss=0.711634, seen=40, correct=17, accuracy=0.425000
2025-10-02 06:59:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:59:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:59:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.550000, curr=0.425000
2025-10-02 06:59:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 06:59:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 06:59:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 06:59:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:59:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:59:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:59:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.648987, avg_loss=0.706614, seen=83, correct=42, accuracy=0.506024
2025-10-02 06:59:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:59:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:59:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:59:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:59:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:59:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.387516, avg_loss=0.709688, seen=40, correct=18, accuracy=0.450000
2025-10-02 06:59:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:59:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:59:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:59:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.550000, curr=0.450000
2025-10-02 06:59:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 06:59:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 06:59:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 06:59:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:59:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:59:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.864189, avg_loss=0.709207, seen=83, correct=37, accuracy=0.445783
2025-10-02 06:59:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:59:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:59:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:59:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:59:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:59:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:59:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.413176, avg_loss=0.710329, seen=40, correct=19, accuracy=0.475000
2025-10-02 06:59:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:59:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:59:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:59:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.550000, curr=0.475000
2025-10-02 06:59:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 06:59:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 06:59:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 06:59:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 06:59:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:59:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 06:59:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.115700, avg_loss=0.712237, seen=83, correct=37, accuracy=0.445783
2025-10-02 06:59:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:59:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:59:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:59:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 06:59:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 06:59:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 06:59:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.223589, avg_loss=0.705590, seen=40, correct=21, accuracy=0.525000
2025-10-02 06:59:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 06:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 06:59:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 06:59:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 06:59:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.550000, curr=0.525000
2025-10-02 07:00:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:00:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:00:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:00:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 07:00:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 07:00:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=58.989811, avg_loss=0.710721, seen=83, correct=36, accuracy=0.433735
2025-10-02 07:00:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:00:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:00:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 07:00:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:00:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.085251, avg_loss=0.702131, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:00:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:00:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:00:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 07:00:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.550000, curr=0.500000
2025-10-02 07:00:12 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:00:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:00:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:00:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:00:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1826MB
2025-10-02 07:00:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:00:13 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:00:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:00:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:00:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:14 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:00:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:00:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:00:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:00:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.701889, avg_loss=0.728509, seen=200, correct=104, accuracy=0.520000
2025-10-02 07:00:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:00:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:00:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1830MB allocated=1810MB
2025-10-02 07:00:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:00:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:00:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.702730, avg_loss=0.642568, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:00:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:00:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:00:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1830MB allocated=1810MB
2025-10-02 07:00:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 07:00:24 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_034.ckpt
2025-10-02 07:00:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:00:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-02 07:00:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:00:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:00:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:00:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:00:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:00:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:00:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:00:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:00:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.320145, avg_loss=0.706601, seen=200, correct=104, accuracy=0.520000
2025-10-02 07:00:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:00:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:00:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:00:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:00:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:00:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.861820, avg_loss=0.646546, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:00:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:00:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:00:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:00:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.625000
2025-10-02 07:00:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:00:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:00:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:00:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:00:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:00:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.357544, avg_loss=0.721788, seen=200, correct=102, accuracy=0.510000
2025-10-02 07:00:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:00:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:00:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:00:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:00:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:00:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:00:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.974058, avg_loss=0.724351, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:00:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:00:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:00:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:00:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:00:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.400000
2025-10-02 07:01:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:01:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:01:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:01:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:01:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:01:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:01:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.801483, avg_loss=0.709007, seen=200, correct=100, accuracy=0.500000
2025-10-02 07:01:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:01:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:01:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:01:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:01:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:01:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:01:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.750725, avg_loss=0.693768, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:01:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:01:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:01:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.475000
2025-10-02 07:01:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:01:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:01:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:01:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:01:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:01:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:01:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.807465, avg_loss=0.699037, seen=200, correct=102, accuracy=0.510000
2025-10-02 07:01:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:01:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:01:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:01:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:01:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:01:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.497599, avg_loss=0.662440, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:01:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:01:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:01:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:01:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.625000
2025-10-02 07:01:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:01:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:01:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:01:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:01:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:01:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:01:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.417282, avg_loss=0.697086, seen=200, correct=104, accuracy=0.520000
2025-10-02 07:01:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:01:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:01:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:01:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:01:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:01:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:01:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.452621, avg_loss=0.661316, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:01:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:01:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:01:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:01:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.625000
2025-10-02 07:01:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:01:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:01:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:01:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:01:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:01:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:02:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:02:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.241074, avg_loss=0.701205, seen=200, correct=102, accuracy=0.510000
2025-10-02 07:02:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:02:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:02:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:02:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:02:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:02:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.152143, avg_loss=0.678804, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:02:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:02:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:02:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:02:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.625000
2025-10-02 07:02:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:02:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:02:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:02:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:02:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:02:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:02:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.471039, avg_loss=0.702355, seen=200, correct=106, accuracy=0.530000
2025-10-02 07:02:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:02:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:02:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:02:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:02:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:02:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:02:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.034687, avg_loss=0.675867, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:02:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:02:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:02:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:02:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.600000
2025-10-02 07:02:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:02:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:02:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:02:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:02:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:02:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.388168, avg_loss=0.701941, seen=200, correct=106, accuracy=0.530000
2025-10-02 07:02:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:02:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:02:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:02:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:02:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:02:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:02:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.487680, avg_loss=0.687192, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:02:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:02:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:02:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:02:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.625000, curr=0.600000
2025-10-02 07:02:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:02:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:02:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:02:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:02:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:02:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.075485, avg_loss=0.705377, seen=200, correct=107, accuracy=0.535000
2025-10-02 07:02:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:02:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:02:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:02:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:02:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:02:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:02:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.033136, avg_loss=0.700828, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:02:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:02:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:02:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:02:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:02:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.625000, curr=0.525000
2025-10-02 07:03:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:03:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:03:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:03:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:03:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:03:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.724487, avg_loss=0.698622, seen=200, correct=101, accuracy=0.505000
2025-10-02 07:03:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:03:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:03:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:03:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:03:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.072039, avg_loss=0.676801, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:03:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:03:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:03:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:03:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.625000, curr=0.575000
2025-10-02 07:03:13 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:03:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:03:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:03:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:03:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1843MB
2025-10-02 07:03:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:03:14 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:03:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:03:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:03:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:14 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:03:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:03:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:03:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:03:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=98.318497, avg_loss=0.717653, seen=137, correct=73, accuracy=0.532847
2025-10-02 07:03:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:03:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:03:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1852MB allocated=1826MB
2025-10-02 07:03:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:03:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:03:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.702242, avg_loss=0.767556, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:03:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:03:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1852MB allocated=1826MB
2025-10-02 07:03:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-02 07:03:21 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_012.ckpt
2025-10-02 07:03:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:03:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-02 07:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:03:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:03:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:03:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:03:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:03:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:03:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:03:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:03:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=98.238129, avg_loss=0.717067, seen=137, correct=71, accuracy=0.518248
2025-10-02 07:03:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:03:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:03:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:03:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:03:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:03:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.733898, avg_loss=0.768347, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:03:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:03:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:03:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:03:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.475000, curr=0.450000
2025-10-02 07:03:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:03:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:03:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:03:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:03:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:03:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=97.065117, avg_loss=0.708505, seen=137, correct=71, accuracy=0.518248
2025-10-02 07:03:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:03:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:03:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:03:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:03:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:03:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:03:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.203754, avg_loss=0.730094, seen=40, correct=15, accuracy=0.375000
2025-10-02 07:03:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:03:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:03:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:03:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.475000, curr=0.375000
2025-10-02 07:04:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:04:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:04:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:04:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:04:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:04:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:04:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=97.491158, avg_loss=0.711614, seen=137, correct=71, accuracy=0.518248
2025-10-02 07:04:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:04:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:04:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:04:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:04:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:04:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.357346, avg_loss=0.708934, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:04:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:04:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:04:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 07:04:12 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_012.ckpt
2025-10-02 07:04:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:04:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:04:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:04:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:04:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:04:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:04:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=97.011414, avg_loss=0.708113, seen=137, correct=67, accuracy=0.489051
2025-10-02 07:04:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:04:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:04:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:04:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:04:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:04:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.110943, avg_loss=0.702774, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:04:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:04:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:04:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.525000
2025-10-02 07:04:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:04:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:04:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:04:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:04:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:04:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=97.749847, avg_loss=0.713503, seen=137, correct=68, accuracy=0.496350
2025-10-02 07:04:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:04:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:04:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:04:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:04:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:04:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.917439, avg_loss=0.697936, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:04:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:04:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:04:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:04:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.525000, curr=0.525000
2025-10-02 07:04:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:04:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:04:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:04:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:04:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:04:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=99.121742, avg_loss=0.723516, seen=137, correct=63, accuracy=0.459854
2025-10-02 07:04:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:04:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:04:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:04:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:04:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:04:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.608156, avg_loss=0.690204, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:04:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:04:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:05:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:05:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.525000, curr=0.525000
2025-10-02 07:05:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:05:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:05:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:05:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:05:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:05:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=97.218796, avg_loss=0.709626, seen=137, correct=63, accuracy=0.459854
2025-10-02 07:05:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:05:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:05:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:05:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:05:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.176571, avg_loss=0.704414, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:05:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:05:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:05:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:05:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.525000, curr=0.475000
2025-10-02 07:05:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:05:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:05:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:05:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:05:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:05:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:05:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=96.670746, avg_loss=0.705626, seen=137, correct=70, accuracy=0.510949
2025-10-02 07:05:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:05:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:05:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:05:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:05:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:05:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.960312, avg_loss=0.724008, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:05:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:05:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:05:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:05:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.525000, curr=0.475000
2025-10-02 07:05:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:05:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:05:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:05:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:05:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:05:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=97.105255, avg_loss=0.708797, seen=137, correct=72, accuracy=0.525547
2025-10-02 07:05:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:05:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:05:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:05:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:05:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.188732, avg_loss=0.754718, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:05:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:05:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:05:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:05:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.525000, curr=0.450000
2025-10-02 07:05:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:05:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:05:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:05:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:05:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:06:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=96.276146, avg_loss=0.702746, seen=137, correct=70, accuracy=0.510949
2025-10-02 07:06:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:06:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:06:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:06:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:06:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:06:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.595098, avg_loss=0.714877, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:06:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:06:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:06:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.525000, curr=0.500000
2025-10-02 07:06:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 07:06:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 07:06:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 07:06:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:06:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:06:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:06:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=96.529686, avg_loss=0.704596, seen=137, correct=69, accuracy=0.503650
2025-10-02 07:06:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:06:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:06:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:06:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.581905, avg_loss=0.689548, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:06:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:06:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:06:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 07:06:21 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_012.ckpt
2025-10-02 07:06:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 07:06:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 07:06:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 07:06:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:06:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:06:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:06:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=96.406616, avg_loss=0.703698, seen=137, correct=65, accuracy=0.474453
2025-10-02 07:06:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:06:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:06:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:06:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:06:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.422617, avg_loss=0.685565, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:06:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:06:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:06:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 07:06:37 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_012.ckpt
2025-10-02 07:06:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 07:06:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 07:06:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 07:06:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:06:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:06:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:06:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=96.022141, avg_loss=0.700892, seen=137, correct=66, accuracy=0.481752
2025-10-02 07:06:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:06:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:06:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:06:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:06:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.595358, avg_loss=0.689884, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:06:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:06:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:06:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:06:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.500000
2025-10-02 07:07:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 07:07:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 07:07:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 07:07:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:07:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:07:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:07:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=95.710800, avg_loss=0.698619, seen=137, correct=69, accuracy=0.503650
2025-10-02 07:07:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:07:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:07:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:07:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:07:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:07:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:07:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.436028, avg_loss=0.685901, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:07:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:07:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:07:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:07:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.500000
2025-10-02 07:07:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 07:07:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 07:07:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 07:07:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:07:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:07:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=97.026550, avg_loss=0.708223, seen=137, correct=67, accuracy=0.489051
2025-10-02 07:07:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:07:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:07:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:07:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:07:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.234221, avg_loss=0.680856, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:07:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:07:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:07:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.575000
2025-10-02 07:07:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 07:07:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 07:07:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 07:07:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:07:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:07:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=96.303436, avg_loss=0.702945, seen=137, correct=70, accuracy=0.510949
2025-10-02 07:07:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:07:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:07:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:07:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:07:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.158764, avg_loss=0.678969, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:07:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:07:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:07:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.550000
2025-10-02 07:07:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 07:07:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 07:07:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 07:07:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:07:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:07:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:07:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=94.917473, avg_loss=0.692828, seen=137, correct=74, accuracy=0.540146
2025-10-02 07:07:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:07:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:07:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:07:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:08:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:08:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.862829, avg_loss=0.696571, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:08:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:08:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:08:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.475000
2025-10-02 07:08:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 07:08:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 07:08:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 07:08:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:08:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:08:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=94.619095, avg_loss=0.690650, seen=137, correct=75, accuracy=0.547445
2025-10-02 07:08:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:08:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:08:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:08:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:08:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:08:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.545856, avg_loss=0.713646, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:08:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:08:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:08:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.500000
2025-10-02 07:08:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 07:08:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 07:08:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 07:08:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:08:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:08:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:08:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=94.445251, avg_loss=0.689381, seen=137, correct=75, accuracy=0.547445
2025-10-02 07:08:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:08:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:08:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:08:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:08:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:08:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.027275, avg_loss=0.700682, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:08:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:08:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:08:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.475000
2025-10-02 07:08:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 07:08:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 07:08:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 07:08:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:08:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:08:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:08:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=95.194542, avg_loss=0.694851, seen=137, correct=72, accuracy=0.525547
2025-10-02 07:08:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:08:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:08:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:08:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:08:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:08:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.569921, avg_loss=0.689248, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:08:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:08:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:08:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:08:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.475000
2025-10-02 07:09:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 07:09:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 07:09:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 07:09:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:09:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:09:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=95.099007, avg_loss=0.694153, seen=137, correct=73, accuracy=0.532847
2025-10-02 07:09:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:09:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:09:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:09:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:09:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:09:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.075317, avg_loss=0.676883, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:09:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:09:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:09:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.550000
2025-10-02 07:09:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=220
2025-10-02 07:09:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=220
2025-10-02 07:09:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=220, splits=['val', 'test']
2025-10-02 07:09:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 07:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 07:09:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=97.098167, avg_loss=0.708746, seen=137, correct=63, accuracy=0.459854
2025-10-02 07:09:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:09:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:09:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:09:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.318146, avg_loss=0.682954, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:09:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:09:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:09:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.600000, curr=0.575000
2025-10-02 07:09:24 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:09:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:09:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:09:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:09:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1860MB
2025-10-02 07:09:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:09:25 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:09:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:09:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:09:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=346, num_train_batch_last_epoch=108, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:25 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:09:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:09:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:09:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:09:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=346, num_train_batch_last_epoch=108, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:09:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=24.922989, avg_loss=0.692305, seen=36, correct=20, accuracy=0.555556
2025-10-02 07:09:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:09:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:09:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1872MB allocated=1843MB
2025-10-02 07:09:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=346, num_train_batch_last_epoch=108, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:09:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.067190, avg_loss=0.726680, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:09:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:09:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1872MB allocated=1843MB
2025-10-02 07:09:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 07:09:32 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_003.ckpt
2025-10-02 07:09:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:09:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-02 07:09:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:09:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=346, num_train_batch_last_epoch=108, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:09:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:09:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:09:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:09:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:09:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:09:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:09:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.600893, avg_loss=0.711136, seen=36, correct=20, accuracy=0.555556
2025-10-02 07:09:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:09:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:09:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:09:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:09:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.316666, avg_loss=0.707917, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:09:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:09:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:09:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.500000
2025-10-02 07:09:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:09:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:09:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:09:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:09:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:09:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:09:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.644075, avg_loss=0.712335, seen=36, correct=20, accuracy=0.555556
2025-10-02 07:09:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:09:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:10:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:10:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:10:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.530981, avg_loss=0.713275, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:10:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:10:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.500000
2025-10-02 07:10:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:10:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:10:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:10:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:10:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:10:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.531853, avg_loss=0.709218, seen=36, correct=20, accuracy=0.555556
2025-10-02 07:10:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:10:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:10:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:10:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:10:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.407276, avg_loss=0.710182, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:10:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.450000
2025-10-02 07:10:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:10:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:10:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:10:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:10:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:10:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.452118, avg_loss=0.707003, seen=36, correct=20, accuracy=0.555556
2025-10-02 07:10:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:10:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:10:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:10:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:10:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.536907, avg_loss=0.713423, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:10:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.425000
2025-10-02 07:10:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:10:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:10:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:10:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:10:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:10:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.618776, avg_loss=0.711633, seen=36, correct=20, accuracy=0.555556
2025-10-02 07:10:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:10:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:10:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:10:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.190926, avg_loss=0.704773, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:10:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.425000
2025-10-02 07:10:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:10:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:10:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:10:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:10:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:10:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.206608, avg_loss=0.700184, seen=36, correct=20, accuracy=0.555556
2025-10-02 07:10:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:10:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:10:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:10:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.915518, avg_loss=0.697888, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:10:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:10:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:10:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:10:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:10:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.550000, curr=0.450000
2025-10-02 07:11:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:11:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:11:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:11:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:11:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:11:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.235023, avg_loss=0.700973, seen=36, correct=21, accuracy=0.583333
2025-10-02 07:11:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:11:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:11:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:11:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:11:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.840342, avg_loss=0.696009, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:11:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:11:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:11:13 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 07:11:13 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_003.ckpt
2025-10-02 07:11:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:11:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:11:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:11:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:11:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:11:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.462814, avg_loss=0.707300, seen=36, correct=21, accuracy=0.583333
2025-10-02 07:11:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:11:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:11:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:11:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:11:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:11:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.888477, avg_loss=0.697212, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:11:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:11:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:11:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.500000
2025-10-02 07:11:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:11:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:11:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:11:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:11:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:11:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.941185, avg_loss=0.720588, seen=36, correct=18, accuracy=0.500000
2025-10-02 07:11:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:11:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:11:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:11:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:11:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.972504, avg_loss=0.699313, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:11:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:11:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:11:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.450000
2025-10-02 07:11:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:11:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:11:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:11:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:11:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:11:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:11:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.712471, avg_loss=0.714235, seen=36, correct=19, accuracy=0.527778
2025-10-02 07:11:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:11:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:11:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:11:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:11:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:11:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:11:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.941690, avg_loss=0.698542, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:11:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:11:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:11:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:11:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.450000
2025-10-02 07:12:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 07:12:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 07:12:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 07:12:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:12:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:12:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:12:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=24.938444, avg_loss=0.692735, seen=36, correct=23, accuracy=0.638889
2025-10-02 07:12:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:12:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:12:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:12:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:12:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:12:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:12:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.485094, avg_loss=0.687127, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:12:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:12:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:12:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:12:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.575000
2025-10-02 07:12:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 07:12:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 07:12:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 07:12:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:12:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:12:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:12:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.671722, avg_loss=0.713103, seen=36, correct=19, accuracy=0.527778
2025-10-02 07:12:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:12:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:12:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:12:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:12:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:12:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:12:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.994156, avg_loss=0.699854, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:12:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:12:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:12:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:12:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.425000
2025-10-02 07:12:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 07:12:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 07:12:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 07:12:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:12:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:12:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.680569, avg_loss=0.713349, seen=36, correct=19, accuracy=0.527778
2025-10-02 07:12:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:12:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:12:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:12:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:12:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:12:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:12:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.572071, avg_loss=0.689302, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:12:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:12:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:12:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.475000
2025-10-02 07:12:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 07:12:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 07:12:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 07:12:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:12:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:12:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.782200, avg_loss=0.716172, seen=36, correct=17, accuracy=0.472222
2025-10-02 07:12:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:12:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:12:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:12:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:12:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:12:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.103127, avg_loss=0.702578, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:12:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:12:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:12:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:12:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.450000
2025-10-02 07:13:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 07:13:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 07:13:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 07:13:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:13:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.397518, avg_loss=0.705487, seen=36, correct=24, accuracy=0.666667
2025-10-02 07:13:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:13:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:13:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:13:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:13:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.625431, avg_loss=0.690636, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:13:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:13:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:13:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:13:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.500000
2025-10-02 07:13:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 07:13:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 07:13:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 07:13:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:13:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.400520, avg_loss=0.705570, seen=36, correct=25, accuracy=0.694444
2025-10-02 07:13:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:13:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:13:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:13:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:13:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:13:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.565750, avg_loss=0.689144, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:13:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:13:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:13:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:13:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.575000
2025-10-02 07:13:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 07:13:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 07:13:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 07:13:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 07:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 07:13:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.608253, avg_loss=0.711340, seen=36, correct=22, accuracy=0.611111
2025-10-02 07:13:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:13:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:13:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:13:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.799715, avg_loss=0.694993, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:13:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:13:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:13:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.600000, curr=0.500000
2025-10-02 07:13:39 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:13:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:13:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:13:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1877MB
2025-10-02 07:13:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:13:40 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:13:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:13:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:13:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:40 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:13:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:13:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:13:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:13:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=80.194168, avg_loss=0.716019, seen=112, correct=67, accuracy=0.598214
2025-10-02 07:13:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:13:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:13:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1860MB
2025-10-02 07:13:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:13:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.486843, avg_loss=0.687171, seen=40, correct=26, accuracy=0.650000
2025-10-02 07:13:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:13:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1894MB allocated=1860MB
2025-10-02 07:13:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-02 07:13:48 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_032.ckpt
2025-10-02 07:13:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:13:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-02 07:13:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:13:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:13:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:13:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:13:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:13:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:13:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:13:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:13:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=79.321075, avg_loss=0.708224, seen=112, correct=54, accuracy=0.482143
2025-10-02 07:13:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:13:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:14:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:14:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:14:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:14:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.615549, avg_loss=0.690389, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:14:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:14:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:14:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:14:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.650000, curr=0.525000
2025-10-02 07:14:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:14:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:14:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:14:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:14:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:14:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=80.301201, avg_loss=0.716975, seen=112, correct=52, accuracy=0.464286
2025-10-02 07:14:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:14:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:14:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:14:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:14:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:14:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.208214, avg_loss=0.730205, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:14:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:14:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:14:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:14:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.650000, curr=0.475000
2025-10-02 07:14:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:14:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:14:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:14:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:14:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:14:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=79.914543, avg_loss=0.713523, seen=112, correct=52, accuracy=0.464286
2025-10-02 07:14:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:14:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:14:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:14:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:14:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.437815, avg_loss=0.735945, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:14:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:14:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:14:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.650000, curr=0.400000
2025-10-02 07:14:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:14:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:14:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:14:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:14:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:14:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=77.866524, avg_loss=0.695237, seen=112, correct=56, accuracy=0.500000
2025-10-02 07:14:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:14:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:14:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:14:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:14:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:14:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.425842, avg_loss=0.710646, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:14:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:14:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:14:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:14:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:14:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.650000, curr=0.475000
2025-10-02 07:15:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:15:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:15:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:15:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:15:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:15:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=77.546036, avg_loss=0.692375, seen=112, correct=63, accuracy=0.562500
2025-10-02 07:15:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:15:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:15:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:15:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:15:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:15:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:15:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.527369, avg_loss=0.688184, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:15:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:15:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:15:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:15:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.650000, curr=0.575000
2025-10-02 07:15:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:15:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:15:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:15:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:15:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:15:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=78.221252, avg_loss=0.698404, seen=112, correct=64, accuracy=0.571429
2025-10-02 07:15:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:15:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:15:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:15:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:15:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:15:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.524597, avg_loss=0.688115, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:15:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:15:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:15:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:15:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.650000, curr=0.550000
2025-10-02 07:15:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:15:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:15:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:15:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:15:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:15:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=77.019608, avg_loss=0.687675, seen=112, correct=67, accuracy=0.598214
2025-10-02 07:15:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:15:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:15:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:15:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:15:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:15:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:15:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.345392, avg_loss=0.683635, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:15:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:15:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:15:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:15:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.650000, curr=0.625000
2025-10-02 07:15:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:15:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:15:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:15:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:15:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:15:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=77.082443, avg_loss=0.688236, seen=112, correct=53, accuracy=0.473214
2025-10-02 07:15:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:15:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:15:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:15:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:15:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.205128, avg_loss=0.705128, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:15:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:15:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:15:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:15:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:15:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.650000, curr=0.475000
2025-10-02 07:16:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:16:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:16:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:16:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:16:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:16:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=78.525818, avg_loss=0.701123, seen=112, correct=55, accuracy=0.491071
2025-10-02 07:16:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:16:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:16:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:16:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:16:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.266262, avg_loss=0.731657, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:16:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:16:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:16:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:16:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.650000, curr=0.400000
2025-10-02 07:16:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:16:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:16:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:16:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 07:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 07:16:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=79.299698, avg_loss=0.708033, seen=112, correct=60, accuracy=0.535714
2025-10-02 07:16:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:16:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:16:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:16:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.956348, avg_loss=0.748909, seen=40, correct=15, accuracy=0.375000
2025-10-02 07:16:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:16:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:16:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.650000, curr=0.375000
2025-10-02 07:16:27 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:16:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:16:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:16:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1942MB allocated=1893MB
2025-10-02 07:16:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:16:28 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:16:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:16:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:16:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:28 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:16:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:16:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:16:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:16:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:16:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=147.013000, avg_loss=0.735065, seen=200, correct=104, accuracy=0.520000
2025-10-02 07:16:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:16:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:16:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1914MB allocated=1877MB
2025-10-02 07:16:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:16:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:16:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.059902, avg_loss=0.726498, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:16:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:16:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1914MB allocated=1877MB
2025-10-02 07:16:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 07:16:37 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_042.ckpt
2025-10-02 07:16:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:16:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-02 07:16:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:16:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:16:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:16:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:16:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:16:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:16:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:16:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.601639, avg_loss=0.718008, seen=200, correct=92, accuracy=0.460000
2025-10-02 07:16:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:16:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:16:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:16:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:16:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:16:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:16:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.752104, avg_loss=0.693803, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:16:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:16:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:16:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:16:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:16:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.550000
2025-10-02 07:17:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:17:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:17:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:17:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:17:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:17:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.924164, avg_loss=0.719621, seen=200, correct=96, accuracy=0.480000
2025-10-02 07:17:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:17:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:17:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:17:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:17:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:17:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.906910, avg_loss=0.697673, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:17:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:17:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:17:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:17:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.500000
2025-10-02 07:17:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:17:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:17:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:17:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:17:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:17:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:17:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.660126, avg_loss=0.718301, seen=200, correct=90, accuracy=0.450000
2025-10-02 07:17:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:17:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:17:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:17:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:17:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:17:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:17:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.661598, avg_loss=0.691540, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:17:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:17:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:17:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:17:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.525000
2025-10-02 07:17:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:17:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:17:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:17:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:17:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:17:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:17:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.726929, avg_loss=0.713635, seen=200, correct=95, accuracy=0.475000
2025-10-02 07:17:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:17:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:17:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:17:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:17:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:17:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.573997, avg_loss=0.689350, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:17:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:17:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:17:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:17:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.525000
2025-10-02 07:17:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:17:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:17:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:17:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:17:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:17:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:17:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:17:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.705612, avg_loss=0.713528, seen=200, correct=101, accuracy=0.505000
2025-10-02 07:17:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:17:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:18:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:18:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:18:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:18:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.311398, avg_loss=0.682785, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:18:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:18:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:18:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:18:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.600000
2025-10-02 07:18:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:18:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:18:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:18:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:18:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:18:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.860352, avg_loss=0.714302, seen=200, correct=103, accuracy=0.515000
2025-10-02 07:18:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:18:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:18:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:18:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:18:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:18:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.775389, avg_loss=0.694385, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:18:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:18:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:18:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:18:22 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 07:18:22 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_042.ckpt
2025-10-02 07:18:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:18:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:18:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:18:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:18:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:18:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:18:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.525055, avg_loss=0.712625, seen=200, correct=95, accuracy=0.475000
2025-10-02 07:18:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:18:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:18:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:18:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:18:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:18:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.418077, avg_loss=0.685452, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:18:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:18:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:18:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.550000
2025-10-02 07:18:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:18:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:18:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:18:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:18:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:18:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.605591, avg_loss=0.718028, seen=200, correct=94, accuracy=0.470000
2025-10-02 07:18:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:18:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:18:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:18:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:18:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:18:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:18:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.147257, avg_loss=0.678681, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:18:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:18:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:18:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:18:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:18:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.600000
2025-10-02 07:19:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:19:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:19:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:19:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:19:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:19:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.105255, avg_loss=0.705526, seen=200, correct=101, accuracy=0.505000
2025-10-02 07:19:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:19:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:19:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:19:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:19:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.956177, avg_loss=0.673904, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:19:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:19:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:19:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.625000
2025-10-02 07:19:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:19:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:19:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:19:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:19:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:19:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:19:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.494873, avg_loss=0.702474, seen=200, correct=103, accuracy=0.515000
2025-10-02 07:19:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:19:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:19:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:19:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:19:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.887743, avg_loss=0.672194, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:19:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:19:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:19:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:19:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.625000
2025-10-02 07:19:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 07:19:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 07:19:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 07:19:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:19:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:19:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.072433, avg_loss=0.720362, seen=200, correct=92, accuracy=0.460000
2025-10-02 07:19:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:19:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:19:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:19:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:19:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.500563, avg_loss=0.687514, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:19:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:19:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:19:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:19:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:19:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.600000
2025-10-02 07:19:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 07:19:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 07:19:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 07:20:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:20:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:20:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=146.020966, avg_loss=0.730105, seen=200, correct=92, accuracy=0.460000
2025-10-02 07:20:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:20:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:20:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:20:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:20:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.721586, avg_loss=0.693040, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:20:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:20:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:20:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.550000
2025-10-02 07:20:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 07:20:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 07:20:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 07:20:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:20:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:20:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:20:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.610352, avg_loss=0.698052, seen=200, correct=102, accuracy=0.510000
2025-10-02 07:20:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:20:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:20:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:20:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:20:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.323229, avg_loss=0.683081, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:20:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:20:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:20:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.625000
2025-10-02 07:20:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 07:20:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 07:20:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 07:20:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:20:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:20:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:20:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.265472, avg_loss=0.701327, seen=200, correct=106, accuracy=0.530000
2025-10-02 07:20:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:20:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:20:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:20:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:20:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:20:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.164026, avg_loss=0.679101, seen=40, correct=26, accuracy=0.650000
2025-10-02 07:20:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:20:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:20:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-02 07:20:44 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_042.ckpt
2025-10-02 07:20:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 07:20:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 07:20:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 07:20:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:20:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:20:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:20:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.098694, avg_loss=0.695493, seen=200, correct=108, accuracy=0.540000
2025-10-02 07:20:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:20:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:20:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:20:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:20:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:20:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:21:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:21:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.798729, avg_loss=0.669968, seen=40, correct=28, accuracy=0.700000
2025-10-02 07:21:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:21:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:21:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-02 07:21:02 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_042.ckpt
2025-10-02 07:21:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 07:21:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 07:21:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 07:21:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:21:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:21:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.173798, avg_loss=0.690869, seen=200, correct=102, accuracy=0.510000
2025-10-02 07:21:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:21:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:21:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:21:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:21:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:21:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.648321, avg_loss=0.666208, seen=40, correct=29, accuracy=0.725000
2025-10-02 07:21:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:21:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:21:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-02 07:21:21 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_042.ckpt
2025-10-02 07:21:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 07:21:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 07:21:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 07:21:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:21:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:21:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.249176, avg_loss=0.701246, seen=200, correct=100, accuracy=0.500000
2025-10-02 07:21:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:21:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:21:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:21:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:21:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:21:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.738647, avg_loss=0.668466, seen=40, correct=26, accuracy=0.650000
2025-10-02 07:21:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:21:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:21:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:21:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.725000, curr=0.650000
2025-10-02 07:21:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 07:21:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 07:21:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 07:21:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:21:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:21:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=146.344070, avg_loss=0.731720, seen=200, correct=97, accuracy=0.485000
2025-10-02 07:21:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:21:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:21:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:21:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:21:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.504917, avg_loss=0.687623, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:21:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:21:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:21:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:21:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:21:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.725000, curr=0.500000
2025-10-02 07:22:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 07:22:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 07:22:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 07:22:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:22:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:22:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:22:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.743057, avg_loss=0.698715, seen=200, correct=98, accuracy=0.490000
2025-10-02 07:22:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:22:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:22:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:22:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:22:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:22:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:22:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.699036, avg_loss=0.667476, seen=40, correct=27, accuracy=0.675000
2025-10-02 07:22:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:22:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:22:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:22:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.725000, curr=0.675000
2025-10-02 07:22:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 07:22:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 07:22:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 07:22:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:22:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:22:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:22:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.417969, avg_loss=0.692090, seen=200, correct=108, accuracy=0.540000
2025-10-02 07:22:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:22:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:22:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:22:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:22:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:22:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:22:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.657463, avg_loss=0.666437, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:22:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:22:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:22:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.725000, curr=0.600000
2025-10-02 07:22:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 07:22:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 07:22:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 07:22:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:22:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.711166, avg_loss=0.698556, seen=200, correct=104, accuracy=0.520000
2025-10-02 07:22:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:22:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:22:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:22:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:22:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.471020, avg_loss=0.686775, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:22:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:22:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:22:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.725000, curr=0.500000
2025-10-02 07:22:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=220
2025-10-02 07:22:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=220
2025-10-02 07:22:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=220, splits=['val', 'test']
2025-10-02 07:22:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:22:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:22:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:23:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:23:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.251801, avg_loss=0.691259, seen=200, correct=109, accuracy=0.545000
2025-10-02 07:23:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:23:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:23:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:23:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:23:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:23:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.731674, avg_loss=0.668292, seen=40, correct=26, accuracy=0.650000
2025-10-02 07:23:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:23:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:23:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:23:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.725000, curr=0.650000
2025-10-02 07:23:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=230
2025-10-02 07:23:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=230
2025-10-02 07:23:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=230, splits=['val', 'test']
2025-10-02 07:23:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:23:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:23:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.773743, avg_loss=0.708869, seen=200, correct=96, accuracy=0.480000
2025-10-02 07:23:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:23:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:23:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:23:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:23:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:23:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.014372, avg_loss=0.675359, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:23:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:23:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:23:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.725000, curr=0.525000
2025-10-02 07:23:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=240
2025-10-02 07:23:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=240
2025-10-02 07:23:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=240, splits=['val', 'test']
2025-10-02 07:23:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:23:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:23:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.211182, avg_loss=0.706056, seen=200, correct=100, accuracy=0.500000
2025-10-02 07:23:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:23:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:23:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:23:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:23:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.992123, avg_loss=0.674803, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:23:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:23:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:23:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.725000, curr=0.575000
2025-10-02 07:23:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=250
2025-10-02 07:23:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=250
2025-10-02 07:23:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=250, splits=['val', 'test']
2025-10-02 07:23:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:23:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:23:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.705292, avg_loss=0.693526, seen=200, correct=107, accuracy=0.535000
2025-10-02 07:23:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:23:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:23:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:23:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:23:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:23:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.727201, avg_loss=0.668180, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:23:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:23:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:23:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:23:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:23:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.725000, curr=0.525000
2025-10-02 07:24:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=260
2025-10-02 07:24:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=260
2025-10-02 07:24:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=260, splits=['val', 'test']
2025-10-02 07:24:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:24:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:24:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:24:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.023972, avg_loss=0.690120, seen=200, correct=108, accuracy=0.540000
2025-10-02 07:24:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:24:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:24:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:24:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:24:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:24:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:24:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.681040, avg_loss=0.667026, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:24:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:24:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:24:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:24:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.725000, curr=0.550000
2025-10-02 07:24:16 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:24:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:24:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:24:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:24:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1984MB allocated=1910MB
2025-10-02 07:24:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:24:17 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #42', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:24:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:24:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:24:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:24:18 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:24:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:24:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:24:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:24:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:24:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:24:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=126.383591, avg_loss=0.743433, seen=170, correct=92, accuracy=0.541176
2025-10-02 07:24:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:24:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:24:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1893MB
2025-10-02 07:24:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:24:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:24:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.324884, avg_loss=0.783122, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:24:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:24:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:24:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1893MB
2025-10-02 07:24:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.400000
2025-10-02 07:24:28 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_030.ckpt
2025-10-02 07:24:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:24:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-02 07:24:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:24:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:24:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:24:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:24:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:24:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:24:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:24:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:24:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:24:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:24:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=122.981430, avg_loss=0.723420, seen=170, correct=83, accuracy=0.488235
2025-10-02 07:24:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:24:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:24:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:24:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:24:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:24:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.953770, avg_loss=0.723844, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:24:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:24:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:24:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 07:24:44 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_030.ckpt
2025-10-02 07:24:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:24:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:24:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:24:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:24:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:24:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:24:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=124.194176, avg_loss=0.730554, seen=170, correct=78, accuracy=0.458824
2025-10-02 07:24:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:24:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:24:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:24:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:24:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:24:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:25:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.355650, avg_loss=0.683891, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:25:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:25:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:25:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 07:25:02 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_030.ckpt
2025-10-02 07:25:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:25:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:25:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:25:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:25:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:25:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:25:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=122.872040, avg_loss=0.722777, seen=170, correct=81, accuracy=0.476471
2025-10-02 07:25:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:25:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:25:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:25:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:25:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:25:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:25:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.455685, avg_loss=0.686392, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:25:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:25:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:25:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.500000
2025-10-02 07:25:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:25:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:25:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:25:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:25:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:25:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:25:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=124.169868, avg_loss=0.730411, seen=170, correct=80, accuracy=0.470588
2025-10-02 07:25:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:25:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:25:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:25:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:25:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:25:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.145002, avg_loss=0.678625, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:25:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:25:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:25:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:25:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 07:25:38 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_030.ckpt
2025-10-02 07:25:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:25:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:25:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:25:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:25:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:25:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:25:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=120.811165, avg_loss=0.710654, seen=170, correct=85, accuracy=0.500000
2025-10-02 07:25:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:25:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:25:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:25:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:25:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:25:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:25:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.652168, avg_loss=0.691304, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:25:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:25:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:25:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:25:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:25:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.475000
2025-10-02 07:26:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:26:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:26:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:26:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:26:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:26:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:26:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=120.506081, avg_loss=0.708859, seen=170, correct=87, accuracy=0.511765
2025-10-02 07:26:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:26:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:26:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:26:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:26:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:26:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.036953, avg_loss=0.700924, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:26:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:26:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:26:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.500000
2025-10-02 07:26:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:26:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:26:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:26:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:26:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:26:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:26:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=119.726540, avg_loss=0.704274, seen=170, correct=88, accuracy=0.517647
2025-10-02 07:26:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:26:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:26:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:26:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:26:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:26:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:26:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.835609, avg_loss=0.695890, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:26:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:26:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:26:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:26:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.550000
2025-10-02 07:26:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:26:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:26:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:26:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:26:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:26:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=119.555717, avg_loss=0.703269, seen=170, correct=81, accuracy=0.476471
2025-10-02 07:26:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:26:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:26:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:26:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:26:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:26:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.136164, avg_loss=0.678404, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:26:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:26:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:26:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:26:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.525000
2025-10-02 07:26:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:26:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:26:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:26:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:26:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:26:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:27:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:27:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=118.361336, avg_loss=0.696243, seen=170, correct=91, accuracy=0.535294
2025-10-02 07:27:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:27:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:27:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:27:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:27:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:27:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:27:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.625578, avg_loss=0.690639, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:27:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:27:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:27:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.475000
2025-10-02 07:27:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:27:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:27:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:27:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:27:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:27:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:27:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=118.834656, avg_loss=0.699027, seen=170, correct=91, accuracy=0.535294
2025-10-02 07:27:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:27:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:27:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:27:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:27:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:27:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.990257, avg_loss=0.699756, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:27:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:27:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:27:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:27:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.550000, curr=0.550000
2025-10-02 07:27:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 07:27:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 07:27:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 07:27:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:27:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:27:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:27:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=118.528336, avg_loss=0.697226, seen=170, correct=87, accuracy=0.511765
2025-10-02 07:27:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:27:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:27:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:27:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:27:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:27:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:27:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.205183, avg_loss=0.680130, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:27:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:27:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:27:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.550000, curr=0.500000
2025-10-02 07:27:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 07:27:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 07:27:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 07:27:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:27:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:27:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=120.821045, avg_loss=0.710712, seen=170, correct=82, accuracy=0.482353
2025-10-02 07:27:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:27:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:27:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:27:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:27:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:27:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:27:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.712891, avg_loss=0.667822, seen=40, correct=26, accuracy=0.650000
2025-10-02 07:27:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:27:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:27:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:27:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:27:55 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-02 07:27:56 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_030.ckpt
2025-10-02 07:28:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 07:28:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 07:28:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 07:28:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:28:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:28:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=123.323151, avg_loss=0.725430, seen=170, correct=84, accuracy=0.494118
2025-10-02 07:28:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:28:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:28:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:28:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:28:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:28:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:28:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.725746, avg_loss=0.668144, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:28:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:28:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:28:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:28:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.650000, curr=0.625000
2025-10-02 07:28:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 07:28:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 07:28:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 07:28:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:28:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:28:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:28:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=120.090897, avg_loss=0.706417, seen=170, correct=81, accuracy=0.476471
2025-10-02 07:28:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:28:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:28:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:28:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:28:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:28:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:28:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.623188, avg_loss=0.665580, seen=40, correct=28, accuracy=0.700000
2025-10-02 07:28:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:28:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:28:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:28:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-02 07:28:28 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_030.ckpt
2025-10-02 07:28:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 07:28:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 07:28:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 07:28:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:28:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:28:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:28:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=117.813263, avg_loss=0.693019, seen=170, correct=94, accuracy=0.552941
2025-10-02 07:28:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:28:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:28:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:28:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:28:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:28:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:28:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.381985, avg_loss=0.684550, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:28:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:28:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:28:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:28:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.700000, curr=0.475000
2025-10-02 07:28:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 07:28:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 07:28:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 07:28:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:28:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:28:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:28:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=118.727249, avg_loss=0.698396, seen=170, correct=93, accuracy=0.547059
2025-10-02 07:28:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:28:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:28:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:28:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:28:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:28:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:29:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:29:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.768230, avg_loss=0.694206, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:29:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:29:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:29:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:29:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.700000, curr=0.500000
2025-10-02 07:29:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 07:29:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 07:29:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 07:29:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:29:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:29:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:29:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=117.991501, avg_loss=0.694068, seen=170, correct=94, accuracy=0.552941
2025-10-02 07:29:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:29:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:29:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:29:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:29:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:29:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.108770, avg_loss=0.677719, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:29:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:29:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:29:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:29:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.700000, curr=0.525000
2025-10-02 07:29:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 07:29:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 07:29:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 07:29:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:29:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:29:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=117.580498, avg_loss=0.691650, seen=170, correct=90, accuracy=0.529412
2025-10-02 07:29:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:29:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:29:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:29:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:29:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:29:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:29:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.995258, avg_loss=0.674881, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:29:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:29:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:29:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:29:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.700000, curr=0.500000
2025-10-02 07:29:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 07:29:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 07:29:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 07:29:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:29:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:29:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:29:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=118.176804, avg_loss=0.695158, seen=170, correct=85, accuracy=0.500000
2025-10-02 07:29:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:29:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:29:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:29:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:29:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:29:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.616058, avg_loss=0.665401, seen=40, correct=26, accuracy=0.650000
2025-10-02 07:29:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:29:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:29:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:29:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:29:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.700000, curr=0.650000
2025-10-02 07:30:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 07:30:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 07:30:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 07:30:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:30:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:30:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:30:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=117.479828, avg_loss=0.691058, seen=170, correct=102, accuracy=0.600000
2025-10-02 07:30:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:30:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:30:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:30:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:30:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:30:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:30:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.128948, avg_loss=0.678224, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:30:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:30:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:30:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:30:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.700000, curr=0.500000
2025-10-02 07:30:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 07:30:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 07:30:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 07:30:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:30:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:30:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:30:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=117.489960, avg_loss=0.691117, seen=170, correct=93, accuracy=0.547059
2025-10-02 07:30:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:30:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:30:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:30:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:30:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:30:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:30:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.410690, avg_loss=0.685267, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:30:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:30:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:30:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:30:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.700000, curr=0.525000
2025-10-02 07:30:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=220
2025-10-02 07:30:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=220
2025-10-02 07:30:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=220, splits=['val', 'test']
2025-10-02 07:30:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:30:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:30:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=117.255981, avg_loss=0.689741, seen=170, correct=96, accuracy=0.564706
2025-10-02 07:30:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:30:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:30:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:30:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:30:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:30:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:30:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.283752, avg_loss=0.682094, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:30:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:30:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:30:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:30:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.700000, curr=0.550000
2025-10-02 07:30:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=230
2025-10-02 07:30:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=230
2025-10-02 07:30:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=230, splits=['val', 'test']
2025-10-02 07:30:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:30:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:30:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:30:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=117.526276, avg_loss=0.691331, seen=170, correct=94, accuracy=0.552941
2025-10-02 07:30:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:30:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:30:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:30:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:30:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:31:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.035677, avg_loss=0.675892, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:31:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:31:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:31:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:31:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.700000, curr=0.550000
2025-10-02 07:31:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=240
2025-10-02 07:31:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=240
2025-10-02 07:31:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=240, splits=['val', 'test']
2025-10-02 07:31:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 07:31:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 07:31:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=119.428566, avg_loss=0.702521, seen=170, correct=81, accuracy=0.476471
2025-10-02 07:31:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:31:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:31:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:31:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:31:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.491192, avg_loss=0.662280, seen=40, correct=27, accuracy=0.675000
2025-10-02 07:31:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:31:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:31:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:31:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.700000, curr=0.675000
2025-10-02 07:31:19 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:31:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:31:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:31:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:31:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1927MB
2025-10-02 07:31:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:31:20 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:31:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:31:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:31:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:20 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:31:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:31:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:31:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:31:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:31:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=93.045494, avg_loss=0.756467, seen=123, correct=55, accuracy=0.447154
2025-10-02 07:31:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:31:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:31:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1910MB
2025-10-02 07:31:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:31:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:31:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.579025, avg_loss=0.789476, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:31:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:31:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:31:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1934MB allocated=1910MB
2025-10-02 07:31:29 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-02 07:31:30 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_027.ckpt
2025-10-02 07:31:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:31:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-02 07:31:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:31:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:31:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:31:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:31:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:31:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:31:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:31:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:31:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=91.040634, avg_loss=0.740168, seen=123, correct=53, accuracy=0.430894
2025-10-02 07:31:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:31:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:31:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:31:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:31:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:31:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.822014, avg_loss=0.745550, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:31:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:31:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:31:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:31:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.425000, curr=0.400000
2025-10-02 07:31:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:31:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:31:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:31:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:31:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:31:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:31:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=89.448975, avg_loss=0.727227, seen=123, correct=56, accuracy=0.455285
2025-10-02 07:31:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:31:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:32:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:32:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:32:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:32:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.157188, avg_loss=0.703930, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:32:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:32:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:32:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:32:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-02 07:32:05 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_027.ckpt
2025-10-02 07:32:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:32:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:32:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:32:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:32:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:32:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:32:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=88.369873, avg_loss=0.718454, seen=123, correct=57, accuracy=0.463415
2025-10-02 07:32:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:32:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:32:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:32:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:32:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:32:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.923683, avg_loss=0.698092, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:32:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:32:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:32:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:32:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.475000, curr=0.475000
2025-10-02 07:32:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:32:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:32:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:32:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:32:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:32:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=88.666206, avg_loss=0.720863, seen=123, correct=56, accuracy=0.455285
2025-10-02 07:32:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:32:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:32:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:32:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:32:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:32:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:32:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.718321, avg_loss=0.692958, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:32:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:32:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:32:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:32:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.475000, curr=0.475000
2025-10-02 07:32:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:32:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:32:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:32:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:32:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:32:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=88.438713, avg_loss=0.719014, seen=123, correct=56, accuracy=0.455285
2025-10-02 07:32:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:32:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:32:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:32:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:32:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:32:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:32:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.290926, avg_loss=0.707273, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:32:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:32:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:32:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:32:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:32:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.475000, curr=0.475000
2025-10-02 07:33:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:33:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:33:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:33:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:33:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:33:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:33:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=88.331329, avg_loss=0.718141, seen=123, correct=55, accuracy=0.447154
2025-10-02 07:33:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:33:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:33:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:33:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:33:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:33:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:33:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.446495, avg_loss=0.711162, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:33:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:33:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:33:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:33:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.475000, curr=0.475000
2025-10-02 07:33:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:33:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:33:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:33:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:33:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:33:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:33:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=87.641678, avg_loss=0.712534, seen=123, correct=56, accuracy=0.455285
2025-10-02 07:33:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:33:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:33:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:33:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:33:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:33:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:33:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.204472, avg_loss=0.705112, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:33:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:33:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:33:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:33:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.475000, curr=0.450000
2025-10-02 07:33:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:33:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:33:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:33:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:33:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:33:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:33:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=86.845360, avg_loss=0.706060, seen=123, correct=59, accuracy=0.479675
2025-10-02 07:33:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:33:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:33:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:33:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:33:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:33:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:33:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.082624, avg_loss=0.677066, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:33:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:33:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:33:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:33:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 07:33:42 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_027.ckpt
2025-10-02 07:33:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:33:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:33:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:33:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:33:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:33:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:33:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=87.093521, avg_loss=0.708077, seen=123, correct=62, accuracy=0.504065
2025-10-02 07:33:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:33:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:33:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:33:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:33:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:33:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.991369, avg_loss=0.674784, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:33:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:33:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:33:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:33:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:33:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.600000
2025-10-02 07:34:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:34:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:34:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:34:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:34:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:34:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:34:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=87.120850, avg_loss=0.708300, seen=123, correct=68, accuracy=0.552846
2025-10-02 07:34:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:34:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:34:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:34:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:34:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:34:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.031185, avg_loss=0.675780, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:34:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:34:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:34:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.600000
2025-10-02 07:34:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 07:34:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 07:34:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 07:34:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:34:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:34:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:34:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=86.860909, avg_loss=0.706186, seen=123, correct=65, accuracy=0.528455
2025-10-02 07:34:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:34:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:34:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:34:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:34:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:34:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.001139, avg_loss=0.675028, seen=40, correct=26, accuracy=0.650000
2025-10-02 07:34:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:34:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:34:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:34:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-02 07:34:32 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_027.ckpt
2025-10-02 07:34:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 07:34:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 07:34:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 07:34:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:34:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:34:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:34:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=87.087051, avg_loss=0.708025, seen=123, correct=53, accuracy=0.430894
2025-10-02 07:34:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:34:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:34:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:34:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:34:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:34:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.269558, avg_loss=0.706739, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:34:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:34:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:34:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:34:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.650000, curr=0.450000
2025-10-02 07:34:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 07:34:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 07:34:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 07:34:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:34:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:34:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:35:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:35:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=87.097580, avg_loss=0.708110, seen=123, correct=54, accuracy=0.439024
2025-10-02 07:35:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:35:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:35:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:35:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:35:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:35:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:35:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.806454, avg_loss=0.720161, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:35:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:35:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:35:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:35:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.650000, curr=0.400000
2025-10-02 07:35:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 07:35:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 07:35:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 07:35:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:35:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:35:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:35:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=86.553856, avg_loss=0.703690, seen=123, correct=57, accuracy=0.463415
2025-10-02 07:35:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:35:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:35:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:35:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:35:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:35:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.532970, avg_loss=0.713324, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:35:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:35:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:35:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:35:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.650000, curr=0.450000
2025-10-02 07:35:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 07:35:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 07:35:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 07:35:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:35:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:35:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:35:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=88.071617, avg_loss=0.716029, seen=123, correct=56, accuracy=0.455285
2025-10-02 07:35:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:35:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:35:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:35:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:35:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:35:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:35:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.458912, avg_loss=0.736473, seen=40, correct=15, accuracy=0.375000
2025-10-02 07:35:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:35:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:35:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:35:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.650000, curr=0.375000
2025-10-02 07:35:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 07:35:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 07:35:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 07:35:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:35:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:35:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=88.142120, avg_loss=0.716603, seen=123, correct=52, accuracy=0.422764
2025-10-02 07:35:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:35:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:35:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:35:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:35:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:35:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:35:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.032738, avg_loss=0.750818, seen=40, correct=14, accuracy=0.350000
2025-10-02 07:35:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:35:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:35:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:35:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:35:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.650000, curr=0.350000
2025-10-02 07:36:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 07:36:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 07:36:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 07:36:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:36:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:36:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:36:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=86.404541, avg_loss=0.702476, seen=123, correct=58, accuracy=0.471545
2025-10-02 07:36:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:36:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:36:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:36:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:36:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:36:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:36:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.447395, avg_loss=0.711185, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:36:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:36:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:36:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:36:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.650000, curr=0.450000
2025-10-02 07:36:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 07:36:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 07:36:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 07:36:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:36:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:36:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:36:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=85.871986, avg_loss=0.698146, seen=123, correct=59, accuracy=0.479675
2025-10-02 07:36:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:36:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:36:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:36:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:36:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:36:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.697224, avg_loss=0.692431, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:36:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:36:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:36:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:36:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.650000, curr=0.500000
2025-10-02 07:36:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 07:36:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 07:36:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 07:36:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:36:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:36:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:36:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=85.840652, avg_loss=0.697891, seen=123, correct=58, accuracy=0.471545
2025-10-02 07:36:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:36:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:36:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:36:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:36:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:36:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.641354, avg_loss=0.691034, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:36:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:36:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:36:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.650000, curr=0.500000
2025-10-02 07:36:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 07:36:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 07:36:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 07:36:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:36:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:36:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:36:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=85.542290, avg_loss=0.695466, seen=123, correct=60, accuracy=0.487805
2025-10-02 07:36:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:36:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:36:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:36:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:36:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:36:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:36:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.011539, avg_loss=0.700288, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:36:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:36:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:37:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.650000, curr=0.475000
2025-10-02 07:37:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 07:37:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 07:37:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 07:37:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 07:37:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 07:37:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=85.600754, avg_loss=0.695941, seen=123, correct=58, accuracy=0.471545
2025-10-02 07:37:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:37:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:37:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:37:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:37:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.131865, avg_loss=0.703297, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:37:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:37:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:37:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.650000, curr=0.475000
2025-10-02 07:37:17 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:37:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:37:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:37:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1944MB
2025-10-02 07:37:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:37:18 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:37:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:37:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:37:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=143, num_train_batch_last_epoch=85, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:18 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:37:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:37:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:37:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:37:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=143, num_train_batch_last_epoch=85, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:37:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.549689, avg_loss=0.610692, seen=14, correct=10, accuracy=0.714286
2025-10-02 07:37:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:37:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1927MB
2025-10-02 07:37:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:37:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=143, num_train_batch_last_epoch=85, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:37:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.736904, avg_loss=0.718423, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:37:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:37:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1954MB allocated=1927MB
2025-10-02 07:37:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 07:37:25 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_005.ckpt
2025-10-02 07:37:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:37:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-02 07:37:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:37:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=143, num_train_batch_last_epoch=85, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:37:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:37:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:37:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:37:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:37:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:37:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.764229, avg_loss=0.626016, seen=14, correct=7, accuracy=0.500000
2025-10-02 07:37:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:37:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:37:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:37:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.427685, avg_loss=0.760692, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:37:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:37:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:37:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.400000
2025-10-02 07:37:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:37:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:37:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:37:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:37:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.807846, avg_loss=0.629132, seen=14, correct=9, accuracy=0.642857
2025-10-02 07:37:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:37:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:37:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:37:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:37:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.629648, avg_loss=0.765741, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:37:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:37:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:37:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:37:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:37:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.400000
2025-10-02 07:38:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:38:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:38:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:38:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:38:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:38:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:38:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.171411, avg_loss=0.655101, seen=14, correct=8, accuracy=0.571429
2025-10-02 07:38:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:38:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:38:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:38:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:38:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:38:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:38:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.950817, avg_loss=0.773770, seen=40, correct=14, accuracy=0.350000
2025-10-02 07:38:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:38:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:38:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.575000, curr=0.350000
2025-10-02 07:38:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:38:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:38:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:38:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:38:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:38:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:38:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.793019, avg_loss=0.628073, seen=14, correct=8, accuracy=0.571429
2025-10-02 07:38:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:38:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:38:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:38:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:38:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:38:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:38:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.764740, avg_loss=0.744118, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:38:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:38:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:38:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:38:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.575000, curr=0.425000
2025-10-02 07:38:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:38:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:38:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:38:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:38:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:38:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:38:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.830751, avg_loss=0.630768, seen=14, correct=9, accuracy=0.642857
2025-10-02 07:38:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:38:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:38:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:38:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:38:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:38:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.453831, avg_loss=0.736346, seen=40, correct=15, accuracy=0.375000
2025-10-02 07:38:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:38:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:38:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:38:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.575000, curr=0.375000
2025-10-02 07:38:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:38:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:38:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:38:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:38:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:38:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:38:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.918673, avg_loss=0.637048, seen=14, correct=9, accuracy=0.642857
2025-10-02 07:38:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:38:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:38:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:38:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:38:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:38:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:38:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.438688, avg_loss=0.735967, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:38:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:38:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:38:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:38:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:38:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.575000, curr=0.425000
2025-10-02 07:39:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:39:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:39:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:39:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:39:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:39:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.832237, avg_loss=0.630874, seen=14, correct=8, accuracy=0.571429
2025-10-02 07:39:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:39:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:39:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:39:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:39:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.613407, avg_loss=0.715335, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:39:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:39:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:39:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.575000, curr=0.425000
2025-10-02 07:39:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:39:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:39:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:39:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:39:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.958517, avg_loss=0.639894, seen=14, correct=9, accuracy=0.642857
2025-10-02 07:39:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:39:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:39:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:39:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:39:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.462940, avg_loss=0.736574, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:39:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:39:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:39:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.575000, curr=0.450000
2025-10-02 07:39:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:39:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:39:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:39:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:39:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.659966, avg_loss=0.618569, seen=14, correct=11, accuracy=0.785714
2025-10-02 07:39:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:39:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:39:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:39:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:39:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.115879, avg_loss=0.702897, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:39:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:39:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:39:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.575000, curr=0.475000
2025-10-02 07:39:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:39:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:39:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:39:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 07:39:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 07:39:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.985155, avg_loss=0.641797, seen=14, correct=10, accuracy=0.714286
2025-10-02 07:39:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:39:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:39:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:39:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:39:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.947186, avg_loss=0.723680, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:39:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:39:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:39:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.575000, curr=0.450000
2025-10-02 07:39:53 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:39:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:39:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:39:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2012MB allocated=1961MB
2025-10-02 07:39:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:39:54 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:39:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:39:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:39:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=311, num_train_batch_last_epoch=178, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:54 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:39:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:39:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:39:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:39:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=311, num_train_batch_last_epoch=178, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:39:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:39:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=24.057899, avg_loss=0.751809, seen=32, correct=13, accuracy=0.406250
2025-10-02 07:39:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:39:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1944MB
2025-10-02 07:39:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:39:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:39:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=311, num_train_batch_last_epoch=178, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:40:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.015141, avg_loss=0.775379, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:40:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:40:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1976MB allocated=1944MB
2025-10-02 07:40:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-02 07:40:02 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_011.ckpt
2025-10-02 07:40:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:40:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-02 07:40:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:40:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=311, num_train_batch_last_epoch=178, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:40:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:40:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:40:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:40:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:40:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:40:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:40:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=23.313480, avg_loss=0.728546, seen=32, correct=13, accuracy=0.406250
2025-10-02 07:40:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:40:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:40:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:40:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:40:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:40:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.024239, avg_loss=0.700606, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:40:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:40:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:40:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:40:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-02 07:40:17 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_011.ckpt
2025-10-02 07:40:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:40:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:40:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:40:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:40:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=23.520746, avg_loss=0.735023, seen=32, correct=17, accuracy=0.531250
2025-10-02 07:40:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:40:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:40:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:40:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:40:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:40:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.140944, avg_loss=0.653524, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:40:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:40:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:40:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:40:29 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 07:40:29 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_011.ckpt
2025-10-02 07:40:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:40:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:40:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:40:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:40:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=23.162308, avg_loss=0.723822, seen=32, correct=13, accuracy=0.406250
2025-10-02 07:40:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:40:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:40:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:40:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.065617, avg_loss=0.701640, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:40:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:40:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:40:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.425000
2025-10-02 07:40:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:40:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:40:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:40:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:40:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=23.480761, avg_loss=0.733774, seen=32, correct=12, accuracy=0.375000
2025-10-02 07:40:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:40:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:40:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:40:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:40:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:40:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:40:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.745651, avg_loss=0.718641, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:40:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:40:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:40:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:40:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:40:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.475000
2025-10-02 07:41:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:41:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:41:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:41:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:41:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:41:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:41:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=23.085112, avg_loss=0.721410, seen=32, correct=12, accuracy=0.375000
2025-10-02 07:41:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:41:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:41:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:41:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:41:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:41:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.148350, avg_loss=0.703709, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:41:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:41:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:41:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.450000
2025-10-02 07:41:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:41:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:41:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:41:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:41:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:41:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:41:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.871647, avg_loss=0.714739, seen=32, correct=18, accuracy=0.562500
2025-10-02 07:41:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:41:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:41:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:41:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:41:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:41:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:41:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.720917, avg_loss=0.668023, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:41:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:41:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:41:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:41:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 07:41:26 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_011.ckpt
2025-10-02 07:41:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:41:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:41:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:41:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:41:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:41:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:41:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.994926, avg_loss=0.718591, seen=32, correct=14, accuracy=0.437500
2025-10-02 07:41:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:41:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:41:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:41:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:41:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:41:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:41:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.985174, avg_loss=0.699629, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:41:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:41:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:41:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:41:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.425000
2025-10-02 07:41:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:41:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:41:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:41:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:41:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:41:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:41:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.872831, avg_loss=0.714776, seen=32, correct=12, accuracy=0.375000
2025-10-02 07:41:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:41:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:41:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:41:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:41:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:41:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.372614, avg_loss=0.709315, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:41:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:41:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:41:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:41:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.425000
2025-10-02 07:42:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:42:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:42:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:42:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:42:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:42:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.825861, avg_loss=0.713308, seen=32, correct=14, accuracy=0.437500
2025-10-02 07:42:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:42:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:42:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:42:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:42:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:42:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.777662, avg_loss=0.694442, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:42:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:42:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:42:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:42:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.450000
2025-10-02 07:42:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:42:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:42:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:42:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:42:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:42:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.465202, avg_loss=0.702038, seen=32, correct=17, accuracy=0.531250
2025-10-02 07:42:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:42:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:42:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:42:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:42:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:42:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.140385, avg_loss=0.678510, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:42:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:42:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:42:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.575000
2025-10-02 07:42:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 07:42:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 07:42:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 07:42:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:42:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:42:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.342398, avg_loss=0.698200, seen=32, correct=15, accuracy=0.468750
2025-10-02 07:42:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:42:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:42:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:42:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:42:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:42:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:42:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.535446, avg_loss=0.713386, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:42:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:42:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:42:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.425000
2025-10-02 07:42:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 07:42:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 07:42:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 07:42:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:42:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:42:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.563011, avg_loss=0.705094, seen=32, correct=13, accuracy=0.406250
2025-10-02 07:42:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:42:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:42:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:42:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:42:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.189693, avg_loss=0.704742, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:42:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:42:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:42:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:42:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.450000
2025-10-02 07:43:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 07:43:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 07:43:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 07:43:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:43:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:43:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:43:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.412155, avg_loss=0.700380, seen=32, correct=16, accuracy=0.500000
2025-10-02 07:43:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:43:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:43:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:43:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:43:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:43:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:43:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.785021, avg_loss=0.694626, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:43:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:43:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:43:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:43:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.450000
2025-10-02 07:43:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 07:43:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 07:43:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 07:43:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:43:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:43:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:43:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.441034, avg_loss=0.701282, seen=32, correct=17, accuracy=0.531250
2025-10-02 07:43:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:43:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:43:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:43:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:43:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:43:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:43:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.060501, avg_loss=0.676513, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:43:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:43:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:43:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:43:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 07:43:21 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_011.ckpt
2025-10-02 07:43:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 07:43:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 07:43:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 07:43:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:43:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:43:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:43:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.560419, avg_loss=0.705013, seen=32, correct=14, accuracy=0.437500
2025-10-02 07:43:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:43:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:43:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:43:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:43:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:43:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:43:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.699831, avg_loss=0.717496, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:43:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:43:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:43:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:43:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.425000
2025-10-02 07:43:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 07:43:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 07:43:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 07:43:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:43:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:43:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:43:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.441727, avg_loss=0.701304, seen=32, correct=15, accuracy=0.468750
2025-10-02 07:43:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:43:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:43:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:43:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:43:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:43:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:43:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.766493, avg_loss=0.719162, seen=40, correct=15, accuracy=0.375000
2025-10-02 07:43:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:43:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:43:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:43:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:43:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.375000
2025-10-02 07:44:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 07:44:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 07:44:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 07:44:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:44:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:44:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:44:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.302620, avg_loss=0.696957, seen=32, correct=16, accuracy=0.500000
2025-10-02 07:44:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:44:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:44:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:44:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:44:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:44:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.503902, avg_loss=0.687598, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:44:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:44:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:44:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:44:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.525000
2025-10-02 07:44:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 07:44:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 07:44:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 07:44:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:44:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:44:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:44:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.182941, avg_loss=0.693217, seen=32, correct=17, accuracy=0.531250
2025-10-02 07:44:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:44:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:44:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:44:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:44:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:44:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:44:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.517563, avg_loss=0.687939, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:44:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:44:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:44:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:44:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.600000
2025-10-02 07:44:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 07:44:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 07:44:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 07:44:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:44:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:44:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:44:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.562271, avg_loss=0.705071, seen=32, correct=16, accuracy=0.500000
2025-10-02 07:44:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:44:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:44:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:44:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:44:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:44:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:44:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.342226, avg_loss=0.733556, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:44:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:44:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:44:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:44:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.400000
2025-10-02 07:44:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 07:44:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 07:44:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 07:44:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:44:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:44:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:44:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.161476, avg_loss=0.692546, seen=32, correct=16, accuracy=0.500000
2025-10-02 07:44:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:44:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:44:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:44:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:44:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:44:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.119064, avg_loss=0.702977, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:44:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:44:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:44:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.450000
2025-10-02 07:44:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 07:44:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 07:44:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 07:44:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:44:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:44:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:44:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:44:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.191662, avg_loss=0.693489, seen=32, correct=18, accuracy=0.562500
2025-10-02 07:44:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:44:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:45:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:45:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.442482, avg_loss=0.686062, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:45:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:45:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:45:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.600000
2025-10-02 07:45:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=220
2025-10-02 07:45:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=220
2025-10-02 07:45:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=220, splits=['val', 'test']
2025-10-02 07:45:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:45:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:45:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=21.967133, avg_loss=0.686473, seen=32, correct=18, accuracy=0.562500
2025-10-02 07:45:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:45:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:45:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:45:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:45:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.995480, avg_loss=0.699887, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:45:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:45:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:45:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.625000, curr=0.500000
2025-10-02 07:45:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=230
2025-10-02 07:45:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=230
2025-10-02 07:45:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=230, splits=['val', 'test']
2025-10-02 07:45:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:45:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.320269, avg_loss=0.697508, seen=32, correct=18, accuracy=0.562500
2025-10-02 07:45:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:45:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:45:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:45:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:45:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.820910, avg_loss=0.745523, seen=40, correct=15, accuracy=0.375000
2025-10-02 07:45:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:45:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:45:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.625000, curr=0.375000
2025-10-02 07:45:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=240
2025-10-02 07:45:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=240
2025-10-02 07:45:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=240, splits=['val', 'test']
2025-10-02 07:45:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 07:45:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 07:45:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.043396, avg_loss=0.688856, seen=32, correct=18, accuracy=0.562500
2025-10-02 07:45:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:45:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:45:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:45:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:45:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.545607, avg_loss=0.713640, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:45:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:45:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:45:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.625000, curr=0.500000
2025-10-02 07:45:46 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:45:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:45:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:45:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2038MB allocated=1977MB
2025-10-02 07:45:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:45:47 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:45:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:45:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:45:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=717, num_train_batch_last_epoch=83, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:47 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:45:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:45:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:45:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=717, num_train_batch_last_epoch=83, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:45:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=57.071590, avg_loss=0.760955, seen=75, correct=33, accuracy=0.440000
2025-10-02 07:45:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:45:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1961MB
2025-10-02 07:45:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:45:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=717, num_train_batch_last_epoch=83, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:45:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.076994, avg_loss=0.701925, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:45:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:45:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:45:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1996MB allocated=1961MB
2025-10-02 07:45:55 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 07:45:55 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_028.ckpt
2025-10-02 07:45:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:45:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-02 07:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:45:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:45:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=717, num_train_batch_last_epoch=83, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:45:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:45:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:46:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:46:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:46:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:46:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:46:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:46:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:46:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=52.996288, avg_loss=0.706617, seen=75, correct=41, accuracy=0.546667
2025-10-02 07:46:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:46:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:46:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:46:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:46:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:46:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:46:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.222212, avg_loss=0.705555, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:46:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:46:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:46:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:46:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.425000
2025-10-02 07:46:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:46:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:46:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:46:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:46:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:46:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:46:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=52.527802, avg_loss=0.700371, seen=75, correct=40, accuracy=0.533333
2025-10-02 07:46:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:46:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:46:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:46:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:46:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:46:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:46:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.863056, avg_loss=0.721576, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:46:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:46:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:46:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:46:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.525000
2025-10-02 07:46:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:46:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:46:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:46:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:46:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:46:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:46:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=52.132248, avg_loss=0.695097, seen=75, correct=41, accuracy=0.546667
2025-10-02 07:46:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:46:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:46:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:46:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:46:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.237415, avg_loss=0.705935, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:46:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:46:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:46:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.450000
2025-10-02 07:46:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:46:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:46:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:46:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:46:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:46:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=52.659187, avg_loss=0.702122, seen=75, correct=38, accuracy=0.506667
2025-10-02 07:46:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:46:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:46:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:46:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:46:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:46:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:46:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.827055, avg_loss=0.695676, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:46:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:46:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:46:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:46:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.400000
2025-10-02 07:47:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:47:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:47:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:47:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:47:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:47:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.899353, avg_loss=0.691991, seen=75, correct=40, accuracy=0.533333
2025-10-02 07:47:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:47:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:47:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:47:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:47:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:47:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.997297, avg_loss=0.699932, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:47:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:47:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:47:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.475000
2025-10-02 07:47:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:47:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:47:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:47:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:47:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:47:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:47:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.689640, avg_loss=0.689195, seen=75, correct=40, accuracy=0.533333
2025-10-02 07:47:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:47:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:47:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:47:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:47:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:47:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:47:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.961803, avg_loss=0.699045, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:47:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:47:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:47:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.425000
2025-10-02 07:47:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:47:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:47:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:47:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:47:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:47:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=52.653015, avg_loss=0.702040, seen=75, correct=35, accuracy=0.466667
2025-10-02 07:47:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:47:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:47:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:47:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:47:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.455763, avg_loss=0.686394, seen=40, correct=17, accuracy=0.425000
2025-10-02 07:47:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:47:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:47:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.425000
2025-10-02 07:47:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:47:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:47:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:47:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:47:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:47:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.749134, avg_loss=0.689988, seen=75, correct=39, accuracy=0.520000
2025-10-02 07:47:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:47:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:48:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:48:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.803961, avg_loss=0.695099, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:48:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:48:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.475000
2025-10-02 07:48:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:48:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:48:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:48:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:48:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:48:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.455666, avg_loss=0.686076, seen=75, correct=44, accuracy=0.586667
2025-10-02 07:48:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:48:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:48:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:48:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.584488, avg_loss=0.689612, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:48:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:48:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.525000
2025-10-02 07:48:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:48:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:48:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:48:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 07:48:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 07:48:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.284416, avg_loss=0.683792, seen=75, correct=40, accuracy=0.533333
2025-10-02 07:48:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:48:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:48:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:48:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.793545, avg_loss=0.694839, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:48:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:48:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:48:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.600000, curr=0.550000
2025-10-02 07:48:34 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:48:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:48:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:48:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=1994MB
2025-10-02 07:48:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:48:35 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:48:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:48:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:48:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:35 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:48:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:48:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:48:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:48:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.690735, avg_loss=0.713454, seen=200, correct=99, accuracy=0.495000
2025-10-02 07:48:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1977MB
2025-10-02 07:48:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:48:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.051998, avg_loss=0.701300, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:48:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:48:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2016MB allocated=1977MB
2025-10-02 07:48:45 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 07:48:45 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_053.ckpt
2025-10-02 07:48:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:48:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-02 07:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:48:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:48:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:48:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:48:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:48:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:48:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:48:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:48:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.091202, avg_loss=0.700456, seen=200, correct=111, accuracy=0.555000
2025-10-02 07:48:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:48:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:48:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:48:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:48:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:48:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:48:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:48:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.107464, avg_loss=0.702687, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:48:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:48:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:49:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:49:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 07:49:01 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_053.ckpt
2025-10-02 07:49:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:49:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:49:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:49:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:49:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:49:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.165970, avg_loss=0.695830, seen=200, correct=103, accuracy=0.515000
2025-10-02 07:49:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:49:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:49:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:49:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:49:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:49:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.934010, avg_loss=0.698350, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:49:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:49:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:49:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:49:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.475000
2025-10-02 07:49:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:49:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:49:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:49:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:49:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:49:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:49:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.752441, avg_loss=0.693762, seen=200, correct=106, accuracy=0.530000
2025-10-02 07:49:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:49:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:49:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:49:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:49:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:49:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:49:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.851948, avg_loss=0.696299, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:49:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:49:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:49:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:49:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.500000
2025-10-02 07:49:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:49:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:49:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:49:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:49:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:49:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.485870, avg_loss=0.692429, seen=200, correct=98, accuracy=0.490000
2025-10-02 07:49:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:49:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:49:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:49:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:49:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:49:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.758175, avg_loss=0.693954, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:49:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:49:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:49:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:49:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:49:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.550000
2025-10-02 07:50:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:50:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:50:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:50:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:50:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:50:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.944153, avg_loss=0.694721, seen=200, correct=103, accuracy=0.515000
2025-10-02 07:50:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:50:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:50:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:50:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:50:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:50:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:50:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.857330, avg_loss=0.696433, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:50:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:50:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:50:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:50:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.475000
2025-10-02 07:50:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:50:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:50:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:50:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:50:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:50:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.837540, avg_loss=0.694188, seen=200, correct=99, accuracy=0.495000
2025-10-02 07:50:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:50:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:50:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:50:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:50:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:50:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:50:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.905506, avg_loss=0.697638, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:50:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:50:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:50:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.550000
2025-10-02 07:50:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:50:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:50:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:50:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:50:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:50:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.017075, avg_loss=0.695085, seen=200, correct=99, accuracy=0.495000
2025-10-02 07:50:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:50:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:50:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:50:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:50:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:50:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.742785, avg_loss=0.693570, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:50:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:50:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:50:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:50:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.550000, curr=0.475000
2025-10-02 07:50:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:50:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:50:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:50:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:50:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:50:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:50:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.866287, avg_loss=0.699331, seen=200, correct=107, accuracy=0.535000
2025-10-02 07:50:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:50:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:50:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:51:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:51:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:51:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:51:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:51:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.309305, avg_loss=0.707733, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:51:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:51:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:51:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:51:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.550000, curr=0.550000
2025-10-02 07:51:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:51:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:51:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:51:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:51:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:51:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:51:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.832977, avg_loss=0.704165, seen=200, correct=107, accuracy=0.535000
2025-10-02 07:51:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:51:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:51:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:51:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:51:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:51:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.288128, avg_loss=0.707203, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:51:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:51:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:51:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:51:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.550000, curr=0.550000
2025-10-02 07:51:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:51:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:51:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:51:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:51:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:51:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:51:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.140549, avg_loss=0.700703, seen=200, correct=107, accuracy=0.535000
2025-10-02 07:51:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:51:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:51:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:51:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:51:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:51:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:51:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.439068, avg_loss=0.710977, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:51:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:51:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:51:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:51:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.550000, curr=0.550000
2025-10-02 07:51:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 07:51:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 07:51:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 07:51:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:51:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:51:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.640594, avg_loss=0.698203, seen=200, correct=109, accuracy=0.545000
2025-10-02 07:51:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:51:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:51:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:51:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:51:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:51:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:51:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.180031, avg_loss=0.704501, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:51:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:51:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:51:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:51:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 07:51:53 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_053.ckpt
2025-10-02 07:52:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 07:52:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 07:52:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 07:52:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:52:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:52:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:52:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.772156, avg_loss=0.688861, seen=200, correct=104, accuracy=0.520000
2025-10-02 07:52:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:52:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:52:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:52:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:52:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:52:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.840584, avg_loss=0.696015, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:52:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:52:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:52:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:52:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.525000
2025-10-02 07:52:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 07:52:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 07:52:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 07:52:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:52:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:52:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:52:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.546082, avg_loss=0.692730, seen=200, correct=102, accuracy=0.510000
2025-10-02 07:52:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:52:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:52:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:52:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:52:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:52:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.927616, avg_loss=0.698190, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:52:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:52:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:52:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:52:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.500000
2025-10-02 07:52:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 07:52:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 07:52:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 07:52:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:52:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:52:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.287735, avg_loss=0.696439, seen=200, correct=101, accuracy=0.505000
2025-10-02 07:52:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:52:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:52:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:52:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:52:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:52:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.443073, avg_loss=0.711077, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:52:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:52:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:52:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:52:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.575000, curr=0.525000
2025-10-02 07:52:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 07:52:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 07:52:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 07:52:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:52:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:52:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:52:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.690887, avg_loss=0.693454, seen=200, correct=102, accuracy=0.510000
2025-10-02 07:52:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:52:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:52:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:53:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:53:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:53:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:53:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:53:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.439571, avg_loss=0.710989, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:53:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:53:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:53:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:53:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.575000, curr=0.550000
2025-10-02 07:53:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 07:53:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 07:53:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 07:53:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:53:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:53:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:53:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.016327, avg_loss=0.690082, seen=200, correct=104, accuracy=0.520000
2025-10-02 07:53:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:53:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:53:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:53:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:53:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.312342, avg_loss=0.707809, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:53:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:53:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:53:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.575000, curr=0.500000
2025-10-02 07:53:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 07:53:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 07:53:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 07:53:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:53:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:53:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:53:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.237244, avg_loss=0.686186, seen=200, correct=110, accuracy=0.550000
2025-10-02 07:53:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:53:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:53:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:53:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:53:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:53:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:53:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.890757, avg_loss=0.697269, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:53:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:53:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:53:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:53:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.575000, curr=0.500000
2025-10-02 07:53:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 07:53:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 07:53:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 07:53:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:53:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:53:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:53:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.356705, avg_loss=0.691784, seen=200, correct=103, accuracy=0.515000
2025-10-02 07:53:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:53:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:53:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:53:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:53:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:53:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.142649, avg_loss=0.703566, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:53:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:53:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:53:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:53:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:53:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.575000, curr=0.525000
2025-10-02 07:54:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 07:54:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 07:54:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 07:54:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:54:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:54:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.055573, avg_loss=0.690278, seen=200, correct=109, accuracy=0.545000
2025-10-02 07:54:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:54:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:54:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:54:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:54:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.913542, avg_loss=0.697839, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:54:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:54:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:54:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:54:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.575000, curr=0.450000
2025-10-02 07:54:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 07:54:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 07:54:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 07:54:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:54:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:54:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.766129, avg_loss=0.698831, seen=200, correct=99, accuracy=0.495000
2025-10-02 07:54:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:54:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:54:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:54:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:54:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:54:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.466610, avg_loss=0.711665, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:54:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:54:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:54:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:54:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.575000, curr=0.575000
2025-10-02 07:54:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 07:54:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 07:54:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 07:54:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:54:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.253586, avg_loss=0.706268, seen=200, correct=102, accuracy=0.510000
2025-10-02 07:54:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:54:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:54:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:54:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:54:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.833555, avg_loss=0.720839, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:54:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:54:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:54:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.575000, curr=0.475000
2025-10-02 07:54:44 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:54:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:54:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:54:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:54:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2072MB allocated=2011MB
2025-10-02 07:54:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:54:45 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #53', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:54:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:54:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:54:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:46 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:54:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:54:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:54:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:54:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:54:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=138.564804, avg_loss=0.717952, seen=193, correct=112, accuracy=0.580311
2025-10-02 07:54:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:54:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:54:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2036MB allocated=1994MB
2025-10-02 07:54:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:54:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:54:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.731733, avg_loss=0.693293, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:54:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:54:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2036MB allocated=1994MB
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_031.ckpt
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-02 07:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:54:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:55:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:55:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:55:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:55:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:55:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:55:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:55:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=139.136719, avg_loss=0.720916, seen=193, correct=94, accuracy=0.487047
2025-10-02 07:55:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:55:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:55:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:55:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:55:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:55:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.972054, avg_loss=0.674301, seen=40, correct=27, accuracy=0.675000
2025-10-02 07:55:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:55:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:55:11 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-02 07:55:11 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_031.ckpt
2025-10-02 07:55:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:55:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:55:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:55:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:55:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:55:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:55:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=145.083191, avg_loss=0.751726, seen=193, correct=89, accuracy=0.461140
2025-10-02 07:55:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:55:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:55:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:55:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:55:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:55:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:55:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.065420, avg_loss=0.676636, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:55:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:55:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:55:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:55:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.675000, curr=0.575000
2025-10-02 07:55:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:55:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:55:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:55:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:55:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:55:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=148.344650, avg_loss=0.768625, seen=193, correct=78, accuracy=0.404145
2025-10-02 07:55:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:55:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:55:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:55:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:55:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:55:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:55:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.545511, avg_loss=0.688638, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:55:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:55:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:55:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:55:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.675000, curr=0.550000
2025-10-02 07:55:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:55:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:55:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:55:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:55:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:55:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:55:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=140.707169, avg_loss=0.729053, seen=193, correct=92, accuracy=0.476684
2025-10-02 07:55:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:55:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:56:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:56:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:56:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:56:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.967403, avg_loss=0.674185, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:56:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:56:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:56:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.675000, curr=0.575000
2025-10-02 07:56:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:56:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:56:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:56:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:56:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:56:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=137.368103, avg_loss=0.711752, seen=193, correct=106, accuracy=0.549223
2025-10-02 07:56:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:56:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:56:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:56:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:56:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:56:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.008959, avg_loss=0.675224, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:56:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:56:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:56:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:56:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.675000, curr=0.500000
2025-10-02 07:56:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:56:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:56:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:56:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:56:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:56:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:56:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=137.058563, avg_loss=0.710148, seen=193, correct=97, accuracy=0.502591
2025-10-02 07:56:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:56:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:56:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:56:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:56:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.784210, avg_loss=0.669605, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:56:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:56:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:56:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:56:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.675000, curr=0.600000
2025-10-02 07:56:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 07:56:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 07:56:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 07:56:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:56:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:56:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:56:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=140.979111, avg_loss=0.730462, seen=193, correct=89, accuracy=0.461140
2025-10-02 07:56:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:56:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:56:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:56:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:56:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:56:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.600115, avg_loss=0.665003, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:56:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:56:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:56:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:56:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:56:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.675000, curr=0.625000
2025-10-02 07:57:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 07:57:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 07:57:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 07:57:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:57:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:57:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:57:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=141.940460, avg_loss=0.735443, seen=193, correct=86, accuracy=0.445596
2025-10-02 07:57:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:57:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:57:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:57:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:57:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:57:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.913979, avg_loss=0.672849, seen=40, correct=23, accuracy=0.575000
2025-10-02 07:57:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:57:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:57:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.675000, curr=0.575000
2025-10-02 07:57:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 07:57:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 07:57:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 07:57:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:57:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:57:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:57:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=137.554749, avg_loss=0.712719, seen=193, correct=99, accuracy=0.512953
2025-10-02 07:57:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:57:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:57:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:57:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:57:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:57:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.646755, avg_loss=0.666169, seen=40, correct=24, accuracy=0.600000
2025-10-02 07:57:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:57:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:57:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:57:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.675000, curr=0.600000
2025-10-02 07:57:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 07:57:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 07:57:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 07:57:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:57:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:57:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:57:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=136.809479, avg_loss=0.708857, seen=193, correct=101, accuracy=0.523316
2025-10-02 07:57:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:57:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:57:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:57:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:57:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:57:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:57:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.663349, avg_loss=0.666584, seen=40, correct=25, accuracy=0.625000
2025-10-02 07:57:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:57:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:57:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:57:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.675000, curr=0.625000
2025-10-02 07:57:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 07:57:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 07:57:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 07:57:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 07:57:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:57:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 07:58:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=136.345230, avg_loss=0.706452, seen=193, correct=105, accuracy=0.544041
2025-10-02 07:58:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:58:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:58:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:58:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:58:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.153072, avg_loss=0.678827, seen=40, correct=22, accuracy=0.550000
2025-10-02 07:58:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:58:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:58:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:58:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.675000, curr=0.550000
2025-10-02 07:58:07 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 07:58:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 07:58:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 07:58:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:58:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2092MB allocated=2028MB
2025-10-02 07:58:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 07:58:08 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {}}
2025-10-02 07:58:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 07:58:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 07:58:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:08 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 07:58:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:58:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 07:58:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:58:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:58:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=150.937775, avg_loss=0.754689, seen=200, correct=85, accuracy=0.425000
2025-10-02 07:58:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:58:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-02 07:58:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:58:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:58:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.550953, avg_loss=0.788774, seen=40, correct=16, accuracy=0.400000
2025-10-02 07:58:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:58:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2011MB
2025-10-02 07:58:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.400000
2025-10-02 07:58:18 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_038.ckpt
2025-10-02 07:58:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 07:58:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-02 07:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 07:58:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 07:58:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 07:58:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 07:58:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 07:58:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 07:58:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:58:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.170105, avg_loss=0.715851, seen=200, correct=98, accuracy=0.490000
2025-10-02 07:58:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:58:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:58:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:58:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.907379, avg_loss=0.722684, seen=40, correct=18, accuracy=0.450000
2025-10-02 07:58:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:58:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:58:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-02 07:58:35 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_038.ckpt
2025-10-02 07:58:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 07:58:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 07:58:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 07:58:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:58:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:58:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.100708, avg_loss=0.715504, seen=200, correct=107, accuracy=0.535000
2025-10-02 07:58:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:58:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:58:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:58:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:58:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:58:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:58:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.351717, avg_loss=0.708793, seen=40, correct=21, accuracy=0.525000
2025-10-02 07:58:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:58:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:58:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:58:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:58:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 07:58:52 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_038.ckpt
2025-10-02 07:58:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 07:58:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 07:58:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 07:59:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:59:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:59:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:59:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.244400, avg_loss=0.716222, seen=200, correct=96, accuracy=0.480000
2025-10-02 07:59:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:59:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:59:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:59:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:59:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:59:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:59:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.162735, avg_loss=0.729068, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:59:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:59:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:59:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.500000
2025-10-02 07:59:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 07:59:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 07:59:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 07:59:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:59:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:59:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.693604, avg_loss=0.718468, seen=200, correct=89, accuracy=0.445000
2025-10-02 07:59:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:59:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:59:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:59:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:59:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:59:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:59:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.208460, avg_loss=0.730211, seen=40, correct=19, accuracy=0.475000
2025-10-02 07:59:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:59:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:59:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.525000, curr=0.475000
2025-10-02 07:59:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 07:59:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 07:59:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 07:59:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:59:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:59:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.556351, avg_loss=0.717782, seen=200, correct=91, accuracy=0.455000
2025-10-02 07:59:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:59:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:59:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:59:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:59:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:59:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 07:59:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.997368, avg_loss=0.724934, seen=40, correct=20, accuracy=0.500000
2025-10-02 07:59:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:59:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:59:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:59:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.525000, curr=0.500000
2025-10-02 07:59:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 07:59:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 07:59:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 07:59:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 07:59:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 07:59:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 07:59:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.625565, avg_loss=0.708128, seen=200, correct=104, accuracy=0.520000
2025-10-02 07:59:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 07:59:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 07:59:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 07:59:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 07:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 07:59:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:00:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:00:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.365788, avg_loss=0.709145, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:00:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:00:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:00:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:00:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.525000, curr=0.450000
2025-10-02 08:00:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:00:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:00:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:00:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:00:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:00:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.432678, avg_loss=0.702163, seen=200, correct=108, accuracy=0.540000
2025-10-02 08:00:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:00:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:00:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:00:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:00:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:00:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:00:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.858961, avg_loss=0.696474, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:00:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:00:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:00:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:00:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.525000, curr=0.475000
2025-10-02 08:00:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:00:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:00:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:00:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:00:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:00:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.026428, avg_loss=0.700132, seen=200, correct=102, accuracy=0.510000
2025-10-02 08:00:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:00:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:00:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:00:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:00:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:00:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:00:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.039669, avg_loss=0.700992, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:00:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:00:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:00:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:00:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.525000, curr=0.500000
2025-10-02 08:00:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:00:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:00:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:00:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:00:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:00:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:00:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.784897, avg_loss=0.703924, seen=200, correct=104, accuracy=0.520000
2025-10-02 08:00:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:00:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:00:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:00:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:00:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:00:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:00:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.112146, avg_loss=0.702804, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:00:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:00:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:00:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:00:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:00:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.525000, curr=0.525000
2025-10-02 08:01:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:01:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:01:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:01:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:01:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:01:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.863419, avg_loss=0.709317, seen=200, correct=88, accuracy=0.440000
2025-10-02 08:01:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:01:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:01:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:01:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:01:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:01:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.901913, avg_loss=0.722548, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:01:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:01:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:01:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:01:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.525000, curr=0.475000
2025-10-02 08:01:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:01:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:01:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:01:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:01:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:01:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.782333, avg_loss=0.713912, seen=200, correct=87, accuracy=0.435000
2025-10-02 08:01:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:01:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:01:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:01:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:01:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.813797, avg_loss=0.720345, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:01:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:01:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:01:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:01:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.525000, curr=0.475000
2025-10-02 08:01:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 08:01:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 08:01:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 08:01:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:01:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:01:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.424744, avg_loss=0.717124, seen=200, correct=88, accuracy=0.440000
2025-10-02 08:01:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:01:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:01:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:01:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:01:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:01:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.047600, avg_loss=0.726190, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:01:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:01:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:01:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:01:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.525000, curr=0.425000
2025-10-02 08:01:46 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:01:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:01:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:01:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:01:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2044MB
2025-10-02 08:01:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:01:47 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #38', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:01:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:01:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:01:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=292, num_train_batch_last_epoch=216, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:47 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:01:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:01:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:01:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:01:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=292, num_train_batch_last_epoch=216, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:01:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=21.859335, avg_loss=0.728644, seen=30, correct=13, accuracy=0.433333
2025-10-02 08:01:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:01:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:01:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2028MB
2025-10-02 08:01:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:01:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=292, num_train_batch_last_epoch=216, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:01:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.159439, avg_loss=0.728986, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:01:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:01:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:01:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2056MB allocated=2028MB
2025-10-02 08:01:55 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 08:01:55 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_023.ckpt
2025-10-02 08:01:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:01:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-02 08:01:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:01:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:01:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=292, num_train_batch_last_epoch=216, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:01:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:01:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:02:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:02:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:02:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:02:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:02:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:02:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:02:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=21.257359, avg_loss=0.708579, seen=30, correct=12, accuracy=0.400000
2025-10-02 08:02:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:02:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:02:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:02:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:02:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:02:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:02:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.581841, avg_loss=0.714546, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:02:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:02:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:02:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:02:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.525000
2025-10-02 08:02:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:02:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:02:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:02:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:02:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:02:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:02:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.838890, avg_loss=0.694630, seen=30, correct=15, accuracy=0.500000
2025-10-02 08:02:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:02:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:02:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:02:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:02:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:02:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.074377, avg_loss=0.701859, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:02:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:02:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:02:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.525000, curr=0.450000
2025-10-02 08:02:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:02:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:02:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:02:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:02:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:02:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.342028, avg_loss=0.678068, seen=30, correct=17, accuracy=0.566667
2025-10-02 08:02:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:02:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:02:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:02:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:02:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:02:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:02:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.146770, avg_loss=0.703669, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:02:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:02:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:02:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:02:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.525000, curr=0.525000
2025-10-02 08:02:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:02:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:02:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:02:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:02:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:02:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:02:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.387039, avg_loss=0.679568, seen=30, correct=17, accuracy=0.566667
2025-10-02 08:02:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:02:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:02:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:02:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:02:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:02:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.983326, avg_loss=0.699583, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:02:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:02:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:02:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:02:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:02:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.525000, curr=0.525000
2025-10-02 08:03:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:03:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:03:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:03:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:03:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:03:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=21.102726, avg_loss=0.703424, seen=30, correct=13, accuracy=0.433333
2025-10-02 08:03:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:03:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:03:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:03:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:03:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:03:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.741919, avg_loss=0.693548, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:03:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:03:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:03:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:03:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.525000, curr=0.450000
2025-10-02 08:03:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:03:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:03:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:03:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:03:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:03:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=21.005997, avg_loss=0.700200, seen=30, correct=14, accuracy=0.466667
2025-10-02 08:03:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:03:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:03:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:03:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:03:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:03:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.529924, avg_loss=0.688248, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:03:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:03:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:03:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:03:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.525000, curr=0.500000
2025-10-02 08:03:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:03:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:03:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:03:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:03:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:03:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.664869, avg_loss=0.688829, seen=30, correct=14, accuracy=0.466667
2025-10-02 08:03:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:03:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:03:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:03:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:03:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:03:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:03:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.496937, avg_loss=0.687423, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:03:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:03:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:03:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:03:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.525000, curr=0.525000
2025-10-02 08:03:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:03:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:03:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:03:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:03:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:03:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:03:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.521561, avg_loss=0.684052, seen=30, correct=18, accuracy=0.600000
2025-10-02 08:03:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:03:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:03:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:03:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:03:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:03:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:03:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.574707, avg_loss=0.689368, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:03:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:03:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:03:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:03:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.525000, curr=0.475000
2025-10-02 08:03:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:03:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:03:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:03:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:03:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:03:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:03:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.977779, avg_loss=0.699259, seen=30, correct=14, accuracy=0.466667
2025-10-02 08:03:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:03:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:03:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:03:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:03:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:04:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:04:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.340946, avg_loss=0.683524, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:04:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:04:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:04:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 08:04:01 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_023.ckpt
2025-10-02 08:04:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:04:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:04:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:04:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:04:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:04:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.626507, avg_loss=0.687550, seen=30, correct=14, accuracy=0.466667
2025-10-02 08:04:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:04:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:04:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:04:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:04:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.966789, avg_loss=0.674170, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:04:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:04:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:04:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 08:04:16 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_023.ckpt
2025-10-02 08:04:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:04:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:04:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:04:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:04:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:04:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:04:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.416363, avg_loss=0.680545, seen=30, correct=16, accuracy=0.533333
2025-10-02 08:04:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:04:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:04:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:04:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:04:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:04:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:04:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.598898, avg_loss=0.689972, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:04:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:04:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:04:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.525000
2025-10-02 08:04:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 08:04:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 08:04:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 08:04:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:04:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:04:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:04:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.796663, avg_loss=0.693222, seen=30, correct=15, accuracy=0.500000
2025-10-02 08:04:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:04:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:04:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:04:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:04:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.247812, avg_loss=0.681195, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:04:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:04:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:04:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.600000
2025-10-02 08:04:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 08:04:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 08:04:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 08:04:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:04:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:04:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=21.145779, avg_loss=0.704859, seen=30, correct=14, accuracy=0.466667
2025-10-02 08:04:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:04:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:04:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:04:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:04:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.248621, avg_loss=0.681216, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:04:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:04:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:04:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:04:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:04:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.550000
2025-10-02 08:05:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 08:05:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 08:05:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 08:05:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:05:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:05:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.680820, avg_loss=0.689361, seen=30, correct=15, accuracy=0.500000
2025-10-02 08:05:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:05:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:05:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:05:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:05:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.450760, avg_loss=0.686269, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:05:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:05:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:05:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.525000
2025-10-02 08:05:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 08:05:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 08:05:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 08:05:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:05:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:05:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:05:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.486803, avg_loss=0.682893, seen=30, correct=16, accuracy=0.533333
2025-10-02 08:05:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:05:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:05:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:05:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:05:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:05:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.318947, avg_loss=0.682974, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:05:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:05:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:05:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:05:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.525000
2025-10-02 08:05:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 08:05:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 08:05:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 08:05:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:05:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:05:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:05:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=21.059956, avg_loss=0.701999, seen=30, correct=15, accuracy=0.500000
2025-10-02 08:05:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:05:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:05:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:05:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:05:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:05:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:05:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.339443, avg_loss=0.683486, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:05:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:05:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:05:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.550000
2025-10-02 08:05:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 08:05:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 08:05:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 08:05:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:05:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:05:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:05:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=21.049431, avg_loss=0.701648, seen=30, correct=14, accuracy=0.466667
2025-10-02 08:05:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:05:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:05:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:05:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:05:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:05:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.086395, avg_loss=0.677160, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:05:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:05:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:05:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:05:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:05:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.575000
2025-10-02 08:06:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 08:06:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 08:06:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 08:06:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:06:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.559790, avg_loss=0.685326, seen=30, correct=16, accuracy=0.533333
2025-10-02 08:06:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:06:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:06:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:06:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.258457, avg_loss=0.681461, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:06:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:06:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:06:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.550000
2025-10-02 08:06:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 08:06:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 08:06:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 08:06:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:06:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:06:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.814934, avg_loss=0.693831, seen=30, correct=17, accuracy=0.566667
2025-10-02 08:06:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:06:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:06:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:06:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.936577, avg_loss=0.673414, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:06:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:06:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:06:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:06:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.575000
2025-10-02 08:06:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 08:06:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 08:06:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 08:06:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 08:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 08:06:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=21.341312, avg_loss=0.711377, seen=30, correct=13, accuracy=0.433333
2025-10-02 08:06:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:06:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:06:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:06:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.093029, avg_loss=0.677326, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:06:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:06:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:06:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.600000, curr=0.500000
2025-10-02 08:06:40 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:06:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:06:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:06:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:06:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-02 08:06:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:06:41 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:06:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:06:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:06:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=658, num_train_batch_last_epoch=142, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:41 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:06:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:06:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:06:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=658, num_train_batch_last_epoch=142, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:06:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=53.172108, avg_loss=0.770610, seen=69, correct=38, accuracy=0.550725
2025-10-02 08:06:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:06:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:06:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2076MB allocated=2044MB
2025-10-02 08:06:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:06:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=658, num_train_batch_last_epoch=142, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:06:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.262808, avg_loss=0.681570, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:06:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:06:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2076MB allocated=2044MB
2025-10-02 08:06:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 08:06:49 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_008.ckpt
2025-10-02 08:06:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:06:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-02 08:06:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:06:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=658, num_train_batch_last_epoch=142, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:06:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:06:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:06:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:06:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:06:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:06:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:06:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:07:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:07:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.995361, avg_loss=0.724570, seen=69, correct=32, accuracy=0.463768
2025-10-02 08:07:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:07:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:07:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:07:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:07:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:07:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:07:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.194639, avg_loss=0.679866, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:07:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:07:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:07:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 08:07:03 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_008.ckpt
2025-10-02 08:07:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:07:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:07:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:07:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:07:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:07:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:07:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.466591, avg_loss=0.716907, seen=69, correct=32, accuracy=0.463768
2025-10-02 08:07:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:07:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:07:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:07:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:07:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.605946, avg_loss=0.690149, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:07:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:07:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:07:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.525000
2025-10-02 08:07:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:07:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:07:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:07:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:07:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:07:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:07:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.878014, avg_loss=0.722870, seen=69, correct=32, accuracy=0.463768
2025-10-02 08:07:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:07:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:07:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:07:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:07:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:07:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:07:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.981724, avg_loss=0.674543, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:07:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:07:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:07:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.500000
2025-10-02 08:07:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:07:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:07:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:07:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:07:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:07:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.674797, avg_loss=0.719925, seen=69, correct=34, accuracy=0.492754
2025-10-02 08:07:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:07:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:07:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:07:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:07:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.896269, avg_loss=0.672407, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:07:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:07:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:07:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.525000
2025-10-02 08:07:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:07:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:07:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:07:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:07:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:08:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:08:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.548904, avg_loss=0.718100, seen=69, correct=30, accuracy=0.434783
2025-10-02 08:08:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:08:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:08:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:08:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:08:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.327993, avg_loss=0.683200, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:08:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:08:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:08:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.575000
2025-10-02 08:08:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:08:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:08:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:08:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:08:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:08:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:08:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.158104, avg_loss=0.712436, seen=69, correct=30, accuracy=0.434783
2025-10-02 08:08:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:08:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:08:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:08:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:08:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:08:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.976004, avg_loss=0.724400, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:08:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:08:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:08:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.500000
2025-10-02 08:08:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:08:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:08:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:08:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:08:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:08:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=48.999447, avg_loss=0.710137, seen=69, correct=32, accuracy=0.463768
2025-10-02 08:08:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:08:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:08:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:08:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:08:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:08:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:08:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.684155, avg_loss=0.692104, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:08:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:08:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:08:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:08:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.550000
2025-10-02 08:08:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:08:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:08:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:08:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:08:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:08:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=48.861446, avg_loss=0.708137, seen=69, correct=30, accuracy=0.434783
2025-10-02 08:08:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:08:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:08:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:08:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:08:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:08:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.000278, avg_loss=0.700007, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:08:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:08:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:08:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:08:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.525000
2025-10-02 08:08:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:08:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:08:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:08:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:08:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:08:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:08:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.464268, avg_loss=0.716873, seen=69, correct=28, accuracy=0.405797
2025-10-02 08:08:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:08:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:08:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:08:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:08:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:09:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.036982, avg_loss=0.700925, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:09:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:09:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:09:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.525000
2025-10-02 08:09:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:09:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:09:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:09:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:09:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.420357, avg_loss=0.716237, seen=69, correct=30, accuracy=0.434783
2025-10-02 08:09:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:09:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:09:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:09:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:09:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:09:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.894299, avg_loss=0.697357, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:09:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:09:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:09:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.475000
2025-10-02 08:09:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:09:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:09:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:09:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 08:09:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 08:09:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.384651, avg_loss=0.715720, seen=69, correct=37, accuracy=0.536232
2025-10-02 08:09:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:09:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:09:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:09:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:09:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.523512, avg_loss=0.688088, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:09:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:09:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:09:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:09:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.600000, curr=0.475000
2025-10-02 08:09:32 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:09:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:09:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:09:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:09:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2134MB allocated=2078MB
2025-10-02 08:09:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:09:33 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:09:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:09:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:09:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:33 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:09:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:09:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:09:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:09:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:09:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=149.310272, avg_loss=0.746551, seen=200, correct=90, accuracy=0.450000
2025-10-02 08:09:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:09:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2096MB allocated=2061MB
2025-10-02 08:09:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:09:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:09:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.725262, avg_loss=0.668132, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:09:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:09:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:09:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2096MB allocated=2061MB
2025-10-02 08:09:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 08:09:43 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_015.ckpt
2025-10-02 08:09:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:09:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-02 08:09:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:09:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:09:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:09:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:09:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:09:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:09:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:09:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:09:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.272064, avg_loss=0.681360, seen=200, correct=109, accuracy=0.545000
2025-10-02 08:09:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:09:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:09:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:09:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:09:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:09:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:10:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:10:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.178280, avg_loss=0.704457, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:10:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:10:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:10:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:10:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.475000
2025-10-02 08:10:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:10:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:10:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:10:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:10:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:10:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.522705, avg_loss=0.682614, seen=200, correct=110, accuracy=0.550000
2025-10-02 08:10:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:10:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:10:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:10:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:10:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:10:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.214653, avg_loss=0.705366, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:10:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:10:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:10:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:10:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.475000
2025-10-02 08:10:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:10:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:10:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:10:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:10:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:10:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:10:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.365189, avg_loss=0.691826, seen=200, correct=116, accuracy=0.580000
2025-10-02 08:10:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:10:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:10:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:10:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:10:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:10:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.134539, avg_loss=0.678363, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:10:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:10:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:10:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.600000
2025-10-02 08:10:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:10:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:10:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:10:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:10:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:10:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:10:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.455994, avg_loss=0.702280, seen=200, correct=113, accuracy=0.565000
2025-10-02 08:10:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:10:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:10:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:10:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:10:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:10:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.823442, avg_loss=0.670586, seen=40, correct=25, accuracy=0.625000
2025-10-02 08:10:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:10:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:10:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:10:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:10:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 08:10:53 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_015.ckpt
2025-10-02 08:11:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:11:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:11:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:11:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:11:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:11:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.930557, avg_loss=0.699653, seen=200, correct=110, accuracy=0.550000
2025-10-02 08:11:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:11:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:11:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:11:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:11:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.134342, avg_loss=0.678359, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:11:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:11:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:11:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.575000
2025-10-02 08:11:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:11:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:11:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:11:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:11:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:11:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:11:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.939789, avg_loss=0.689699, seen=200, correct=118, accuracy=0.590000
2025-10-02 08:11:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:11:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:11:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:11:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:11:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:11:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.203646, avg_loss=0.680091, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:11:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:11:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:11:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.600000
2025-10-02 08:11:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:11:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:11:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:11:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:11:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:11:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.693069, avg_loss=0.678465, seen=200, correct=110, accuracy=0.550000
2025-10-02 08:11:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:11:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:11:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:11:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.956985, avg_loss=0.698925, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:11:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:11:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:11:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.450000
2025-10-02 08:11:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:11:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:11:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:11:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:11:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:11:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:11:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.527740, avg_loss=0.677639, seen=200, correct=111, accuracy=0.555000
2025-10-02 08:11:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:11:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:11:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:11:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:12:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:12:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.645500, avg_loss=0.691138, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:12:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:12:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:12:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:12:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.525000
2025-10-02 08:12:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:12:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:12:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:12:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:12:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:12:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:12:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.441086, avg_loss=0.692205, seen=200, correct=119, accuracy=0.595000
2025-10-02 08:12:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:12:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:12:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:12:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:12:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.083027, avg_loss=0.677076, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:12:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:12:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:12:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.600000
2025-10-02 08:12:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:12:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:12:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:12:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:12:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:12:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:12:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.365265, avg_loss=0.681826, seen=200, correct=111, accuracy=0.555000
2025-10-02 08:12:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:12:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:12:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:12:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:12:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.425808, avg_loss=0.685645, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:12:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:12:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:12:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:12:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.575000
2025-10-02 08:12:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:12:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:12:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:12:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:12:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:12:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:12:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.074890, avg_loss=0.675374, seen=200, correct=113, accuracy=0.565000
2025-10-02 08:12:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:12:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:12:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:12:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:12:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:12:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.653757, avg_loss=0.716344, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:12:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:12:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:12:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:12:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:12:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.425000
2025-10-02 08:13:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 08:13:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 08:13:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 08:13:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:13:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.482468, avg_loss=0.677412, seen=200, correct=109, accuracy=0.545000
2025-10-02 08:13:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:13:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:13:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:13:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:13:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.132917, avg_loss=0.703323, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:13:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:13:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:13:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.625000, curr=0.425000
2025-10-02 08:13:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 08:13:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 08:13:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 08:13:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:13:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.138123, avg_loss=0.685691, seen=200, correct=118, accuracy=0.590000
2025-10-02 08:13:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:13:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:13:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:13:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:13:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.308796, avg_loss=0.682720, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:13:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:13:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:13:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.625000, curr=0.575000
2025-10-02 08:13:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 08:13:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 08:13:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 08:13:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:13:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.328812, avg_loss=0.691644, seen=200, correct=113, accuracy=0.565000
2025-10-02 08:13:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:13:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:13:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:13:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.260731, avg_loss=0.681518, seen=40, correct=25, accuracy=0.625000
2025-10-02 08:13:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:13:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:13:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:13:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.625000, curr=0.625000
2025-10-02 08:13:42 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:13:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:13:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:13:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-02 08:13:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:13:43 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:13:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:13:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:13:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:44 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:13:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:13:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:13:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:13:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=156.855957, avg_loss=0.784280, seen=200, correct=87, accuracy=0.435000
2025-10-02 08:13:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:13:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2116MB allocated=2078MB
2025-10-02 08:13:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:13:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:13:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.811337, avg_loss=0.795283, seen=40, correct=15, accuracy=0.375000
2025-10-02 08:13:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:13:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:13:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2116MB allocated=2078MB
2025-10-02 08:13:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.375000
2025-10-02 08:13:53 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_035.ckpt
2025-10-02 08:13:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:13:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-02 08:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:13:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:13:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:13:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:13:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:14:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:14:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:14:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:14:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:14:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:14:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:14:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.888306, avg_loss=0.714442, seen=200, correct=101, accuracy=0.505000
2025-10-02 08:14:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:14:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:14:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:14:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:14:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.061367, avg_loss=0.726534, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:14:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:14:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:14:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:14:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 08:14:10 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_035.ckpt
2025-10-02 08:14:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:14:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:14:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:14:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:14:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:14:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.067551, avg_loss=0.695338, seen=200, correct=108, accuracy=0.540000
2025-10-02 08:14:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:14:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:14:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:14:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:14:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:14:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:14:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.378834, avg_loss=0.709471, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:14:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:14:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:14:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.425000
2025-10-02 08:14:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:14:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:14:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:14:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:14:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:14:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.178253, avg_loss=0.690891, seen=200, correct=107, accuracy=0.535000
2025-10-02 08:14:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:14:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:14:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:14:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:14:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:14:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.149471, avg_loss=0.703737, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:14:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:14:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:14:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.500000, curr=0.475000
2025-10-02 08:14:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:14:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:14:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:14:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:14:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:14:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:14:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.704971, avg_loss=0.693525, seen=200, correct=109, accuracy=0.545000
2025-10-02 08:14:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:14:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:14:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:14:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:15:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:15:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.364565, avg_loss=0.709114, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:15:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:15:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:15:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:15:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.500000, curr=0.425000
2025-10-02 08:15:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:15:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:15:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:15:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:15:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:15:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.248169, avg_loss=0.701241, seen=200, correct=105, accuracy=0.525000
2025-10-02 08:15:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:15:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:15:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:15:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:15:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.362207, avg_loss=0.709055, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:15:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:15:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:15:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.500000, curr=0.425000
2025-10-02 08:15:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:15:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:15:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:15:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:15:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:15:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:15:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.392349, avg_loss=0.711962, seen=200, correct=97, accuracy=0.485000
2025-10-02 08:15:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:15:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:15:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:15:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:15:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:15:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.835863, avg_loss=0.720897, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:15:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:15:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:15:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:15:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 08:15:35 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_035.ckpt
2025-10-02 08:15:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:15:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:15:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:15:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:15:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:15:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.087814, avg_loss=0.715439, seen=200, correct=98, accuracy=0.490000
2025-10-02 08:15:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:15:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:15:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:15:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:15:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.742144, avg_loss=0.718554, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:15:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:15:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:15:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:15:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.425000
2025-10-02 08:16:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:16:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:16:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:16:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:16:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:16:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:16:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.712234, avg_loss=0.698561, seen=200, correct=106, accuracy=0.530000
2025-10-02 08:16:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:16:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:16:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:16:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:16:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:16:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.956841, avg_loss=0.698921, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:16:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:16:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:16:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:16:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.525000, curr=0.525000
2025-10-02 08:16:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:16:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:16:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:16:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:16:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:16:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:16:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.342743, avg_loss=0.691714, seen=200, correct=107, accuracy=0.535000
2025-10-02 08:16:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:16:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:16:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:16:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:16:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.815426, avg_loss=0.695386, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:16:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:16:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:16:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.525000, curr=0.475000
2025-10-02 08:16:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:16:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:16:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:16:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:16:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:16:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:16:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.817871, avg_loss=0.694089, seen=200, correct=108, accuracy=0.540000
2025-10-02 08:16:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:16:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:16:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:16:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:16:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.756403, avg_loss=0.693910, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:16:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:16:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:16:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:16:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 08:16:47 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_035.ckpt
2025-10-02 08:16:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:16:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:16:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:16:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:16:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:16:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:17:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:17:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.682251, avg_loss=0.678411, seen=200, correct=117, accuracy=0.585000
2025-10-02 08:17:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:17:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:17:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:17:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:17:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:17:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:17:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.267540, avg_loss=0.681688, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:17:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:17:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:17:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:17:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.500000
2025-10-02 08:17:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 08:17:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 08:17:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 08:17:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:17:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:17:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.561371, avg_loss=0.677807, seen=200, correct=114, accuracy=0.570000
2025-10-02 08:17:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:17:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:17:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:17:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:17:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.051956, avg_loss=0.676299, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:17:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:17:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:17:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:17:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.550000
2025-10-02 08:17:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 08:17:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 08:17:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 08:17:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:17:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:17:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:17:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.668030, avg_loss=0.708340, seen=200, correct=100, accuracy=0.500000
2025-10-02 08:17:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:17:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:17:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:17:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:17:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:17:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.999275, avg_loss=0.699982, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:17:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:17:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:17:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:17:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.500000
2025-10-02 08:17:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 08:17:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 08:17:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 08:17:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:17:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:17:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:17:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.624847, avg_loss=0.708124, seen=200, correct=102, accuracy=0.510000
2025-10-02 08:17:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:17:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:17:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:17:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:17:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:17:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:17:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.179743, avg_loss=0.704494, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:17:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:17:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:17:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:17:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:17:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.500000
2025-10-02 08:18:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 08:18:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 08:18:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 08:18:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:18:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:18:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:18:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.462219, avg_loss=0.702311, seen=200, correct=104, accuracy=0.520000
2025-10-02 08:18:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:18:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:18:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:18:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:18:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:18:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:18:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.870655, avg_loss=0.696766, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:18:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:18:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:18:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:18:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.550000
2025-10-02 08:18:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 08:18:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 08:18:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 08:18:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:18:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:18:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.758087, avg_loss=0.693790, seen=200, correct=106, accuracy=0.530000
2025-10-02 08:18:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:18:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:18:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:18:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:18:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:18:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:18:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.323874, avg_loss=0.683097, seen=40, correct=25, accuracy=0.625000
2025-10-02 08:18:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:18:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:18:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 08:18:29 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_035.ckpt
2025-10-02 08:18:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 08:18:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 08:18:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 08:18:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:18:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:18:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:18:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.350174, avg_loss=0.696751, seen=200, correct=104, accuracy=0.520000
2025-10-02 08:18:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:18:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:18:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:18:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:18:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:18:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.591440, avg_loss=0.689786, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:18:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:18:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:18:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:18:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.525000
2025-10-02 08:18:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 08:18:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 08:18:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 08:18:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:18:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:18:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:18:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.109528, avg_loss=0.705548, seen=200, correct=98, accuracy=0.490000
2025-10-02 08:18:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:18:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:19:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:19:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:19:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:19:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.841736, avg_loss=0.696043, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:19:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:19:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:19:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:19:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.500000
2025-10-02 08:19:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 08:19:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 08:19:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 08:19:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:19:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:19:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.522324, avg_loss=0.702612, seen=200, correct=99, accuracy=0.495000
2025-10-02 08:19:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:19:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:19:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:19:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:19:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:19:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:19:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.114700, avg_loss=0.702868, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:19:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:19:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:19:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.525000
2025-10-02 08:19:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 08:19:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 08:19:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 08:19:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:19:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:19:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:19:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.158524, avg_loss=0.710793, seen=200, correct=94, accuracy=0.470000
2025-10-02 08:19:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:19:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:19:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:19:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:19:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:19:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.084705, avg_loss=0.702118, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:19:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:19:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:19:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.425000
2025-10-02 08:19:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 08:19:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 08:19:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 08:19:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:19:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:19:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:19:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.203705, avg_loss=0.706019, seen=200, correct=99, accuracy=0.495000
2025-10-02 08:19:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:19:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:19:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:19:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:19:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:19:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:19:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.014011, avg_loss=0.700350, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:19:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:19:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:19:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:19:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:19:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.450000
2025-10-02 08:20:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=220
2025-10-02 08:20:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=220
2025-10-02 08:20:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=220, splits=['val', 'test']
2025-10-02 08:20:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:20:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:20:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:20:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.174255, avg_loss=0.685871, seen=200, correct=108, accuracy=0.540000
2025-10-02 08:20:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:20:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:20:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:20:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:20:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:20:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.218752, avg_loss=0.680469, seen=40, correct=27, accuracy=0.675000
2025-10-02 08:20:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:20:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:20:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:20:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-02 08:20:14 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_035.ckpt
2025-10-02 08:20:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=230
2025-10-02 08:20:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=230
2025-10-02 08:20:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=230, splits=['val', 'test']
2025-10-02 08:20:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:20:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:20:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.458344, avg_loss=0.677292, seen=200, correct=111, accuracy=0.555000
2025-10-02 08:20:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:20:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:20:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:20:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:20:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.687319, avg_loss=0.667183, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:20:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:20:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:20:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:20:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.675000, curr=0.575000
2025-10-02 08:20:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=240
2025-10-02 08:20:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=240
2025-10-02 08:20:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=240, splits=['val', 'test']
2025-10-02 08:20:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:20:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:20:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.348450, avg_loss=0.686742, seen=200, correct=108, accuracy=0.540000
2025-10-02 08:20:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:20:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:20:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:20:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:20:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.014948, avg_loss=0.675374, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:20:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:20:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:20:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:20:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.675000, curr=0.575000
2025-10-02 08:20:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=250
2025-10-02 08:20:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=250
2025-10-02 08:20:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=250, splits=['val', 'test']
2025-10-02 08:20:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:20:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:20:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:21:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:21:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.627106, avg_loss=0.678136, seen=200, correct=111, accuracy=0.555000
2025-10-02 08:21:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:21:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:21:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:21:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:21:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:21:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:21:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.618584, avg_loss=0.665465, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:21:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:21:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:21:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:21:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.675000, curr=0.600000
2025-10-02 08:21:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=260
2025-10-02 08:21:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=260
2025-10-02 08:21:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=260, splits=['val', 'test']
2025-10-02 08:21:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:21:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:21:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.742493, avg_loss=0.673712, seen=200, correct=118, accuracy=0.590000
2025-10-02 08:21:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:21:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:21:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:21:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:21:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:21:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.470173, avg_loss=0.661754, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:21:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:21:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:21:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.675000, curr=0.575000
2025-10-02 08:21:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=270
2025-10-02 08:21:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=270
2025-10-02 08:21:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=270, splits=['val', 'test']
2025-10-02 08:21:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:21:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:21:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.422485, avg_loss=0.687112, seen=200, correct=113, accuracy=0.565000
2025-10-02 08:21:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:21:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:21:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:21:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:21:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:21:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.113258, avg_loss=0.677831, seen=40, correct=25, accuracy=0.625000
2025-10-02 08:21:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:21:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:21:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:21:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.675000, curr=0.625000
2025-10-02 08:21:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=280
2025-10-02 08:21:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=280
2025-10-02 08:21:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=280, splits=['val', 'test']
2025-10-02 08:21:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:21:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:21:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.892197, avg_loss=0.694461, seen=200, correct=103, accuracy=0.515000
2025-10-02 08:21:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:21:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:21:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:21:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:21:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:21:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:21:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.159035, avg_loss=0.678976, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:21:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:21:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:21:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:21:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.675000, curr=0.550000
2025-10-02 08:22:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=290
2025-10-02 08:22:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=290
2025-10-02 08:22:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=290, splits=['val', 'test']
2025-10-02 08:22:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:22:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:22:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:22:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.139160, avg_loss=0.695696, seen=200, correct=107, accuracy=0.535000
2025-10-02 08:22:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:22:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:22:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:22:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:22:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:22:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.201086, avg_loss=0.680027, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:22:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:22:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:22:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.675000, curr=0.575000
2025-10-02 08:22:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=300
2025-10-02 08:22:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=300
2025-10-02 08:22:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=300, splits=['val', 'test']
2025-10-02 08:22:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:22:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:22:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.964722, avg_loss=0.699824, seen=200, correct=100, accuracy=0.500000
2025-10-02 08:22:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:22:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:22:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:22:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:22:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:22:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.130163, avg_loss=0.678254, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:22:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:22:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:22:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.675000, curr=0.550000
2025-10-02 08:22:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=310
2025-10-02 08:22:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=310
2025-10-02 08:22:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=310, splits=['val', 'test']
2025-10-02 08:22:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:22:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:22:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:22:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.226517, avg_loss=0.676133, seen=200, correct=118, accuracy=0.590000
2025-10-02 08:22:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:22:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:22:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:22:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:22:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:22:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:22:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.260416, avg_loss=0.656510, seen=40, correct=26, accuracy=0.650000
2025-10-02 08:22:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:22:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:22:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:22:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:22:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.675000, curr=0.650000
2025-10-02 08:23:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=320
2025-10-02 08:23:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=320
2025-10-02 08:23:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=320, splits=['val', 'test']
2025-10-02 08:23:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:23:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:23:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.760132, avg_loss=0.683801, seen=200, correct=107, accuracy=0.535000
2025-10-02 08:23:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:23:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:23:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:23:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.711855, avg_loss=0.667796, seen=40, correct=26, accuracy=0.650000
2025-10-02 08:23:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:23:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:23:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:23:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.675000, curr=0.650000
2025-10-02 08:23:09 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:23:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:23:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:23:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2112MB
2025-10-02 08:23:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:23:10 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:23:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:23:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:23:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:10 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:23:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:23:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:23:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:23:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=92.466560, avg_loss=0.700504, seen=132, correct=66, accuracy=0.500000
2025-10-02 08:23:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:23:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:23:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2136MB allocated=2095MB
2025-10-02 08:23:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:23:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:23:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.837635, avg_loss=0.795941, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:23:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:23:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:23:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2136MB allocated=2095MB
2025-10-02 08:23:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-02 08:23:19 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_049.ckpt
2025-10-02 08:23:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:23:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-02 08:23:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:23:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:23:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:23:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:23:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:23:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:23:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:23:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:23:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=89.458565, avg_loss=0.677716, seen=132, correct=76, accuracy=0.575758
2025-10-02 08:23:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:23:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:23:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:23:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:23:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:23:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.858696, avg_loss=0.771467, seen=40, correct=15, accuracy=0.375000
2025-10-02 08:23:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:23:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:23:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.450000, curr=0.375000
2025-10-02 08:23:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:23:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:23:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:23:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:23:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=94.337120, avg_loss=0.714675, seen=132, correct=70, accuracy=0.530303
2025-10-02 08:23:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:23:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:23:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:23:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:23:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:23:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:23:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.533331, avg_loss=0.788333, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:23:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:23:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:23:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:23:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-02 08:23:53 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_049.ckpt
2025-10-02 08:24:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:24:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:24:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:24:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:24:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:24:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:24:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=90.935493, avg_loss=0.688905, seen=132, correct=69, accuracy=0.522727
2025-10-02 08:24:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:24:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:24:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:24:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:24:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:24:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.637249, avg_loss=0.765931, seen=40, correct=15, accuracy=0.375000
2025-10-02 08:24:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:24:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:24:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:24:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.475000, curr=0.375000
2025-10-02 08:24:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:24:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:24:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:24:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:24:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:24:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:24:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=89.302940, avg_loss=0.676537, seen=132, correct=75, accuracy=0.568182
2025-10-02 08:24:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:24:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:24:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:24:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:24:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.702871, avg_loss=0.742572, seen=40, correct=14, accuracy=0.350000
2025-10-02 08:24:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:24:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:24:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:24:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.475000, curr=0.350000
2025-10-02 08:24:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:24:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:24:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:24:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:24:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:24:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:24:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=89.803665, avg_loss=0.680331, seen=132, correct=76, accuracy=0.575758
2025-10-02 08:24:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:24:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:24:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:24:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:24:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:24:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.063942, avg_loss=0.751599, seen=40, correct=15, accuracy=0.375000
2025-10-02 08:24:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:24:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:24:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.475000, curr=0.375000
2025-10-02 08:24:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:24:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:24:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:24:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:24:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:24:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:24:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=89.807632, avg_loss=0.680361, seen=132, correct=73, accuracy=0.553030
2025-10-02 08:24:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:24:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:24:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:24:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:24:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:24:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:25:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.431578, avg_loss=0.735789, seen=40, correct=15, accuracy=0.375000
2025-10-02 08:25:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:25:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:25:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.475000, curr=0.375000
2025-10-02 08:25:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:25:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:25:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:25:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:25:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:25:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:25:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=89.552048, avg_loss=0.678425, seen=132, correct=76, accuracy=0.575758
2025-10-02 08:25:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:25:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:25:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:25:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:25:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:25:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:25:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.691910, avg_loss=0.742298, seen=40, correct=15, accuracy=0.375000
2025-10-02 08:25:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:25:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:25:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:25:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.475000, curr=0.375000
2025-10-02 08:25:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:25:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:25:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:25:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:25:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:25:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:25:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=90.167343, avg_loss=0.683086, seen=132, correct=71, accuracy=0.537879
2025-10-02 08:25:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:25:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:25:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:25:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:25:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:25:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.662043, avg_loss=0.741551, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:25:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:25:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:25:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:25:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.475000, curr=0.450000
2025-10-02 08:25:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:25:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:25:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:25:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:25:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:25:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:25:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=89.878860, avg_loss=0.680900, seen=132, correct=77, accuracy=0.583333
2025-10-02 08:25:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:25:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:25:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:25:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:25:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:25:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:25:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.287634, avg_loss=0.732191, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:25:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:25:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:25:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:25:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:25:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.475000, curr=0.425000
2025-10-02 08:26:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:26:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:26:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:26:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:26:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:26:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=90.397247, avg_loss=0.684828, seen=132, correct=72, accuracy=0.545455
2025-10-02 08:26:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:26:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:26:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:26:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:26:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:26:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.280626, avg_loss=0.732016, seen=40, correct=13, accuracy=0.325000
2025-10-02 08:26:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:26:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:26:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.475000, curr=0.325000
2025-10-02 08:26:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:26:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:26:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:26:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:26:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:26:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=90.089104, avg_loss=0.682493, seen=132, correct=75, accuracy=0.568182
2025-10-02 08:26:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:26:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:26:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:26:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:26:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:26:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.328842, avg_loss=0.733221, seen=40, correct=15, accuracy=0.375000
2025-10-02 08:26:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:26:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:26:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:26:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.475000, curr=0.375000
2025-10-02 08:26:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 08:26:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 08:26:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 08:26:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 08:26:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 08:26:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=89.913605, avg_loss=0.681164, seen=132, correct=76, accuracy=0.575758
2025-10-02 08:26:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:26:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:26:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:26:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:26:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.106894, avg_loss=0.727672, seen=40, correct=14, accuracy=0.350000
2025-10-02 08:26:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:26:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:26:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:26:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.475000, curr=0.350000
2025-10-02 08:26:41 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:26:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:26:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:26:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:26:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2128MB
2025-10-02 08:26:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:26:42 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #49', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:26:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:26:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:26:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:42 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:26:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:26:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:26:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=82.421997, avg_loss=0.749291, seen=110, correct=59, accuracy=0.536364
2025-10-02 08:26:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:26:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:26:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-02 08:26:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:26:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.651703, avg_loss=0.716293, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:26:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:26:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:26:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2112MB
2025-10-02 08:26:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 08:26:50 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_019.ckpt
2025-10-02 08:26:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:26:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-02 08:26:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:26:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:26:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:26:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:26:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:26:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:26:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:26:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:27:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:27:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:27:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:27:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=81.887642, avg_loss=0.744433, seen=110, correct=58, accuracy=0.527273
2025-10-02 08:27:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:27:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:27:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:27:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:27:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:27:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.542187, avg_loss=0.713555, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:27:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:27:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:27:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:27:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.575000
2025-10-02 08:27:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:27:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:27:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:27:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:27:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:27:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:27:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=79.594048, avg_loss=0.723582, seen=110, correct=57, accuracy=0.518182
2025-10-02 08:27:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:27:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:27:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:27:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:27:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:27:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:27:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.868385, avg_loss=0.721710, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:27:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:27:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:27:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:27:23 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 08:27:23 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_019.ckpt
2025-10-02 08:27:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:27:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:27:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:27:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:27:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:27:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:27:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=79.971550, avg_loss=0.727014, seen=110, correct=52, accuracy=0.472727
2025-10-02 08:27:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:27:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:27:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:27:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:27:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:27:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:27:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.622337, avg_loss=0.740558, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:27:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:27:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:27:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.475000
2025-10-02 08:27:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:27:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:27:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:27:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:27:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:27:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=80.072205, avg_loss=0.727929, seen=110, correct=54, accuracy=0.490909
2025-10-02 08:27:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:27:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:27:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:27:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:27:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:27:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:27:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.917088, avg_loss=0.747927, seen=40, correct=16, accuracy=0.400000
2025-10-02 08:27:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:27:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:27:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:27:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:27:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.400000
2025-10-02 08:28:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:28:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:28:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:28:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:28:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:28:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=78.618973, avg_loss=0.714718, seen=110, correct=58, accuracy=0.527273
2025-10-02 08:28:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:28:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:28:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:28:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:28:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:28:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.414093, avg_loss=0.710352, seen=40, correct=25, accuracy=0.625000
2025-10-02 08:28:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:28:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:28:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:28:11 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 08:28:12 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_019.ckpt
2025-10-02 08:28:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:28:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:28:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:28:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:28:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:28:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:28:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=78.855385, avg_loss=0.716867, seen=110, correct=58, accuracy=0.527273
2025-10-02 08:28:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:28:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:28:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:28:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:28:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:28:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:28:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.614651, avg_loss=0.715366, seen=40, correct=26, accuracy=0.650000
2025-10-02 08:28:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:28:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:28:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:28:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-02 08:28:28 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_019.ckpt
2025-10-02 08:28:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:28:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:28:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:28:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:28:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:28:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:28:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=78.569557, avg_loss=0.714269, seen=110, correct=60, accuracy=0.545455
2025-10-02 08:28:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:28:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:28:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:28:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:28:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:28:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:28:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.784115, avg_loss=0.719603, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:28:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:28:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:28:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:28:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.650000, curr=0.550000
2025-10-02 08:28:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:28:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:28:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:28:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:28:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:28:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:28:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=78.006683, avg_loss=0.709152, seen=110, correct=62, accuracy=0.563636
2025-10-02 08:28:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:28:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:28:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:28:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:28:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:28:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:28:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.852709, avg_loss=0.721318, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:28:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:28:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:29:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:29:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.650000, curr=0.575000
2025-10-02 08:29:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:29:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:29:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:29:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:29:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:29:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:29:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=78.186203, avg_loss=0.710784, seen=110, correct=59, accuracy=0.536364
2025-10-02 08:29:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:29:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:29:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:29:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:29:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.640160, avg_loss=0.716004, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:29:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:29:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:29:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:29:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.650000, curr=0.575000
2025-10-02 08:29:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:29:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:29:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:29:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:29:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:29:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:29:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=78.214073, avg_loss=0.711037, seen=110, correct=54, accuracy=0.490909
2025-10-02 08:29:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:29:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:29:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:29:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:29:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:29:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.820610, avg_loss=0.720515, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:29:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:29:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:29:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:29:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.650000, curr=0.475000
2025-10-02 08:29:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:29:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:29:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:29:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:29:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:29:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:29:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=77.765190, avg_loss=0.706956, seen=110, correct=61, accuracy=0.554545
2025-10-02 08:29:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:29:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:29:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:29:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:29:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:29:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.635149, avg_loss=0.715879, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:29:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:29:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:29:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:29:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:29:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.650000, curr=0.500000
2025-10-02 08:30:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 08:30:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 08:30:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 08:30:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:30:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:30:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=77.799683, avg_loss=0.707270, seen=110, correct=63, accuracy=0.572727
2025-10-02 08:30:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:30:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:30:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:30:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:30:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:30:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:30:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.390598, avg_loss=0.709765, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:30:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:30:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:30:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:30:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.650000, curr=0.600000
2025-10-02 08:30:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 08:30:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 08:30:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 08:30:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:30:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:30:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:30:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=78.786522, avg_loss=0.716241, seen=110, correct=52, accuracy=0.472727
2025-10-02 08:30:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:30:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:30:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:30:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:30:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:30:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.036465, avg_loss=0.700912, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:30:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:30:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:30:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:30:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.650000, curr=0.525000
2025-10-02 08:30:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 08:30:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 08:30:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 08:30:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:30:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:30:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:30:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=79.098579, avg_loss=0.719078, seen=110, correct=54, accuracy=0.490909
2025-10-02 08:30:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:30:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:30:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:30:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:30:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:30:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:30:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.878599, avg_loss=0.696965, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:30:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:30:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:30:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:30:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.650000, curr=0.575000
2025-10-02 08:30:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 08:30:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 08:30:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 08:30:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:30:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:30:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:30:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=77.627144, avg_loss=0.705701, seen=110, correct=57, accuracy=0.518182
2025-10-02 08:30:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:30:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:30:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:30:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:30:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:30:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:30:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.168201, avg_loss=0.704205, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:30:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:30:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:30:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:30:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:30:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.650000, curr=0.550000
2025-10-02 08:31:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 08:31:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 08:31:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 08:31:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:31:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:31:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=77.480217, avg_loss=0.704366, seen=110, correct=62, accuracy=0.563636
2025-10-02 08:31:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:31:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:31:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:31:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:31:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:31:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.508919, avg_loss=0.712723, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:31:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:31:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:31:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.650000, curr=0.525000
2025-10-02 08:31:13 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:31:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:31:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:31:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:31:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2145MB
2025-10-02 08:31:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:31:14 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:31:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:31:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:31:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=790, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:31:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:31:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:31:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=790, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:31:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.550327, avg_loss=0.729522, seen=83, correct=39, accuracy=0.469880
2025-10-02 08:31:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:31:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:31:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2128MB
2025-10-02 08:31:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:31:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=790, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:31:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.645502, avg_loss=0.716138, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:31:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:31:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2156MB allocated=2128MB
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_051.ckpt
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-02 08:31:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=790, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:31:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:31:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:31:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:31:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:31:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:31:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:31:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.564415, avg_loss=0.681499, seen=83, correct=50, accuracy=0.602410
2025-10-02 08:31:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:31:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:31:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:31:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:31:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:31:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.022066, avg_loss=0.725552, seen=40, correct=16, accuracy=0.400000
2025-10-02 08:31:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:31:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:31:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:31:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.400000
2025-10-02 08:31:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:31:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:31:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:31:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:31:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:31:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.370094, avg_loss=0.679158, seen=83, correct=48, accuracy=0.578313
2025-10-02 08:31:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:31:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:31:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:31:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:31:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:31:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:31:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.166016, avg_loss=0.754150, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:31:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:31:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:31:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:31:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:31:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.475000
2025-10-02 08:32:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:32:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:32:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:32:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:32:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:32:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.394936, avg_loss=0.679457, seen=83, correct=50, accuracy=0.602410
2025-10-02 08:32:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:32:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:32:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:32:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:32:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:32:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:32:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.646555, avg_loss=0.716164, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:32:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:32:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:32:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.425000
2025-10-02 08:32:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:32:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:32:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:32:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:32:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:32:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:32:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.897324, avg_loss=0.673462, seen=83, correct=51, accuracy=0.614458
2025-10-02 08:32:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:32:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:32:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:32:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:32:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:32:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:32:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.676235, avg_loss=0.716906, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:32:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:32:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:32:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:32:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.425000
2025-10-02 08:32:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:32:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:32:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:32:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:32:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:32:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:32:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.457947, avg_loss=0.680216, seen=83, correct=49, accuracy=0.590361
2025-10-02 08:32:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:32:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:32:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:32:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:32:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:32:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:32:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.404291, avg_loss=0.710107, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:32:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:32:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:32:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:32:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.450000
2025-10-02 08:32:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:32:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:32:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:32:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:32:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:32:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.441730, avg_loss=0.680021, seen=83, correct=50, accuracy=0.602410
2025-10-02 08:32:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:32:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:32:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:32:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:32:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:32:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:32:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.818047, avg_loss=0.695451, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:32:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:32:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:32:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:32:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:32:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.475000
2025-10-02 08:33:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:33:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:33:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:33:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:33:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:33:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.784698, avg_loss=0.672105, seen=83, correct=49, accuracy=0.590361
2025-10-02 08:33:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:33:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:33:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:33:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:33:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.850439, avg_loss=0.696261, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:33:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:33:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:33:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.525000
2025-10-02 08:33:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:33:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:33:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:33:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:33:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:33:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.171204, avg_loss=0.676761, seen=83, correct=48, accuracy=0.578313
2025-10-02 08:33:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:33:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:33:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:33:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.802490, avg_loss=0.695062, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:33:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:33:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:33:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.500000
2025-10-02 08:33:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:33:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:33:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:33:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:33:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:33:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.801556, avg_loss=0.672308, seen=83, correct=51, accuracy=0.614458
2025-10-02 08:33:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:33:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:33:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:33:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:33:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.842075, avg_loss=0.696052, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:33:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:33:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:33:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.525000
2025-10-02 08:33:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:33:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:33:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:33:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 08:33:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 08:33:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.993423, avg_loss=0.686668, seen=83, correct=44, accuracy=0.530120
2025-10-02 08:33:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:33:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:33:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:33:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:33:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.417334, avg_loss=0.685433, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:33:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:33:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:33:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.600000, curr=0.500000
2025-10-02 08:33:53 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:33:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:33:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-02 08:33:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:33:54 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #51', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:33:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:33:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:33:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=515, num_train_batch_last_epoch=285, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:54 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:33:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:33:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:33:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:33:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=515, num_train_batch_last_epoch=285, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:33:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:33:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=39.379314, avg_loss=0.729247, seen=54, correct=30, accuracy=0.555556
2025-10-02 08:33:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:33:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:33:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2176MB allocated=2145MB
2025-10-02 08:33:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:33:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:33:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=515, num_train_batch_last_epoch=285, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:34:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:34:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.903999, avg_loss=0.672600, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:34:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:34:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2176MB allocated=2145MB
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_036.ckpt
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-02 08:34:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=515, num_train_batch_last_epoch=285, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:34:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:34:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:34:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:34:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:34:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:34:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:34:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:34:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.366547, avg_loss=0.691973, seen=54, correct=29, accuracy=0.537037
2025-10-02 08:34:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:34:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:34:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:34:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:34:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:34:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.511848, avg_loss=0.687796, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:34:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:34:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:34:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:34:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.600000
2025-10-02 08:34:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:34:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:34:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:34:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:34:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:34:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:34:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=38.866383, avg_loss=0.719748, seen=54, correct=28, accuracy=0.518519
2025-10-02 08:34:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:34:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:34:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:34:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:34:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:34:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.865944, avg_loss=0.746649, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:34:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:34:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:34:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:34:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.475000
2025-10-02 08:34:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:34:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:34:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:34:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:34:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:34:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:34:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.367218, avg_loss=0.691986, seen=54, correct=29, accuracy=0.537037
2025-10-02 08:34:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:34:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:34:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:34:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:34:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:34:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.268763, avg_loss=0.681719, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:34:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:34:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:34:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:34:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.575000
2025-10-02 08:34:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:34:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:34:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:34:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:34:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:34:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:34:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.950283, avg_loss=0.684265, seen=54, correct=31, accuracy=0.574074
2025-10-02 08:34:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:34:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:34:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:34:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:34:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:34:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:35:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:35:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.746414, avg_loss=0.668660, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:35:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:35:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:35:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:35:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.600000
2025-10-02 08:35:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:35:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:35:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:35:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:35:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:35:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.864937, avg_loss=0.682684, seen=54, correct=30, accuracy=0.555556
2025-10-02 08:35:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:35:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:35:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:35:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:35:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:35:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.647573, avg_loss=0.666189, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:35:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:35:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:35:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:35:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.600000
2025-10-02 08:35:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:35:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:35:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:35:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:35:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:35:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:35:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.513901, avg_loss=0.676183, seen=54, correct=30, accuracy=0.555556
2025-10-02 08:35:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:35:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:35:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:35:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:35:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:35:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:35:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.733377, avg_loss=0.668334, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:35:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:35:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:35:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:35:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.600000
2025-10-02 08:35:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:35:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:35:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:35:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:35:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:35:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:35:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.161430, avg_loss=0.688175, seen=54, correct=25, accuracy=0.462963
2025-10-02 08:35:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:35:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:35:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:35:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:35:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:35:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:35:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.949327, avg_loss=0.698733, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:35:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:35:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:35:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.575000
2025-10-02 08:35:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:35:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:35:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:35:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:35:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:35:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:36:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:36:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=38.483654, avg_loss=0.712660, seen=54, correct=29, accuracy=0.537037
2025-10-02 08:36:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:36:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:36:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:36:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:36:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:36:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:36:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.796320, avg_loss=0.744908, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:36:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:36:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:36:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:36:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.450000
2025-10-02 08:36:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:36:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:36:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:36:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:36:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:36:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:36:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.080650, avg_loss=0.686679, seen=54, correct=28, accuracy=0.518519
2025-10-02 08:36:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:36:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:36:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:36:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:36:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:36:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.528992, avg_loss=0.688225, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:36:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:36:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:36:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:36:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.600000
2025-10-02 08:36:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:36:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:36:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:36:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:36:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:36:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:36:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.883133, avg_loss=0.683021, seen=54, correct=27, accuracy=0.500000
2025-10-02 08:36:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:36:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:36:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:36:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:36:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.401529, avg_loss=0.660038, seen=40, correct=25, accuracy=0.625000
2025-10-02 08:36:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:36:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:36:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 08:36:32 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_036.ckpt
2025-10-02 08:36:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:36:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:36:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:36:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:36:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:36:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.525562, avg_loss=0.676399, seen=54, correct=31, accuracy=0.574074
2025-10-02 08:36:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:36:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:36:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:36:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:36:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:36:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:36:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.338303, avg_loss=0.658458, seen=40, correct=25, accuracy=0.625000
2025-10-02 08:36:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:36:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:36:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:36:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.625000
2025-10-02 08:36:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 08:36:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 08:36:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 08:36:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:36:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:36:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:36:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.387962, avg_loss=0.673851, seen=54, correct=29, accuracy=0.537037
2025-10-02 08:36:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:36:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:36:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:37:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:37:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:37:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:37:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.450041, avg_loss=0.661251, seen=40, correct=25, accuracy=0.625000
2025-10-02 08:37:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:37:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:37:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.625000
2025-10-02 08:37:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 08:37:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 08:37:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 08:37:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:37:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:37:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.861816, avg_loss=0.682626, seen=54, correct=31, accuracy=0.574074
2025-10-02 08:37:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:37:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:37:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:37:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:37:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:37:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.305834, avg_loss=0.682646, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:37:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:37:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:37:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:37:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.550000
2025-10-02 08:37:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 08:37:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 08:37:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 08:37:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:37:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:37:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:37:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=38.197414, avg_loss=0.707360, seen=54, correct=29, accuracy=0.537037
2025-10-02 08:37:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:37:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:37:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:37:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:37:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:37:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:37:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.007812, avg_loss=0.725195, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:37:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:37:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:37:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:37:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.475000
2025-10-02 08:37:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 08:37:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 08:37:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 08:37:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:37:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:37:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:37:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.572948, avg_loss=0.695795, seen=54, correct=25, accuracy=0.462963
2025-10-02 08:37:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:37:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:37:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:37:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:37:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:37:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:37:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.574734, avg_loss=0.714368, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:37:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:37:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:37:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.500000
2025-10-02 08:37:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 08:37:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 08:37:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 08:37:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:37:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:37:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:37:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.676109, avg_loss=0.679187, seen=54, correct=31, accuracy=0.574074
2025-10-02 08:37:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:37:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:37:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:38:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:38:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:38:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:38:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:38:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.004026, avg_loss=0.675101, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:38:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:38:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:38:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:38:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.600000
2025-10-02 08:38:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 08:38:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 08:38:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 08:38:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:38:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:38:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:38:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.691933, avg_loss=0.679480, seen=54, correct=30, accuracy=0.555556
2025-10-02 08:38:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:38:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:38:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:38:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:38:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:38:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:38:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.299725, avg_loss=0.657493, seen=40, correct=28, accuracy=0.700000
2025-10-02 08:38:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:38:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:38:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:38:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-02 08:38:17 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_036.ckpt
2025-10-02 08:38:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 08:38:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 08:38:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 08:38:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:38:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:38:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:38:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.772495, avg_loss=0.680972, seen=54, correct=30, accuracy=0.555556
2025-10-02 08:38:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:38:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:38:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:38:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:38:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:38:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.217106, avg_loss=0.655428, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:38:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:38:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:38:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:38:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.700000, curr=0.600000
2025-10-02 08:38:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 08:38:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 08:38:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 08:38:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:38:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:38:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:38:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.990749, avg_loss=0.685014, seen=54, correct=29, accuracy=0.537037
2025-10-02 08:38:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:38:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:38:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:38:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:38:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:38:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:38:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.787613, avg_loss=0.669690, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:38:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:38:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:38:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:38:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.700000, curr=0.575000
2025-10-02 08:38:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 08:38:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 08:38:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 08:38:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:38:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:38:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:38:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:38:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.510231, avg_loss=0.694634, seen=54, correct=27, accuracy=0.500000
2025-10-02 08:38:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:38:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:39:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:39:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:39:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:39:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.861788, avg_loss=0.696545, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:39:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:39:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:39:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:39:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.700000, curr=0.525000
2025-10-02 08:39:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 08:39:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 08:39:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 08:39:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:39:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:39:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:39:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=38.659500, avg_loss=0.715917, seen=54, correct=29, accuracy=0.537037
2025-10-02 08:39:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:39:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:39:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:39:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:39:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:39:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.026653, avg_loss=0.725666, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:39:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:39:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:39:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.700000, curr=0.475000
2025-10-02 08:39:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=220
2025-10-02 08:39:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=220
2025-10-02 08:39:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=220, splits=['val', 'test']
2025-10-02 08:39:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:39:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:39:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:39:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.184628, avg_loss=0.688604, seen=54, correct=26, accuracy=0.481481
2025-10-02 08:39:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:39:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:39:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:39:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:39:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:39:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.702591, avg_loss=0.692565, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:39:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:39:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:39:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.700000, curr=0.550000
2025-10-02 08:39:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=230
2025-10-02 08:39:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=230
2025-10-02 08:39:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=230, splits=['val', 'test']
2025-10-02 08:39:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:39:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:39:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:39:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.878242, avg_loss=0.682930, seen=54, correct=28, accuracy=0.518519
2025-10-02 08:39:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:39:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:39:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:39:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:39:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:39:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.316055, avg_loss=0.657901, seen=40, correct=27, accuracy=0.675000
2025-10-02 08:39:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:39:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:39:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:39:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.700000, curr=0.675000
2025-10-02 08:39:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=240
2025-10-02 08:39:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=240
2025-10-02 08:39:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=240, splits=['val', 'test']
2025-10-02 08:39:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:39:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:39:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.742874, avg_loss=0.680424, seen=54, correct=28, accuracy=0.518519
2025-10-02 08:39:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:39:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:39:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:40:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:40:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.199577, avg_loss=0.654989, seen=40, correct=27, accuracy=0.675000
2025-10-02 08:40:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:40:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:40:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.700000, curr=0.675000
2025-10-02 08:40:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=250
2025-10-02 08:40:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=250
2025-10-02 08:40:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=250, splits=['val', 'test']
2025-10-02 08:40:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:40:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:40:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.873234, avg_loss=0.682838, seen=54, correct=28, accuracy=0.518519
2025-10-02 08:40:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:40:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:40:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:40:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:40:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.418829, avg_loss=0.660471, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:40:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:40:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:40:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.700000, curr=0.600000
2025-10-02 08:40:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=260
2025-10-02 08:40:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=260
2025-10-02 08:40:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=260, splits=['val', 'test']
2025-10-02 08:40:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:40:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=37.340160, avg_loss=0.691484, seen=54, correct=27, accuracy=0.500000
2025-10-02 08:40:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:40:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:40:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:40:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.610649, avg_loss=0.690266, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:40:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:40:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:40:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.700000, curr=0.550000
2025-10-02 08:40:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=270
2025-10-02 08:40:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=270
2025-10-02 08:40:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=270, splits=['val', 'test']
2025-10-02 08:40:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 08:40:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 08:40:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=38.723099, avg_loss=0.717094, seen=54, correct=28, accuracy=0.518519
2025-10-02 08:40:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:40:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:40:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:40:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:40:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.351646, avg_loss=0.733791, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:40:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:40:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.700000, curr=0.500000
2025-10-02 08:40:48 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:40:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:40:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:40:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2230MB allocated=2179MB
2025-10-02 08:40:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:40:49 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #36', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:40:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:40:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:40:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:49 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:40:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:40:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:40:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:40:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=98.504547, avg_loss=0.724298, seen=136, correct=69, accuracy=0.507353
2025-10-02 08:40:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:40:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2196MB allocated=2162MB
2025-10-02 08:40:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:40:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:40:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.830856, avg_loss=0.695771, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:40:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:40:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:40:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2196MB allocated=2162MB
2025-10-02 08:40:58 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 08:40:58 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_016.ckpt
2025-10-02 08:40:58 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:40:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-02 08:40:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:40:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:40:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:40:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:40:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:41:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:41:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:41:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:41:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:41:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:41:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:41:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=97.646820, avg_loss=0.717991, seen=136, correct=75, accuracy=0.551471
2025-10-02 08:41:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:41:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:41:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:41:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:41:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:41:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.642992, avg_loss=0.691075, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:41:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:41:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:41:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:41:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 08:41:15 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_016.ckpt
2025-10-02 08:41:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:41:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:41:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:41:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:41:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:41:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:41:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=97.968994, avg_loss=0.720360, seen=136, correct=60, accuracy=0.441176
2025-10-02 08:41:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:41:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:41:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:41:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:41:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:41:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.679871, avg_loss=0.716997, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:41:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:41:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:41:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.475000
2025-10-02 08:41:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:41:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:41:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:41:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:41:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:41:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=100.802872, avg_loss=0.741198, seen=136, correct=63, accuracy=0.463235
2025-10-02 08:41:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:41:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:41:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:41:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:41:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:41:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.016823, avg_loss=0.750421, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:41:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:41:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:41:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:41:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.500000
2025-10-02 08:41:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:41:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:41:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:41:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:41:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:41:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:42:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:42:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=96.087776, avg_loss=0.706528, seen=136, correct=66, accuracy=0.485294
2025-10-02 08:42:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:42:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:42:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:42:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:42:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:42:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:42:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.867321, avg_loss=0.696683, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:42:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:42:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:42:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.575000, curr=0.525000
2025-10-02 08:42:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:42:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:42:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:42:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:42:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:42:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:42:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=96.086761, avg_loss=0.706520, seen=136, correct=67, accuracy=0.492647
2025-10-02 08:42:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:42:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:42:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:42:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:42:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:42:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.730242, avg_loss=0.693256, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:42:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:42:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:42:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.575000, curr=0.500000
2025-10-02 08:42:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:42:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:42:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:42:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:42:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:42:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:42:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=96.004799, avg_loss=0.705918, seen=136, correct=71, accuracy=0.522059
2025-10-02 08:42:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:42:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:42:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:42:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:42:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:42:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.771496, avg_loss=0.694287, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:42:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:42:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:42:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.575000, curr=0.500000
2025-10-02 08:42:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:42:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:42:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:42:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:42:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:42:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:42:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=95.857590, avg_loss=0.704835, seen=136, correct=74, accuracy=0.544118
2025-10-02 08:42:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:42:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:42:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:42:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:42:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:42:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.817623, avg_loss=0.695441, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:42:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:42:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:42:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:42:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.575000, curr=0.475000
2025-10-02 08:43:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:43:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:43:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:43:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:43:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:43:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:43:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=95.723045, avg_loss=0.703846, seen=136, correct=73, accuracy=0.536765
2025-10-02 08:43:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:43:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:43:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:43:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:43:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:43:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:43:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.381441, avg_loss=0.684536, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:43:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:43:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:43:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:43:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.575000, curr=0.525000
2025-10-02 08:43:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:43:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:43:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:43:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:43:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:43:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:43:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=95.462288, avg_loss=0.701929, seen=136, correct=78, accuracy=0.573529
2025-10-02 08:43:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:43:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:43:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:43:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:43:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:43:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:43:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.402248, avg_loss=0.685056, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:43:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:43:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:43:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:43:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.575000, curr=0.475000
2025-10-02 08:43:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:43:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:43:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:43:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:43:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:43:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:43:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=95.125366, avg_loss=0.699451, seen=136, correct=69, accuracy=0.507353
2025-10-02 08:43:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:43:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:43:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:43:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:43:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:43:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:43:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.819374, avg_loss=0.695484, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:43:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:43:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:43:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:43:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.575000, curr=0.525000
2025-10-02 08:43:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:43:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:43:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:43:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 08:43:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:43:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:43:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=97.315933, avg_loss=0.715558, seen=136, correct=67, accuracy=0.492647
2025-10-02 08:43:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:43:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:43:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:44:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:44:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:44:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.428122, avg_loss=0.735703, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:44:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:44:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.575000, curr=0.475000
2025-10-02 08:44:03 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:44:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:44:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:44:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-02 08:44:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:44:04 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:44:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:44:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:44:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:04 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:44:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:44:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:44:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:44:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:44:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=99.870087, avg_loss=0.745299, seen=134, correct=61, accuracy=0.455224
2025-10-02 08:44:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:44:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2179MB
2025-10-02 08:44:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:44:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:44:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.400597, avg_loss=0.710015, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:44:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2179MB
2025-10-02 08:44:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 08:44:12 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_006.ckpt
2025-10-02 08:44:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:44:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-02 08:44:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:44:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:44:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:44:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:44:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:44:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:44:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:44:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:44:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=96.079254, avg_loss=0.717009, seen=134, correct=65, accuracy=0.485075
2025-10-02 08:44:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:44:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:44:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:44:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.603954, avg_loss=0.690099, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:44:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:44:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:44:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 08:44:30 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_006.ckpt
2025-10-02 08:44:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:44:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:44:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:44:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:44:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:44:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=93.961334, avg_loss=0.701204, seen=134, correct=68, accuracy=0.507463
2025-10-02 08:44:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:44:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:44:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:44:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:44:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.618368, avg_loss=0.690459, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:44:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:44:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:44:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.500000
2025-10-02 08:44:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:44:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:44:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:44:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:44:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:44:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:44:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=93.878906, avg_loss=0.700589, seen=134, correct=70, accuracy=0.522388
2025-10-02 08:44:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:44:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:44:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:44:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:45:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:45:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:45:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:45:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.858028, avg_loss=0.696451, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:45:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:45:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:45:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.450000
2025-10-02 08:45:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:45:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:45:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:45:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:45:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:45:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=93.890541, avg_loss=0.700676, seen=134, correct=66, accuracy=0.492537
2025-10-02 08:45:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:45:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:45:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:45:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:45:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:45:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:45:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.965170, avg_loss=0.699129, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:45:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:45:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:45:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:45:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.525000
2025-10-02 08:45:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:45:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:45:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:45:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:45:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:45:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:45:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=93.947235, avg_loss=0.701099, seen=134, correct=69, accuracy=0.514925
2025-10-02 08:45:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:45:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:45:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:45:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:45:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:45:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.753387, avg_loss=0.693835, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:45:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:45:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:45:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.425000
2025-10-02 08:45:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:45:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:45:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:45:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:45:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:45:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:45:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=94.138069, avg_loss=0.702523, seen=134, correct=67, accuracy=0.500000
2025-10-02 08:45:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:45:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:45:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:45:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:45:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:45:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:45:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.599628, avg_loss=0.689991, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:45:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:45:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:45:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:45:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:45:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.550000
2025-10-02 08:46:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:46:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:46:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:46:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:46:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:46:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:46:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=93.357193, avg_loss=0.696695, seen=134, correct=70, accuracy=0.522388
2025-10-02 08:46:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:46:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:46:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:46:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:46:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:46:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:46:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.360344, avg_loss=0.684009, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:46:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:46:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:46:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:46:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.550000, curr=0.550000
2025-10-02 08:46:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:46:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:46:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:46:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:46:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:46:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:46:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=92.860130, avg_loss=0.692986, seen=134, correct=76, accuracy=0.567164
2025-10-02 08:46:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:46:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:46:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:46:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:46:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:46:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:46:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.439407, avg_loss=0.685985, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:46:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:46:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:46:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:46:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.550000, curr=0.500000
2025-10-02 08:46:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:46:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:46:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:46:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:46:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:46:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:46:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=93.039520, avg_loss=0.694325, seen=134, correct=74, accuracy=0.552239
2025-10-02 08:46:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:46:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:46:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:46:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:46:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.349100, avg_loss=0.683728, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:46:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:46:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:46:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:46:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.550000, curr=0.500000
2025-10-02 08:46:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:46:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:46:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:46:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:46:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:46:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=92.694000, avg_loss=0.691746, seen=134, correct=74, accuracy=0.552239
2025-10-02 08:46:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:46:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:46:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:46:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:46:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:46:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:46:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.467232, avg_loss=0.686681, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:46:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:46:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:46:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:46:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:46:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.550000, curr=0.500000
2025-10-02 08:47:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:47:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:47:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:47:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 08:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 08:47:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=92.801521, avg_loss=0.692549, seen=134, correct=70, accuracy=0.522388
2025-10-02 08:47:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:47:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:47:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:47:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:47:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.452446, avg_loss=0.686311, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:47:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:47:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:47:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:47:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.550000, curr=0.525000
2025-10-02 08:47:16 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:47:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:47:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:47:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:47:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2212MB
2025-10-02 08:47:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:47:17 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:47:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:47:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:47:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:18 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:47:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:47:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:47:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:47:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.376999, avg_loss=0.726885, seen=200, correct=106, accuracy=0.530000
2025-10-02 08:47:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:47:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:47:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2236MB allocated=2196MB
2025-10-02 08:47:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:47:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:47:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.430231, avg_loss=0.685756, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:47:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:47:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:47:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2236MB allocated=2196MB
2025-10-02 08:47:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 08:47:27 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_029.ckpt
2025-10-02 08:47:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:47:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-02 08:47:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:47:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:47:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:47:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:47:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:47:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:47:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:47:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:47:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.182327, avg_loss=0.725912, seen=200, correct=91, accuracy=0.455000
2025-10-02 08:47:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:47:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:47:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:47:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:47:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.154598, avg_loss=0.753865, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:47:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:47:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:47:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:47:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.475000
2025-10-02 08:47:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:47:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:47:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:47:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:47:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:47:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.166504, avg_loss=0.715833, seen=200, correct=99, accuracy=0.495000
2025-10-02 08:47:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:47:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:47:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:47:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:47:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.505726, avg_loss=0.712643, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:47:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:47:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:47:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:48:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:48:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.500000
2025-10-02 08:48:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:48:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:48:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:48:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:48:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:48:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:48:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.551453, avg_loss=0.712757, seen=200, correct=103, accuracy=0.515000
2025-10-02 08:48:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:48:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:48:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:48:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:48:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:48:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.673601, avg_loss=0.691840, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:48:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:48:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:48:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.575000, curr=0.500000
2025-10-02 08:48:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:48:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:48:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:48:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:48:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:48:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:48:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.270126, avg_loss=0.716351, seen=200, correct=101, accuracy=0.505000
2025-10-02 08:48:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:48:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:48:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:48:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:48:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:48:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.579290, avg_loss=0.689482, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:48:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:48:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:48:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.575000, curr=0.525000
2025-10-02 08:48:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:48:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:48:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:48:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:48:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:48:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.984985, avg_loss=0.709925, seen=200, correct=104, accuracy=0.520000
2025-10-02 08:48:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:48:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:48:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:48:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:48:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:48:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:48:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.196846, avg_loss=0.704921, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:48:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:48:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:48:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:48:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:48:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.575000, curr=0.450000
2025-10-02 08:49:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:49:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:49:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:49:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:49:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:49:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.103577, avg_loss=0.710518, seen=200, correct=101, accuracy=0.505000
2025-10-02 08:49:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:49:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:49:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:49:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:49:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.636683, avg_loss=0.715917, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:49:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:49:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:49:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.575000, curr=0.450000
2025-10-02 08:49:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:49:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:49:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:49:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:49:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:49:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:49:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.250793, avg_loss=0.716254, seen=200, correct=96, accuracy=0.480000
2025-10-02 08:49:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:49:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:49:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:49:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:49:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:49:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:49:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.621857, avg_loss=0.740546, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:49:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:49:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:49:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:49:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.575000, curr=0.525000
2025-10-02 08:49:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:49:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:49:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:49:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:49:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:49:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:49:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.733917, avg_loss=0.708670, seen=200, correct=103, accuracy=0.515000
2025-10-02 08:49:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:49:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:49:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:49:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:49:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:49:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.493147, avg_loss=0.712329, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:49:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:49:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:49:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.575000, curr=0.450000
2025-10-02 08:49:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:49:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:49:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:49:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:49:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:49:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:49:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:49:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.894241, avg_loss=0.704471, seen=200, correct=104, accuracy=0.520000
2025-10-02 08:49:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:49:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:50:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:50:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:50:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:50:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.748850, avg_loss=0.693721, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:50:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:50:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:50:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.575000, curr=0.450000
2025-10-02 08:50:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:50:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:50:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:50:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:50:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:50:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:50:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.048218, avg_loss=0.705241, seen=200, correct=102, accuracy=0.510000
2025-10-02 08:50:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:50:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:50:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:50:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:50:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:50:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:50:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.560907, avg_loss=0.714023, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:50:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:50:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:50:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:50:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.575000, curr=0.425000
2025-10-02 08:50:22 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:50:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:50:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:50:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:50:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2300MB allocated=2229MB
2025-10-02 08:50:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:50:23 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:50:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:50:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:50:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:50:24 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:50:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:50:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:50:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:50:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:50:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=148.812073, avg_loss=0.744060, seen=200, correct=99, accuracy=0.495000
2025-10-02 08:50:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:50:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:50:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2256MB allocated=2212MB
2025-10-02 08:50:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:50:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:50:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:50:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.069813, avg_loss=0.701745, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:50:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:50:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:50:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2256MB allocated=2212MB
2025-10-02 08:50:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 08:50:33 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_017.ckpt
2025-10-02 08:50:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:50:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-02 08:50:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:50:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:50:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:50:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:50:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:50:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:50:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:50:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:50:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:50:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:50:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.491821, avg_loss=0.727459, seen=200, correct=102, accuracy=0.510000
2025-10-02 08:50:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:50:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:50:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:50:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:50:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:50:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:50:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.241617, avg_loss=0.681040, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:50:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:50:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:50:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 08:50:51 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_017.ckpt
2025-10-02 08:50:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:50:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:50:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:50:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:50:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:50:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:51:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:51:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.617355, avg_loss=0.718087, seen=200, correct=92, accuracy=0.460000
2025-10-02 08:51:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:51:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:51:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:51:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:51:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:51:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.033588, avg_loss=0.675840, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:51:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:51:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:51:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:51:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.500000
2025-10-02 08:51:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:51:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:51:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:51:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:51:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:51:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:51:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.099472, avg_loss=0.715497, seen=200, correct=96, accuracy=0.480000
2025-10-02 08:51:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:51:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:51:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:51:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:51:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:51:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.904858, avg_loss=0.672621, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:51:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:51:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:51:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:51:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 08:51:25 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_017.ckpt
2025-10-02 08:51:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:51:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:51:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:51:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:51:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:51:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:51:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.631470, avg_loss=0.713157, seen=200, correct=94, accuracy=0.470000
2025-10-02 08:51:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:51:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:51:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:51:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:51:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:51:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.942228, avg_loss=0.673556, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:51:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:51:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:51:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:51:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.575000
2025-10-02 08:51:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:51:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:51:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:51:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:51:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:51:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.062408, avg_loss=0.705312, seen=200, correct=101, accuracy=0.505000
2025-10-02 08:51:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:51:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:51:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:51:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:51:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:52:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:52:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.056973, avg_loss=0.676424, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:52:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:52:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:52:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:52:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.550000
2025-10-02 08:52:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:52:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:52:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:52:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:52:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:52:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:52:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.917160, avg_loss=0.704586, seen=200, correct=100, accuracy=0.500000
2025-10-02 08:52:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:52:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:52:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:52:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:52:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:52:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.115612, avg_loss=0.677890, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:52:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:52:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:52:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:52:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.525000
2025-10-02 08:52:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:52:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:52:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:52:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:52:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:52:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:52:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.037704, avg_loss=0.700189, seen=200, correct=103, accuracy=0.515000
2025-10-02 08:52:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:52:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:52:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:52:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:52:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:52:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:52:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.001667, avg_loss=0.675042, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:52:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:52:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:52:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.500000
2025-10-02 08:52:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:52:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:52:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:52:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:52:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:52:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.614304, avg_loss=0.698072, seen=200, correct=94, accuracy=0.470000
2025-10-02 08:52:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:52:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:52:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:52:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:52:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:52:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:52:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.787477, avg_loss=0.669687, seen=40, correct=25, accuracy=0.625000
2025-10-02 08:52:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:52:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:52:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:52:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:52:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 08:52:53 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_017.ckpt
2025-10-02 08:53:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:53:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:53:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:53:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:53:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:53:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.364914, avg_loss=0.696825, seen=200, correct=93, accuracy=0.465000
2025-10-02 08:53:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:53:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:53:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:53:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:53:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:53:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:53:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.958406, avg_loss=0.673960, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:53:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:53:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:53:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.600000
2025-10-02 08:53:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:53:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:53:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:53:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:53:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:53:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:53:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.112183, avg_loss=0.705561, seen=200, correct=106, accuracy=0.530000
2025-10-02 08:53:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:53:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:53:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:53:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:53:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:53:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:53:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.352489, avg_loss=0.683812, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:53:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:53:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:53:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:53:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.500000
2025-10-02 08:53:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:53:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:53:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:53:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:53:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:53:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:53:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.074707, avg_loss=0.710374, seen=200, correct=108, accuracy=0.540000
2025-10-02 08:53:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:53:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:53:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:53:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:53:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:53:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:53:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.097538, avg_loss=0.702438, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:53:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:53:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:53:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:53:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.475000
2025-10-02 08:53:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 08:53:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 08:53:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 08:53:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:53:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:53:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:54:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:54:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.958359, avg_loss=0.694792, seen=200, correct=91, accuracy=0.455000
2025-10-02 08:54:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:54:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:54:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:54:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:54:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:54:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:54:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.135056, avg_loss=0.678376, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:54:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:54:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:54:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:54:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.525000
2025-10-02 08:54:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 08:54:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 08:54:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 08:54:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:54:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:54:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:54:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.316360, avg_loss=0.696582, seen=200, correct=95, accuracy=0.475000
2025-10-02 08:54:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:54:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:54:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:54:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:54:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:54:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:54:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.017200, avg_loss=0.675430, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:54:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:54:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:54:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:54:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.550000
2025-10-02 08:54:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 08:54:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 08:54:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 08:54:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:54:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:54:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:54:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.579346, avg_loss=0.687897, seen=200, correct=97, accuracy=0.485000
2025-10-02 08:54:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:54:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:54:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:54:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:54:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.074423, avg_loss=0.676861, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:54:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:54:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:54:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:54:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.600000
2025-10-02 08:54:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 08:54:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 08:54:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 08:54:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:54:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:54:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:54:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.464340, avg_loss=0.692322, seen=200, correct=95, accuracy=0.475000
2025-10-02 08:54:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:54:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:54:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:54:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:54:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:54:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:54:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.981226, avg_loss=0.674531, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:54:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:54:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:54:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:54:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:54:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.575000
2025-10-02 08:55:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 08:55:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 08:55:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 08:55:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:55:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.854065, avg_loss=0.694270, seen=200, correct=95, accuracy=0.475000
2025-10-02 08:55:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:55:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:55:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:55:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:55:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.969854, avg_loss=0.674246, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:55:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:55:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:55:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:55:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.625000, curr=0.500000
2025-10-02 08:55:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 08:55:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 08:55:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 08:55:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:55:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:55:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.059052, avg_loss=0.690295, seen=200, correct=96, accuracy=0.480000
2025-10-02 08:55:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:55:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:55:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:55:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:55:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:55:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.679226, avg_loss=0.666981, seen=40, correct=23, accuracy=0.575000
2025-10-02 08:55:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:55:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:55:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:55:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.625000, curr=0.575000
2025-10-02 08:55:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 08:55:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 08:55:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 08:55:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 08:55:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 08:55:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.180023, avg_loss=0.685900, seen=200, correct=111, accuracy=0.555000
2025-10-02 08:55:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:55:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:55:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:55:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:55:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.740650, avg_loss=0.668516, seen=40, correct=24, accuracy=0.600000
2025-10-02 08:55:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:55:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:55:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:55:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.625000, curr=0.600000
2025-10-02 08:55:50 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 08:55:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 08:55:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 08:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:55:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2318MB allocated=2246MB
2025-10-02 08:55:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 08:55:51 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {}}
2025-10-02 08:55:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 08:55:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 08:55:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:52 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 08:55:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:55:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 08:55:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:55:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:55:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=79.170242, avg_loss=0.719729, seen=110, correct=60, accuracy=0.545455
2025-10-02 08:55:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:55:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:55:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2256MB allocated=2229MB
2025-10-02 08:55:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:55:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.383682, avg_loss=0.809592, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:55:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:55:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2256MB allocated=2229MB
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_046.ckpt
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-02 08:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 08:55:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 08:56:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 08:56:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 08:56:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 08:56:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:56:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:56:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:56:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=78.655869, avg_loss=0.715053, seen=110, correct=60, accuracy=0.545455
2025-10-02 08:56:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:56:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:56:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:56:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:56:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.491440, avg_loss=0.812286, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:56:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:56:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:56:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.425000, curr=0.425000
2025-10-02 08:56:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 08:56:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 08:56:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 08:56:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:56:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:56:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:56:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=76.775520, avg_loss=0.697959, seen=110, correct=58, accuracy=0.527273
2025-10-02 08:56:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:56:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:56:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:56:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:56:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:56:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:56:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.164404, avg_loss=0.754110, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:56:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:56:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:56:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:56:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.425000, curr=0.425000
2025-10-02 08:56:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 08:56:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 08:56:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 08:56:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:56:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:56:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:56:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=76.158897, avg_loss=0.692354, seen=110, correct=58, accuracy=0.527273
2025-10-02 08:56:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:56:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:56:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:56:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:56:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:56:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.541365, avg_loss=0.738534, seen=40, correct=17, accuracy=0.425000
2025-10-02 08:56:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:56:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:56:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:56:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.425000, curr=0.425000
2025-10-02 08:56:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 08:56:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 08:56:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 08:56:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:56:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:56:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=76.032005, avg_loss=0.691200, seen=110, correct=58, accuracy=0.527273
2025-10-02 08:56:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:56:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:56:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:57:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:57:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:57:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:57:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:57:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.157543, avg_loss=0.728939, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:57:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:57:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:57:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:57:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 08:57:02 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_046.ckpt
2025-10-02 08:57:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 08:57:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 08:57:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 08:57:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:57:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:57:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:57:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.909767, avg_loss=0.690089, seen=110, correct=61, accuracy=0.554545
2025-10-02 08:57:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:57:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:57:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:57:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:57:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.687176, avg_loss=0.742179, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:57:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:57:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:57:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.475000
2025-10-02 08:57:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 08:57:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 08:57:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 08:57:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:57:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:57:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:57:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.732742, avg_loss=0.688479, seen=110, correct=59, accuracy=0.536364
2025-10-02 08:57:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:57:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:57:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:57:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:57:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:57:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.344086, avg_loss=0.733602, seen=40, correct=18, accuracy=0.450000
2025-10-02 08:57:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:57:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:57:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.500000, curr=0.450000
2025-10-02 08:57:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 08:57:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 08:57:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 08:57:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:57:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:57:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=76.218056, avg_loss=0.692891, seen=110, correct=58, accuracy=0.527273
2025-10-02 08:57:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:57:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:57:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:57:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:57:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:57:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:57:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.131454, avg_loss=0.703286, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:57:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:57:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:57:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:57:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 08:57:46 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_046.ckpt
2025-10-02 08:57:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 08:57:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 08:57:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 08:57:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:57:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:57:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:57:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=77.011757, avg_loss=0.700107, seen=110, correct=60, accuracy=0.545455
2025-10-02 08:57:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:57:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:57:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:57:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:57:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:57:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:57:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:57:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.142401, avg_loss=0.703560, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:57:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:57:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:58:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:58:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.550000
2025-10-02 08:58:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 08:58:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 08:58:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 08:58:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:58:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:58:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:58:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.798668, avg_loss=0.689079, seen=110, correct=59, accuracy=0.536364
2025-10-02 08:58:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:58:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:58:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:58:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:58:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:58:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:58:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.504427, avg_loss=0.712611, seen=40, correct=20, accuracy=0.500000
2025-10-02 08:58:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:58:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:58:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.500000
2025-10-02 08:58:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 08:58:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 08:58:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 08:58:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:58:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:58:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.181030, avg_loss=0.683464, seen=110, correct=62, accuracy=0.563636
2025-10-02 08:58:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:58:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:58:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:58:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:58:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:58:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.879911, avg_loss=0.721998, seen=40, correct=19, accuracy=0.475000
2025-10-02 08:58:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:58:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:58:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.475000
2025-10-02 08:58:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 08:58:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 08:58:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 08:58:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:58:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:58:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:58:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.175323, avg_loss=0.683412, seen=110, correct=59, accuracy=0.536364
2025-10-02 08:58:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:58:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:58:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:58:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:58:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:58:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:58:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.577072, avg_loss=0.714427, seen=40, correct=21, accuracy=0.525000
2025-10-02 08:58:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:58:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:58:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:58:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.525000
2025-10-02 08:58:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 08:58:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 08:58:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 08:58:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:58:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:59:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:59:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=76.561447, avg_loss=0.696013, seen=110, correct=57, accuracy=0.518182
2025-10-02 08:59:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:59:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:59:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:59:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:59:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:59:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:59:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.585918, avg_loss=0.689648, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:59:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:59:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:59:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:59:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.550000
2025-10-02 08:59:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 08:59:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 08:59:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 08:59:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:59:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:59:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:59:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=76.188995, avg_loss=0.692627, seen=110, correct=58, accuracy=0.527273
2025-10-02 08:59:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:59:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:59:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:59:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:59:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:59:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:59:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.171677, avg_loss=0.679292, seen=40, correct=22, accuracy=0.550000
2025-10-02 08:59:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:59:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:59:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.550000, curr=0.550000
2025-10-02 08:59:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 08:59:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 08:59:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 08:59:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:59:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:59:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.373871, avg_loss=0.685217, seen=110, correct=59, accuracy=0.536364
2025-10-02 08:59:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:59:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:59:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:59:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:59:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:59:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:59:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.971685, avg_loss=0.724292, seen=40, correct=16, accuracy=0.400000
2025-10-02 08:59:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:59:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:59:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:59:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.550000, curr=0.400000
2025-10-02 08:59:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 08:59:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 08:59:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 08:59:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 08:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:59:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 08:59:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.702026, avg_loss=0.688200, seen=110, correct=60, accuracy=0.545455
2025-10-02 08:59:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:59:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:59:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:59:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 08:59:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 08:59:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 08:59:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.467638, avg_loss=0.736691, seen=40, correct=15, accuracy=0.375000
2025-10-02 08:59:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 08:59:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 08:59:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 08:59:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 08:59:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.550000, curr=0.375000
2025-10-02 08:59:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 08:59:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 08:59:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 09:00:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 09:00:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 09:00:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.511719, avg_loss=0.686470, seen=110, correct=58, accuracy=0.527273
2025-10-02 09:00:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:00:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:00:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 09:00:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:00:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:00:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.976330, avg_loss=0.724408, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:00:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:00:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:00:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 09:00:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.550000, curr=0.425000
2025-10-02 09:00:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 09:00:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 09:00:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 09:00:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 09:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 09:00:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=74.861214, avg_loss=0.680556, seen=110, correct=62, accuracy=0.563636
2025-10-02 09:00:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:00:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:00:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 09:00:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:00:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:00:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.043816, avg_loss=0.701095, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:00:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:00:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:00:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 09:00:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.550000, curr=0.500000
2025-10-02 09:00:22 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:00:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:00:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:00:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:00:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2322MB allocated=2263MB
2025-10-02 09:00:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:00:23 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #46', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:00:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:00:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:00:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:23 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:00:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:00:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:00:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:00:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:00:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=116.550919, avg_loss=0.761771, seen=153, correct=75, accuracy=0.490196
2025-10-02 09:00:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:00:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:00:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2246MB
2025-10-02 09:00:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:00:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:00:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.982487, avg_loss=0.649562, seen=40, correct=29, accuracy=0.725000
2025-10-02 09:00:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:00:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2276MB allocated=2246MB
2025-10-02 09:00:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-02 09:00:33 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_021.ckpt
2025-10-02 09:00:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:00:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-02 09:00:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:00:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:00:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:00:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:00:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:00:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:00:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:00:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:00:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=108.700073, avg_loss=0.710458, seen=153, correct=83, accuracy=0.542484
2025-10-02 09:00:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:00:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:00:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:00:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:00:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:00:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.679325, avg_loss=0.691983, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:00:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:00:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:00:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:00:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.725000, curr=0.500000
2025-10-02 09:00:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:00:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:00:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:00:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:00:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:00:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:01:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:01:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=107.441460, avg_loss=0.702232, seen=153, correct=78, accuracy=0.509804
2025-10-02 09:01:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:01:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:01:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:01:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:01:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:01:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:01:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.964027, avg_loss=0.749101, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:01:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:01:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:01:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.725000, curr=0.425000
2025-10-02 09:01:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:01:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:01:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:01:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:01:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:01:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:01:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=106.852020, avg_loss=0.698379, seen=153, correct=83, accuracy=0.542484
2025-10-02 09:01:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:01:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:01:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:01:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:01:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:01:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.402321, avg_loss=0.735058, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:01:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:01:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:01:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:01:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.725000, curr=0.400000
2025-10-02 09:01:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:01:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:01:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:01:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:01:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:01:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:01:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=107.286865, avg_loss=0.701221, seen=153, correct=82, accuracy=0.535948
2025-10-02 09:01:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:01:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:01:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:01:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:01:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:01:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:01:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.583563, avg_loss=0.714589, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:01:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:01:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:01:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.725000, curr=0.425000
2025-10-02 09:01:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:01:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:01:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:01:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:01:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:01:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:01:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=107.521019, avg_loss=0.702752, seen=153, correct=84, accuracy=0.549020
2025-10-02 09:01:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:01:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:01:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:01:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:01:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:01:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:01:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.975767, avg_loss=0.699394, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:01:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:01:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:01:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:01:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:01:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.725000, curr=0.425000
2025-10-02 09:02:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:02:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:02:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:02:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:02:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:02:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:02:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=106.577896, avg_loss=0.696588, seen=153, correct=84, accuracy=0.549020
2025-10-02 09:02:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:02:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:02:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:02:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:02:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:02:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:02:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.929459, avg_loss=0.723236, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:02:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:02:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:02:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.725000, curr=0.400000
2025-10-02 09:02:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:02:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:02:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:02:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:02:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:02:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:02:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=107.464127, avg_loss=0.702380, seen=153, correct=81, accuracy=0.529412
2025-10-02 09:02:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:02:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:02:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:02:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:02:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:02:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:02:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.177284, avg_loss=0.704432, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:02:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:02:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:02:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:02:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.725000, curr=0.450000
2025-10-02 09:02:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:02:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:02:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:02:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:02:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:02:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:02:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=107.876648, avg_loss=0.705076, seen=153, correct=81, accuracy=0.529412
2025-10-02 09:02:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:02:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:02:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:02:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:02:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:02:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:02:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.181108, avg_loss=0.679528, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:02:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:02:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:02:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:02:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.725000, curr=0.500000
2025-10-02 09:02:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:02:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:02:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:02:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:02:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:02:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:02:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=106.141083, avg_loss=0.693733, seen=153, correct=80, accuracy=0.522876
2025-10-02 09:02:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:02:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:02:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:02:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:02:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:02:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:02:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.352097, avg_loss=0.733802, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:02:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:02:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:03:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:03:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.725000, curr=0.450000
2025-10-02 09:03:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:03:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:03:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:03:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 09:03:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:03:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 09:03:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=106.956772, avg_loss=0.699064, seen=153, correct=83, accuracy=0.542484
2025-10-02 09:03:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:03:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:03:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:03:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:03:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:03:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.673916, avg_loss=0.766848, seen=40, correct=14, accuracy=0.350000
2025-10-02 09:03:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:03:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:03:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.725000, curr=0.350000
2025-10-02 09:03:18 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:03:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:03:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:03:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:03:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2340MB allocated=2279MB
2025-10-02 09:03:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:03:19 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:03:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:03:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:03:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:03:20 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:03:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:03:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:03:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:03:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:03:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=110.972748, avg_loss=0.754917, seen=147, correct=70, accuracy=0.476190
2025-10-02 09:03:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:03:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:03:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2263MB
2025-10-02 09:03:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:03:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:03:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:03:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.935368, avg_loss=0.748384, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:03:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:03:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2296MB allocated=2263MB
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_047.ckpt
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-02 09:03:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:03:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:03:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:03:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:03:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:03:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:03:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:03:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:03:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=108.857407, avg_loss=0.740527, seen=147, correct=67, accuracy=0.455782
2025-10-02 09:03:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:03:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:03:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:03:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:03:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:03:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:03:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.113304, avg_loss=0.702833, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:03:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:03:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:03:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:03:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 09:03:46 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_047.ckpt
2025-10-02 09:03:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:03:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:03:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:03:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:03:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:03:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=110.269485, avg_loss=0.750133, seen=147, correct=70, accuracy=0.476190
2025-10-02 09:03:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:03:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:03:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:03:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:03:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:04:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:04:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.104090, avg_loss=0.727602, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:04:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:04:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:04:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.425000
2025-10-02 09:04:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:04:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:04:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:04:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:04:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:04:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:04:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=107.834618, avg_loss=0.733569, seen=147, correct=71, accuracy=0.482993
2025-10-02 09:04:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:04:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:04:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:04:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:04:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:04:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:04:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.595222, avg_loss=0.714881, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:04:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:04:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:04:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.450000
2025-10-02 09:04:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:04:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:04:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:04:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:04:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:04:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=107.783730, avg_loss=0.733223, seen=147, correct=68, accuracy=0.462585
2025-10-02 09:04:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:04:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:04:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:04:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:04:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:04:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.945644, avg_loss=0.723641, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:04:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:04:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:04:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.550000
2025-10-02 09:04:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:04:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:04:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:04:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:04:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:04:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=106.785667, avg_loss=0.726433, seen=147, correct=73, accuracy=0.496599
2025-10-02 09:04:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:04:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:04:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:04:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:04:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:04:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.048981, avg_loss=0.701225, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:04:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:04:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:04:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:04:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.550000
2025-10-02 09:05:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:05:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:05:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:05:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:05:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:05:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:05:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=106.623993, avg_loss=0.725333, seen=147, correct=67, accuracy=0.455782
2025-10-02 09:05:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:05:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:05:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:05:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:05:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.989195, avg_loss=0.699730, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:05:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:05:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:05:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.500000
2025-10-02 09:05:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:05:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:05:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:05:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:05:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:05:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=106.416313, avg_loss=0.723920, seen=147, correct=68, accuracy=0.462585
2025-10-02 09:05:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:05:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:05:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:05:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:05:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:05:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:05:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.138393, avg_loss=0.703460, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:05:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:05:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:05:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:05:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.550000, curr=0.525000
2025-10-02 09:05:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:05:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:05:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:05:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:05:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:05:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=106.234131, avg_loss=0.722681, seen=147, correct=67, accuracy=0.455782
2025-10-02 09:05:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:05:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:05:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:05:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:05:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:05:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:05:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.889957, avg_loss=0.697249, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:05:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:05:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:05:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:05:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.550000, curr=0.550000
2025-10-02 09:05:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:05:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:05:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:05:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:05:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:06:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=105.990036, avg_loss=0.721021, seen=147, correct=69, accuracy=0.469388
2025-10-02 09:06:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:06:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:06:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:06:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.651951, avg_loss=0.691299, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:06:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:06:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:06:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.550000, curr=0.525000
2025-10-02 09:06:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:06:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:06:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:06:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:06:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=105.991547, avg_loss=0.721031, seen=147, correct=68, accuracy=0.462585
2025-10-02 09:06:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:06:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:06:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:06:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:06:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.148846, avg_loss=0.703721, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:06:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:06:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:06:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.550000, curr=0.550000
2025-10-02 09:06:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:06:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:06:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:06:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 09:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 09:06:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=105.428535, avg_loss=0.717201, seen=147, correct=70, accuracy=0.476190
2025-10-02 09:06:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:06:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:06:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:06:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.649628, avg_loss=0.691241, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:06:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:06:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:06:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.550000, curr=0.550000
2025-10-02 09:06:40 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:06:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:06:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:06:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:06:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2352MB allocated=2296MB
2025-10-02 09:06:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:06:41 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #47', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:06:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:06:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:06:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:41 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:06:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:06:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:06:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:06:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=137.918365, avg_loss=0.733608, seen=188, correct=98, accuracy=0.521277
2025-10-02 09:06:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:06:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:06:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2316MB allocated=2279MB
2025-10-02 09:06:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:06:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.488033, avg_loss=0.812201, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:06:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:06:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:06:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2316MB allocated=2279MB
2025-10-02 09:06:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-02 09:06:50 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_009.ckpt
2025-10-02 09:06:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:06:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-02 09:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:06:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:06:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:06:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:06:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:06:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:06:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:06:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:07:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:07:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:07:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:07:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=135.461060, avg_loss=0.720538, seen=188, correct=94, accuracy=0.500000
2025-10-02 09:07:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:07:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:07:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:07:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:07:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:07:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.330482, avg_loss=0.783262, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:07:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:07:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:07:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:07:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-02 09:07:08 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_009.ckpt
2025-10-02 09:07:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:07:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:07:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:07:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:07:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:07:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=134.372086, avg_loss=0.714745, seen=188, correct=88, accuracy=0.468085
2025-10-02 09:07:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:07:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:07:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:07:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:07:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.228096, avg_loss=0.755702, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:07:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:07:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:07:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:07:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.450000, curr=0.425000
2025-10-02 09:07:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:07:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:07:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:07:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:07:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:07:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:07:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=135.103745, avg_loss=0.718637, seen=188, correct=84, accuracy=0.446809
2025-10-02 09:07:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:07:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:07:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:07:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:07:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:07:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:07:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.478117, avg_loss=0.736953, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:07:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:07:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:07:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.450000, curr=0.450000
2025-10-02 09:07:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:07:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:07:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:07:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:07:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:07:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:07:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=133.942932, avg_loss=0.712462, seen=188, correct=83, accuracy=0.441489
2025-10-02 09:07:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:07:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:07:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:07:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:08:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:08:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.803955, avg_loss=0.745099, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:08:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:08:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:08:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.450000, curr=0.425000
2025-10-02 09:08:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:08:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:08:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:08:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:08:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:08:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=133.668198, avg_loss=0.711001, seen=188, correct=86, accuracy=0.457447
2025-10-02 09:08:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:08:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:08:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:08:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:08:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.469265, avg_loss=0.736732, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:08:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:08:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:08:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:08:18 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 09:08:18 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_009.ckpt
2025-10-02 09:08:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:08:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:08:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:08:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:08:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:08:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:08:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=133.879364, avg_loss=0.712124, seen=188, correct=84, accuracy=0.446809
2025-10-02 09:08:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:08:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:08:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:08:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:08:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:08:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.308868, avg_loss=0.732722, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:08:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:08:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:08:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.425000
2025-10-02 09:08:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:08:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:08:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:08:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:08:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:08:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:08:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=132.531418, avg_loss=0.704954, seen=188, correct=91, accuracy=0.484043
2025-10-02 09:08:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:08:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:08:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:08:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:08:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:08:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.775448, avg_loss=0.744386, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:08:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:08:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:08:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:08:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:08:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.500000, curr=0.475000
2025-10-02 09:09:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:09:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:09:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:09:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:09:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:09:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:09:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=133.255341, avg_loss=0.708805, seen=188, correct=88, accuracy=0.468085
2025-10-02 09:09:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:09:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:09:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:09:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:09:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:09:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.564556, avg_loss=0.739114, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:09:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:09:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:09:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:09:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.500000, curr=0.500000
2025-10-02 09:09:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:09:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:09:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:09:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:09:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:09:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:09:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=132.289490, avg_loss=0.703667, seen=188, correct=83, accuracy=0.441489
2025-10-02 09:09:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:09:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:09:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:09:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:09:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.006557, avg_loss=0.725164, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:09:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:09:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:09:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:09:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.500000, curr=0.450000
2025-10-02 09:09:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:09:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:09:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:09:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:09:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:09:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=132.738831, avg_loss=0.706058, seen=188, correct=89, accuracy=0.473404
2025-10-02 09:09:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:09:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:09:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:09:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:09:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:09:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:09:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.658279, avg_loss=0.716457, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:09:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:09:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:09:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:09:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.500000, curr=0.425000
2025-10-02 09:09:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:09:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:09:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:09:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:09:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:09:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:09:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=132.738052, avg_loss=0.706053, seen=188, correct=90, accuracy=0.478723
2025-10-02 09:09:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:09:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:09:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:09:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:09:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:09:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.572300, avg_loss=0.714307, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:09:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:09:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:09:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:09:57 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.500000, curr=0.450000
2025-10-02 09:10:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:10:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:10:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:10:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:10:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:10:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:10:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.143265, avg_loss=0.697571, seen=188, correct=86, accuracy=0.457447
2025-10-02 09:10:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:10:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:10:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2376MB allocated=2313MB
2025-10-02 09:10:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:10:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:10:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:10:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.037106, avg_loss=0.725928, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:10:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:10:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:10:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.500000, curr=0.475000
2025-10-02 09:10:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 09:10:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 09:10:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 09:10:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:10:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:10:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=130.965149, avg_loss=0.696623, seen=188, correct=101, accuracy=0.537234
2025-10-02 09:10:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:10:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:10:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:10:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:10:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:10:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.278925, avg_loss=0.731973, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:10:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:10:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:10:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.500000, curr=0.450000
2025-10-02 09:10:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 09:10:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 09:10:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 09:10:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:10:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:10:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:10:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.685715, avg_loss=0.700456, seen=188, correct=88, accuracy=0.468085
2025-10-02 09:10:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:10:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:10:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:10:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:10:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.966015, avg_loss=0.724150, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:10:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:10:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:10:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:10:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.500000, curr=0.475000
2025-10-02 09:10:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 09:10:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 09:10:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 09:10:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:10:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:10:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:11:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.127029, avg_loss=0.697484, seen=188, correct=96, accuracy=0.510638
2025-10-02 09:11:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:11:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:11:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:11:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:11:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:11:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.750305, avg_loss=0.718758, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:11:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:11:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:11:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:11:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.500000, curr=0.450000
2025-10-02 09:11:05 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:11:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:11:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:11:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2374MB allocated=2313MB
2025-10-02 09:11:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:11:06 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:11:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:11:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:11:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:06 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:11:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:11:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:11:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=118.994926, avg_loss=0.743718, seen=160, correct=85, accuracy=0.531250
2025-10-02 09:11:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:11:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:11:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2336MB allocated=2296MB
2025-10-02 09:11:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:11:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.359970, avg_loss=0.708999, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:11:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:11:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:11:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2336MB allocated=2296MB
2025-10-02 09:11:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 09:11:16 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_014.ckpt
2025-10-02 09:11:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:11:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-02 09:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:11:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:11:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:11:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:11:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:11:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:11:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:11:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=118.755753, avg_loss=0.742223, seen=160, correct=83, accuracy=0.518750
2025-10-02 09:11:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:11:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:11:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:11:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:11:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.672161, avg_loss=0.716804, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:11:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:11:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:11:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:11:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.500000
2025-10-02 09:11:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:11:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:11:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:11:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:11:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:11:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=116.876511, avg_loss=0.730478, seen=160, correct=71, accuracy=0.443750
2025-10-02 09:11:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:11:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:11:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:11:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:11:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.483765, avg_loss=0.662094, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:11:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:11:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:11:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:11:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 09:11:50 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_014.ckpt
2025-10-02 09:12:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:12:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:12:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:12:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:12:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:12:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=133.858093, avg_loss=0.836613, seen=160, correct=74, accuracy=0.462500
2025-10-02 09:12:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:12:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:12:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:12:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:12:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:12:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:12:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.432453, avg_loss=0.710811, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:12:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:12:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:12:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:12:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.550000
2025-10-02 09:12:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:12:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:12:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:12:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:12:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:12:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=119.058884, avg_loss=0.744118, seen=160, correct=70, accuracy=0.437500
2025-10-02 09:12:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:12:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:12:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:12:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:12:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:12:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:12:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.696781, avg_loss=0.667420, seen=40, correct=25, accuracy=0.625000
2025-10-02 09:12:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:12:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:12:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:12:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 09:12:24 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_014.ckpt
2025-10-02 09:12:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:12:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:12:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:12:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:12:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:12:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=115.283112, avg_loss=0.720519, seen=160, correct=78, accuracy=0.487500
2025-10-02 09:12:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:12:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:12:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:12:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:12:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.020071, avg_loss=0.675502, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:12:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:12:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:12:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:12:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.500000
2025-10-02 09:12:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:12:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:12:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:12:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:12:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:12:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=114.603836, avg_loss=0.716274, seen=160, correct=84, accuracy=0.525000
2025-10-02 09:12:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:12:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:12:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:12:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:12:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:12:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.198980, avg_loss=0.679975, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:12:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:12:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:12:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:13:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:13:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.550000
2025-10-02 09:13:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:13:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:13:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:13:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:13:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:13:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=114.515984, avg_loss=0.715725, seen=160, correct=82, accuracy=0.512500
2025-10-02 09:13:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:13:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:13:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:13:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:13:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.514339, avg_loss=0.687858, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:13:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:13:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:13:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.575000
2025-10-02 09:13:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:13:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:13:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:13:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:13:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:13:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:13:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=113.963951, avg_loss=0.712275, seen=160, correct=80, accuracy=0.500000
2025-10-02 09:13:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:13:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:13:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:13:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:13:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.255360, avg_loss=0.681384, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:13:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:13:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:13:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.525000
2025-10-02 09:13:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:13:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:13:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:13:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:13:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:13:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=113.721069, avg_loss=0.710757, seen=160, correct=80, accuracy=0.500000
2025-10-02 09:13:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:13:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:13:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:13:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:13:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:13:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:13:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.265797, avg_loss=0.681645, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:13:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:13:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:13:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:13:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.575000
2025-10-02 09:14:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:14:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:14:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:14:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:14:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:14:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=113.135666, avg_loss=0.707098, seen=160, correct=82, accuracy=0.512500
2025-10-02 09:14:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:14:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:14:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:14:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:14:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:14:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.724726, avg_loss=0.693118, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:14:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:14:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:14:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.550000
2025-10-02 09:14:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:14:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:14:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:14:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:14:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:14:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=113.528244, avg_loss=0.709552, seen=160, correct=77, accuracy=0.481250
2025-10-02 09:14:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:14:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:14:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:14:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:14:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.042448, avg_loss=0.676061, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:14:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:14:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:14:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.575000
2025-10-02 09:14:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:14:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:14:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:14:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:14:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:14:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=116.025993, avg_loss=0.725162, seen=160, correct=72, accuracy=0.450000
2025-10-02 09:14:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:14:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:14:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:14:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:14:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:14:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.659401, avg_loss=0.666485, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:14:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:14:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:14:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:14:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.625000, curr=0.600000
2025-10-02 09:14:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 09:14:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 09:14:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 09:14:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:14:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:14:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:14:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=115.208633, avg_loss=0.720054, seen=160, correct=73, accuracy=0.456250
2025-10-02 09:14:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:14:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:14:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:14:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:14:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:14:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:15:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.709866, avg_loss=0.667747, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:15:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:15:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:15:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.625000, curr=0.600000
2025-10-02 09:15:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 09:15:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 09:15:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 09:15:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 09:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 09:15:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=112.956978, avg_loss=0.705981, seen=160, correct=76, accuracy=0.475000
2025-10-02 09:15:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:15:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:15:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:15:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:15:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.035753, avg_loss=0.675894, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:15:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:15:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:15:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.625000, curr=0.525000
2025-10-02 09:15:17 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:15:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:15:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:15:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2380MB allocated=2330MB
2025-10-02 09:15:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:15:18 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:15:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:15:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:15:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:19 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:15:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:15:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:15:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:15:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:15:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=114.509453, avg_loss=0.711239, seen=161, correct=85, accuracy=0.527950
2025-10-02 09:15:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:15:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2356MB allocated=2313MB
2025-10-02 09:15:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:15:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.828533, avg_loss=0.720713, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:15:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:15:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:15:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2356MB allocated=2313MB
2025-10-02 09:15:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 09:15:28 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_026.ckpt
2025-10-02 09:15:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:15:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-02 09:15:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:15:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:15:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:15:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:15:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:15:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:15:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:15:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:15:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=111.714096, avg_loss=0.693876, seen=161, correct=84, accuracy=0.521739
2025-10-02 09:15:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:15:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:15:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:15:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:15:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:15:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.542948, avg_loss=0.688574, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:15:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:15:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:15:45 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 09:15:45 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_026.ckpt
2025-10-02 09:15:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:15:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:15:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:15:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:15:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:15:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=110.408218, avg_loss=0.685765, seen=161, correct=89, accuracy=0.552795
2025-10-02 09:15:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:15:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:15:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:16:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:16:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:16:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:16:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:16:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.254677, avg_loss=0.681367, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:16:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:16:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:16:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:16:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.550000
2025-10-02 09:16:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:16:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:16:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:16:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:16:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:16:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=109.988808, avg_loss=0.683160, seen=161, correct=88, accuracy=0.546584
2025-10-02 09:16:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:16:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:16:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:16:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:16:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.624432, avg_loss=0.665611, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:16:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:16:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:16:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:16:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.575000
2025-10-02 09:16:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:16:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:16:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:16:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:16:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:16:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=110.770508, avg_loss=0.688016, seen=161, correct=89, accuracy=0.552795
2025-10-02 09:16:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:16:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:16:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:16:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:16:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.704987, avg_loss=0.667625, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:16:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:16:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:16:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.550000
2025-10-02 09:16:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:16:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:16:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:16:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:16:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:16:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=109.371590, avg_loss=0.679327, seen=161, correct=90, accuracy=0.559006
2025-10-02 09:16:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:16:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:16:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:16:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:16:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.768095, avg_loss=0.669202, seen=40, correct=25, accuracy=0.625000
2025-10-02 09:16:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:16:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:16:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:16:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:16:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 09:16:54 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_026.ckpt
2025-10-02 09:17:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:17:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:17:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:17:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:17:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:17:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:17:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=109.664429, avg_loss=0.681146, seen=161, correct=89, accuracy=0.552795
2025-10-02 09:17:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:17:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:17:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:17:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:17:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:17:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.989069, avg_loss=0.674727, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:17:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:17:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:17:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:17:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.575000
2025-10-02 09:17:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:17:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:17:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:17:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:17:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:17:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=109.072433, avg_loss=0.677469, seen=161, correct=86, accuracy=0.534161
2025-10-02 09:17:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:17:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:17:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:17:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:17:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.917871, avg_loss=0.672947, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:17:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:17:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:17:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:17:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.550000
2025-10-02 09:17:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:17:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:17:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:17:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:17:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:17:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:17:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=109.250290, avg_loss=0.678573, seen=161, correct=93, accuracy=0.577640
2025-10-02 09:17:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:17:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:17:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:17:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:17:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:17:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.772644, avg_loss=0.669316, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:17:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:17:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:17:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:17:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.550000
2025-10-02 09:17:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:17:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:17:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:17:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:17:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:17:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=109.025375, avg_loss=0.677176, seen=161, correct=93, accuracy=0.577640
2025-10-02 09:17:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:17:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:18:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:18:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:18:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:18:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.566845, avg_loss=0.664171, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:18:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:18:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:18:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.575000
2025-10-02 09:18:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:18:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:18:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:18:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:18:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:18:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:18:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=108.756821, avg_loss=0.675508, seen=161, correct=89, accuracy=0.552795
2025-10-02 09:18:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:18:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:18:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:18:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:18:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.614645, avg_loss=0.665366, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:18:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:18:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:18:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:18:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.600000
2025-10-02 09:18:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:18:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:18:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:18:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:18:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:18:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:18:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=108.641991, avg_loss=0.674795, seen=161, correct=89, accuracy=0.552795
2025-10-02 09:18:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:18:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:18:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:18:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:18:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:18:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:18:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.989506, avg_loss=0.674738, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:18:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:18:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:18:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.600000
2025-10-02 09:18:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:18:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:18:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:18:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:18:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=108.416870, avg_loss=0.673397, seen=161, correct=93, accuracy=0.577640
2025-10-02 09:18:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:18:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:18:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:18:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.716909, avg_loss=0.667923, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:18:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:18:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:18:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:18:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.600000
2025-10-02 09:19:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 09:19:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 09:19:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 09:19:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:19:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:19:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=108.051804, avg_loss=0.671129, seen=161, correct=94, accuracy=0.583851
2025-10-02 09:19:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:19:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:19:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:19:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:19:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.579483, avg_loss=0.664487, seen=40, correct=25, accuracy=0.625000
2025-10-02 09:19:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:19:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:19:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.625000, curr=0.625000
2025-10-02 09:19:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 09:19:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 09:19:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 09:19:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:19:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:19:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=108.105087, avg_loss=0.671460, seen=161, correct=87, accuracy=0.540373
2025-10-02 09:19:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:19:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:19:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:19:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:19:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.390297, avg_loss=0.659757, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:19:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:19:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:19:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:19:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.625000, curr=0.600000
2025-10-02 09:19:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 09:19:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 09:19:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 09:19:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 09:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 09:19:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=107.652390, avg_loss=0.668648, seen=161, correct=91, accuracy=0.565217
2025-10-02 09:19:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:19:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:19:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:19:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:19:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:19:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.510540, avg_loss=0.662764, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:19:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:19:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:19:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:19:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.625000, curr=0.575000
2025-10-02 09:19:46 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:19:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:19:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:19:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:19:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2400MB allocated=2347MB
2025-10-02 09:19:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:19:47 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:19:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:19:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:19:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:47 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:19:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:19:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:19:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=106.410828, avg_loss=0.788228, seen=135, correct=62, accuracy=0.459259
2025-10-02 09:19:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:19:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2356MB allocated=2330MB
2025-10-02 09:19:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:19:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:19:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.487499, avg_loss=0.787187, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:19:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:19:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:19:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2356MB allocated=2330MB
2025-10-02 09:19:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.400000
2025-10-02 09:19:56 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_018.ckpt
2025-10-02 09:19:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:19:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-02 09:19:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:19:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:19:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:19:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:19:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:20:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:20:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:20:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:20:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:20:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:20:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:20:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=100.297150, avg_loss=0.742942, seen=135, correct=60, accuracy=0.444444
2025-10-02 09:20:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:20:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:20:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:20:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:20:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:20:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:20:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.289951, avg_loss=0.707249, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:20:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:20:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:20:13 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 09:20:13 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_018.ckpt
2025-10-02 09:20:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:20:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:20:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:20:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:20:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:20:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=100.838089, avg_loss=0.746949, seen=135, correct=58, accuracy=0.429630
2025-10-02 09:20:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:20:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:20:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:20:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:20:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:20:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.498222, avg_loss=0.712456, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:20:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:20:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:20:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.475000
2025-10-02 09:20:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:20:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:20:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:20:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:20:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:20:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=100.701126, avg_loss=0.745934, seen=135, correct=54, accuracy=0.400000
2025-10-02 09:20:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:20:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:20:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:20:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:20:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:20:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:20:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.791225, avg_loss=0.719781, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:20:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:20:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:20:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.500000, curr=0.425000
2025-10-02 09:20:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:20:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:20:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:20:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:20:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:20:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:20:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=101.514771, avg_loss=0.751961, seen=135, correct=55, accuracy=0.407407
2025-10-02 09:20:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:20:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:20:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:21:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:21:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:21:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:21:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.122070, avg_loss=0.728052, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:21:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:21:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:21:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.500000, curr=0.425000
2025-10-02 09:21:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:21:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:21:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:21:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:21:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:21:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:21:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=100.991760, avg_loss=0.748087, seen=135, correct=59, accuracy=0.437037
2025-10-02 09:21:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:21:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:21:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:21:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:21:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:21:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.941610, avg_loss=0.723540, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:21:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:21:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:21:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.500000, curr=0.400000
2025-10-02 09:21:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:21:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:21:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:21:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:21:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:21:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=98.570908, avg_loss=0.730155, seen=135, correct=56, accuracy=0.414815
2025-10-02 09:21:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:21:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:21:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:21:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:21:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.282156, avg_loss=0.682054, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:21:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:21:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:21:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:21:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 09:21:37 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_018.ckpt
2025-10-02 09:21:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:21:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:21:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:21:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:21:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:21:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=97.418411, avg_loss=0.721618, seen=135, correct=67, accuracy=0.496296
2025-10-02 09:21:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:21:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:21:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:21:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:21:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:21:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:21:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.269371, avg_loss=0.656734, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:21:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:21:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:21:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:21:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 09:21:53 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_018.ckpt
2025-10-02 09:22:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:22:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:22:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:22:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:22:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:22:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:22:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=97.274750, avg_loss=0.720554, seen=135, correct=59, accuracy=0.437037
2025-10-02 09:22:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:22:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:22:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:22:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:22:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:22:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:22:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.824203, avg_loss=0.670605, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:22:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:22:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:22:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:22:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.550000
2025-10-02 09:22:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:22:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:22:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:22:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:22:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:22:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:22:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=97.284142, avg_loss=0.720623, seen=135, correct=58, accuracy=0.429630
2025-10-02 09:22:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:22:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:22:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:22:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:22:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:22:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.753582, avg_loss=0.693840, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:22:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:22:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:22:23 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.425000
2025-10-02 09:22:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:22:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:22:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:22:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:22:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:22:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=97.428398, avg_loss=0.721692, seen=135, correct=55, accuracy=0.407407
2025-10-02 09:22:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:22:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:22:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:22:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:22:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.497953, avg_loss=0.687449, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:22:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:22:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:22:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.475000
2025-10-02 09:22:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:22:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:22:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:22:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:22:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:22:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=95.966072, avg_loss=0.710860, seen=135, correct=65, accuracy=0.481481
2025-10-02 09:22:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:22:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:22:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:22:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:22:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:22:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:22:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.312305, avg_loss=0.657808, seen=40, correct=25, accuracy=0.625000
2025-10-02 09:22:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:22:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:22:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:22:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:22:57 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 09:22:57 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_018.ckpt
2025-10-02 09:23:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:23:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:23:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:23:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:23:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:23:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:23:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=96.362755, avg_loss=0.713798, seen=135, correct=67, accuracy=0.496296
2025-10-02 09:23:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:23:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:23:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:23:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:23:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.865574, avg_loss=0.646639, seen=40, correct=27, accuracy=0.675000
2025-10-02 09:23:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:23:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:23:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-02 09:23:14 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_018.ckpt
2025-10-02 09:23:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 09:23:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 09:23:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 09:23:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:23:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:23:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=96.669785, avg_loss=0.716072, seen=135, correct=67, accuracy=0.496296
2025-10-02 09:23:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:23:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:23:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:23:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:23:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:23:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:23:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.332590, avg_loss=0.633315, seen=40, correct=27, accuracy=0.675000
2025-10-02 09:23:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:23:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:23:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:23:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.675000, curr=0.675000
2025-10-02 09:23:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 09:23:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 09:23:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 09:23:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:23:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:23:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=96.806747, avg_loss=0.717087, seen=135, correct=57, accuracy=0.422222
2025-10-02 09:23:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:23:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:23:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:23:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:23:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:23:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.594660, avg_loss=0.689866, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:23:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:23:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:23:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:23:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.675000, curr=0.425000
2025-10-02 09:23:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 09:23:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 09:23:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 09:23:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:23:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:23:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:24:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:24:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=98.626236, avg_loss=0.730565, seen=135, correct=64, accuracy=0.474074
2025-10-02 09:24:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:24:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:24:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:24:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:24:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:24:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:24:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.917992, avg_loss=0.747950, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:24:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:24:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:24:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:24:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.675000, curr=0.400000
2025-10-02 09:24:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 09:24:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 09:24:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 09:24:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:24:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:24:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:24:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=96.000748, avg_loss=0.711117, seen=135, correct=61, accuracy=0.451852
2025-10-02 09:24:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:24:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:24:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:24:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:24:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:24:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:24:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.500286, avg_loss=0.687507, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:24:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:24:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:24:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:24:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.675000, curr=0.450000
2025-10-02 09:24:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 09:24:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 09:24:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 09:24:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:24:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:24:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:24:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=95.488777, avg_loss=0.707324, seen=135, correct=66, accuracy=0.488889
2025-10-02 09:24:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:24:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:24:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:24:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:24:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:24:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.598158, avg_loss=0.664954, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:24:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:24:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:24:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.675000, curr=0.550000
2025-10-02 09:24:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 09:24:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 09:24:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 09:24:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:24:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:24:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:24:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=96.204613, avg_loss=0.712627, seen=135, correct=69, accuracy=0.511111
2025-10-02 09:24:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:24:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:24:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:24:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:24:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:24:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.387930, avg_loss=0.709698, seen=40, correct=15, accuracy=0.375000
2025-10-02 09:24:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:24:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:24:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:24:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:24:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.675000, curr=0.375000
2025-10-02 09:25:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 09:25:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 09:25:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 09:25:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:25:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:25:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=98.140579, avg_loss=0.726967, seen=135, correct=62, accuracy=0.459259
2025-10-02 09:25:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:25:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:25:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:25:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:25:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:25:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:25:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.963612, avg_loss=0.749090, seen=40, correct=15, accuracy=0.375000
2025-10-02 09:25:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:25:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:25:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:25:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.675000, curr=0.375000
2025-10-02 09:25:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 09:25:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 09:25:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 09:25:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:25:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:25:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=97.126083, avg_loss=0.719452, seen=135, correct=63, accuracy=0.466667
2025-10-02 09:25:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:25:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:25:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:25:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:25:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:25:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:25:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.595001, avg_loss=0.739875, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:25:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:25:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:25:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:25:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.675000, curr=0.400000
2025-10-02 09:25:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 09:25:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 09:25:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 09:25:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:25:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:25:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=96.798042, avg_loss=0.717023, seen=135, correct=62, accuracy=0.459259
2025-10-02 09:25:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:25:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:25:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:25:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:25:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:25:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.911831, avg_loss=0.722796, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:25:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:25:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:25:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:25:41 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.675000, curr=0.400000
2025-10-02 09:25:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=220
2025-10-02 09:25:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=220
2025-10-02 09:25:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=220, splits=['val', 'test']
2025-10-02 09:25:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 09:25:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:25:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 09:25:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=95.384773, avg_loss=0.706554, seen=135, correct=59, accuracy=0.437037
2025-10-02 09:25:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:25:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:25:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:25:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:25:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:25:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:25:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.363407, avg_loss=0.684085, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:25:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:25:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:25:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:25:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.675000, curr=0.450000
2025-10-02 09:25:55 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:25:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:25:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:25:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:25:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2414MB allocated=2363MB
2025-10-02 09:25:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:25:56 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:25:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:25:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:25:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:25:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:25:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:25:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:25:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:25:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:26:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:26:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=137.790649, avg_loss=0.732929, seen=188, correct=92, accuracy=0.489362
2025-10-02 09:26:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:26:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:26:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2376MB allocated=2347MB
2025-10-02 09:26:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:26:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:26:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:26:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.844505, avg_loss=0.721113, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:26:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:26:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:26:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2376MB allocated=2347MB
2025-10-02 09:26:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 09:26:05 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_052.ckpt
2025-10-02 09:26:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:26:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-02 09:26:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:26:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:26:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:26:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:26:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:26:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:26:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:26:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:26:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:26:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:26:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=134.365326, avg_loss=0.714709, seen=188, correct=92, accuracy=0.489362
2025-10-02 09:26:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:26:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:26:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:26:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:26:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:26:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:26:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.492935, avg_loss=0.712323, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:26:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:26:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:26:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:26:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.500000
2025-10-02 09:26:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:26:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:26:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:26:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:26:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:26:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:26:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=132.091339, avg_loss=0.702614, seen=188, correct=97, accuracy=0.515957
2025-10-02 09:26:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:26:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:26:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:26:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:26:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:26:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:26:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.199951, avg_loss=0.704999, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:26:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:26:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:26:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:26:39 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 09:26:39 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_052.ckpt
2025-10-02 09:26:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:26:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:26:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:26:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:26:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:26:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:26:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=133.052689, avg_loss=0.707727, seen=188, correct=104, accuracy=0.553191
2025-10-02 09:26:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:26:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:26:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:26:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:26:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:26:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:26:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.027912, avg_loss=0.725698, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:26:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:26:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:26:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:26:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:26:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.475000
2025-10-02 09:27:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:27:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:27:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:27:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:27:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:27:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:27:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.812347, avg_loss=0.701130, seen=188, correct=106, accuracy=0.563830
2025-10-02 09:27:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:27:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:27:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:27:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:27:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:27:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:27:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.426865, avg_loss=0.710672, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:27:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:27:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:27:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:27:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.425000
2025-10-02 09:27:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:27:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:27:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:27:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:27:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:27:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:27:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.759262, avg_loss=0.700847, seen=188, correct=94, accuracy=0.500000
2025-10-02 09:27:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:27:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:27:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:27:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:27:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:27:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:27:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.261631, avg_loss=0.706541, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:27:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:27:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:27:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:27:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.450000
2025-10-02 09:27:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:27:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:27:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:27:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:27:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:27:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:27:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.669861, avg_loss=0.700372, seen=188, correct=93, accuracy=0.494681
2025-10-02 09:27:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:27:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:27:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:27:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:27:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:27:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.021915, avg_loss=0.700548, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:27:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:27:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:27:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:27:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:27:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.475000
2025-10-02 09:28:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:28:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:28:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:28:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:28:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:28:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:28:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.665497, avg_loss=0.700348, seen=188, correct=100, accuracy=0.531915
2025-10-02 09:28:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:28:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:28:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:28:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:28:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.126154, avg_loss=0.703154, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:28:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:28:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:28:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:28:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.425000
2025-10-02 09:28:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:28:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:28:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:28:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:28:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:28:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:28:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.504547, avg_loss=0.699492, seen=188, correct=98, accuracy=0.521277
2025-10-02 09:28:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:28:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:28:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:28:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:28:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:28:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:28:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.141130, avg_loss=0.703528, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:28:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:28:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:28:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:28:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.550000, curr=0.475000
2025-10-02 09:28:35 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:28:35 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:28:35 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:28:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:28:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:28:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:28:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.274796, avg_loss=0.698270, seen=188, correct=94, accuracy=0.500000
2025-10-02 09:28:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:28:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:28:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:28:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:28:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:28:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:28:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.990259, avg_loss=0.699756, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:28:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:28:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:28:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:28:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.550000, curr=0.500000
2025-10-02 09:28:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:28:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:28:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:28:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:28:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:28:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:28:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=130.157272, avg_loss=0.692326, seen=188, correct=95, accuracy=0.505319
2025-10-02 09:28:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:28:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:28:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:28:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:28:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:28:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:28:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.797779, avg_loss=0.694944, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:28:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:28:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:28:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:28:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.550000, curr=0.475000
2025-10-02 09:29:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:29:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:29:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:29:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:29:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:29:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=130.570419, avg_loss=0.694524, seen=188, correct=100, accuracy=0.531915
2025-10-02 09:29:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:29:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:29:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:29:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:29:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:29:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.298134, avg_loss=0.707453, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:29:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:29:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:29:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:29:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.550000, curr=0.525000
2025-10-02 09:29:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:29:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:29:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:29:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 09:29:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 09:29:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=130.186340, avg_loss=0.692481, seen=188, correct=103, accuracy=0.547872
2025-10-02 09:29:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:29:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:29:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:29:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:29:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.240040, avg_loss=0.706001, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:29:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:29:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:29:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:29:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.550000, curr=0.525000
2025-10-02 09:29:33 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:29:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:29:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:29:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:29:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2380MB
2025-10-02 09:29:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:29:34 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #52', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:29:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:29:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:29:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:34 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:29:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:29:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:29:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:29:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:29:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=69.477013, avg_loss=0.780641, seen=89, correct=40, accuracy=0.449438
2025-10-02 09:29:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:29:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:29:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2396MB allocated=2363MB
2025-10-02 09:29:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:29:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:29:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.641369, avg_loss=0.691034, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:29:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:29:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2396MB allocated=2363MB
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_043.ckpt
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-02 09:29:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:29:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:29:51 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:29:51 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:29:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:29:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:29:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:29:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=64.525909, avg_loss=0.725010, seen=89, correct=42, accuracy=0.471910
2025-10-02 09:29:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:29:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:29:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:29:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:29:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:29:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:29:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.468781, avg_loss=0.686720, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:29:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:29:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:29:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:29:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:29:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.550000
2025-10-02 09:30:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:30:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:30:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:30:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:30:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:30:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:30:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=64.533089, avg_loss=0.725091, seen=89, correct=41, accuracy=0.460674
2025-10-02 09:30:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:30:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:30:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:30:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:30:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:30:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:30:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.280920, avg_loss=0.732023, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:30:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:30:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:30:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:30:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.450000
2025-10-02 09:30:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:30:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:30:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:30:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:30:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:30:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:30:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.996811, avg_loss=0.719065, seen=89, correct=39, accuracy=0.438202
2025-10-02 09:30:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:30:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:30:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:30:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:30:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:30:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:30:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.797550, avg_loss=0.694939, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:30:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:30:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:30:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:30:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 09:30:28 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_043.ckpt
2025-10-02 09:30:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:30:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:30:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:30:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:30:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:30:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:30:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=64.589333, avg_loss=0.725723, seen=89, correct=42, accuracy=0.471910
2025-10-02 09:30:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:30:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:30:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:30:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:30:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:30:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:30:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.369001, avg_loss=0.684225, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:30:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:30:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:30:43 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.575000
2025-10-02 09:30:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:30:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:30:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:30:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:30:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:30:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:30:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=64.692223, avg_loss=0.726879, seen=89, correct=44, accuracy=0.494382
2025-10-02 09:30:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:30:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:30:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:30:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:30:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:30:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:30:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.210281, avg_loss=0.680257, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:30:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:30:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:30:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:30:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.550000
2025-10-02 09:31:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:31:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:31:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:31:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:31:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:31:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:31:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.637497, avg_loss=0.715028, seen=89, correct=42, accuracy=0.471910
2025-10-02 09:31:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:31:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:31:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:31:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:31:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:31:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:31:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.568266, avg_loss=0.689207, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:31:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:31:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:31:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:31:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 09:31:14 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_043.ckpt
2025-10-02 09:31:24 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:31:24 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:31:24 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:31:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:31:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:31:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:31:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.659409, avg_loss=0.715274, seen=89, correct=42, accuracy=0.471910
2025-10-02 09:31:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:31:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:31:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:31:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:31:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:31:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:31:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.179211, avg_loss=0.679480, seen=40, correct=26, accuracy=0.650000
2025-10-02 09:31:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:31:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:31:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:31:31 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-02 09:31:31 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_043.ckpt
2025-10-02 09:31:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:31:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:31:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:31:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:31:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:31:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:31:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=64.038300, avg_loss=0.719531, seen=89, correct=44, accuracy=0.494382
2025-10-02 09:31:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:31:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:31:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:31:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:31:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:31:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:31:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.342915, avg_loss=0.683573, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:31:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:31:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:31:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:31:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.650000, curr=0.525000
2025-10-02 09:31:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:31:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:31:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:31:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:31:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:31:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.435989, avg_loss=0.712764, seen=89, correct=42, accuracy=0.471910
2025-10-02 09:31:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:31:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:31:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:31:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:31:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:31:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.153374, avg_loss=0.678834, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:31:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:31:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:31:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:32:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:32:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.650000, curr=0.550000
2025-10-02 09:32:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:32:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:32:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:32:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:32:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:32:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:32:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=64.399475, avg_loss=0.723590, seen=89, correct=44, accuracy=0.494382
2025-10-02 09:32:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:32:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:32:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:32:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:32:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:32:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:32:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.148209, avg_loss=0.678705, seen=40, correct=25, accuracy=0.625000
2025-10-02 09:32:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:32:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:32:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.650000, curr=0.625000
2025-10-02 09:32:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:32:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:32:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:32:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:32:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:32:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:32:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.652809, avg_loss=0.715200, seen=89, correct=43, accuracy=0.483146
2025-10-02 09:32:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:32:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:32:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:32:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:32:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:32:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:32:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.238541, avg_loss=0.680964, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:32:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:32:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:32:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.650000, curr=0.575000
2025-10-02 09:32:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:32:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:32:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:32:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:32:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:32:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:32:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.750046, avg_loss=0.705057, seen=89, correct=41, accuracy=0.460674
2025-10-02 09:32:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:32:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:32:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:32:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:32:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.251255, avg_loss=0.681281, seen=40, correct=25, accuracy=0.625000
2025-10-02 09:32:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:32:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:32:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.650000, curr=0.625000
2025-10-02 09:32:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 09:32:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 09:32:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 09:32:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:32:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:32:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:33:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:33:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.717865, avg_loss=0.704695, seen=89, correct=44, accuracy=0.494382
2025-10-02 09:33:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:33:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:33:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:33:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:33:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:33:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.346455, avg_loss=0.708661, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:33:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:33:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:33:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:33:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.650000, curr=0.425000
2025-10-02 09:33:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 09:33:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 09:33:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 09:33:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:33:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:33:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:33:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.855690, avg_loss=0.706244, seen=89, correct=40, accuracy=0.449438
2025-10-02 09:33:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:33:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:33:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:33:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:33:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:33:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.365248, avg_loss=0.684131, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:33:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:33:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:33:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:33:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.650000, curr=0.600000
2025-10-02 09:33:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 09:33:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 09:33:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 09:33:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:33:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:33:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:33:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.284519, avg_loss=0.711062, seen=89, correct=46, accuracy=0.516854
2025-10-02 09:33:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:33:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:33:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:33:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:33:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:33:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.918571, avg_loss=0.672964, seen=40, correct=25, accuracy=0.625000
2025-10-02 09:33:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:33:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:33:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.650000, curr=0.625000
2025-10-02 09:33:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 09:33:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 09:33:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 09:33:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:33:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:33:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:33:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.127144, avg_loss=0.709294, seen=89, correct=43, accuracy=0.483146
2025-10-02 09:33:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:33:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:33:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:33:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:33:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:33:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:33:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.154976, avg_loss=0.678874, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:33:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:33:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:33:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:33:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:33:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.650000, curr=0.550000
2025-10-02 09:34:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 09:34:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 09:34:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 09:34:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 09:34:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 09:34:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.409531, avg_loss=0.701231, seen=89, correct=42, accuracy=0.471910
2025-10-02 09:34:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:34:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:34:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:34:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.123749, avg_loss=0.678094, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:34:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:34:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:34:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.650000, curr=0.600000
2025-10-02 09:34:08 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:34:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:34:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:34:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2460MB allocated=2397MB
2025-10-02 09:34:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:34:09 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #43', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:34:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:34:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:34:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=107, num_train_batch_last_epoch=51, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:09 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:34:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:34:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:34:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=107, num_train_batch_last_epoch=51, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:34:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.887947, avg_loss=0.807995, seen=11, correct=4, accuracy=0.363636
2025-10-02 09:34:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2416MB allocated=2380MB
2025-10-02 09:34:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:34:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=107, num_train_batch_last_epoch=51, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:34:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.087929, avg_loss=0.802198, seen=40, correct=14, accuracy=0.350000
2025-10-02 09:34:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2416MB allocated=2380MB
2025-10-02 09:34:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.350000
2025-10-02 09:34:15 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_002.ckpt
2025-10-02 09:34:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:34:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-02 09:34:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:34:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=107, num_train_batch_last_epoch=51, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:34:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:34:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:34:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:34:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:34:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:34:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:34:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.792192, avg_loss=0.708381, seen=11, correct=6, accuracy=0.545455
2025-10-02 09:34:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:34:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:34:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:34:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.863552, avg_loss=0.696589, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:34:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:34:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:34:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 09:34:30 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_002.ckpt
2025-10-02 09:34:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:34:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:34:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:34:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:34:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:34:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.364143, avg_loss=0.669468, seen=11, correct=8, accuracy=0.727273
2025-10-02 09:34:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:34:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:34:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:34:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:34:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.582607, avg_loss=0.664565, seen=40, correct=25, accuracy=0.625000
2025-10-02 09:34:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:34:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 09:34:46 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_002.ckpt
2025-10-02 09:34:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:34:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:34:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:34:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:34:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:34:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:34:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.699308, avg_loss=0.699937, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:34:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:34:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:34:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:34:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:34:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:34:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:35:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:35:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.797092, avg_loss=0.694927, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:35:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:35:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:35:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:35:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.525000
2025-10-02 09:35:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:35:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:35:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:35:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:35:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:35:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.582668, avg_loss=0.689333, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:35:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:35:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:35:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:35:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:35:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:35:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.363165, avg_loss=0.709079, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:35:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:35:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:35:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.500000
2025-10-02 09:35:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:35:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:35:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:35:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:35:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:35:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:35:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.581864, avg_loss=0.689260, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:35:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:35:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:35:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:35:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:35:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:35:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:35:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.378544, avg_loss=0.709464, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:35:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:35:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:35:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:35:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.475000
2025-10-02 09:35:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:35:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:35:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:35:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:35:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:35:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:35:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.572021, avg_loss=0.688366, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:35:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:35:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:35:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:35:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:35:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:35:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:35:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.179333, avg_loss=0.704483, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:35:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:35:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:35:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:35:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.500000
2025-10-02 09:35:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:35:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:35:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:35:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:35:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:35:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:35:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.625541, avg_loss=0.693231, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:35:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:35:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:35:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:35:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:35:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:35:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:36:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:36:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.826685, avg_loss=0.695667, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:36:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:36:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:36:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:36:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.500000
2025-10-02 09:36:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:36:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:36:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:36:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:36:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:36:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:36:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.628228, avg_loss=0.693475, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:36:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:36:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:36:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:36:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:36:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:36:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:36:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.063787, avg_loss=0.701595, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:36:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:36:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:36:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.500000
2025-10-02 09:36:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:36:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:36:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:36:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:36:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:36:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.787286, avg_loss=0.707935, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:36:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:36:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:36:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:36:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:36:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:36:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.709536, avg_loss=0.717738, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:36:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:36:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:36:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.400000
2025-10-02 09:36:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:36:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:36:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:36:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:36:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:36:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:36:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.826075, avg_loss=0.711461, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:36:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:36:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:36:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:36:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:36:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:36:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:36:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.430492, avg_loss=0.710762, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:36:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:36:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:36:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:36:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.625000, curr=0.475000
2025-10-02 09:36:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:36:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:36:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:36:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:36:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:36:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:36:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.814908, avg_loss=0.710446, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:36:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:36:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:36:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:36:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:36:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:36:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:36:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.410954, avg_loss=0.710274, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:36:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:36:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:36:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:36:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:36:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.625000, curr=0.425000
2025-10-02 09:37:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:37:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:37:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:37:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:37:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:37:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.837156, avg_loss=0.712469, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:37:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:37:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:37:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:37:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:37:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.502737, avg_loss=0.712568, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:37:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:37:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:37:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:37:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.625000, curr=0.475000
2025-10-02 09:37:14 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:37:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:37:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:37:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2414MB
2025-10-02 09:37:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:37:15 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:37:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:37:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:37:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=686, num_train_batch_last_epoch=114, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:37:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:37:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:37:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:37:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=686, num_train_batch_last_epoch=114, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:37:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=53.502380, avg_loss=0.743089, seen=72, correct=38, accuracy=0.527778
2025-10-02 09:37:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:37:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:37:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2397MB
2025-10-02 09:37:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:37:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=686, num_train_batch_last_epoch=114, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:37:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.372459, avg_loss=0.759311, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:37:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:37:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2436MB allocated=2397MB
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_013.ckpt
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-02 09:37:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=686, num_train_batch_last_epoch=114, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:37:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:37:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:37:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:37:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:37:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:37:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:37:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=53.243458, avg_loss=0.739492, seen=72, correct=38, accuracy=0.527778
2025-10-02 09:37:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:37:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:37:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:37:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:37:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:37:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.316282, avg_loss=0.757907, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:37:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:37:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:37:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-02 09:37:37 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_013.ckpt
2025-10-02 09:37:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:37:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:37:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:37:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:37:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=52.744625, avg_loss=0.732564, seen=72, correct=35, accuracy=0.486111
2025-10-02 09:37:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:37:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:37:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:37:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:37:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.738920, avg_loss=0.718473, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:37:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:37:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:37:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:37:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:37:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.450000, curr=0.425000
2025-10-02 09:38:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:38:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:38:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:38:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:38:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:38:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:38:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=54.725136, avg_loss=0.760071, seen=72, correct=30, accuracy=0.416667
2025-10-02 09:38:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:38:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:38:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:38:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:38:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:38:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:38:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.081024, avg_loss=0.677026, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:38:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:38:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:38:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:38:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 09:38:08 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_013.ckpt
2025-10-02 09:38:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:38:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:38:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:38:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:38:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:38:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:38:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=53.349586, avg_loss=0.740966, seen=72, correct=28, accuracy=0.388889
2025-10-02 09:38:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:38:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:38:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:38:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:38:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:38:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:38:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.319603, avg_loss=0.682990, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:38:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:38:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:38:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:38:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.575000
2025-10-02 09:38:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:38:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:38:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:38:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:38:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:38:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:38:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=52.623032, avg_loss=0.730875, seen=72, correct=29, accuracy=0.402778
2025-10-02 09:38:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:38:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:38:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:38:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:38:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:38:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:38:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.415520, avg_loss=0.685388, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:38:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:38:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:38:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:38:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.550000
2025-10-02 09:38:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:38:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:38:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:38:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:38:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:38:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:38:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=51.945934, avg_loss=0.721471, seen=72, correct=30, accuracy=0.416667
2025-10-02 09:38:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:38:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:38:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:38:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:38:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:38:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:38:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.593895, avg_loss=0.689847, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:38:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:38:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:38:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:38:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:38:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.450000
2025-10-02 09:39:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:39:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:39:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:39:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:39:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:39:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=51.974140, avg_loss=0.721863, seen=72, correct=29, accuracy=0.402778
2025-10-02 09:39:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:39:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:39:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:39:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:39:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:39:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:39:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.288952, avg_loss=0.682224, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:39:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:39:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:39:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:39:06 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.525000
2025-10-02 09:39:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:39:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:39:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:39:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:39:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:39:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:39:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=52.774220, avg_loss=0.732975, seen=72, correct=31, accuracy=0.430556
2025-10-02 09:39:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:39:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:39:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:39:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:39:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.050735, avg_loss=0.676268, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:39:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:39:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:39:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.600000
2025-10-02 09:39:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:39:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:39:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:39:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:39:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:39:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:39:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=52.065380, avg_loss=0.723130, seen=72, correct=33, accuracy=0.458333
2025-10-02 09:39:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:39:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:39:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:39:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:39:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.257481, avg_loss=0.681437, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:39:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:39:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:39:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.500000
2025-10-02 09:39:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:39:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:39:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:39:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:39:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:39:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=51.594345, avg_loss=0.716588, seen=72, correct=32, accuracy=0.444444
2025-10-02 09:39:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:39:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:39:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:39:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:39:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:39:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:39:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.250504, avg_loss=0.706263, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:39:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:39:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:39:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:39:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.425000
2025-10-02 09:39:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:39:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:39:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:39:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:39:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:39:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=52.057083, avg_loss=0.723015, seen=72, correct=32, accuracy=0.444444
2025-10-02 09:39:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:39:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:39:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:40:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:40:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:40:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.138899, avg_loss=0.703472, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:40:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:40:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.400000
2025-10-02 09:40:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:40:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:40:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:40:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:40:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:40:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=51.586514, avg_loss=0.716479, seen=72, correct=30, accuracy=0.416667
2025-10-02 09:40:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:40:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:40:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:40:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:40:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.410780, avg_loss=0.685269, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:40:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:40:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:40:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.450000
2025-10-02 09:40:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 09:40:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 09:40:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 09:40:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 09:40:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 09:40:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=51.729836, avg_loss=0.718470, seen=72, correct=30, accuracy=0.416667
2025-10-02 09:40:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:40:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:40:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:40:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.108311, avg_loss=0.677708, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:40:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:40:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:40:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.600000, curr=0.575000
2025-10-02 09:40:34 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:40:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:40:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:40:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2500MB allocated=2430MB
2025-10-02 09:40:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:40:35 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:40:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:40:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:40:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:36 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:40:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:40:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:40:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:40:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=86.778275, avg_loss=0.729229, seen=119, correct=55, accuracy=0.462185
2025-10-02 09:40:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2456MB allocated=2414MB
2025-10-02 09:40:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:40:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.995031, avg_loss=0.699876, seen=40, correct=24, accuracy=0.600000
2025-10-02 09:40:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:40:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2456MB allocated=2414MB
2025-10-02 09:40:45 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 09:40:45 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_041.ckpt
2025-10-02 09:40:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:40:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-02 09:40:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:40:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:40:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:40:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:40:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:40:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:40:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:40:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.888397, avg_loss=0.704945, seen=119, correct=62, accuracy=0.521008
2025-10-02 09:40:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:40:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:40:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:40:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:40:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:40:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:40:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.567722, avg_loss=0.739193, seen=40, correct=14, accuracy=0.350000
2025-10-02 09:40:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:40:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:40:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:41:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:41:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.350000
2025-10-02 09:41:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:41:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:41:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:41:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:41:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:41:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:41:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.907227, avg_loss=0.705103, seen=119, correct=62, accuracy=0.521008
2025-10-02 09:41:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:41:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:41:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:41:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:41:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:41:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.342579, avg_loss=0.733564, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:41:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:41:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:41:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:41:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.450000
2025-10-02 09:41:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:41:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:41:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:41:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:41:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:41:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:41:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.876892, avg_loss=0.704848, seen=119, correct=58, accuracy=0.487395
2025-10-02 09:41:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:41:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:41:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:41:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:41:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.162355, avg_loss=0.704059, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:41:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:41:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:41:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.450000
2025-10-02 09:41:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:41:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:41:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:41:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:41:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:41:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.101761, avg_loss=0.698334, seen=119, correct=63, accuracy=0.529412
2025-10-02 09:41:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:41:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:41:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:41:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:41:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:41:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:41:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.733423, avg_loss=0.718336, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:41:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:41:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:41:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:41:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:41:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.450000
2025-10-02 09:42:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:42:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:42:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:42:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:42:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:42:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:42:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=82.878540, avg_loss=0.696458, seen=119, correct=61, accuracy=0.512605
2025-10-02 09:42:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:42:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:42:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:42:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:42:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.205635, avg_loss=0.705141, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:42:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:42:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:42:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:42:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.600000, curr=0.450000
2025-10-02 09:42:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:42:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:42:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:42:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:42:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:42:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.883148, avg_loss=0.704900, seen=119, correct=59, accuracy=0.495798
2025-10-02 09:42:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:42:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:42:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:42:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:42:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:42:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.439236, avg_loss=0.685981, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:42:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:42:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:42:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.600000, curr=0.500000
2025-10-02 09:42:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:42:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:42:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:42:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:42:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:42:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=85.125946, avg_loss=0.715344, seen=119, correct=58, accuracy=0.487395
2025-10-02 09:42:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:42:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:42:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:42:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:42:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:42:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.336220, avg_loss=0.683405, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:42:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:42:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:42:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:42:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.600000, curr=0.550000
2025-10-02 09:42:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:42:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:42:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:42:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:42:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:42:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:42:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.114655, avg_loss=0.698442, seen=119, correct=59, accuracy=0.495798
2025-10-02 09:42:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:42:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:42:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:42:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:42:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:42:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.777496, avg_loss=0.694437, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:42:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:42:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:42:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:42:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:42:56 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.600000, curr=0.500000
2025-10-02 09:43:05 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:43:05 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:43:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:43:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:43:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:43:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=84.678383, avg_loss=0.711583, seen=119, correct=61, accuracy=0.512605
2025-10-02 09:43:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:43:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:43:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:43:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:43:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:43:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.866936, avg_loss=0.746673, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:43:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:43:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:43:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.600000, curr=0.475000
2025-10-02 09:43:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:43:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:43:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:43:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 09:43:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 09:43:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.535507, avg_loss=0.701979, seen=119, correct=65, accuracy=0.546218
2025-10-02 09:43:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:43:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:43:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:43:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:43:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:43:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.826443, avg_loss=0.720661, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:43:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:43:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:43:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:43:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.600000, curr=0.500000
2025-10-02 09:43:29 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:43:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:43:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:43:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:43:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2447MB
2025-10-02 09:43:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:43:30 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #41', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:43:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:43:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:43:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:30 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:43:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:43:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:43:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:43:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:43:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=146.895203, avg_loss=0.734476, seen=200, correct=102, accuracy=0.510000
2025-10-02 09:43:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:43:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:43:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2456MB allocated=2430MB
2025-10-02 09:43:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:43:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:43:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.473801, avg_loss=0.811845, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:43:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:43:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:43:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2456MB allocated=2430MB
2025-10-02 09:43:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-02 09:43:40 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_025.ckpt
2025-10-02 09:43:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:43:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-02 09:43:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:43:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:43:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:43:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:43:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:43:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:43:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:43:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.828125, avg_loss=0.719141, seen=200, correct=97, accuracy=0.485000
2025-10-02 09:43:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:43:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:43:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:43:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:43:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:43:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:43:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.004707, avg_loss=0.750118, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:43:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:43:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:43:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:43:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:43:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 09:44:00 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_025.ckpt
2025-10-02 09:44:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:44:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:44:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:44:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:44:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:44:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:44:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=150.994110, avg_loss=0.754971, seen=200, correct=95, accuracy=0.475000
2025-10-02 09:44:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:44:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:44:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:44:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:44:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:44:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:44:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.363476, avg_loss=0.684087, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:44:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:44:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:44:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:44:18 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 09:44:18 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_025.ckpt
2025-10-02 09:44:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:44:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:44:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:44:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:44:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:44:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.928589, avg_loss=0.724643, seen=200, correct=91, accuracy=0.455000
2025-10-02 09:44:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:44:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:44:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:44:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:44:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:44:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:44:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.883389, avg_loss=0.697085, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:44:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:44:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:44:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:44:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.550000
2025-10-02 09:44:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:44:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:44:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:44:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:44:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:44:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.633881, avg_loss=0.713169, seen=200, correct=103, accuracy=0.515000
2025-10-02 09:44:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:44:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:44:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:44:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:44:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:44:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.994347, avg_loss=0.724859, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:44:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:44:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:44:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:44:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:44:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.475000
2025-10-02 09:45:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:45:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:45:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:45:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:45:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:45:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.626587, avg_loss=0.713133, seen=200, correct=95, accuracy=0.475000
2025-10-02 09:45:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:45:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:45:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:45:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:45:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:45:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:45:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.741226, avg_loss=0.718531, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:45:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:45:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:45:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.575000, curr=0.450000
2025-10-02 09:45:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:45:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:45:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:45:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:45:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:45:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:45:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.053268, avg_loss=0.715266, seen=200, correct=92, accuracy=0.460000
2025-10-02 09:45:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:45:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:45:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:45:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:45:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:45:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:45:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.108150, avg_loss=0.702704, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:45:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:45:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:45:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:45:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.575000, curr=0.550000
2025-10-02 09:45:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:45:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:45:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:45:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:45:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:45:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:45:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.870926, avg_loss=0.719355, seen=200, correct=93, accuracy=0.465000
2025-10-02 09:45:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:45:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:45:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:45:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:45:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:45:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:45:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.976191, avg_loss=0.699405, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:45:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:45:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:45:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:45:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.575000, curr=0.550000
2025-10-02 09:45:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:45:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:45:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:45:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:45:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:45:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:45:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.281296, avg_loss=0.706406, seen=200, correct=96, accuracy=0.480000
2025-10-02 09:45:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:45:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:45:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:45:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:45:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:45:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:45:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.814774, avg_loss=0.720369, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:45:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:45:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:45:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:46:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:46:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.575000, curr=0.450000
2025-10-02 09:46:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:46:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:46:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:46:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:46:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:46:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:46:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.490692, avg_loss=0.707453, seen=200, correct=99, accuracy=0.495000
2025-10-02 09:46:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:46:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:46:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:46:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:46:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:46:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.027746, avg_loss=0.750694, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:46:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:46:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:46:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:46:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.575000, curr=0.400000
2025-10-02 09:46:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:46:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:46:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:46:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:46:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:46:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.151260, avg_loss=0.705756, seen=200, correct=101, accuracy=0.505000
2025-10-02 09:46:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:46:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:46:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:46:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:46:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:46:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.029240, avg_loss=0.750731, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:46:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:46:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:46:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.575000, curr=0.400000
2025-10-02 09:46:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:46:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:46:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:46:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:46:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:46:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:46:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.957321, avg_loss=0.704787, seen=200, correct=100, accuracy=0.500000
2025-10-02 09:46:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:46:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:46:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:46:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:46:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:46:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:46:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.919380, avg_loss=0.747985, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:46:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:46:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:46:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:46:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.575000, curr=0.425000
2025-10-02 09:47:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:47:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:47:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:47:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:47:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:47:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.636353, avg_loss=0.708182, seen=200, correct=98, accuracy=0.490000
2025-10-02 09:47:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:47:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:47:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:47:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:47:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.636087, avg_loss=0.715902, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:47:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:47:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:47:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.575000, curr=0.425000
2025-10-02 09:47:10 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:47:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:47:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:47:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:47:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2502MB allocated=2464MB
2025-10-02 09:47:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:47:11 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:47:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:47:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:47:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=544, num_train_batch_last_epoch=256, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:11 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:47:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:47:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:47:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=544, num_train_batch_last_epoch=256, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:47:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=39.896362, avg_loss=0.699936, seen=57, correct=31, accuracy=0.543860
2025-10-02 09:47:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:47:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:47:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2447MB
2025-10-02 09:47:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:47:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=544, num_train_batch_last_epoch=256, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:47:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.752110, avg_loss=0.768803, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:47:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:47:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:47:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2476MB allocated=2447MB
2025-10-02 09:47:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 09:47:19 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_007.ckpt
2025-10-02 09:47:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:47:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-02 09:47:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:47:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=544, num_train_batch_last_epoch=256, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:47:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:47:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:47:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:47:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:47:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:47:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:47:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.891350, avg_loss=0.682304, seen=57, correct=33, accuracy=0.578947
2025-10-02 09:47:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:47:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:47:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:47:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:47:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:47:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.875534, avg_loss=0.721888, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:47:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:47:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:47:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:47:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.450000
2025-10-02 09:47:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:47:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:47:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:47:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:47:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=40.019859, avg_loss=0.702103, seen=57, correct=30, accuracy=0.526316
2025-10-02 09:47:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:47:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:47:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:47:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.787928, avg_loss=0.719698, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:47:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:47:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:47:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:47:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.500000, curr=0.450000
2025-10-02 09:47:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:47:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:47:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:47:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:47:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:47:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:47:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.950726, avg_loss=0.683346, seen=57, correct=31, accuracy=0.543860
2025-10-02 09:47:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:47:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:48:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:48:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:48:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.289101, avg_loss=0.707228, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:48:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:48:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:48:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.500000, curr=0.450000
2025-10-02 09:48:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:48:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:48:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:48:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:48:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:48:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:48:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=39.229431, avg_loss=0.688236, seen=57, correct=30, accuracy=0.526316
2025-10-02 09:48:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:48:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:48:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:48:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:48:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:48:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.484760, avg_loss=0.712119, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:48:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:48:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:48:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.500000, curr=0.450000
2025-10-02 09:48:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:48:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:48:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:48:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:48:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:48:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:48:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.808239, avg_loss=0.680846, seen=57, correct=32, accuracy=0.561404
2025-10-02 09:48:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:48:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:48:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:48:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:48:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:48:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:48:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.027954, avg_loss=0.700699, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:48:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:48:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:48:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.500000, curr=0.425000
2025-10-02 09:48:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:48:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:48:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:48:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:48:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:48:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.676537, avg_loss=0.678536, seen=57, correct=33, accuracy=0.578947
2025-10-02 09:48:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:48:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:48:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:48:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:48:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:48:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:48:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.257563, avg_loss=0.706439, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:48:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:48:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:48:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:48:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.500000, curr=0.475000
2025-10-02 09:48:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:48:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:48:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:48:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:48:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:48:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:48:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:48:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.627426, avg_loss=0.677674, seen=57, correct=32, accuracy=0.561404
2025-10-02 09:48:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:48:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:49:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:49:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:49:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.111202, avg_loss=0.702780, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:49:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:49:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:49:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.500000, curr=0.475000
2025-10-02 09:49:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:49:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:49:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:49:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:49:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:49:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.818707, avg_loss=0.681030, seen=57, correct=34, accuracy=0.596491
2025-10-02 09:49:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:49:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:49:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:49:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.671535, avg_loss=0.691788, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:49:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:49:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:49:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.500000, curr=0.450000
2025-10-02 09:49:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:49:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:49:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:49:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:49:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:49:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.892326, avg_loss=0.682322, seen=57, correct=32, accuracy=0.561404
2025-10-02 09:49:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:49:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:49:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:49:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:49:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.719351, avg_loss=0.692984, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:49:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:49:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:49:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.500000, curr=0.475000
2025-10-02 09:49:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:49:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:49:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:49:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 09:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 09:49:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.779331, avg_loss=0.680339, seen=57, correct=33, accuracy=0.578947
2025-10-02 09:49:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:49:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:49:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.647358, avg_loss=0.691184, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:49:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:49:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:49:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.500000, curr=0.500000
2025-10-02 09:49:49 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:49:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:49:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:49:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2538MB allocated=2481MB
2025-10-02 09:49:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:49:50 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:49:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:49:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:49:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:50 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:49:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:49:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:49:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:49:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:49:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=148.845642, avg_loss=0.744228, seen=200, correct=104, accuracy=0.520000
2025-10-02 09:49:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:49:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2496MB allocated=2464MB
2025-10-02 09:49:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:49:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:49:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=33.474701, avg_loss=0.836868, seen=40, correct=14, accuracy=0.350000
2025-10-02 09:49:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:49:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2496MB allocated=2464MB
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.350000
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_024.ckpt
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-02 09:49:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:49:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:50:06 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:50:06 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:50:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:50:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:50:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:50:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.313538, avg_loss=0.721568, seen=200, correct=111, accuracy=0.555000
2025-10-02 09:50:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:50:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:50:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:50:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:50:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:50:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:50:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.061092, avg_loss=0.801527, seen=40, correct=14, accuracy=0.350000
2025-10-02 09:50:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:50:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:50:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:50:15 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.350000, curr=0.350000
2025-10-02 09:50:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:50:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:50:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:50:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:50:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:50:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=144.129562, avg_loss=0.720648, seen=200, correct=94, accuracy=0.470000
2025-10-02 09:50:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:50:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:50:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:50:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:50:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:50:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:50:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.921886, avg_loss=0.748047, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:50:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:50:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:50:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:50:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 09:50:33 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_024.ckpt
2025-10-02 09:50:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:50:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:50:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:50:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:50:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:50:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:50:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.758926, avg_loss=0.703795, seen=200, correct=96, accuracy=0.480000
2025-10-02 09:50:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:50:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:50:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:50:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:50:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:50:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:50:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.988621, avg_loss=0.749716, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:50:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:50:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:50:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:50:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.400000
2025-10-02 09:51:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:51:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:51:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:51:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:51:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:51:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:51:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.224335, avg_loss=0.701122, seen=200, correct=99, accuracy=0.495000
2025-10-02 09:51:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:51:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:51:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:51:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:51:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:51:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.330555, avg_loss=0.758264, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:51:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:51:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:51:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:51:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.500000, curr=0.400000
2025-10-02 09:51:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:51:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:51:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:51:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:51:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:51:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:51:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.802490, avg_loss=0.704012, seen=200, correct=102, accuracy=0.510000
2025-10-02 09:51:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:51:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:51:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:51:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:51:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:51:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.056894, avg_loss=0.751422, seen=40, correct=15, accuracy=0.375000
2025-10-02 09:51:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:51:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:51:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:51:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.500000, curr=0.375000
2025-10-02 09:51:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:51:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:51:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:51:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:51:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:51:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:51:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.834381, avg_loss=0.699172, seen=200, correct=107, accuracy=0.535000
2025-10-02 09:51:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:51:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:51:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:51:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:51:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:51:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:51:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.127804, avg_loss=0.753195, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:51:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:51:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:51:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:51:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.500000, curr=0.400000
2025-10-02 09:51:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:51:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:51:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:51:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:51:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:51:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.859192, avg_loss=0.699296, seen=200, correct=102, accuracy=0.510000
2025-10-02 09:51:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:51:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:52:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:52:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:52:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:52:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.728218, avg_loss=0.743205, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:52:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:52:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:52:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.500000, curr=0.425000
2025-10-02 09:52:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:52:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:52:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:52:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:52:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:52:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:52:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.480194, avg_loss=0.697401, seen=200, correct=106, accuracy=0.530000
2025-10-02 09:52:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:52:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:52:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:52:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:52:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:52:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.486605, avg_loss=0.737165, seen=40, correct=15, accuracy=0.375000
2025-10-02 09:52:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:52:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:52:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:52:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.500000, curr=0.375000
2025-10-02 09:52:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:52:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:52:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:52:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:52:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:52:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:52:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.304962, avg_loss=0.696525, seen=200, correct=107, accuracy=0.535000
2025-10-02 09:52:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:52:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:52:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:52:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:52:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:52:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.554966, avg_loss=0.738874, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:52:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:52:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:52:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.500000, curr=0.425000
2025-10-02 09:52:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:52:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:52:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:52:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:52:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:52:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:52:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.658798, avg_loss=0.693294, seen=200, correct=106, accuracy=0.530000
2025-10-02 09:52:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:52:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:52:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:52:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:52:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:52:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.756548, avg_loss=0.718914, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:52:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:52:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:52:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:52:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:52:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.500000, curr=0.475000
2025-10-02 09:53:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:53:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:53:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:53:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:53:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:53:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.691650, avg_loss=0.698458, seen=200, correct=98, accuracy=0.490000
2025-10-02 09:53:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:53:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:53:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:53:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:53:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:53:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.573072, avg_loss=0.714327, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:53:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:53:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:53:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:53:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.500000, curr=0.500000
2025-10-02 09:53:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:53:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:53:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:53:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:53:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:53:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.388031, avg_loss=0.696940, seen=200, correct=99, accuracy=0.495000
2025-10-02 09:53:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:53:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:53:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:53:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:53:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:53:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.761410, avg_loss=0.719035, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:53:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:53:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:53:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:53:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.500000, curr=0.475000
2025-10-02 09:53:30 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:53:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:53:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:53:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:53:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2540MB allocated=2498MB
2025-10-02 09:53:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:53:31 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:53:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:53:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:53:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:31 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:53:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:53:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:53:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:53:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:53:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.129150, avg_loss=0.725646, seen=200, correct=105, accuracy=0.525000
2025-10-02 09:53:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:53:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:53:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2516MB allocated=2481MB
2025-10-02 09:53:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:53:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.932970, avg_loss=0.698324, seen=40, correct=23, accuracy=0.575000
2025-10-02 09:53:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:53:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:53:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2516MB allocated=2481MB
2025-10-02 09:53:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 09:53:42 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_037.ckpt
2025-10-02 09:53:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:53:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-02 09:53:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:53:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:53:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:53:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:53:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:53:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:53:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:53:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:53:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.386673, avg_loss=0.716933, seen=200, correct=98, accuracy=0.490000
2025-10-02 09:53:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:53:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:53:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:53:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:53:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:53:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:53:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.573751, avg_loss=0.714344, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:53:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:53:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:54:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:54:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.450000
2025-10-02 09:54:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:54:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:54:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:54:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:54:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:54:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=154.729584, avg_loss=0.773648, seen=200, correct=92, accuracy=0.460000
2025-10-02 09:54:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:54:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:54:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:54:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:54:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:54:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:54:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.349815, avg_loss=0.808745, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:54:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:54:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:54:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:54:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.425000
2025-10-02 09:54:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:54:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:54:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:54:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:54:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:54:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:54:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=145.499298, avg_loss=0.727496, seen=200, correct=93, accuracy=0.465000
2025-10-02 09:54:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:54:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:54:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:54:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:54:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:54:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:54:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.631187, avg_loss=0.740780, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:54:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:54:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:54:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.575000, curr=0.525000
2025-10-02 09:54:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:54:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:54:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:54:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:54:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:54:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:54:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.618958, avg_loss=0.698095, seen=200, correct=111, accuracy=0.555000
2025-10-02 09:54:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:54:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:54:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:54:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:54:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:54:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:54:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.606813, avg_loss=0.690170, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:54:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:54:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:54:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:54:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:54:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.575000, curr=0.450000
2025-10-02 09:55:03 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:55:03 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:55:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:55:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:55:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:55:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:55:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.234375, avg_loss=0.696172, seen=200, correct=110, accuracy=0.550000
2025-10-02 09:55:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:55:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:55:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:55:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:55:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.651897, avg_loss=0.691297, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:55:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:55:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:55:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.575000, curr=0.500000
2025-10-02 09:55:21 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:55:21 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:55:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:55:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:55:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:55:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.577591, avg_loss=0.702888, seen=200, correct=97, accuracy=0.485000
2025-10-02 09:55:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:55:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:55:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:55:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:55:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.476974, avg_loss=0.711924, seen=40, correct=18, accuracy=0.450000
2025-10-02 09:55:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:55:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:55:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:55:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.575000, curr=0.450000
2025-10-02 09:55:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:55:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:55:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:55:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:55:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:55:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:55:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.302704, avg_loss=0.696514, seen=200, correct=109, accuracy=0.545000
2025-10-02 09:55:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:55:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:55:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:55:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:55:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:55:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:55:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.795307, avg_loss=0.694883, seen=40, correct=15, accuracy=0.375000
2025-10-02 09:55:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:55:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:55:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:55:46 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.575000, curr=0.375000
2025-10-02 09:55:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:55:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:55:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:55:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:55:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:55:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:55:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:55:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.512268, avg_loss=0.692561, seen=200, correct=109, accuracy=0.545000
2025-10-02 09:55:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:56:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:56:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.807083, avg_loss=0.695177, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:56:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:56:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:56:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.575000, curr=0.500000
2025-10-02 09:56:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:56:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:56:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:56:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:56:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.617004, avg_loss=0.698085, seen=200, correct=99, accuracy=0.495000
2025-10-02 09:56:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:56:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:56:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:56:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:56:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.321455, avg_loss=0.708036, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:56:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:56:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:56:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.575000, curr=0.425000
2025-10-02 09:56:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:56:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:56:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:56:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 09:56:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 09:56:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.054611, avg_loss=0.700273, seen=200, correct=100, accuracy=0.500000
2025-10-02 09:56:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:56:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:56:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:56:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.451649, avg_loss=0.711291, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:56:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:56:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.575000, curr=0.475000
2025-10-02 09:56:39 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 09:56:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 09:56:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 09:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2560MB allocated=2514MB
2025-10-02 09:56:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 09:56:40 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #37', 'Round': 0, 'Results_raw': {}}
2025-10-02 09:56:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 09:56:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 09:56:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=112, num_train_batch_last_epoch=16, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:41 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 09:56:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:56:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 09:56:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=112, num_train_batch_last_epoch=16, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:56:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.891737, avg_loss=0.717431, seen=11, correct=6, accuracy=0.545455
2025-10-02 09:56:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2536MB allocated=2498MB
2025-10-02 09:56:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:56:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=112, num_train_batch_last_epoch=16, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:56:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.021839, avg_loss=0.775546, seen=40, correct=16, accuracy=0.400000
2025-10-02 09:56:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:56:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2536MB allocated=2498MB
2025-10-02 09:56:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.400000
2025-10-02 09:56:47 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_022.ckpt
2025-10-02 09:56:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 09:56:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-02 09:56:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 09:56:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=112, num_train_batch_last_epoch=16, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 09:56:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 09:56:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 09:56:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 09:56:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 09:56:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:56:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:56:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.043999, avg_loss=0.731273, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:56:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:56:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:56:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:56:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:56:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:57:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:57:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.543430, avg_loss=0.763586, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:57:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:57:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:57:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:57:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-02 09:57:03 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_022.ckpt
2025-10-02 09:57:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 09:57:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 09:57:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 09:57:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:57:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:57:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.766207, avg_loss=0.796928, seen=11, correct=6, accuracy=0.545455
2025-10-02 09:57:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:57:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:57:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:57:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:57:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.233820, avg_loss=0.730845, seen=40, correct=19, accuracy=0.475000
2025-10-02 09:57:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:57:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:57:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:57:19 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-02 09:57:19 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_022.ckpt
2025-10-02 09:57:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 09:57:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 09:57:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 09:57:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:57:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:57:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.710079, avg_loss=0.791825, seen=11, correct=6, accuracy=0.545455
2025-10-02 09:57:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:57:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:57:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:57:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:57:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:57:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.982122, avg_loss=0.724553, seen=40, correct=17, accuracy=0.425000
2025-10-02 09:57:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:57:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:57:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:57:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.475000, curr=0.425000
2025-10-02 09:57:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 09:57:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 09:57:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 09:57:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:57:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:57:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:57:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.034090, avg_loss=0.730372, seen=11, correct=4, accuracy=0.363636
2025-10-02 09:57:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:57:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:57:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:57:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:57:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:57:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:57:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.923006, avg_loss=0.723075, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:57:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:57:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:57:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:57:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:57:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-02 09:57:51 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_022.ckpt
2025-10-02 09:58:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 09:58:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 09:58:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 09:58:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:58:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:58:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.951746, avg_loss=0.722886, seen=11, correct=4, accuracy=0.363636
2025-10-02 09:58:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:58:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:58:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:58:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:58:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:58:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.481112, avg_loss=0.712028, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:58:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:58:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:58:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.550000, curr=0.525000
2025-10-02 09:58:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 09:58:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 09:58:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 09:58:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:58:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:58:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.943850, avg_loss=0.722168, seen=11, correct=4, accuracy=0.363636
2025-10-02 09:58:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:58:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:58:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:58:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:58:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.814024, avg_loss=0.720351, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:58:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:58:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:58:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:58:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.550000, curr=0.500000
2025-10-02 09:58:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 09:58:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 09:58:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 09:58:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:58:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:58:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.938727, avg_loss=0.721702, seen=11, correct=4, accuracy=0.363636
2025-10-02 09:58:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:58:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:58:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:58:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:58:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.293447, avg_loss=0.707336, seen=40, correct=21, accuracy=0.525000
2025-10-02 09:58:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:58:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:58:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.550000, curr=0.525000
2025-10-02 09:58:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 09:58:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 09:58:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 09:58:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:58:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:58:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:58:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.600702, avg_loss=0.690973, seen=11, correct=4, accuracy=0.363636
2025-10-02 09:58:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:58:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:58:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:58:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:58:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:58:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:58:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.434649, avg_loss=0.710866, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:58:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:58:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:58:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:58:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.550000, curr=0.500000
2025-10-02 09:58:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 09:58:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 09:58:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 09:58:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:58:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:58:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:58:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.610307, avg_loss=0.691846, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:58:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:59:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:59:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:59:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:59:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:59:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.514931, avg_loss=0.712873, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:59:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:59:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:59:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:59:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.550000, curr=0.500000
2025-10-02 09:59:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 09:59:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 09:59:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 09:59:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:59:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:59:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:59:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.545247, avg_loss=0.685932, seen=11, correct=5, accuracy=0.454545
2025-10-02 09:59:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:59:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:59:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:59:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:59:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.872341, avg_loss=0.696809, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:59:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:59:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:59:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:59:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.550000, curr=0.550000
2025-10-02 09:59:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 09:59:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 09:59:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 09:59:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:59:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:59:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.546865, avg_loss=0.686079, seen=11, correct=4, accuracy=0.363636
2025-10-02 09:59:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:59:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:59:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:59:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:59:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:59:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.320042, avg_loss=0.708001, seen=40, correct=20, accuracy=0.500000
2025-10-02 09:59:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:59:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:59:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:59:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.550000, curr=0.500000
2025-10-02 09:59:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 09:59:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 09:59:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 09:59:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:59:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:59:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:59:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.309722, avg_loss=0.664520, seen=11, correct=4, accuracy=0.363636
2025-10-02 09:59:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:59:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:59:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:59:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:59:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 09:59:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.445454, avg_loss=0.711136, seen=40, correct=22, accuracy=0.550000
2025-10-02 09:59:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:59:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:59:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:59:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.550000, curr=0.550000
2025-10-02 09:59:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 09:59:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 09:59:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 09:59:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 09:59:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 09:59:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 09:59:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.435463, avg_loss=0.675951, seen=11, correct=4, accuracy=0.363636
2025-10-02 09:59:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 09:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 09:59:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 09:59:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 09:59:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 09:59:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:00:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:00:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.393896, avg_loss=0.709847, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:00:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:00:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:00:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:00:02 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 10:00:02 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_022.ckpt
2025-10-02 10:00:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 10:00:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 10:00:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 10:00:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:00:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:00:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:00:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.526986, avg_loss=0.684271, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:00:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:00:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:00:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:00:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:00:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:00:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:00:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.147205, avg_loss=0.703680, seen=40, correct=24, accuracy=0.600000
2025-10-02 10:00:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:00:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:00:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:00:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 10:00:17 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_022.ckpt
2025-10-02 10:00:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 10:00:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 10:00:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 10:00:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:00:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:00:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:00:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.588986, avg_loss=0.689908, seen=11, correct=6, accuracy=0.545455
2025-10-02 10:00:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:00:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:00:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:00:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:00:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:00:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.583981, avg_loss=0.714600, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:00:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:00:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:00:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.550000
2025-10-02 10:00:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 10:00:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 10:00:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 10:00:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:00:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:00:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:00:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.551762, avg_loss=0.686524, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:00:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:00:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:00:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:00:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:00:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:00:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:00:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.184273, avg_loss=0.704607, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:00:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:00:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:00:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-02 10:00:46 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_022.ckpt
2025-10-02 10:00:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 10:00:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 10:00:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 10:00:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:00:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:00:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:00:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.328583, avg_loss=0.666235, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:00:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:00:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:00:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:00:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:00:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:00:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:01:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:01:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.983986, avg_loss=0.724600, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:01:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:01:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:01:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:01:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.650000, curr=0.550000
2025-10-02 10:01:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 10:01:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 10:01:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 10:01:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:01:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:01:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:01:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.840007, avg_loss=0.712728, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:01:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:01:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:01:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:01:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:01:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:01:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.950356, avg_loss=0.723759, seen=40, correct=24, accuracy=0.600000
2025-10-02 10:01:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:01:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:01:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:01:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.650000, curr=0.600000
2025-10-02 10:01:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 10:01:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 10:01:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 10:01:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:01:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:01:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.654690, avg_loss=0.786790, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:01:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:01:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:01:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:01:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:01:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:01:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.837584, avg_loss=0.720940, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:01:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:01:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:01:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:01:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.650000, curr=0.575000
2025-10-02 10:01:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 10:01:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 10:01:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 10:01:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:01:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:01:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:01:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=6.940399, avg_loss=0.630945, seen=11, correct=7, accuracy=0.636364
2025-10-02 10:01:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:01:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:01:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:01:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:01:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:01:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:01:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.918322, avg_loss=0.747958, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:01:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:01:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:01:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:01:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.650000, curr=0.550000
2025-10-02 10:01:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 10:01:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 10:01:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 10:01:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:01:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:01:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:01:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=6.951733, avg_loss=0.631976, seen=11, correct=8, accuracy=0.727273
2025-10-02 10:01:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:01:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:01:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:02:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:02:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:02:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:02:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.768261, avg_loss=0.744207, seen=40, correct=24, accuracy=0.600000
2025-10-02 10:02:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:02:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:02:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.650000, curr=0.600000
2025-10-02 10:02:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=220
2025-10-02 10:02:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=220
2025-10-02 10:02:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=220, splits=['val', 'test']
2025-10-02 10:02:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:02:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:02:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:02:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.819921, avg_loss=0.801811, seen=11, correct=4, accuracy=0.363636
2025-10-02 10:02:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:02:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:02:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:02:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:02:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:02:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:02:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.100704, avg_loss=0.727518, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:02:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:02:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:02:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:02:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.650000, curr=0.575000
2025-10-02 10:02:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=230
2025-10-02 10:02:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=230
2025-10-02 10:02:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=230, splits=['val', 'test']
2025-10-02 10:02:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:02:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:02:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:02:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.602797, avg_loss=0.691163, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:02:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:02:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:02:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:02:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:02:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:02:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:02:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.001003, avg_loss=0.775025, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:02:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:02:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:02:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.650000, curr=0.550000
2025-10-02 10:02:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=240
2025-10-02 10:02:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=240
2025-10-02 10:02:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=240, splits=['val', 'test']
2025-10-02 10:02:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:02:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:02:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:02:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=5.950319, avg_loss=0.540938, seen=11, correct=9, accuracy=0.818182
2025-10-02 10:02:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:02:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:02:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:02:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:02:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:02:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:02:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=34.405636, avg_loss=0.860141, seen=40, correct=25, accuracy=0.625000
2025-10-02 10:02:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:02:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:02:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:02:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.650000, curr=0.625000
2025-10-02 10:02:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=250
2025-10-02 10:02:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=250
2025-10-02 10:02:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=250, splits=['val', 'test']
2025-10-02 10:02:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:02:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:02:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:02:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=6.573978, avg_loss=0.597634, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:02:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:02:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:02:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:03:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:03:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:03:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=33.362907, avg_loss=0.834073, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:03:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:03:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:03:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.650000, curr=0.550000
2025-10-02 10:03:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=260
2025-10-02 10:03:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=260
2025-10-02 10:03:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=260, splits=['val', 'test']
2025-10-02 10:03:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:03:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:03:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.720898, avg_loss=0.701900, seen=11, correct=6, accuracy=0.545455
2025-10-02 10:03:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:03:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:03:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:03:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:03:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=32.715042, avg_loss=0.817876, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:03:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:03:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:03:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.650000, curr=0.650000
2025-10-02 10:03:18 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 10:03:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 10:03:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 10:03:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:03:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2606MB allocated=2531MB
2025-10-02 10:03:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 10:03:19 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {}}
2025-10-02 10:03:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:03:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 10:03:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:20 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 10:03:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 10:03:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:03:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:03:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=90.246597, avg_loss=0.716243, seen=126, correct=64, accuracy=0.507937
2025-10-02 10:03:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:03:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:03:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2556MB allocated=2514MB
2025-10-02 10:03:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:03:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:03:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.697920, avg_loss=0.617448, seen=40, correct=28, accuracy=0.700000
2025-10-02 10:03:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:03:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:03:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2556MB allocated=2514MB
2025-10-02 10:03:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-02 10:03:28 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_020.ckpt
2025-10-02 10:03:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 10:03:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-02 10:03:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 10:03:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 10:03:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 10:03:38 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 10:03:38 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 10:03:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 10:03:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:03:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:03:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=88.352211, avg_loss=0.701208, seen=126, correct=69, accuracy=0.547619
2025-10-02 10:03:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:03:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:03:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:03:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:03:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:03:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.955044, avg_loss=0.623876, seen=40, correct=24, accuracy=0.600000
2025-10-02 10:03:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:03:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:03:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:03:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.700000, curr=0.600000
2025-10-02 10:03:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 10:03:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 10:03:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 10:03:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:03:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=88.201508, avg_loss=0.700012, seen=126, correct=66, accuracy=0.523810
2025-10-02 10:03:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:03:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:03:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:03:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:03:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:03:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:03:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.081451, avg_loss=0.627036, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:03:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:03:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:04:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:04:01 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.700000, curr=0.575000
2025-10-02 10:04:11 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 10:04:11 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 10:04:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 10:04:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:04:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:04:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=88.320518, avg_loss=0.700956, seen=126, correct=66, accuracy=0.523810
2025-10-02 10:04:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:04:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:04:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:04:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:04:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:04:16 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.063904, avg_loss=0.626598, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:04:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:04:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:04:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:04:17 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.700000, curr=0.650000
2025-10-02 10:04:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 10:04:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 10:04:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 10:04:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:04:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:04:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=87.831177, avg_loss=0.697073, seen=126, correct=70, accuracy=0.555556
2025-10-02 10:04:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:04:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:04:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:04:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:04:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:04:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:04:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.109568, avg_loss=0.627739, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:04:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:04:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:04:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:04:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.700000, curr=0.650000
2025-10-02 10:04:43 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 10:04:43 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 10:04:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 10:04:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:04:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:04:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:04:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=89.322014, avg_loss=0.708905, seen=126, correct=69, accuracy=0.547619
2025-10-02 10:04:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:04:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:04:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:04:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:04:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:04:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:04:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.926962, avg_loss=0.673174, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:04:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:04:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:04:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:04:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:04:51 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.700000, curr=0.650000
2025-10-02 10:05:00 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 10:05:00 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 10:05:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 10:05:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:05:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:05:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=88.873779, avg_loss=0.705347, seen=126, correct=68, accuracy=0.539683
2025-10-02 10:05:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:05:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:05:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:05:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:05:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.286480, avg_loss=0.657162, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:05:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:05:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:05:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.700000, curr=0.650000
2025-10-02 10:05:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 10:05:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 10:05:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 10:05:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:05:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:05:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:05:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=87.464325, avg_loss=0.694161, seen=126, correct=73, accuracy=0.579365
2025-10-02 10:05:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:05:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:05:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:05:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:05:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:05:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:05:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.956581, avg_loss=0.648915, seen=40, correct=27, accuracy=0.675000
2025-10-02 10:05:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:05:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:05:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.700000, curr=0.675000
2025-10-02 10:05:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 10:05:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 10:05:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 10:05:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:05:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:05:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=87.188690, avg_loss=0.691974, seen=126, correct=68, accuracy=0.539683
2025-10-02 10:05:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:05:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:05:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:05:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:05:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.459225, avg_loss=0.636481, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:05:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:05:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:05:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.700000, curr=0.650000
2025-10-02 10:05:52 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 10:05:52 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 10:05:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 10:05:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:05:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:05:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:05:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=87.683235, avg_loss=0.695899, seen=126, correct=73, accuracy=0.579365
2025-10-02 10:05:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:05:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:05:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:05:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:05:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:05:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:05:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.010262, avg_loss=0.650257, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:05:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:05:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:05:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:05:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.700000, curr=0.650000
2025-10-02 10:06:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 10:06:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 10:06:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 10:06:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:06:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=87.298630, avg_loss=0.692846, seen=126, correct=67, accuracy=0.531746
2025-10-02 10:06:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:06:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:06:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:06:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:06:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.473074, avg_loss=0.636827, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:06:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:06:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:06:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.700000, curr=0.650000
2025-10-02 10:06:16 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 10:06:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 10:06:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 10:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:06:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2618MB allocated=2548MB
2025-10-02 10:06:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 10:06:17 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {}}
2025-10-02 10:06:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:06:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 10:06:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=605, num_train_batch_last_epoch=195, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:18 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 10:06:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:06:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 10:06:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=605, num_train_batch_last_epoch=195, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:06:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=53.251514, avg_loss=0.845262, seen=63, correct=23, accuracy=0.365079
2025-10-02 10:06:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:06:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2576MB allocated=2531MB
2025-10-02 10:06:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=605, num_train_batch_last_epoch=195, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:06:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.176966, avg_loss=0.779424, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:06:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:06:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:06:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2576MB allocated=2531MB
2025-10-02 10:06:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-02 10:06:24 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_010.ckpt
2025-10-02 10:06:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 10:06:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-02 10:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 10:06:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=605, num_train_batch_last_epoch=195, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 10:06:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 10:06:34 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 10:06:34 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 10:06:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 10:06:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:06:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=48.954212, avg_loss=0.777051, seen=63, correct=25, accuracy=0.396825
2025-10-02 10:06:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:06:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:06:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:06:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:06:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.982754, avg_loss=0.774569, seen=40, correct=13, accuracy=0.325000
2025-10-02 10:06:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:06:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:06:40 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.475000, curr=0.325000
2025-10-02 10:06:49 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 10:06:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 10:06:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 10:06:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:06:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:06:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=46.864750, avg_loss=0.743885, seen=63, correct=31, accuracy=0.492063
2025-10-02 10:06:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:06:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:06:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:06:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:06:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.891432, avg_loss=0.797286, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:06:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:06:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:06:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:06:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.475000, curr=0.425000
2025-10-02 10:07:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 10:07:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 10:07:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 10:07:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:07:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:07:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:07:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=48.560360, avg_loss=0.770799, seen=63, correct=25, accuracy=0.396825
2025-10-02 10:07:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:07:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:07:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:07:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:07:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.760014, avg_loss=0.769000, seen=40, correct=13, accuracy=0.325000
2025-10-02 10:07:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:07:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:07:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.475000, curr=0.325000
2025-10-02 10:07:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 10:07:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 10:07:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 10:07:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:07:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:07:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:07:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=47.881077, avg_loss=0.760017, seen=63, correct=27, accuracy=0.428571
2025-10-02 10:07:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:07:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:07:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:07:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:07:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:07:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:07:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.428440, avg_loss=0.760711, seen=40, correct=14, accuracy=0.350000
2025-10-02 10:07:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:07:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:07:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.475000, curr=0.350000
2025-10-02 10:07:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 10:07:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 10:07:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 10:07:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:07:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:07:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=48.298416, avg_loss=0.766642, seen=63, correct=23, accuracy=0.365079
2025-10-02 10:07:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:07:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:07:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:07:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:07:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:07:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:07:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.557238, avg_loss=0.763931, seen=40, correct=14, accuracy=0.350000
2025-10-02 10:07:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:07:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:07:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.475000, curr=0.350000
2025-10-02 10:07:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 10:07:49 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 10:07:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 10:07:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:07:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:07:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=46.945232, avg_loss=0.745162, seen=63, correct=23, accuracy=0.365079
2025-10-02 10:07:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:07:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:07:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:07:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:07:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:07:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:07:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.735291, avg_loss=0.743382, seen=40, correct=14, accuracy=0.350000
2025-10-02 10:07:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:07:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:07:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:07:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:07:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.475000, curr=0.350000
2025-10-02 10:08:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 10:08:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 10:08:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 10:08:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:08:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:08:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=46.324757, avg_loss=0.735314, seen=63, correct=28, accuracy=0.444444
2025-10-02 10:08:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:08:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:08:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:08:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:08:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:08:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.721130, avg_loss=0.743028, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:08:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:08:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:08:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:08:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.475000, curr=0.425000
2025-10-02 10:08:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 10:08:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 10:08:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 10:08:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:08:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:08:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:08:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=46.212627, avg_loss=0.733534, seen=63, correct=25, accuracy=0.396825
2025-10-02 10:08:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:08:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:08:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:08:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:08:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:08:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.656181, avg_loss=0.741405, seen=40, correct=14, accuracy=0.350000
2025-10-02 10:08:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:08:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:08:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.475000, curr=0.350000
2025-10-02 10:08:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 10:08:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 10:08:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 10:08:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:08:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:08:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:08:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=46.742035, avg_loss=0.741937, seen=63, correct=26, accuracy=0.412698
2025-10-02 10:08:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:08:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:08:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:08:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:08:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:08:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.475479, avg_loss=0.736887, seen=40, correct=16, accuracy=0.400000
2025-10-02 10:08:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:08:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:08:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:08:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.475000, curr=0.400000
2025-10-02 10:08:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 10:08:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 10:08:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 10:08:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:08:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:08:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=45.988243, avg_loss=0.729972, seen=63, correct=24, accuracy=0.380952
2025-10-02 10:08:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:08:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:08:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:08:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:08:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:08:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.420429, avg_loss=0.735511, seen=40, correct=12, accuracy=0.300000
2025-10-02 10:08:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:08:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:08:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:08:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.475000, curr=0.300000
2025-10-02 10:08:55 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 10:08:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 10:08:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 10:08:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:08:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2565MB
2025-10-02 10:08:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 10:08:56 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {}}
2025-10-02 10:08:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:08:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 10:08:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:08:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 10:08:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:08:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 10:08:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:08:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:08:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:09:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:09:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.468307, avg_loss=0.692342, seen=200, correct=117, accuracy=0.585000
2025-10-02 10:09:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:09:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:09:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2576MB allocated=2548MB
2025-10-02 10:09:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:09:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:09:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.004435, avg_loss=0.725111, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:09:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:09:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:09:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2576MB allocated=2548MB
2025-10-02 10:09:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 10:09:06 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_040.ckpt
2025-10-02 10:09:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 10:09:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-02 10:09:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 10:09:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:09:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 10:09:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 10:09:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 10:09:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 10:09:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 10:09:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:09:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:09:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.757080, avg_loss=0.693785, seen=200, correct=98, accuracy=0.490000
2025-10-02 10:09:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:09:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:09:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:09:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:09:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:09:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.380636, avg_loss=0.709516, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:09:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:09:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:09:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.475000
2025-10-02 10:09:33 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 10:09:33 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 10:09:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 10:09:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:09:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:09:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:09:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.505615, avg_loss=0.687528, seen=200, correct=99, accuracy=0.495000
2025-10-02 10:09:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:09:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:09:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:09:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:09:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:09:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:09:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.286106, avg_loss=0.707153, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:09:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:09:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:09:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:09:42 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.500000, curr=0.450000
2025-10-02 10:09:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 10:09:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 10:09:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 10:09:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:09:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:09:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:09:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.741074, avg_loss=0.683705, seen=200, correct=103, accuracy=0.515000
2025-10-02 10:09:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:09:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:09:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:09:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:09:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:09:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.086052, avg_loss=0.702151, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:09:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:09:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:09:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:09:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 10:09:59 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_040.ckpt
2025-10-02 10:10:08 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 10:10:08 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 10:10:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 10:10:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:10:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:10:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.117111, avg_loss=0.695586, seen=200, correct=105, accuracy=0.525000
2025-10-02 10:10:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:10:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:10:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:10:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:10:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.548359, avg_loss=0.713709, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:10:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:10:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:10:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.450000
2025-10-02 10:10:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 10:10:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 10:10:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 10:10:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:10:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:10:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.581314, avg_loss=0.677907, seen=200, correct=110, accuracy=0.550000
2025-10-02 10:10:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:10:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:10:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:10:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:10:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:10:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.035845, avg_loss=0.700896, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:10:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:10:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:10:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:10:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 10:10:33 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_040.ckpt
2025-10-02 10:10:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 10:10:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 10:10:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 10:10:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:10:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:10:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.453323, avg_loss=0.677267, seen=200, correct=113, accuracy=0.565000
2025-10-02 10:10:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:10:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:10:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:10:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:10:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.315437, avg_loss=0.707886, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:10:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:10:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:10:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:10:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.575000
2025-10-02 10:11:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 10:11:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 10:11:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 10:11:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:11:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:11:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.437347, avg_loss=0.677187, seen=200, correct=106, accuracy=0.530000
2025-10-02 10:11:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:11:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:11:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:11:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:11:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.930433, avg_loss=0.698261, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:11:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:11:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:11:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.550000
2025-10-02 10:11:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 10:11:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 10:11:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 10:11:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:11:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:11:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.976791, avg_loss=0.679884, seen=200, correct=103, accuracy=0.515000
2025-10-02 10:11:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:11:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:11:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:11:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:11:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:11:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.163593, avg_loss=0.704090, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:11:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:11:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:11:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.575000, curr=0.500000
2025-10-02 10:11:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 10:11:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 10:11:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 10:11:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:11:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:11:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.435822, avg_loss=0.687179, seen=200, correct=106, accuracy=0.530000
2025-10-02 10:11:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:11:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:11:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:11:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:11:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:11:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.992847, avg_loss=0.699821, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:11:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:11:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:11:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:11:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.575000, curr=0.525000
2025-10-02 10:11:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 10:11:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 10:11:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 10:11:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:11:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:11:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.547867, avg_loss=0.707739, seen=200, correct=104, accuracy=0.520000
2025-10-02 10:11:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:11:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:12:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:12:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:12:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:12:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:12:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.608641, avg_loss=0.715216, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:12:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:12:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:12:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.575000, curr=0.450000
2025-10-02 10:12:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 10:12:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 10:12:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 10:12:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:12:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:12:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:12:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.072708, avg_loss=0.705364, seen=200, correct=104, accuracy=0.520000
2025-10-02 10:12:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:12:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:12:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:12:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:12:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:12:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.481991, avg_loss=0.712050, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:12:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:12:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:12:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.575000, curr=0.425000
2025-10-02 10:12:29 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 10:12:29 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 10:12:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 10:12:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:12:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:12:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:12:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.826904, avg_loss=0.679135, seen=200, correct=115, accuracy=0.575000
2025-10-02 10:12:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:12:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:12:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:12:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:12:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.476000, avg_loss=0.686900, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:12:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:12:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:12:38 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.575000, curr=0.550000
2025-10-02 10:12:47 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 10:12:47 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 10:12:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 10:12:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:12:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:12:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.239655, avg_loss=0.676198, seen=200, correct=117, accuracy=0.585000
2025-10-02 10:12:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:12:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:12:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:12:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:12:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.286766, avg_loss=0.682169, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:12:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:12:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:12:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:12:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.575000, curr=0.575000
2025-10-02 10:13:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 10:13:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 10:13:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 10:13:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:13:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:13:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.579895, avg_loss=0.672899, seen=200, correct=114, accuracy=0.570000
2025-10-02 10:13:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:13:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:13:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:13:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:13:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:13:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.301691, avg_loss=0.682542, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:13:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:13:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:13:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:13:13 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.575000, curr=0.575000
2025-10-02 10:13:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 10:13:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 10:13:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 10:13:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:13:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:13:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:13:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.347321, avg_loss=0.676737, seen=200, correct=118, accuracy=0.590000
2025-10-02 10:13:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:13:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:13:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:13:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:13:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:13:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.512676, avg_loss=0.687817, seen=40, correct=24, accuracy=0.600000
2025-10-02 10:13:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:13:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:13:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-02 10:13:30 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_040.ckpt
2025-10-02 10:13:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 10:13:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 10:13:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 10:13:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:13:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:13:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:13:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.296646, avg_loss=0.676483, seen=200, correct=117, accuracy=0.585000
2025-10-02 10:13:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:13:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:13:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:13:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:13:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:13:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:13:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.565180, avg_loss=0.689129, seen=40, correct=24, accuracy=0.600000
2025-10-02 10:13:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:13:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:13:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.600000, curr=0.600000
2025-10-02 10:13:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 10:13:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 10:13:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 10:13:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:13:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:14:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:14:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.926941, avg_loss=0.674635, seen=200, correct=116, accuracy=0.580000
2025-10-02 10:14:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:14:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:14:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:14:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:14:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.520166, avg_loss=0.688004, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:14:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:14:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:14:07 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.600000, curr=0.575000
2025-10-02 10:14:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 10:14:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 10:14:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 10:14:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:14:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:14:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.089127, avg_loss=0.675446, seen=200, correct=106, accuracy=0.530000
2025-10-02 10:14:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:14:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:14:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:14:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:14:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.231205, avg_loss=0.680780, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:14:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:14:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:14:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.600000, curr=0.550000
2025-10-02 10:14:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 10:14:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 10:14:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 10:14:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:14:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:14:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:14:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.927658, avg_loss=0.679638, seen=200, correct=105, accuracy=0.525000
2025-10-02 10:14:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:14:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:14:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:14:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:14:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:14:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:14:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.336733, avg_loss=0.683418, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:14:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:14:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:14:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:14:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.600000, curr=0.550000
2025-10-02 10:14:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=200
2025-10-02 10:14:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=200
2025-10-02 10:14:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=200, splits=['val', 'test']
2025-10-02 10:14:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:14:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:14:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.604279, avg_loss=0.678021, seen=200, correct=117, accuracy=0.585000
2025-10-02 10:14:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:14:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:14:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:15:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:15:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:15:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:15:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:15:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.219736, avg_loss=0.680493, seen=40, correct=25, accuracy=0.625000
2025-10-02 10:15:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:15:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:15:03 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 10:15:03 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_040.ckpt
2025-10-02 10:15:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=210
2025-10-02 10:15:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=210
2025-10-02 10:15:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=210, splits=['val', 'test']
2025-10-02 10:15:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:15:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:15:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.107605, avg_loss=0.690538, seen=200, correct=107, accuracy=0.535000
2025-10-02 10:15:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:15:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:15:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:15:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:15:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.955359, avg_loss=0.698884, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:15:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:15:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:15:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.425000
2025-10-02 10:15:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=220
2025-10-02 10:15:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=220
2025-10-02 10:15:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=220, splits=['val', 'test']
2025-10-02 10:15:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:15:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:15:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:15:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.595337, avg_loss=0.677977, seen=200, correct=121, accuracy=0.605000
2025-10-02 10:15:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:15:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:15:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:15:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:15:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:15:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.389061, avg_loss=0.684727, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:15:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:15:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:15:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:15:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.575000
2025-10-02 10:15:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=230
2025-10-02 10:15:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=230
2025-10-02 10:15:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=230, splits=['val', 'test']
2025-10-02 10:15:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:15:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:15:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.349426, avg_loss=0.671747, seen=200, correct=113, accuracy=0.565000
2025-10-02 10:15:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:15:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:15:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:15:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:15:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.288925, avg_loss=0.682223, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:15:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:15:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:15:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:15:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.550000
2025-10-02 10:16:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=240
2025-10-02 10:16:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=240
2025-10-02 10:16:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=240, splits=['val', 'test']
2025-10-02 10:16:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:16:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:16:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.238007, avg_loss=0.676190, seen=200, correct=112, accuracy=0.560000
2025-10-02 10:16:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:16:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:16:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:16:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:16:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:16:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.594954, avg_loss=0.689874, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:16:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:16:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:16:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:16:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.575000
2025-10-02 10:16:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=250
2025-10-02 10:16:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=250
2025-10-02 10:16:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=250, splits=['val', 'test']
2025-10-02 10:16:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:16:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:16:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.144699, avg_loss=0.685723, seen=200, correct=112, accuracy=0.560000
2025-10-02 10:16:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:16:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:16:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:16:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:16:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:16:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.949083, avg_loss=0.698727, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:16:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:16:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:16:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.475000
2025-10-02 10:16:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=260
2025-10-02 10:16:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=260
2025-10-02 10:16:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=260, splits=['val', 'test']
2025-10-02 10:16:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:16:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:16:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:16:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.674210, avg_loss=0.688371, seen=200, correct=112, accuracy=0.560000
2025-10-02 10:16:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:16:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:16:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:16:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:16:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:16:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.995487, avg_loss=0.699887, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:16:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:16:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:16:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:16:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.450000
2025-10-02 10:16:54 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=270
2025-10-02 10:16:54 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=270
2025-10-02 10:16:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=270, splits=['val', 'test']
2025-10-02 10:16:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:16:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:16:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.770386, avg_loss=0.673852, seen=200, correct=117, accuracy=0.585000
2025-10-02 10:16:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:16:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:16:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:16:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:16:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:17:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:17:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.510223, avg_loss=0.687756, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:17:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:17:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:17:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:17:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.550000
2025-10-02 10:17:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=280
2025-10-02 10:17:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=280
2025-10-02 10:17:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=280, splits=['val', 'test']
2025-10-02 10:17:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:17:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:17:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:17:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.355789, avg_loss=0.671779, seen=200, correct=116, accuracy=0.580000
2025-10-02 10:17:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:17:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:17:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:17:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:17:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.359612, avg_loss=0.683990, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:17:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:17:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:17:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.625000, curr=0.575000
2025-10-02 10:17:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=290
2025-10-02 10:17:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=290
2025-10-02 10:17:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=290, splits=['val', 'test']
2025-10-02 10:17:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:17:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:17:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.948700, avg_loss=0.684743, seen=200, correct=115, accuracy=0.575000
2025-10-02 10:17:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:17:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:17:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:17:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:17:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.007601, avg_loss=0.700190, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:17:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:17:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:17:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.625000, curr=0.500000
2025-10-02 10:17:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=300
2025-10-02 10:17:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=300
2025-10-02 10:17:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=300, splits=['val', 'test']
2025-10-02 10:17:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:17:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:17:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:17:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.771667, avg_loss=0.673858, seen=200, correct=116, accuracy=0.580000
2025-10-02 10:17:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:17:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:17:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:17:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:17:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:17:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:17:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.552248, avg_loss=0.688806, seen=40, correct=25, accuracy=0.625000
2025-10-02 10:17:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:17:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:17:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:17:54 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.625000, curr=0.625000
2025-10-02 10:17:54 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 10:17:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 10:17:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 10:17:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:17:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2622MB allocated=2582MB
2025-10-02 10:17:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 10:17:55 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #40', 'Round': 0, 'Results_raw': {}}
2025-10-02 10:17:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:17:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 10:17:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:17:55 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 10:17:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 10:17:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:17:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:17:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:17:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=97.797638, avg_loss=0.735321, seen=133, correct=71, accuracy=0.533835
2025-10-02 10:17:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:17:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:18:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2596MB allocated=2565MB
2025-10-02 10:18:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:18:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:18:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.165619, avg_loss=0.754140, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:18:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:18:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:18:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2596MB allocated=2565MB
2025-10-02 10:18:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 10:18:05 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_050.ckpt
2025-10-02 10:18:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 10:18:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-02 10:18:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 10:18:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:18:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 10:18:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 10:18:15 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 10:18:15 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 10:18:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 10:18:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:18:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:18:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=93.740540, avg_loss=0.704816, seen=133, correct=66, accuracy=0.496241
2025-10-02 10:18:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:18:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:18:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:18:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:18:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.528267, avg_loss=0.738207, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:18:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:18:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:18:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:18:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.475000
2025-10-02 10:18:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 10:18:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 10:18:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 10:18:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:18:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:18:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:18:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=96.578476, avg_loss=0.726154, seen=133, correct=71, accuracy=0.533835
2025-10-02 10:18:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:18:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:18:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:18:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:18:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.318239, avg_loss=0.757956, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:18:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:18:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:18:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.525000, curr=0.450000
2025-10-02 10:18:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 10:18:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 10:18:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 10:18:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:18:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:18:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=94.369362, avg_loss=0.709544, seen=133, correct=72, accuracy=0.541353
2025-10-02 10:18:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:18:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:18:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:18:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:18:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.306925, avg_loss=0.732673, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:18:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:18:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:18:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:18:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.525000, curr=0.425000
2025-10-02 10:19:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 10:19:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 10:19:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 10:19:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:19:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:19:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:19:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=93.276001, avg_loss=0.701323, seen=133, correct=66, accuracy=0.496241
2025-10-02 10:19:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:19:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:19:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:19:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:19:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:19:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.805500, avg_loss=0.720138, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:19:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:19:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:19:08 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.525000, curr=0.450000
2025-10-02 10:19:16 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 10:19:16 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 10:19:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 10:19:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:19:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:19:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:19:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=92.644844, avg_loss=0.696578, seen=133, correct=66, accuracy=0.496241
2025-10-02 10:19:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:19:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:19:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:19:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:19:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:19:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:19:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.018456, avg_loss=0.725461, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:19:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:19:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:19:24 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.525000, curr=0.475000
2025-10-02 10:19:32 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 10:19:32 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 10:19:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 10:19:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:19:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:19:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=92.755035, avg_loss=0.697406, seen=133, correct=67, accuracy=0.503759
2025-10-02 10:19:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:19:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:19:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:19:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:19:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:19:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:19:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.722422, avg_loss=0.718061, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:19:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:19:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:19:39 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.525000, curr=0.425000
2025-10-02 10:19:48 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 10:19:48 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 10:19:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 10:19:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:19:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:19:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:19:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=92.836922, avg_loss=0.698022, seen=133, correct=65, accuracy=0.488722
2025-10-02 10:19:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:19:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:19:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:19:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:19:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:19:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.896948, avg_loss=0.722424, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:19:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:19:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:19:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:19:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:19:55 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.525000, curr=0.475000
2025-10-02 10:20:04 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 10:20:04 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 10:20:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 10:20:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:20:06 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=92.214653, avg_loss=0.693343, seen=133, correct=70, accuracy=0.526316
2025-10-02 10:20:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:20:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:20:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:20:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:20:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:20:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.992222, avg_loss=0.724806, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:20:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:20:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:20:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:20:11 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.525000, curr=0.500000
2025-10-02 10:20:20 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 10:20:20 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 10:20:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 10:20:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:20:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:20:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=92.925774, avg_loss=0.698690, seen=133, correct=73, accuracy=0.548872
2025-10-02 10:20:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:20:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:20:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:20:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:20:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.160900, avg_loss=0.729023, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:20:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:20:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:20:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.525000, curr=0.450000
2025-10-02 10:20:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 10:20:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 10:20:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 10:20:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:20:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=92.917641, avg_loss=0.698629, seen=133, correct=70, accuracy=0.526316
2025-10-02 10:20:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:20:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:20:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:20:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.849184, avg_loss=0.721230, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:20:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:20:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:20:45 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.525000, curr=0.525000
2025-10-02 10:20:45 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 10:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 10:20:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 10:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:20:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2642MB allocated=2598MB
2025-10-02 10:20:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 10:20:46 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #50', 'Round': 0, 'Results_raw': {}}
2025-10-02 10:20:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:20:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 10:20:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=107, num_train_batch_last_epoch=51, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:47 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 10:20:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 10:20:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:20:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=107, num_train_batch_last_epoch=51, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:20:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.162638, avg_loss=0.832967, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:20:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:20:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:20:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2616MB allocated=2582MB
2025-10-02 10:20:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:20:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=107, num_train_batch_last_epoch=51, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:20:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=34.393097, avg_loss=0.859827, seen=40, correct=13, accuracy=0.325000
2025-10-02 10:20:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:20:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2616MB allocated=2582MB
2025-10-02 10:20:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.325000
2025-10-02 10:20:52 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_004.ckpt
2025-10-02 10:20:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 10:20:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-02 10:20:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:20:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 10:20:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=107, num_train_batch_last_epoch=51, num_train_epoch=8, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:20:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 10:20:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 10:21:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 10:21:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 10:21:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 10:21:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:21:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:21:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:21:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.808264, avg_loss=0.891660, seen=11, correct=1, accuracy=0.090909
2025-10-02 10:21:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:21:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:21:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:21:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:21:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:21:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:21:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.250870, avg_loss=0.706272, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:21:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:21:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:21:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:21:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 10:21:08 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_004.ckpt
2025-10-02 10:21:17 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 10:21:17 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 10:21:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 10:21:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:21:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:21:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:21:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.700339, avg_loss=0.881849, seen=11, correct=2, accuracy=0.181818
2025-10-02 10:21:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:21:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:21:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:21:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:21:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:21:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:21:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.915392, avg_loss=0.697885, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:21:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:21:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:21:22 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.500000
2025-10-02 10:21:31 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 10:21:31 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 10:21:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 10:21:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:21:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:21:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.365110, avg_loss=0.851374, seen=11, correct=1, accuracy=0.090909
2025-10-02 10:21:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:21:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:21:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:21:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:21:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.506365, avg_loss=0.712659, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:21:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:21:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:21:36 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.500000, curr=0.425000
2025-10-02 10:21:44 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 10:21:44 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 10:21:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 10:21:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:21:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:21:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:21:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.161781, avg_loss=0.832889, seen=11, correct=1, accuracy=0.090909
2025-10-02 10:21:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:21:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:21:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:21:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:21:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:21:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.271694, avg_loss=0.706792, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:21:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:21:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:21:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:21:50 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.500000, curr=0.475000
2025-10-02 10:21:59 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 10:21:59 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 10:21:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 10:22:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:22:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:22:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.069952, avg_loss=0.824541, seen=11, correct=1, accuracy=0.090909
2025-10-02 10:22:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:22:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:22:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:22:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:22:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.920992, avg_loss=0.698025, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:22:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:22:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:22:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-02 10:22:05 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_004.ckpt
2025-10-02 10:22:13 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 10:22:13 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 10:22:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 10:22:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:22:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.699570, avg_loss=0.790870, seen=11, correct=2, accuracy=0.181818
2025-10-02 10:22:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:22:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:22:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:22:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.831476, avg_loss=0.695787, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:22:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:22:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:22:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.525000, curr=0.500000
2025-10-02 10:22:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 10:22:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 10:22:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 10:22:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:22:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:22:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.593655, avg_loss=0.781241, seen=11, correct=1, accuracy=0.090909
2025-10-02 10:22:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:22:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:22:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:22:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:22:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.828979, avg_loss=0.695724, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:22:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:22:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:22:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.525000, curr=0.475000
2025-10-02 10:22:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 10:22:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 10:22:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 10:22:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:22:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:22:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.634991, avg_loss=0.784999, seen=11, correct=1, accuracy=0.090909
2025-10-02 10:22:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:22:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:22:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:22:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:22:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.285423, avg_loss=0.682136, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:22:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:22:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:22:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.525000, curr=0.525000
2025-10-02 10:22:55 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 10:22:55 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 10:22:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 10:22:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:22:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:22:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.403114, avg_loss=0.763919, seen=11, correct=2, accuracy=0.181818
2025-10-02 10:22:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:22:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:22:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:22:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:22:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:22:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.448925, avg_loss=0.686223, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:22:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:22:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:23:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:23:00 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 10:23:00 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_004.ckpt
2025-10-02 10:23:10 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 10:23:10 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 10:23:10 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 10:23:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:23:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:23:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.981870, avg_loss=0.725625, seen=11, correct=4, accuracy=0.363636
2025-10-02 10:23:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:23:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:23:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:23:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:23:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.791294, avg_loss=0.694782, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:23:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:23:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:23:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.575000
2025-10-02 10:23:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 10:23:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 10:23:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 10:23:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:23:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:23:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.105089, avg_loss=0.736826, seen=11, correct=6, accuracy=0.545455
2025-10-02 10:23:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:23:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:23:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:23:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:23:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.779417, avg_loss=0.694485, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:23:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:23:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:23:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:23:29 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.550000
2025-10-02 10:23:39 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 10:23:39 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 10:23:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 10:23:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:23:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:23:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.196372, avg_loss=0.745125, seen=11, correct=4, accuracy=0.363636
2025-10-02 10:23:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:23:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:23:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:23:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:23:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.211029, avg_loss=0.680276, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:23:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:23:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:23:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.575000, curr=0.575000
2025-10-02 10:23:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 10:23:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 10:23:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 10:23:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:23:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:23:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.058899, avg_loss=0.732627, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:23:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:23:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:23:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:23:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:23:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.589788, avg_loss=0.689745, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:23:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:23:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:23:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:23:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.575000, curr=0.550000
2025-10-02 10:24:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 10:24:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 10:24:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 10:24:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:24:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:24:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:24:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.936628, avg_loss=0.721512, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:24:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:24:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:24:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:24:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:24:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:24:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.520817, avg_loss=0.713020, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:24:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:24:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:24:12 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.575000, curr=0.575000
2025-10-02 10:24:22 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 10:24:22 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 10:24:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 10:24:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:24:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:24:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:24:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.093597, avg_loss=0.735782, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:24:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:24:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:24:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:24:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:24:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:24:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.593042, avg_loss=0.714826, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:24:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:24:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:24:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:24:28 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.575000, curr=0.550000
2025-10-02 10:24:37 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 10:24:37 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 10:24:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 10:24:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:24:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:24:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:24:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.100341, avg_loss=0.736395, seen=11, correct=6, accuracy=0.545455
2025-10-02 10:24:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:24:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:24:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:24:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:24:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:24:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.526485, avg_loss=0.713162, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:24:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:24:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:24:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:24:44 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.575000, curr=0.550000
2025-10-02 10:24:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 10:24:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 10:24:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 10:24:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:24:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:24:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:24:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.144403, avg_loss=0.740400, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:24:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:24:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:24:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:24:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:24:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:24:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:24:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.997620, avg_loss=0.724940, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:24:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:24:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:24:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:24:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:24:59 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.575000, curr=0.525000
2025-10-02 10:25:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 10:25:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 10:25:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 10:25:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:25:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.771359, avg_loss=0.706487, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:25:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:25:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:25:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:25:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:25:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:25:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.436628, avg_loss=0.735916, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:25:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:25:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:25:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:25:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.575000, curr=0.525000
2025-10-02 10:25:25 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 10:25:25 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 10:25:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 10:25:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:25:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:25:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.836800, avg_loss=0.712436, seen=11, correct=5, accuracy=0.454545
2025-10-02 10:25:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:25:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:25:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:25:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:25:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:25:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.164722, avg_loss=0.754118, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:25:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:25:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:25:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:25:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.575000, curr=0.500000
2025-10-02 10:25:30 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 10:25:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 10:25:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 10:25:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:25:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2674MB allocated=2615MB
2025-10-02 10:25:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 10:25:31 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {}}
2025-10-02 10:25:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:25:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 10:25:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:32 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 10:25:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 10:25:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:25:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:25:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=107.220184, avg_loss=0.734385, seen=146, correct=77, accuracy=0.527397
2025-10-02 10:25:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:25:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2598MB
2025-10-02 10:25:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:25:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:25:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.671253, avg_loss=0.741781, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:25:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:25:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2636MB allocated=2598MB
2025-10-02 10:25:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-02 10:25:41 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_001.ckpt
2025-10-02 10:25:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 10:25:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-02 10:25:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 10:25:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 10:25:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 10:25:50 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 10:25:50 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 10:25:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 10:25:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:25:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:25:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=102.999405, avg_loss=0.705475, seen=146, correct=78, accuracy=0.534247
2025-10-02 10:25:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:25:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:25:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:25:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:25:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:25:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:25:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.431797, avg_loss=0.710795, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:25:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:25:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:25:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:25:58 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.575000, curr=0.550000
2025-10-02 10:26:07 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 10:26:07 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 10:26:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 10:26:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:26:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:26:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:26:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=101.003525, avg_loss=0.691805, seen=146, correct=86, accuracy=0.589041
2025-10-02 10:26:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:26:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:26:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:26:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:26:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:26:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.813395, avg_loss=0.720335, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:26:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:26:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:26:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:26:14 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.575000, curr=0.475000
2025-10-02 10:26:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 10:26:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 10:26:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 10:26:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:26:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:26:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:26:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=101.205765, avg_loss=0.693190, seen=146, correct=85, accuracy=0.582192
2025-10-02 10:26:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:26:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:26:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:26:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:26:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:26:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:26:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.458431, avg_loss=0.711461, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:26:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:26:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:26:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:26:31 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.575000, curr=0.450000
2025-10-02 10:26:41 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 10:26:41 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 10:26:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 10:26:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:26:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:26:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:26:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=101.147659, avg_loss=0.692792, seen=146, correct=84, accuracy=0.575342
2025-10-02 10:26:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:26:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:26:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:26:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:26:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:26:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.526819, avg_loss=0.713170, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:26:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:26:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:26:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.575000, curr=0.475000
2025-10-02 10:26:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 10:26:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 10:26:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 10:26:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:26:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:26:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:26:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:26:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=101.312958, avg_loss=0.693924, seen=146, correct=84, accuracy=0.575342
2025-10-02 10:26:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:26:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:27:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:27:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:27:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:27:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:27:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.358246, avg_loss=0.708956, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:27:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:27:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:27:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.575000, curr=0.525000
2025-10-02 10:27:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 10:27:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 10:27:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 10:27:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:27:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:27:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:27:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=100.342712, avg_loss=0.687279, seen=146, correct=85, accuracy=0.582192
2025-10-02 10:27:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:27:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:27:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:27:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:27:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:27:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.952078, avg_loss=0.698802, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:27:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:27:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:27:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:27:20 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.575000, curr=0.450000
2025-10-02 10:27:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 10:27:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 10:27:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 10:27:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:27:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:27:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:27:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=99.794983, avg_loss=0.683527, seen=146, correct=85, accuracy=0.582192
2025-10-02 10:27:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:27:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:27:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:27:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:27:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:27:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:27:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.201160, avg_loss=0.705029, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:27:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:27:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:27:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:27:35 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.575000, curr=0.525000
2025-10-02 10:27:45 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 10:27:45 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 10:27:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 10:27:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:27:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:27:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:27:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=100.384995, avg_loss=0.687568, seen=146, correct=83, accuracy=0.568493
2025-10-02 10:27:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:27:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:27:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:27:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:27:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:27:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:27:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.107845, avg_loss=0.702696, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:27:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:27:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:27:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:27:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:27:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.575000, curr=0.500000
2025-10-02 10:28:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 10:28:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 10:28:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 10:28:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:28:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=100.356285, avg_loss=0.687372, seen=146, correct=87, accuracy=0.595890
2025-10-02 10:28:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:28:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:28:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:28:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.922966, avg_loss=0.698074, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:28:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:28:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:28:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:28:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.575000, curr=0.525000
2025-10-02 10:28:19 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 10:28:19 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 10:28:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 10:28:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:28:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:28:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=100.225044, avg_loss=0.686473, seen=146, correct=70, accuracy=0.479452
2025-10-02 10:28:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:28:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:28:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:28:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:28:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:28:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.179295, avg_loss=0.704482, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:28:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:28:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:28:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:28:26 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.575000, curr=0.550000
2025-10-02 10:28:26 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 10:28:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 10:28:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 10:28:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:28:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2682MB allocated=2632MB
2025-10-02 10:28:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 10:28:27 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {}}
2025-10-02 10:28:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:28:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 10:28:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=440, num_train_batch_last_epoch=360, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:28 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 10:28:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:28:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 10:28:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:28:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=440, num_train_batch_last_epoch=360, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:28:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=34.529030, avg_loss=0.750631, seen=46, correct=20, accuracy=0.434783
2025-10-02 10:28:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:28:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:28:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2656MB allocated=2615MB
2025-10-02 10:28:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:28:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=440, num_train_batch_last_epoch=360, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:28:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.251173, avg_loss=0.656279, seen=40, correct=25, accuracy=0.625000
2025-10-02 10:28:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:28:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:28:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2656MB allocated=2615MB
2025-10-02 10:28:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-02 10:28:33 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_048.ckpt
2025-10-02 10:28:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 10:28:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-02 10:28:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 10:28:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=440, num_train_batch_last_epoch=360, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 10:28:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 10:28:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 10:28:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 10:28:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 10:28:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:28:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:28:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=33.495720, avg_loss=0.728168, seen=46, correct=22, accuracy=0.478261
2025-10-02 10:28:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:28:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:28:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:28:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:28:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:28:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.790825, avg_loss=0.669771, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:28:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:28:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:28:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:28:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.625000, curr=0.550000
2025-10-02 10:28:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 10:28:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 10:28:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 10:28:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:28:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:28:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:28:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.974352, avg_loss=0.695095, seen=46, correct=23, accuracy=0.500000
2025-10-02 10:28:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:28:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:29:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:29:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:29:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:29:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:29:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.222923, avg_loss=0.730573, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:29:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:29:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:29:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:29:03 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.625000, curr=0.525000
2025-10-02 10:29:12 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 10:29:12 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 10:29:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 10:29:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:29:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:29:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.767628, avg_loss=0.690601, seen=46, correct=26, accuracy=0.565217
2025-10-02 10:29:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:29:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:29:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:29:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:29:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:29:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.510609, avg_loss=0.712765, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:29:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:29:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:29:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:29:18 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.625000, curr=0.475000
2025-10-02 10:29:27 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 10:29:27 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 10:29:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 10:29:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:29:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:29:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.585630, avg_loss=0.686644, seen=46, correct=26, accuracy=0.565217
2025-10-02 10:29:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:29:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:29:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:29:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:29:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:29:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.334583, avg_loss=0.708365, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:29:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:29:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:29:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:29:33 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.625000, curr=0.525000
2025-10-02 10:29:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 10:29:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 10:29:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 10:29:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:29:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:29:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:29:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.271133, avg_loss=0.679807, seen=46, correct=23, accuracy=0.500000
2025-10-02 10:29:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:29:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:29:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:29:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:29:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:29:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:29:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.390219, avg_loss=0.734755, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:29:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:29:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:29:48 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.625000, curr=0.425000
2025-10-02 10:29:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 10:29:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 10:29:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 10:29:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:29:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:29:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:29:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.308922, avg_loss=0.680629, seen=46, correct=23, accuracy=0.500000
2025-10-02 10:29:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:29:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:30:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:30:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:30:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:30:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:30:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.623215, avg_loss=0.715580, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:30:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:30:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:30:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:30:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.625000, curr=0.500000
2025-10-02 10:30:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 10:30:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 10:30:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 10:30:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:30:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:30:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:30:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.739851, avg_loss=0.689997, seen=46, correct=23, accuracy=0.500000
2025-10-02 10:30:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:30:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:30:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:30:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:30:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:30:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:30:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.088732, avg_loss=0.677218, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:30:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:30:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:30:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:30:19 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.625000, curr=0.550000
2025-10-02 10:30:28 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 10:30:28 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 10:30:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 10:30:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:30:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:30:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:30:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.809790, avg_loss=0.691517, seen=46, correct=24, accuracy=0.521739
2025-10-02 10:30:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:30:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:30:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:30:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:30:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:30:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:30:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.768272, avg_loss=0.669207, seen=40, correct=25, accuracy=0.625000
2025-10-02 10:30:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:30:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:30:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:30:34 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.625000, curr=0.625000
2025-10-02 10:30:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 10:30:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 10:30:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 10:30:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:30:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:30:43 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.634766, avg_loss=0.687712, seen=46, correct=25, accuracy=0.543478
2025-10-02 10:30:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:30:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:30:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:30:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:30:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:30:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:30:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.060932, avg_loss=0.676523, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:30:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:30:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:30:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:30:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.625000, curr=0.550000
2025-10-02 10:30:57 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 10:30:57 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 10:30:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 10:30:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:30:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:30:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:30:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.074305, avg_loss=0.675528, seen=46, correct=24, accuracy=0.521739
2025-10-02 10:30:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:30:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:30:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:30:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:31:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.909473, avg_loss=0.722737, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:31:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:31:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:31:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:31:02 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.625000, curr=0.425000
2025-10-02 10:31:02 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 10:31:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 10:31:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 10:31:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:31:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2722MB allocated=2649MB
2025-10-02 10:31:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 10:31:03 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #48', 'Round': 0, 'Results_raw': {}}
2025-10-02 10:31:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:31:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-02 10:31:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:03 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-02 10:31:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:31:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-02 10:31:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:31:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:31:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=82.807846, avg_loss=0.828078, seen=100, correct=37, accuracy=0.370000
2025-10-02 10:31:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:31:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:31:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2676MB allocated=2632MB
2025-10-02 10:31:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:31:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:31:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.461647, avg_loss=0.786541, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:31:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:31:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2676MB allocated=2632MB
2025-10-02 10:31:12 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-02 10:31:12 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_045.ckpt
2025-10-02 10:31:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-02 10:31:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-02 10:31:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-02 10:31:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=800, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=800, grad_accum_step=2 (=> total micro-batches = 1600)
2025-10-02 10:31:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=800, grad_accum_step=2, will_run_step(loops)=1600
2025-10-02 10:31:23 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-10-02 10:31:23 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-10-02 10:31:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-10-02 10:31:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:31:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:31:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=77.748741, avg_loss=0.777487, seen=100, correct=40, accuracy=0.400000
2025-10-02 10:31:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:31:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:31:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:31:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:31:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:31:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.059589, avg_loss=0.751490, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:31:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:31:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:31:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:31:30 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.475000, curr=0.425000
2025-10-02 10:31:40 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-10-02 10:31:40 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-10-02 10:31:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-10-02 10:31:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:31:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:31:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=77.204193, avg_loss=0.772042, seen=100, correct=41, accuracy=0.410000
2025-10-02 10:31:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:31:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:31:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:31:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:31:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:31:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.037319, avg_loss=0.750933, seen=40, correct=16, accuracy=0.400000
2025-10-02 10:31:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:31:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:31:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:31:47 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.475000, curr=0.400000
2025-10-02 10:31:56 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-10-02 10:31:56 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-10-02 10:31:56 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-10-02 10:31:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:31:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:31:58 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=77.943947, avg_loss=0.779439, seen=100, correct=38, accuracy=0.380000
2025-10-02 10:31:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:31:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:31:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:32:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:32:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:32:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:32:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:32:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.585585, avg_loss=0.764640, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:32:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:32:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:32:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:32:04 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.475000, curr=0.425000
2025-10-02 10:32:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-10-02 10:32:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-10-02 10:32:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-10-02 10:32:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:32:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:32:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:32:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=75.609253, avg_loss=0.756093, seen=100, correct=41, accuracy=0.410000
2025-10-02 10:32:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:32:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:32:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:32:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:32:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:32:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:32:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.513611, avg_loss=0.737840, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:32:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:32:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:32:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:32:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.475000, curr=0.450000
2025-10-02 10:32:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-10-02 10:32:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-10-02 10:32:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-10-02 10:32:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:32:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:32:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=72.469536, avg_loss=0.724695, seen=100, correct=47, accuracy=0.470000
2025-10-02 10:32:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:32:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:32:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:32:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:32:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:32:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:32:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.499065, avg_loss=0.737477, seen=40, correct=16, accuracy=0.400000
2025-10-02 10:32:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:32:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:32:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:32:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.475000, curr=0.400000
2025-10-02 10:32:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-10-02 10:32:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-10-02 10:32:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-10-02 10:32:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:32:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:32:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=72.779770, avg_loss=0.727798, seen=100, correct=46, accuracy=0.460000
2025-10-02 10:32:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:32:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:32:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:32:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:32:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:32:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:32:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.326374, avg_loss=0.733159, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:32:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:32:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:32:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:32:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:32:53 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.475000, curr=0.425000
2025-10-02 10:33:01 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-10-02 10:33:01 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-10-02 10:33:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-10-02 10:33:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:33:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:33:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:33:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=74.667709, avg_loss=0.746677, seen=100, correct=43, accuracy=0.430000
2025-10-02 10:33:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:33:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:33:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:33:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:33:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:33:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:33:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.596668, avg_loss=0.739917, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:33:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:33:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:33:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:33:10 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.475000, curr=0.425000
2025-10-02 10:33:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-10-02 10:33:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-10-02 10:33:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-10-02 10:33:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:33:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:33:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:33:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=76.472725, avg_loss=0.764727, seen=100, correct=38, accuracy=0.380000
2025-10-02 10:33:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:33:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:33:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:33:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:33:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:33:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:33:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.075695, avg_loss=0.751892, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:33:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:33:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:33:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:33:27 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.475000, curr=0.475000
2025-10-02 10:33:36 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-10-02 10:33:36 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-10-02 10:33:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-10-02 10:33:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:33:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:33:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:33:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=74.830986, avg_loss=0.748310, seen=100, correct=41, accuracy=0.410000
2025-10-02 10:33:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:33:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:33:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:33:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:33:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:33:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:33:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.191004, avg_loss=0.729775, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:33:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:33:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:33:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-02 10:33:43 (federatedscope.llm.trainer.trainer:1463) INFO: [local-only] saved(best) -> ./checkpoints_1.0_local_only/local_only_tldr_choice_qwen_client_045.ckpt
2025-10-02 10:33:53 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-10-02 10:33:53 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-10-02 10:33:53 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-10-02 10:33:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:33:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:33:54 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=70.337662, avg_loss=0.703377, seen=100, correct=52, accuracy=0.520000
2025-10-02 10:33:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:33:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:33:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:33:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:33:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:33:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:33:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.026672, avg_loss=0.725667, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:33:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:33:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:33:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:34:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:34:00 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=1/10), best=0.500000, curr=0.450000
2025-10-02 10:34:09 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=110
2025-10-02 10:34:09 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=110
2025-10-02 10:34:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=110, splits=['val', 'test']
2025-10-02 10:34:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:34:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:34:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:34:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=70.958954, avg_loss=0.709590, seen=100, correct=50, accuracy=0.500000
2025-10-02 10:34:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:34:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:34:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:34:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:34:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.768124, avg_loss=0.719203, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:34:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:34:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:34:16 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=2/10), best=0.500000, curr=0.450000
2025-10-02 10:34:26 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=120
2025-10-02 10:34:26 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=120
2025-10-02 10:34:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=120, splits=['val', 'test']
2025-10-02 10:34:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:34:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:34:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=72.155640, avg_loss=0.721556, seen=100, correct=43, accuracy=0.430000
2025-10-02 10:34:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:34:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:34:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:34:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:34:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:34:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:34:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.104427, avg_loss=0.727611, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:34:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:34:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:34:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:34:32 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=3/10), best=0.500000, curr=0.475000
2025-10-02 10:34:42 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=130
2025-10-02 10:34:42 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=130
2025-10-02 10:34:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=130, splits=['val', 'test']
2025-10-02 10:34:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:34:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:34:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:34:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=73.831787, avg_loss=0.738318, seen=100, correct=38, accuracy=0.380000
2025-10-02 10:34:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:34:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:34:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:34:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:34:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:34:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.892664, avg_loss=0.722317, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:34:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:34:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:34:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:34:49 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=4/10), best=0.500000, curr=0.500000
2025-10-02 10:34:58 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=140
2025-10-02 10:34:58 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=140
2025-10-02 10:34:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=140, splits=['val', 'test']
2025-10-02 10:34:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:34:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:34:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:34:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:34:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=76.478996, avg_loss=0.764790, seen=100, correct=37, accuracy=0.370000
2025-10-02 10:34:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:34:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:35:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:35:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:35:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:35:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:35:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.626421, avg_loss=0.740661, seen=40, correct=16, accuracy=0.400000
2025-10-02 10:35:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:35:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:35:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:35:05 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=5/10), best=0.500000, curr=0.400000
2025-10-02 10:35:14 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=150
2025-10-02 10:35:14 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=150
2025-10-02 10:35:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=150, splits=['val', 'test']
2025-10-02 10:35:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:35:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:35:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:35:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=77.844482, avg_loss=0.778445, seen=100, correct=37, accuracy=0.370000
2025-10-02 10:35:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:35:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:35:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:35:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:35:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:35:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.904417, avg_loss=0.747610, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:35:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:35:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:35:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:35:21 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=6/10), best=0.500000, curr=0.425000
2025-10-02 10:35:30 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=160
2025-10-02 10:35:30 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=160
2025-10-02 10:35:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=160, splits=['val', 'test']
2025-10-02 10:35:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:35:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:35:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:35:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=72.346680, avg_loss=0.723467, seen=100, correct=41, accuracy=0.410000
2025-10-02 10:35:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:35:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:35:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:35:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:35:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:35:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:35:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.936928, avg_loss=0.723423, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:35:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:35:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:35:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:35:37 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=7/10), best=0.500000, curr=0.450000
2025-10-02 10:35:46 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=170
2025-10-02 10:35:46 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=170
2025-10-02 10:35:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=170, splits=['val', 'test']
2025-10-02 10:35:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:35:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:35:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:35:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=69.967865, avg_loss=0.699679, seen=100, correct=50, accuracy=0.500000
2025-10-02 10:35:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:35:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:35:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:35:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:35:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:35:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.836267, avg_loss=0.720907, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:35:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:35:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:35:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:35:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:35:52 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=8/10), best=0.500000, curr=0.425000
2025-10-02 10:36:02 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=180
2025-10-02 10:36:02 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=180
2025-10-02 10:36:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=180, splits=['val', 'test']
2025-10-02 10:36:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:36:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:36:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=70.734634, avg_loss=0.707346, seen=100, correct=50, accuracy=0.500000
2025-10-02 10:36:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:36:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:36:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:36:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.724859, avg_loss=0.718121, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:36:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:36:09 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=9/10), best=0.500000, curr=0.450000
2025-10-02 10:36:18 (federatedscope.llm.trainer.trainer:1128) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=190
2025-10-02 10:36:18 (federatedscope.llm.trainer.trainer:1242) INFO: [MID-EVAL-TRIGGER] at optimizer_step=190
2025-10-02 10:36:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=190, splits=['val', 'test']
2025-10-02 10:36:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:36:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:36:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=73.323936, avg_loss=0.733239, seen=100, correct=40, accuracy=0.400000
2025-10-02 10:36:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:36:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:36:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:36:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.885946, avg_loss=0.722149, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:36:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:36:25 (federatedscope.llm.trainer.trainer:1795) INFO: [EarlyStop] no improvement (wait=10/10), best=0.500000, curr=0.475000
2025-10-02 10:36:25 (federatedscope.llm.trainer.trainer:1800) INFO: [EarlyStop] patience reached -> request stop
2025-10-02 10:36:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-10-02 10:36:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-10-02 10:36:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2728MB allocated=2665MB
2025-10-02 10:36:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-10-02 10:36:26 (federatedscope.core.workers.client:642) INFO: {'Role': 'Client #45', 'Round': 0, 'Results_raw': {}}
2025-10-02 10:36:26 (federatedscope.core.workers.server:546) INFO: [in-place aggregation 시작] round=0
2025-10-02 10:36:26 (federatedscope.core.workers.server:433) INFO: Server: Training is finished! Starting evaluation.
2025-10-02 10:36:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:36:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-02 10:36:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:36:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=100.076492, avg_loss=0.685455, seen=146, correct=82, accuracy=0.561644
2025-10-02 10:36:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:31 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 146, 'val_loss': 100.07649230957031, 'val_avg_loss': 0.6854554267778789, 'val_seen': 146, 'val_correct': 82, 'val_acc': 0.5616438356164384}
2025-10-02 10:36:31 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:36:31 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 146, 'val_loss': 100.07649230957031, 'val_avg_loss': 0.6854554267778789, 'val_seen': 146, 'val_correct': 82, 'val_acc': 0.5616438356164384}
2025-10-02 10:36:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 37, 'val_loss': 24.733186960220337, 'val_avg_loss': 0.6684645124383874, 'val_seen': 37, 'val_correct': 22, 'val_acc': 0.5945945945945946}}
2025-10-02 10:36:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 146, 'val_loss': 100.07649230957031, 'val_avg_loss': 0.6854554267778789, 'val_seen': 146, 'val_correct': 82, 'val_acc': 0.5616438356164384}}
2025-10-02 10:36:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:36:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.621729, avg_loss=0.690543, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:36:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:33 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.621728897094727, 'test_avg_loss': 0.6905432224273682, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:36:33 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 146, 'val_loss': 100.07649230957031, 'val_avg_loss': 0.6854554267778789, 'val_seen': 146, 'val_correct': 82, 'val_acc': 0.5616438356164384}
2025-10-02 10:36:33 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.621728897094727, 'test_avg_loss': 0.6905432224273682, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:36:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.013804078102112, 'test_avg_loss': 0.7013804078102112, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:36:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.621728897094727, 'test_avg_loss': 0.6905432224273682, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-10-02 10:36:33 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.621728897094727, 'test_avg_loss': 0.6905432224273682, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 146, 'val_loss': 100.07649230957031, 'val_avg_loss': 0.6854554267778789, 'val_seen': 146, 'val_correct': 82, 'val_acc': 0.5616438356164384, 'test_total': 40, 'test_loss': 27.621728897094727, 'test_avg_loss': 0.6905432224273682, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:36:33 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:36:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:36:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:36:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:36:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.780365, avg_loss=0.707306, seen=11, correct=4, accuracy=0.363636
2025-10-02 10:36:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:35 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 7.780364990234375, 'val_avg_loss': 0.707305908203125, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-10-02 10:36:35 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:36:35 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 7.780364990234375, 'val_avg_loss': 0.707305908203125, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-10-02 10:36:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 2.521131992340088, 'val_avg_loss': 0.8403773307800293, 'val_seen': 3, 'val_correct': 0, 'val_acc': 0.0}}
2025-10-02 10:36:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 7.780364990234375, 'val_avg_loss': 0.707305908203125, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}}
2025-10-02 10:36:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:36:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:36:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.851814, avg_loss=0.721295, seen=40, correct=14, accuracy=0.350000
2025-10-02 10:36:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:39 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.85181427001953, 'test_avg_loss': 0.7212953567504883, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-10-02 10:36:39 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 7.780364990234375, 'val_avg_loss': 0.707305908203125, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-10-02 10:36:39 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.85181427001953, 'test_avg_loss': 0.7212953567504883, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-10-02 10:36:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.6271281242370605, 'test_avg_loss': 0.762712812423706, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-10-02 10:36:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.85181427001953, 'test_avg_loss': 0.7212953567504883, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}}
2025-10-02 10:36:39 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.85181427001953, 'test_avg_loss': 0.7212953567504883, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}, metrics={'val_total': 11, 'val_loss': 7.780364990234375, 'val_avg_loss': 0.707305908203125, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365, 'test_total': 40, 'test_loss': 28.85181427001953, 'test_avg_loss': 0.7212953567504883, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-10-02 10:36:39 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:36:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:36:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-02 10:36:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-02 10:36:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=25.014095, avg_loss=0.694836, seen=36, correct=23, accuracy=0.638889
2025-10-02 10:36:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:41 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 36, 'val_loss': 25.014095306396484, 'val_avg_loss': 0.6948359807332357, 'val_seen': 36, 'val_correct': 23, 'val_acc': 0.6388888888888888}
2025-10-02 10:36:41 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:36:41 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 36, 'val_loss': 25.014095306396484, 'val_avg_loss': 0.6948359807332357, 'val_seen': 36, 'val_correct': 23, 'val_acc': 0.6388888888888888}
2025-10-02 10:36:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 9, 'val_loss': 6.78961455821991, 'val_avg_loss': 0.7544016175799899, 'val_seen': 9, 'val_correct': 5, 'val_acc': 0.5555555555555556}}
2025-10-02 10:36:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 36, 'val_loss': 25.014095306396484, 'val_avg_loss': 0.6948359807332357, 'val_seen': 36, 'val_correct': 23, 'val_acc': 0.6388888888888888}}
2025-10-02 10:36:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:36:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:36:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.327202, avg_loss=0.683180, seen=40, correct=24, accuracy=0.600000
2025-10-02 10:36:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:43 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.32720184326172, 'test_avg_loss': 0.6831800460815429, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-10-02 10:36:43 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 36, 'val_loss': 25.014095306396484, 'val_avg_loss': 0.6948359807332357, 'val_seen': 36, 'val_correct': 23, 'val_acc': 0.6388888888888888}
2025-10-02 10:36:43 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.32720184326172, 'test_avg_loss': 0.6831800460815429, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-10-02 10:36:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.205371022224426, 'test_avg_loss': 0.7205371022224426, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:36:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.32720184326172, 'test_avg_loss': 0.6831800460815429, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-10-02 10:36:43 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.32720184326172, 'test_avg_loss': 0.6831800460815429, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 36, 'val_loss': 25.014095306396484, 'val_avg_loss': 0.6948359807332357, 'val_seen': 36, 'val_correct': 23, 'val_acc': 0.6388888888888888, 'test_total': 40, 'test_loss': 27.32720184326172, 'test_avg_loss': 0.6831800460815429, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-10-02 10:36:43 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:36:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:36:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:36:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:36:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.377499, avg_loss=0.761591, seen=11, correct=4, accuracy=0.363636
2025-10-02 10:36:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:45 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 8.377498626708984, 'val_avg_loss': 0.7615907842462714, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-10-02 10:36:45 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:36:45 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 8.377498626708984, 'val_avg_loss': 0.7615907842462714, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-10-02 10:36:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 2.731881022453308, 'val_avg_loss': 0.910627007484436, 'val_seen': 3, 'val_correct': 0, 'val_acc': 0.0}}
2025-10-02 10:36:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 8.377498626708984, 'val_avg_loss': 0.7615907842462714, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}}
2025-10-02 10:36:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:36:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:36:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.328142, avg_loss=0.733204, seen=40, correct=15, accuracy=0.375000
2025-10-02 10:36:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:48 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.328142166137695, 'test_avg_loss': 0.7332035541534424, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-10-02 10:36:48 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 8.377498626708984, 'val_avg_loss': 0.7615907842462714, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-10-02 10:36:48 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.328142166137695, 'test_avg_loss': 0.7332035541534424, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-10-02 10:36:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.920728921890259, 'test_avg_loss': 0.7920728921890259, 'test_seen': 10, 'test_correct': 2, 'test_acc': 0.2}}
2025-10-02 10:36:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.328142166137695, 'test_avg_loss': 0.7332035541534424, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}}
2025-10-02 10:36:48 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.328142166137695, 'test_avg_loss': 0.7332035541534424, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}, metrics={'val_total': 11, 'val_loss': 8.377498626708984, 'val_avg_loss': 0.7615907842462714, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365, 'test_total': 40, 'test_loss': 29.328142166137695, 'test_avg_loss': 0.7332035541534424, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-10-02 10:36:48 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:36:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:36:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-02 10:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-02 10:36:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=8.996771, avg_loss=0.642626, seen=14, correct=9, accuracy=0.642857
2025-10-02 10:36:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:50 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 14, 'val_loss': 8.996770858764648, 'val_avg_loss': 0.6426264899117606, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-10-02 10:36:50 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:36:50 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 14, 'val_loss': 8.996770858764648, 'val_avg_loss': 0.6426264899117606, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-10-02 10:36:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 4, 'val_loss': 2.6225205659866333, 'val_avg_loss': 0.6556301414966583, 'val_seen': 4, 'val_correct': 2, 'val_acc': 0.5}}
2025-10-02 10:36:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 14, 'val_loss': 8.996770858764648, 'val_avg_loss': 0.6426264899117606, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}}
2025-10-02 10:36:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:36:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:36:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.097933, avg_loss=0.702448, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:36:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:53 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.097932815551758, 'test_avg_loss': 0.7024483203887939, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:36:53 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 14, 'val_loss': 8.996770858764648, 'val_avg_loss': 0.6426264899117606, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-10-02 10:36:53 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.097932815551758, 'test_avg_loss': 0.7024483203887939, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:36:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.879466652870178, 'test_avg_loss': 0.6879466652870179, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:36:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.097932815551758, 'test_avg_loss': 0.7024483203887939, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-10-02 10:36:53 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.097932815551758, 'test_avg_loss': 0.7024483203887939, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 14, 'val_loss': 8.996770858764648, 'val_avg_loss': 0.6426264899117606, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429, 'test_total': 40, 'test_loss': 28.097932815551758, 'test_avg_loss': 0.7024483203887939, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:36:53 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:36:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:36:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-02 10:36:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:36:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=92.972244, avg_loss=0.693823, seen=134, correct=70, accuracy=0.522388
2025-10-02 10:36:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:36:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:36:57 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 134, 'val_loss': 92.97224426269531, 'val_avg_loss': 0.6938227183783232, 'val_seen': 134, 'val_correct': 70, 'val_acc': 0.5223880597014925}
2025-10-02 10:36:57 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:36:57 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 134, 'val_loss': 92.97224426269531, 'val_avg_loss': 0.6938227183783232, 'val_seen': 134, 'val_correct': 70, 'val_acc': 0.5223880597014925}
2025-10-02 10:36:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 22.633156776428223, 'val_avg_loss': 0.6656810816596536, 'val_seen': 34, 'val_correct': 22, 'val_acc': 0.6470588235294118}}
2025-10-02 10:36:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 134, 'val_loss': 92.97224426269531, 'val_avg_loss': 0.6938227183783232, 'val_seen': 134, 'val_correct': 70, 'val_acc': 0.5223880597014925}}
2025-10-02 10:36:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:36:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:36:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.528620, avg_loss=0.688215, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:36:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:36:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:36:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:01 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.52861976623535, 'test_avg_loss': 0.6882154941558838, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:37:01 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 134, 'val_loss': 92.97224426269531, 'val_avg_loss': 0.6938227183783232, 'val_seen': 134, 'val_correct': 70, 'val_acc': 0.5223880597014925}
2025-10-02 10:37:01 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.52861976623535, 'test_avg_loss': 0.6882154941558838, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:37:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.44156014919281, 'test_avg_loss': 0.744156014919281, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:37:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.52861976623535, 'test_avg_loss': 0.6882154941558838, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-10-02 10:37:01 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.52861976623535, 'test_avg_loss': 0.6882154941558838, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 134, 'val_loss': 92.97224426269531, 'val_avg_loss': 0.6938227183783232, 'val_seen': 134, 'val_correct': 70, 'val_acc': 0.5223880597014925, 'test_total': 40, 'test_loss': 27.52861976623535, 'test_avg_loss': 0.6882154941558838, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:37:01 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:37:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:37:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-02 10:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-02 10:37:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.517853, avg_loss=0.675752, seen=57, correct=31, accuracy=0.543860
2025-10-02 10:37:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:04 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 57, 'val_loss': 38.517852783203125, 'val_avg_loss': 0.6757518032140899, 'val_seen': 57, 'val_correct': 31, 'val_acc': 0.543859649122807}
2025-10-02 10:37:04 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:37:04 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 57, 'val_loss': 38.517852783203125, 'val_avg_loss': 0.6757518032140899, 'val_seen': 57, 'val_correct': 31, 'val_acc': 0.543859649122807}
2025-10-02 10:37:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 15, 'val_loss': 10.205955982208252, 'val_avg_loss': 0.6803970654805501, 'val_seen': 15, 'val_correct': 8, 'val_acc': 0.5333333333333333}}
2025-10-02 10:37:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 57, 'val_loss': 38.517852783203125, 'val_avg_loss': 0.6757518032140899, 'val_seen': 57, 'val_correct': 31, 'val_acc': 0.543859649122807}}
2025-10-02 10:37:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:37:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:37:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.379833, avg_loss=0.709496, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:37:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:07 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.379833221435547, 'test_avg_loss': 0.7094958305358887, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:37:07 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 57, 'val_loss': 38.517852783203125, 'val_avg_loss': 0.6757518032140899, 'val_seen': 57, 'val_correct': 31, 'val_acc': 0.543859649122807}
2025-10-02 10:37:07 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.379833221435547, 'test_avg_loss': 0.7094958305358887, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:37:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.005095481872559, 'test_avg_loss': 0.7005095481872559, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:37:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.379833221435547, 'test_avg_loss': 0.7094958305358887, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-10-02 10:37:07 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.379833221435547, 'test_avg_loss': 0.7094958305358887, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 57, 'val_loss': 38.517852783203125, 'val_avg_loss': 0.6757518032140899, 'val_seen': 57, 'val_correct': 31, 'val_acc': 0.543859649122807, 'test_total': 40, 'test_loss': 28.379833221435547, 'test_avg_loss': 0.7094958305358887, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:37:07 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:37:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:37:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-02 10:37:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 10:37:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.227890, avg_loss=0.713448, seen=69, correct=39, accuracy=0.565217
2025-10-02 10:37:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:10 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 69, 'val_loss': 49.22789001464844, 'val_avg_loss': 0.7134476813717164, 'val_seen': 69, 'val_correct': 39, 'val_acc': 0.5652173913043478}
2025-10-02 10:37:10 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:37:10 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 69, 'val_loss': 49.22789001464844, 'val_avg_loss': 0.7134476813717164, 'val_seen': 69, 'val_correct': 39, 'val_acc': 0.5652173913043478}
2025-10-02 10:37:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 18, 'val_loss': 12.807422995567322, 'val_avg_loss': 0.7115234997537401, 'val_seen': 18, 'val_correct': 10, 'val_acc': 0.5555555555555556}}
2025-10-02 10:37:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 69, 'val_loss': 49.22789001464844, 'val_avg_loss': 0.7134476813717164, 'val_seen': 69, 'val_correct': 39, 'val_acc': 0.5652173913043478}}
2025-10-02 10:37:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:37:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:37:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.291874, avg_loss=0.682297, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:37:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:13 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.291873931884766, 'test_avg_loss': 0.6822968482971191, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:37:13 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 69, 'val_loss': 49.22789001464844, 'val_avg_loss': 0.7134476813717164, 'val_seen': 69, 'val_correct': 39, 'val_acc': 0.5652173913043478}
2025-10-02 10:37:13 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.291873931884766, 'test_avg_loss': 0.6822968482971191, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:37:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.248284816741943, 'test_avg_loss': 0.7248284816741943, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:37:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.291873931884766, 'test_avg_loss': 0.6822968482971191, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-10-02 10:37:13 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.291873931884766, 'test_avg_loss': 0.6822968482971191, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 69, 'val_loss': 49.22789001464844, 'val_avg_loss': 0.7134476813717164, 'val_seen': 69, 'val_correct': 39, 'val_acc': 0.5652173913043478, 'test_total': 40, 'test_loss': 27.291873931884766, 'test_avg_loss': 0.6822968482971191, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:37:13 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:37:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:37:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 10:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 10:37:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=132.600113, avg_loss=0.705320, seen=188, correct=94, accuracy=0.500000
2025-10-02 10:37:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:20 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 188, 'val_loss': 132.60011291503906, 'val_avg_loss': 0.7053197495480801, 'val_seen': 188, 'val_correct': 94, 'val_acc': 0.5}
2025-10-02 10:37:20 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:37:20 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 188, 'val_loss': 132.60011291503906, 'val_avg_loss': 0.7053197495480801, 'val_seen': 188, 'val_correct': 94, 'val_acc': 0.5}
2025-10-02 10:37:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 47, 'val_loss': 32.59909039735794, 'val_avg_loss': 0.6935976680288923, 'val_seen': 47, 'val_correct': 25, 'val_acc': 0.5319148936170213}}
2025-10-02 10:37:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 188, 'val_loss': 132.60011291503906, 'val_avg_loss': 0.7053197495480801, 'val_seen': 188, 'val_correct': 94, 'val_acc': 0.5}}
2025-10-02 10:37:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:37:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:37:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.168304, avg_loss=0.729208, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:37:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:23 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.168304443359375, 'test_avg_loss': 0.7292076110839844, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:37:23 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 188, 'val_loss': 132.60011291503906, 'val_avg_loss': 0.7053197495480801, 'val_seen': 188, 'val_correct': 94, 'val_acc': 0.5}
2025-10-02 10:37:23 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.168304443359375, 'test_avg_loss': 0.7292076110839844, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:37:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.555516719818115, 'test_avg_loss': 0.7555516719818115, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-10-02 10:37:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.168304443359375, 'test_avg_loss': 0.7292076110839844, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-10-02 10:37:23 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.168304443359375, 'test_avg_loss': 0.7292076110839844, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 188, 'val_loss': 132.60011291503906, 'val_avg_loss': 0.7053197495480801, 'val_seen': 188, 'val_correct': 94, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 29.168304443359375, 'test_avg_loss': 0.7292076110839844, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:37:23 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:37:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:37:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-02 10:37:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-02 10:37:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=46.735973, avg_loss=0.741841, seen=63, correct=25, accuracy=0.396825
2025-10-02 10:37:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:26 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 63, 'val_loss': 46.7359733581543, 'val_avg_loss': 0.7418408469548301, 'val_seen': 63, 'val_correct': 25, 'val_acc': 0.3968253968253968}
2025-10-02 10:37:26 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:37:26 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 63, 'val_loss': 46.7359733581543, 'val_avg_loss': 0.7418408469548301, 'val_seen': 63, 'val_correct': 25, 'val_acc': 0.3968253968253968}
2025-10-02 10:37:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 16, 'val_loss': 11.212374687194824, 'val_avg_loss': 0.7007734179496765, 'val_seen': 16, 'val_correct': 8, 'val_acc': 0.5}}
2025-10-02 10:37:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 63, 'val_loss': 46.7359733581543, 'val_avg_loss': 0.7418408469548301, 'val_seen': 63, 'val_correct': 25, 'val_acc': 0.3968253968253968}}
2025-10-02 10:37:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:37:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:37:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.038601, avg_loss=0.725965, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:37:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:29 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.03860092163086, 'test_avg_loss': 0.7259650230407715, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:37:29 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 63, 'val_loss': 46.7359733581543, 'val_avg_loss': 0.7418408469548301, 'val_seen': 63, 'val_correct': 25, 'val_acc': 0.3968253968253968}
2025-10-02 10:37:29 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.03860092163086, 'test_avg_loss': 0.7259650230407715, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:37:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.509626388549805, 'test_avg_loss': 0.7509626388549805, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:37:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.03860092163086, 'test_avg_loss': 0.7259650230407715, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-10-02 10:37:29 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.03860092163086, 'test_avg_loss': 0.7259650230407715, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 63, 'val_loss': 46.7359733581543, 'val_avg_loss': 0.7418408469548301, 'val_seen': 63, 'val_correct': 25, 'val_acc': 0.3968253968253968, 'test_total': 40, 'test_loss': 29.03860092163086, 'test_avg_loss': 0.7259650230407715, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:37:29 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:37:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:37:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-02 10:37:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 10:37:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=22.505182, avg_loss=0.703287, seen=32, correct=14, accuracy=0.437500
2025-10-02 10:37:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:32 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 32, 'val_loss': 22.50518226623535, 'val_avg_loss': 0.7032869458198547, 'val_seen': 32, 'val_correct': 14, 'val_acc': 0.4375}
2025-10-02 10:37:32 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:37:32 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 32, 'val_loss': 22.50518226623535, 'val_avg_loss': 0.7032869458198547, 'val_seen': 32, 'val_correct': 14, 'val_acc': 0.4375}
2025-10-02 10:37:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 8, 'val_loss': 5.66430127620697, 'val_avg_loss': 0.7080376595258713, 'val_seen': 8, 'val_correct': 3, 'val_acc': 0.375}}
2025-10-02 10:37:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 32, 'val_loss': 22.50518226623535, 'val_avg_loss': 0.7032869458198547, 'val_seen': 32, 'val_correct': 14, 'val_acc': 0.4375}}
2025-10-02 10:37:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:37:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:37:33 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.323547, avg_loss=0.683089, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:37:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:35 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.32354736328125, 'test_avg_loss': 0.6830886840820313, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:37:35 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 32, 'val_loss': 22.50518226623535, 'val_avg_loss': 0.7032869458198547, 'val_seen': 32, 'val_correct': 14, 'val_acc': 0.4375}
2025-10-02 10:37:35 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.32354736328125, 'test_avg_loss': 0.6830886840820313, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:37:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.117604315280914, 'test_avg_loss': 0.6117604315280915, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-10-02 10:37:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.32354736328125, 'test_avg_loss': 0.6830886840820313, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-10-02 10:37:35 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.32354736328125, 'test_avg_loss': 0.6830886840820313, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 32, 'val_loss': 22.50518226623535, 'val_avg_loss': 0.7032869458198547, 'val_seen': 32, 'val_correct': 14, 'val_acc': 0.4375, 'test_total': 40, 'test_loss': 27.32354736328125, 'test_avg_loss': 0.6830886840820313, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:37:35 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:37:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:37:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-02 10:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-02 10:37:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=95.204163, avg_loss=0.694921, seen=137, correct=80, accuracy=0.583942
2025-10-02 10:37:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:39 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 137, 'val_loss': 95.20416259765625, 'val_avg_loss': 0.6949208948734032, 'val_seen': 137, 'val_correct': 80, 'val_acc': 0.583941605839416}
2025-10-02 10:37:39 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:37:39 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 137, 'val_loss': 95.20416259765625, 'val_avg_loss': 0.6949208948734032, 'val_seen': 137, 'val_correct': 80, 'val_acc': 0.583941605839416}
2025-10-02 10:37:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 35, 'val_loss': 24.02350091934204, 'val_avg_loss': 0.6863857405526298, 'val_seen': 35, 'val_correct': 21, 'val_acc': 0.6}}
2025-10-02 10:37:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 137, 'val_loss': 95.20416259765625, 'val_avg_loss': 0.6949208948734032, 'val_seen': 137, 'val_correct': 80, 'val_acc': 0.583941605839416}}
2025-10-02 10:37:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:37:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:37:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.205326, avg_loss=0.705133, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:37:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:42 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.205326080322266, 'test_avg_loss': 0.7051331520080566, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:37:42 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 137, 'val_loss': 95.20416259765625, 'val_avg_loss': 0.6949208948734032, 'val_seen': 137, 'val_correct': 80, 'val_acc': 0.583941605839416}
2025-10-02 10:37:42 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.205326080322266, 'test_avg_loss': 0.7051331520080566, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:37:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.488656044006348, 'test_avg_loss': 0.6488656044006348, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-10-02 10:37:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.205326080322266, 'test_avg_loss': 0.7051331520080566, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}}
2025-10-02 10:37:42 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.205326080322266, 'test_avg_loss': 0.7051331520080566, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}, metrics={'val_total': 137, 'val_loss': 95.20416259765625, 'val_avg_loss': 0.6949208948734032, 'val_seen': 137, 'val_correct': 80, 'val_acc': 0.583941605839416, 'test_total': 40, 'test_loss': 28.205326080322266, 'test_avg_loss': 0.7051331520080566, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:37:42 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:37:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:37:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-02 10:37:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-02 10:37:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=50.332672, avg_loss=0.699065, seen=72, correct=37, accuracy=0.513889
2025-10-02 10:37:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:46 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 72, 'val_loss': 50.332672119140625, 'val_avg_loss': 0.6990648905436198, 'val_seen': 72, 'val_correct': 37, 'val_acc': 0.5138888888888888}
2025-10-02 10:37:46 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:37:46 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 72, 'val_loss': 50.332672119140625, 'val_avg_loss': 0.6990648905436198, 'val_seen': 72, 'val_correct': 37, 'val_acc': 0.5138888888888888}
2025-10-02 10:37:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 18, 'val_loss': 12.267355918884277, 'val_avg_loss': 0.6815197732713487, 'val_seen': 18, 'val_correct': 8, 'val_acc': 0.4444444444444444}}
2025-10-02 10:37:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 72, 'val_loss': 50.332672119140625, 'val_avg_loss': 0.6990648905436198, 'val_seen': 72, 'val_correct': 37, 'val_acc': 0.5138888888888888}}
2025-10-02 10:37:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:37:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.089165, avg_loss=0.702229, seen=40, correct=16, accuracy=0.400000
2025-10-02 10:37:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:49 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.08916473388672, 'test_avg_loss': 0.702229118347168, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:37:49 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 72, 'val_loss': 50.332672119140625, 'val_avg_loss': 0.6990648905436198, 'val_seen': 72, 'val_correct': 37, 'val_acc': 0.5138888888888888}
2025-10-02 10:37:49 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.08916473388672, 'test_avg_loss': 0.702229118347168, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:37:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.302313566207886, 'test_avg_loss': 0.7302313566207885, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:37:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.08916473388672, 'test_avg_loss': 0.702229118347168, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}}
2025-10-02 10:37:49 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.08916473388672, 'test_avg_loss': 0.702229118347168, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}, metrics={'val_total': 72, 'val_loss': 50.332672119140625, 'val_avg_loss': 0.6990648905436198, 'val_seen': 72, 'val_correct': 37, 'val_acc': 0.5138888888888888, 'test_total': 40, 'test_loss': 28.08916473388672, 'test_avg_loss': 0.702229118347168, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:37:49 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:37:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:37:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-02 10:37:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-02 10:37:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=113.356651, avg_loss=0.708479, seen=160, correct=79, accuracy=0.493750
2025-10-02 10:37:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:54 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 160, 'val_loss': 113.35665130615234, 'val_avg_loss': 0.7084790706634522, 'val_seen': 160, 'val_correct': 79, 'val_acc': 0.49375}
2025-10-02 10:37:54 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:37:54 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 160, 'val_loss': 113.35665130615234, 'val_avg_loss': 0.7084790706634522, 'val_seen': 160, 'val_correct': 79, 'val_acc': 0.49375}
2025-10-02 10:37:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 40, 'val_loss': 28.711769580841064, 'val_avg_loss': 0.7177942395210266, 'val_seen': 40, 'val_correct': 19, 'val_acc': 0.475}}
2025-10-02 10:37:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 160, 'val_loss': 113.35665130615234, 'val_avg_loss': 0.7084790706634522, 'val_seen': 160, 'val_correct': 79, 'val_acc': 0.49375}}
2025-10-02 10:37:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:37:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:37:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:37:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.558525, avg_loss=0.688963, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:37:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:37:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:37:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:37:57 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.55852508544922, 'test_avg_loss': 0.6889631271362304, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:37:57 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 160, 'val_loss': 113.35665130615234, 'val_avg_loss': 0.7084790706634522, 'val_seen': 160, 'val_correct': 79, 'val_acc': 0.49375}
2025-10-02 10:37:57 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.55852508544922, 'test_avg_loss': 0.6889631271362304, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:37:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.5582133531570435, 'test_avg_loss': 0.6558213353157043, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:37:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.55852508544922, 'test_avg_loss': 0.6889631271362304, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-10-02 10:37:57 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.55852508544922, 'test_avg_loss': 0.6889631271362304, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 160, 'val_loss': 113.35665130615234, 'val_avg_loss': 0.7084790706634522, 'val_seen': 160, 'val_correct': 79, 'val_acc': 0.49375, 'test_total': 40, 'test_loss': 27.55852508544922, 'test_avg_loss': 0.6889631271362304, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:37:57 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:37:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:37:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:37:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:37:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:38:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.173309, avg_loss=0.690867, seen=200, correct=107, accuracy=0.535000
2025-10-02 10:38:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:03 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 138.17330932617188, 'val_avg_loss': 0.6908665466308593, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-10-02 10:38:03 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:38:03 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 138.17330932617188, 'val_avg_loss': 0.6908665466308593, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-10-02 10:38:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.775620222091675, 'val_avg_loss': 0.6955124044418335, 'val_seen': 50, 'val_correct': 25, 'val_acc': 0.5}}
2025-10-02 10:38:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 138.17330932617188, 'val_avg_loss': 0.6908665466308593, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}}
2025-10-02 10:38:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:38:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:38:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.437729, avg_loss=0.660943, seen=40, correct=27, accuracy=0.675000
2025-10-02 10:38:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:06 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.437728881835938, 'test_avg_loss': 0.6609432220458984, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-10-02 10:38:06 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 138.17330932617188, 'val_avg_loss': 0.6908665466308593, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-10-02 10:38:06 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.437728881835938, 'test_avg_loss': 0.6609432220458984, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-10-02 10:38:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.66100287437439, 'test_avg_loss': 0.666100287437439, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-10-02 10:38:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.437728881835938, 'test_avg_loss': 0.6609432220458984, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-10-02 10:38:06 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.437728881835938, 'test_avg_loss': 0.6609432220458984, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 200, 'val_loss': 138.17330932617188, 'val_avg_loss': 0.6908665466308593, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535, 'test_total': 40, 'test_loss': 26.437728881835938, 'test_avg_loss': 0.6609432220458984, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-10-02 10:38:06 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:38:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:38:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-02 10:38:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:38:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=93.351669, avg_loss=0.686409, seen=136, correct=80, accuracy=0.588235
2025-10-02 10:38:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:11 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 136, 'val_loss': 93.35166931152344, 'val_avg_loss': 0.6864093331729665, 'val_seen': 136, 'val_correct': 80, 'val_acc': 0.5882352941176471}
2025-10-02 10:38:11 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:38:11 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 136, 'val_loss': 93.35166931152344, 'val_avg_loss': 0.6864093331729665, 'val_seen': 136, 'val_correct': 80, 'val_acc': 0.5882352941176471}
2025-10-02 10:38:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 23.263249397277832, 'val_avg_loss': 0.684213217566995, 'val_seen': 34, 'val_correct': 22, 'val_acc': 0.6470588235294118}}
2025-10-02 10:38:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 136, 'val_loss': 93.35166931152344, 'val_avg_loss': 0.6864093331729665, 'val_seen': 136, 'val_correct': 80, 'val_acc': 0.5882352941176471}}
2025-10-02 10:38:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:38:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:38:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.548304, avg_loss=0.688708, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:38:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:14 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.548303604125977, 'test_avg_loss': 0.6887075901031494, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:38:14 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 136, 'val_loss': 93.35166931152344, 'val_avg_loss': 0.6864093331729665, 'val_seen': 136, 'val_correct': 80, 'val_acc': 0.5882352941176471}
2025-10-02 10:38:14 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.548303604125977, 'test_avg_loss': 0.6887075901031494, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:38:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.047415375709534, 'test_avg_loss': 0.7047415375709534, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:38:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.548303604125977, 'test_avg_loss': 0.6887075901031494, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-10-02 10:38:14 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.548303604125977, 'test_avg_loss': 0.6887075901031494, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 136, 'val_loss': 93.35166931152344, 'val_avg_loss': 0.6864093331729665, 'val_seen': 136, 'val_correct': 80, 'val_acc': 0.5882352941176471, 'test_total': 40, 'test_loss': 27.548303604125977, 'test_avg_loss': 0.6887075901031494, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:38:14 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:38:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:38:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:38:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:38:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.969864, avg_loss=0.684849, seen=200, correct=116, accuracy=0.580000
2025-10-02 10:38:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:20 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 136.96986389160156, 'val_avg_loss': 0.6848493194580079, 'val_seen': 200, 'val_correct': 116, 'val_acc': 0.58}
2025-10-02 10:38:20 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:38:20 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 136.96986389160156, 'val_avg_loss': 0.6848493194580079, 'val_seen': 200, 'val_correct': 116, 'val_acc': 0.58}
2025-10-02 10:38:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 33.92487668991089, 'val_avg_loss': 0.6784975337982178, 'val_seen': 50, 'val_correct': 27, 'val_acc': 0.54}}
2025-10-02 10:38:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 136.96986389160156, 'val_avg_loss': 0.6848493194580079, 'val_seen': 200, 'val_correct': 116, 'val_acc': 0.58}}
2025-10-02 10:38:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:38:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:38:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.934544, avg_loss=0.673364, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:38:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:23 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.93454360961914, 'test_avg_loss': 0.6733635902404785, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:38:23 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 136.96986389160156, 'val_avg_loss': 0.6848493194580079, 'val_seen': 200, 'val_correct': 116, 'val_acc': 0.58}
2025-10-02 10:38:23 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.93454360961914, 'test_avg_loss': 0.6733635902404785, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:38:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.840144634246826, 'test_avg_loss': 0.6840144634246826, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:38:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.93454360961914, 'test_avg_loss': 0.6733635902404785, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-10-02 10:38:23 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.93454360961914, 'test_avg_loss': 0.6733635902404785, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 200, 'val_loss': 136.96986389160156, 'val_avg_loss': 0.6848493194580079, 'val_seen': 200, 'val_correct': 116, 'val_acc': 0.58, 'test_total': 40, 'test_loss': 26.93454360961914, 'test_avg_loss': 0.6733635902404785, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:38:23 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:38:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:38:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-02 10:38:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:38:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=95.598175, avg_loss=0.708135, seen=135, correct=63, accuracy=0.466667
2025-10-02 10:38:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:27 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 135, 'val_loss': 95.59817504882812, 'val_avg_loss': 0.7081346299913195, 'val_seen': 135, 'val_correct': 63, 'val_acc': 0.4666666666666667}
2025-10-02 10:38:27 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:38:27 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 135, 'val_loss': 95.59817504882812, 'val_avg_loss': 0.7081346299913195, 'val_seen': 135, 'val_correct': 63, 'val_acc': 0.4666666666666667}
2025-10-02 10:38:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 23.742547869682312, 'val_avg_loss': 0.6983102314612445, 'val_seen': 34, 'val_correct': 17, 'val_acc': 0.5}}
2025-10-02 10:38:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 135, 'val_loss': 95.59817504882812, 'val_avg_loss': 0.7081346299913195, 'val_seen': 135, 'val_correct': 63, 'val_acc': 0.4666666666666667}}
2025-10-02 10:38:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:38:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.568371, avg_loss=0.714209, seen=40, correct=15, accuracy=0.375000
2025-10-02 10:38:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:29 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.568370819091797, 'test_avg_loss': 0.7142092704772949, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-10-02 10:38:29 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 135, 'val_loss': 95.59817504882812, 'val_avg_loss': 0.7081346299913195, 'val_seen': 135, 'val_correct': 63, 'val_acc': 0.4666666666666667}
2025-10-02 10:38:29 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.568370819091797, 'test_avg_loss': 0.7142092704772949, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-10-02 10:38:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.325261235237122, 'test_avg_loss': 0.7325261235237122, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-10-02 10:38:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.568370819091797, 'test_avg_loss': 0.7142092704772949, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}}
2025-10-02 10:38:29 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.568370819091797, 'test_avg_loss': 0.7142092704772949, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}, metrics={'val_total': 135, 'val_loss': 95.59817504882812, 'val_avg_loss': 0.7081346299913195, 'val_seen': 135, 'val_correct': 63, 'val_acc': 0.4666666666666667, 'test_total': 40, 'test_loss': 28.568370819091797, 'test_avg_loss': 0.7142092704772949, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-10-02 10:38:29 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:38:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:38:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 10:38:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 10:38:31 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=77.849304, avg_loss=0.707721, seen=110, correct=56, accuracy=0.509091
2025-10-02 10:38:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:33 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 110, 'val_loss': 77.84930419921875, 'val_avg_loss': 0.707720947265625, 'val_seen': 110, 'val_correct': 56, 'val_acc': 0.509090909090909}
2025-10-02 10:38:33 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:38:33 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 110, 'val_loss': 77.84930419921875, 'val_avg_loss': 0.707720947265625, 'val_seen': 110, 'val_correct': 56, 'val_acc': 0.509090909090909}
2025-10-02 10:38:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 20.166008353233337, 'val_avg_loss': 0.7202145840440478, 'val_seen': 28, 'val_correct': 10, 'val_acc': 0.35714285714285715}}
2025-10-02 10:38:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 110, 'val_loss': 77.84930419921875, 'val_avg_loss': 0.707720947265625, 'val_seen': 110, 'val_correct': 56, 'val_acc': 0.509090909090909}}
2025-10-02 10:38:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:38:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:38:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.288685, avg_loss=0.707217, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:38:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:36 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.288684844970703, 'test_avg_loss': 0.7072171211242676, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:38:36 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 110, 'val_loss': 77.84930419921875, 'val_avg_loss': 0.707720947265625, 'val_seen': 110, 'val_correct': 56, 'val_acc': 0.509090909090909}
2025-10-02 10:38:36 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.288684844970703, 'test_avg_loss': 0.7072171211242676, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:38:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.4265793561935425, 'test_avg_loss': 0.7426579356193542, 'test_seen': 10, 'test_correct': 2, 'test_acc': 0.2}}
2025-10-02 10:38:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.288684844970703, 'test_avg_loss': 0.7072171211242676, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-10-02 10:38:36 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.288684844970703, 'test_avg_loss': 0.7072171211242676, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 110, 'val_loss': 77.84930419921875, 'val_avg_loss': 0.707720947265625, 'val_seen': 110, 'val_correct': 56, 'val_acc': 0.509090909090909, 'test_total': 40, 'test_loss': 28.288684844970703, 'test_avg_loss': 0.7072171211242676, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:38:36 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:38:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:38:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-02 10:38:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-02 10:38:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=86.622604, avg_loss=0.687481, seen=126, correct=69, accuracy=0.547619
2025-10-02 10:38:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:41 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 126, 'val_loss': 86.62260437011719, 'val_avg_loss': 0.6874809870644222, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477}
2025-10-02 10:38:41 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:38:41 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 126, 'val_loss': 86.62260437011719, 'val_avg_loss': 0.6874809870644222, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477}
2025-10-02 10:38:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 32, 'val_loss': 21.664656698703766, 'val_avg_loss': 0.6770205218344927, 'val_seen': 32, 'val_correct': 17, 'val_acc': 0.53125}}
2025-10-02 10:38:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 126, 'val_loss': 86.62260437011719, 'val_avg_loss': 0.6874809870644222, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477}}
2025-10-02 10:38:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:38:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:38:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.012064, avg_loss=0.625302, seen=40, correct=31, accuracy=0.775000
2025-10-02 10:38:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:44 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}
2025-10-02 10:38:44 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 126, 'val_loss': 86.62260437011719, 'val_avg_loss': 0.6874809870644222, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477}
2025-10-02 10:38:44 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}
2025-10-02 10:38:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.3186012506484985, 'test_avg_loss': 0.6318601250648499, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-10-02 10:38:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}}
2025-10-02 10:38:44 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}, metrics={'val_total': 126, 'val_loss': 86.62260437011719, 'val_avg_loss': 0.6874809870644222, 'val_seen': 126, 'val_correct': 69, 'val_acc': 0.5476190476190477, 'test_total': 40, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_seen': 40, 'test_correct': 31, 'test_acc': 0.775}
2025-10-02 10:38:44 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:38:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:38:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-02 10:38:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-02 10:38:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=107.881561, avg_loss=0.705108, seen=153, correct=78, accuracy=0.509804
2025-10-02 10:38:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:49 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 153, 'val_loss': 107.88156127929688, 'val_avg_loss': 0.7051082436555351, 'val_seen': 153, 'val_correct': 78, 'val_acc': 0.5098039215686274}
2025-10-02 10:38:49 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:38:49 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 153, 'val_loss': 107.88156127929688, 'val_avg_loss': 0.7051082436555351, 'val_seen': 153, 'val_correct': 78, 'val_acc': 0.5098039215686274}
2025-10-02 10:38:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 39, 'val_loss': 26.43083345890045, 'val_avg_loss': 0.677713678433345, 'val_seen': 39, 'val_correct': 21, 'val_acc': 0.5384615384615384}}
2025-10-02 10:38:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 153, 'val_loss': 107.88156127929688, 'val_avg_loss': 0.7051082436555351, 'val_seen': 153, 'val_correct': 78, 'val_acc': 0.5098039215686274}}
2025-10-02 10:38:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:38:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:38:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.679270, avg_loss=0.666982, seen=40, correct=25, accuracy=0.625000
2025-10-02 10:38:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:53 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.679269790649414, 'test_avg_loss': 0.6669817447662354, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-10-02 10:38:53 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 153, 'val_loss': 107.88156127929688, 'val_avg_loss': 0.7051082436555351, 'val_seen': 153, 'val_correct': 78, 'val_acc': 0.5098039215686274}
2025-10-02 10:38:53 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.679269790649414, 'test_avg_loss': 0.6669817447662354, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-10-02 10:38:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.504980564117432, 'test_avg_loss': 0.6504980564117432, 'test_seen': 10, 'test_correct': 9, 'test_acc': 0.9}}
2025-10-02 10:38:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.679269790649414, 'test_avg_loss': 0.6669817447662354, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-10-02 10:38:53 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.679269790649414, 'test_avg_loss': 0.6669817447662354, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 153, 'val_loss': 107.88156127929688, 'val_avg_loss': 0.7051082436555351, 'val_seen': 153, 'val_correct': 78, 'val_acc': 0.5098039215686274, 'test_total': 40, 'test_loss': 26.679269790649414, 'test_avg_loss': 0.6669817447662354, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-10-02 10:38:53 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:38:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:38:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-02 10:38:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-02 10:38:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.752235, avg_loss=0.704749, seen=11, correct=6, accuracy=0.545455
2025-10-02 10:38:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:55 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 7.752235412597656, 'val_avg_loss': 0.7047486738725142, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-10-02 10:38:55 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:38:55 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 7.752235412597656, 'val_avg_loss': 0.7047486738725142, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-10-02 10:38:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 2.1209238171577454, 'val_avg_loss': 0.7069746057192484, 'val_seen': 3, 'val_correct': 2, 'val_acc': 0.6666666666666666}}
2025-10-02 10:38:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 7.752235412597656, 'val_avg_loss': 0.7047486738725142, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}}
2025-10-02 10:38:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:38:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:38:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.819391, avg_loss=0.720485, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:38:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:38:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:38:57 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.81939125061035, 'test_avg_loss': 0.7204847812652588, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:38:57 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 7.752235412597656, 'val_avg_loss': 0.7047486738725142, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-10-02 10:38:57 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.81939125061035, 'test_avg_loss': 0.7204847812652588, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:38:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.611857056617737, 'test_avg_loss': 0.7611857056617737, 'test_seen': 10, 'test_correct': 2, 'test_acc': 0.2}}
2025-10-02 10:38:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.81939125061035, 'test_avg_loss': 0.7204847812652588, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-10-02 10:38:57 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.81939125061035, 'test_avg_loss': 0.7204847812652588, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 11, 'val_loss': 7.752235412597656, 'val_avg_loss': 0.7047486738725142, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454, 'test_total': 40, 'test_loss': 28.81939125061035, 'test_avg_loss': 0.7204847812652588, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:38:57 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:38:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:38:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-02 10:38:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:38:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-02 10:38:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.608414, avg_loss=0.686947, seen=30, correct=15, accuracy=0.500000
2025-10-02 10:38:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:38:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:38:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:00 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 30, 'val_loss': 20.608413696289062, 'val_avg_loss': 0.6869471232096355, 'val_seen': 30, 'val_correct': 15, 'val_acc': 0.5}
2025-10-02 10:39:00 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:39:00 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 30, 'val_loss': 20.608413696289062, 'val_avg_loss': 0.6869471232096355, 'val_seen': 30, 'val_correct': 15, 'val_acc': 0.5}
2025-10-02 10:39:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 8, 'val_loss': 6.004290223121643, 'val_avg_loss': 0.7505362778902054, 'val_seen': 8, 'val_correct': 3, 'val_acc': 0.375}}
2025-10-02 10:39:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 30, 'val_loss': 20.608413696289062, 'val_avg_loss': 0.6869471232096355, 'val_seen': 30, 'val_correct': 15, 'val_acc': 0.5}}
2025-10-02 10:39:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:39:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:39:01 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.083382, avg_loss=0.677085, seen=40, correct=25, accuracy=0.625000
2025-10-02 10:39:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:03 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.08338165283203, 'test_avg_loss': 0.6770845413208008, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-10-02 10:39:03 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 30, 'val_loss': 20.608413696289062, 'val_avg_loss': 0.6869471232096355, 'val_seen': 30, 'val_correct': 15, 'val_acc': 0.5}
2025-10-02 10:39:03 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.08338165283203, 'test_avg_loss': 0.6770845413208008, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-10-02 10:39:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.6176512241363525, 'test_avg_loss': 0.6617651224136353, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:39:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.08338165283203, 'test_avg_loss': 0.6770845413208008, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-10-02 10:39:03 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.08338165283203, 'test_avg_loss': 0.6770845413208008, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 30, 'val_loss': 20.608413696289062, 'val_avg_loss': 0.6869471232096355, 'val_seen': 30, 'val_correct': 15, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 27.08338165283203, 'test_avg_loss': 0.6770845413208008, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-10-02 10:39:03 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:39:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:39:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:39:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:39:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.650208, avg_loss=0.693251, seen=200, correct=110, accuracy=0.550000
2025-10-02 10:39:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:10 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 138.65020751953125, 'val_avg_loss': 0.6932510375976563, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-10-02 10:39:10 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:39:10 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 138.65020751953125, 'val_avg_loss': 0.6932510375976563, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-10-02 10:39:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.92184782028198, 'val_avg_loss': 0.6984369564056396, 'val_seen': 50, 'val_correct': 25, 'val_acc': 0.5}}
2025-10-02 10:39:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 138.65020751953125, 'val_avg_loss': 0.6932510375976563, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}}
2025-10-02 10:39:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:39:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:39:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.317083, avg_loss=0.732927, seen=40, correct=14, accuracy=0.350000
2025-10-02 10:39:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:13 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.31708335876465, 'test_avg_loss': 0.7329270839691162, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-10-02 10:39:13 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 138.65020751953125, 'val_avg_loss': 0.6932510375976563, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-10-02 10:39:13 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.31708335876465, 'test_avg_loss': 0.7329270839691162, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-10-02 10:39:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.847612619400024, 'test_avg_loss': 0.6847612619400024, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:39:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.31708335876465, 'test_avg_loss': 0.7329270839691162, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}}
2025-10-02 10:39:13 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.31708335876465, 'test_avg_loss': 0.7329270839691162, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}, metrics={'val_total': 200, 'val_loss': 138.65020751953125, 'val_avg_loss': 0.6932510375976563, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55, 'test_total': 40, 'test_loss': 29.31708335876465, 'test_avg_loss': 0.7329270839691162, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-10-02 10:39:13 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:39:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:39:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:39:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:39:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.621140, avg_loss=0.703106, seen=200, correct=98, accuracy=0.490000
2025-10-02 10:39:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:18 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 140.6211395263672, 'val_avg_loss': 0.703105697631836, 'val_seen': 200, 'val_correct': 98, 'val_acc': 0.49}
2025-10-02 10:39:18 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:39:18 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 140.6211395263672, 'val_avg_loss': 0.703105697631836, 'val_seen': 200, 'val_correct': 98, 'val_acc': 0.49}
2025-10-02 10:39:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.50182044506073, 'val_avg_loss': 0.7100364089012146, 'val_seen': 50, 'val_correct': 21, 'val_acc': 0.42}}
2025-10-02 10:39:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 140.6211395263672, 'val_avg_loss': 0.703105697631836, 'val_seen': 200, 'val_correct': 98, 'val_acc': 0.49}}
2025-10-02 10:39:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:39:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.679832, avg_loss=0.741996, seen=40, correct=16, accuracy=0.400000
2025-10-02 10:39:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:21 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.679832458496094, 'test_avg_loss': 0.7419958114624023, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:39:21 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 140.6211395263672, 'val_avg_loss': 0.703105697631836, 'val_seen': 200, 'val_correct': 98, 'val_acc': 0.49}
2025-10-02 10:39:21 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.679832458496094, 'test_avg_loss': 0.7419958114624023, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:39:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.167885661125183, 'test_avg_loss': 0.8167885661125183, 'test_seen': 10, 'test_correct': 1, 'test_acc': 0.1}}
2025-10-02 10:39:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.679832458496094, 'test_avg_loss': 0.7419958114624023, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}}
2025-10-02 10:39:21 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.679832458496094, 'test_avg_loss': 0.7419958114624023, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}, metrics={'val_total': 200, 'val_loss': 140.6211395263672, 'val_avg_loss': 0.703105697631836, 'val_seen': 200, 'val_correct': 98, 'val_acc': 0.49, 'test_total': 40, 'test_loss': 29.679832458496094, 'test_avg_loss': 0.7419958114624023, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:39:21 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:39:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:39:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-02 10:39:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-02 10:39:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=108.432503, avg_loss=0.673494, seen=161, correct=81, accuracy=0.503106
2025-10-02 10:39:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:27 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 161, 'val_loss': 108.43250274658203, 'val_avg_loss': 0.6734938058793914, 'val_seen': 161, 'val_correct': 81, 'val_acc': 0.5031055900621118}
2025-10-02 10:39:27 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:39:27 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 161, 'val_loss': 108.43250274658203, 'val_avg_loss': 0.6734938058793914, 'val_seen': 161, 'val_correct': 81, 'val_acc': 0.5031055900621118}
2025-10-02 10:39:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 41, 'val_loss': 28.26190748810768, 'val_avg_loss': 0.6893148167831141, 'val_seen': 41, 'val_correct': 21, 'val_acc': 0.5121951219512195}}
2025-10-02 10:39:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 161, 'val_loss': 108.43250274658203, 'val_avg_loss': 0.6734938058793914, 'val_seen': 161, 'val_correct': 81, 'val_acc': 0.5031055900621118}}
2025-10-02 10:39:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:39:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:39:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.390863, avg_loss=0.659772, seen=40, correct=26, accuracy=0.650000
2025-10-02 10:39:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:30 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.3908634185791, 'test_avg_loss': 0.6597715854644776, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-10-02 10:39:30 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 161, 'val_loss': 108.43250274658203, 'val_avg_loss': 0.6734938058793914, 'val_seen': 161, 'val_correct': 81, 'val_acc': 0.5031055900621118}
2025-10-02 10:39:30 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.3908634185791, 'test_avg_loss': 0.6597715854644776, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-10-02 10:39:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.106333017349243, 'test_avg_loss': 0.6106333017349244, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-10-02 10:39:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.3908634185791, 'test_avg_loss': 0.6597715854644776, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-10-02 10:39:30 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.3908634185791, 'test_avg_loss': 0.6597715854644776, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 161, 'val_loss': 108.43250274658203, 'val_avg_loss': 0.6734938058793914, 'val_seen': 161, 'val_correct': 81, 'val_acc': 0.5031055900621118, 'test_total': 40, 'test_loss': 26.3908634185791, 'test_avg_loss': 0.6597715854644776, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-10-02 10:39:30 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:39:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:39:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-02 10:39:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-02 10:39:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=87.101891, avg_loss=0.708145, seen=123, correct=62, accuracy=0.504065
2025-10-02 10:39:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:35 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 123, 'val_loss': 87.10189056396484, 'val_avg_loss': 0.7081454517395516, 'val_seen': 123, 'val_correct': 62, 'val_acc': 0.5040650406504065}
2025-10-02 10:39:35 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:39:35 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 123, 'val_loss': 87.10189056396484, 'val_avg_loss': 0.7081454517395516, 'val_seen': 123, 'val_correct': 62, 'val_acc': 0.5040650406504065}
2025-10-02 10:39:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 31, 'val_loss': 20.852964341640472, 'val_avg_loss': 0.6726762690851765, 'val_seen': 31, 'val_correct': 20, 'val_acc': 0.6451612903225806}}
2025-10-02 10:39:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 123, 'val_loss': 87.10189056396484, 'val_avg_loss': 0.7081454517395516, 'val_seen': 123, 'val_correct': 62, 'val_acc': 0.5040650406504065}}
2025-10-02 10:39:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:39:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:39:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.149542, avg_loss=0.703739, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:39:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:38 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.1495418548584, 'test_avg_loss': 0.7037385463714599, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:39:38 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 123, 'val_loss': 87.10189056396484, 'val_avg_loss': 0.7081454517395516, 'val_seen': 123, 'val_correct': 62, 'val_acc': 0.5040650406504065}
2025-10-02 10:39:38 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.1495418548584, 'test_avg_loss': 0.7037385463714599, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:39:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.35133957862854, 'test_avg_loss': 0.635133957862854, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:39:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.1495418548584, 'test_avg_loss': 0.7037385463714599, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}}
2025-10-02 10:39:38 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.1495418548584, 'test_avg_loss': 0.7037385463714599, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}, metrics={'val_total': 123, 'val_loss': 87.10189056396484, 'val_avg_loss': 0.7081454517395516, 'val_seen': 123, 'val_correct': 62, 'val_acc': 0.5040650406504065, 'test_total': 40, 'test_loss': 28.1495418548584, 'test_avg_loss': 0.7037385463714599, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:39:38 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:39:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:39:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-02 10:39:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 10:39:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=51.366417, avg_loss=0.684886, seen=75, correct=43, accuracy=0.573333
2025-10-02 10:39:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:41 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 75, 'val_loss': 51.366416931152344, 'val_avg_loss': 0.6848855590820313, 'val_seen': 75, 'val_correct': 43, 'val_acc': 0.5733333333333334}
2025-10-02 10:39:41 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:39:41 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 75, 'val_loss': 51.366416931152344, 'val_avg_loss': 0.6848855590820313, 'val_seen': 75, 'val_correct': 43, 'val_acc': 0.5733333333333334}
2025-10-02 10:39:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 19, 'val_loss': 13.877671957015991, 'val_avg_loss': 0.730403787211368, 'val_seen': 19, 'val_correct': 10, 'val_acc': 0.5263157894736842}}
2025-10-02 10:39:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 75, 'val_loss': 51.366416931152344, 'val_avg_loss': 0.6848855590820313, 'val_seen': 75, 'val_correct': 43, 'val_acc': 0.5733333333333334}}
2025-10-02 10:39:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:39:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:39:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.659269, avg_loss=0.691482, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:39:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:44 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.659269332885742, 'test_avg_loss': 0.6914817333221436, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:39:44 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 75, 'val_loss': 51.366416931152344, 'val_avg_loss': 0.6848855590820313, 'val_seen': 75, 'val_correct': 43, 'val_acc': 0.5733333333333334}
2025-10-02 10:39:44 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.659269332885742, 'test_avg_loss': 0.6914817333221436, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:39:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.839593410491943, 'test_avg_loss': 0.6839593410491943, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:39:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.659269332885742, 'test_avg_loss': 0.6914817333221436, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-10-02 10:39:44 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.659269332885742, 'test_avg_loss': 0.6914817333221436, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 75, 'val_loss': 51.366416931152344, 'val_avg_loss': 0.6848855590820313, 'val_seen': 75, 'val_correct': 43, 'val_acc': 0.5733333333333334, 'test_total': 40, 'test_loss': 27.659269332885742, 'test_avg_loss': 0.6914817333221436, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:39:44 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:39:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:39:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:39:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:39:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.770828, avg_loss=0.698854, seen=200, correct=94, accuracy=0.470000
2025-10-02 10:39:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:50 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 139.7708282470703, 'val_avg_loss': 0.6988541412353516, 'val_seen': 200, 'val_correct': 94, 'val_acc': 0.47}
2025-10-02 10:39:50 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:39:50 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 139.7708282470703, 'val_avg_loss': 0.6988541412353516, 'val_seen': 200, 'val_correct': 94, 'val_acc': 0.47}
2025-10-02 10:39:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.292174220085144, 'val_avg_loss': 0.7058434844017029, 'val_seen': 50, 'val_correct': 24, 'val_acc': 0.48}}
2025-10-02 10:39:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 139.7708282470703, 'val_avg_loss': 0.6988541412353516, 'val_seen': 200, 'val_correct': 94, 'val_acc': 0.47}}
2025-10-02 10:39:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:39:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:39:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.143476, avg_loss=0.678587, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:39:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:52 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.143476486206055, 'test_avg_loss': 0.6785869121551513, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:39:52 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 139.7708282470703, 'val_avg_loss': 0.6988541412353516, 'val_seen': 200, 'val_correct': 94, 'val_acc': 0.47}
2025-10-02 10:39:52 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.143476486206055, 'test_avg_loss': 0.6785869121551513, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:39:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.14627206325531, 'test_avg_loss': 0.714627206325531, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:39:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.143476486206055, 'test_avg_loss': 0.6785869121551513, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-10-02 10:39:52 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.143476486206055, 'test_avg_loss': 0.6785869121551513, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 200, 'val_loss': 139.7708282470703, 'val_avg_loss': 0.6988541412353516, 'val_seen': 200, 'val_correct': 94, 'val_acc': 0.47, 'test_total': 40, 'test_loss': 27.143476486206055, 'test_avg_loss': 0.6785869121551513, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:39:52 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:39:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:39:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-02 10:39:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:39:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-02 10:39:57 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=117.856628, avg_loss=0.693274, seen=170, correct=94, accuracy=0.552941
2025-10-02 10:39:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:39:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:39:58 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 170, 'val_loss': 117.85662841796875, 'val_avg_loss': 0.6932742848115809, 'val_seen': 170, 'val_correct': 94, 'val_acc': 0.5529411764705883}
2025-10-02 10:39:58 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:39:58 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 170, 'val_loss': 117.85662841796875, 'val_avg_loss': 0.6932742848115809, 'val_seen': 170, 'val_correct': 94, 'val_acc': 0.5529411764705883}
2025-10-02 10:39:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 43, 'val_loss': 30.005830943584442, 'val_avg_loss': 0.6978100219438242, 'val_seen': 43, 'val_correct': 22, 'val_acc': 0.5116279069767442}}
2025-10-02 10:39:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 170, 'val_loss': 117.85662841796875, 'val_avg_loss': 0.6932742848115809, 'val_seen': 170, 'val_correct': 94, 'val_acc': 0.5529411764705883}}
2025-10-02 10:39:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:39:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:39:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:40:00 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.592085, avg_loss=0.689802, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:40:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:01 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.592084884643555, 'test_avg_loss': 0.6898021221160888, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:40:01 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 170, 'val_loss': 117.85662841796875, 'val_avg_loss': 0.6932742848115809, 'val_seen': 170, 'val_correct': 94, 'val_acc': 0.5529411764705883}
2025-10-02 10:40:01 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.592084884643555, 'test_avg_loss': 0.6898021221160888, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:40:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.270730495452881, 'test_avg_loss': 0.7270730495452881, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:40:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.592084884643555, 'test_avg_loss': 0.6898021221160888, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-10-02 10:40:01 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.592084884643555, 'test_avg_loss': 0.6898021221160888, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 170, 'val_loss': 117.85662841796875, 'val_avg_loss': 0.6932742848115809, 'val_seen': 170, 'val_correct': 94, 'val_acc': 0.5529411764705883, 'test_total': 40, 'test_loss': 27.592084884643555, 'test_avg_loss': 0.6898021221160888, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:40:01 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:40:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:40:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-02 10:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-02 10:40:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=135.359711, avg_loss=0.701346, seen=193, correct=104, accuracy=0.538860
2025-10-02 10:40:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:08 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 193, 'val_loss': 135.35971069335938, 'val_avg_loss': 0.7013456512609294, 'val_seen': 193, 'val_correct': 104, 'val_acc': 0.538860103626943}
2025-10-02 10:40:08 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:40:08 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 193, 'val_loss': 135.35971069335938, 'val_avg_loss': 0.7013456512609294, 'val_seen': 193, 'val_correct': 104, 'val_acc': 0.538860103626943}
2025-10-02 10:40:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 49, 'val_loss': 33.46252393722534, 'val_avg_loss': 0.6829086517801091, 'val_seen': 49, 'val_correct': 31, 'val_acc': 0.6326530612244898}}
2025-10-02 10:40:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 193, 'val_loss': 135.35971069335938, 'val_avg_loss': 0.7013456512609294, 'val_seen': 193, 'val_correct': 104, 'val_acc': 0.538860103626943}}
2025-10-02 10:40:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:40:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:40:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.766964, avg_loss=0.669174, seen=40, correct=24, accuracy=0.600000
2025-10-02 10:40:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:11 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.766963958740234, 'test_avg_loss': 0.6691740989685059, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-10-02 10:40:11 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 193, 'val_loss': 135.35971069335938, 'val_avg_loss': 0.7013456512609294, 'val_seen': 193, 'val_correct': 104, 'val_acc': 0.538860103626943}
2025-10-02 10:40:11 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.766963958740234, 'test_avg_loss': 0.6691740989685059, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-10-02 10:40:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.523031949996948, 'test_avg_loss': 0.6523031949996948, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:40:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.766963958740234, 'test_avg_loss': 0.6691740989685059, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-10-02 10:40:11 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.766963958740234, 'test_avg_loss': 0.6691740989685059, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 193, 'val_loss': 135.35971069335938, 'val_avg_loss': 0.7013456512609294, 'val_seen': 193, 'val_correct': 104, 'val_acc': 0.538860103626943, 'test_total': 40, 'test_loss': 26.766963958740234, 'test_avg_loss': 0.6691740989685059, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-10-02 10:40:11 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:40:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:40:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-02 10:40:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 10:40:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=76.042328, avg_loss=0.678949, seen=112, correct=66, accuracy=0.589286
2025-10-02 10:40:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:16 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 112, 'val_loss': 76.04232788085938, 'val_avg_loss': 0.6789493560791016, 'val_seen': 112, 'val_correct': 66, 'val_acc': 0.5892857142857143}
2025-10-02 10:40:16 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:40:16 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 112, 'val_loss': 76.04232788085938, 'val_avg_loss': 0.6789493560791016, 'val_seen': 112, 'val_correct': 66, 'val_acc': 0.5892857142857143}
2025-10-02 10:40:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 18.884796619415283, 'val_avg_loss': 0.6744570221219744, 'val_seen': 28, 'val_correct': 15, 'val_acc': 0.5357142857142857}}
2025-10-02 10:40:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 112, 'val_loss': 76.04232788085938, 'val_avg_loss': 0.6789493560791016, 'val_seen': 112, 'val_correct': 66, 'val_acc': 0.5892857142857143}}
2025-10-02 10:40:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:40:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:40:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.863192, avg_loss=0.696580, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:40:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:18 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.863191604614258, 'test_avg_loss': 0.6965797901153564, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:40:18 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 112, 'val_loss': 76.04232788085938, 'val_avg_loss': 0.6789493560791016, 'val_seen': 112, 'val_correct': 66, 'val_acc': 0.5892857142857143}
2025-10-02 10:40:18 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.863191604614258, 'test_avg_loss': 0.6965797901153564, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:40:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.72926652431488, 'test_avg_loss': 0.6729266524314881, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-10-02 10:40:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.863191604614258, 'test_avg_loss': 0.6965797901153564, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-10-02 10:40:18 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.863191604614258, 'test_avg_loss': 0.6965797901153564, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 112, 'val_loss': 76.04232788085938, 'val_avg_loss': 0.6789493560791016, 'val_seen': 112, 'val_correct': 66, 'val_acc': 0.5892857142857143, 'test_total': 40, 'test_loss': 27.863191604614258, 'test_avg_loss': 0.6965797901153564, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:40:18 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:40:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:40:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-02 10:40:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-02 10:40:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=50.829781, avg_loss=0.686889, seen=74, correct=41, accuracy=0.554054
2025-10-02 10:40:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:22 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 74, 'val_loss': 50.82978057861328, 'val_avg_loss': 0.6868889267380173, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541}
2025-10-02 10:40:22 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:40:22 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 74, 'val_loss': 50.82978057861328, 'val_avg_loss': 0.6868889267380173, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541}
2025-10-02 10:40:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 19, 'val_loss': 13.39307188987732, 'val_avg_loss': 0.7048985205198589, 'val_seen': 19, 'val_correct': 9, 'val_acc': 0.47368421052631576}}
2025-10-02 10:40:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 74, 'val_loss': 50.82978057861328, 'val_avg_loss': 0.6868889267380173, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541}}
2025-10-02 10:40:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:40:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:40:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.431290, avg_loss=0.710782, seen=40, correct=16, accuracy=0.400000
2025-10-02 10:40:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:25 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.431289672851562, 'test_avg_loss': 0.710782241821289, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:40:25 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 74, 'val_loss': 50.82978057861328, 'val_avg_loss': 0.6868889267380173, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541}
2025-10-02 10:40:25 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.431289672851562, 'test_avg_loss': 0.710782241821289, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:40:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.430893301963806, 'test_avg_loss': 0.7430893301963806, 'test_seen': 10, 'test_correct': 2, 'test_acc': 0.2}}
2025-10-02 10:40:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.431289672851562, 'test_avg_loss': 0.710782241821289, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}}
2025-10-02 10:40:25 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.431289672851562, 'test_avg_loss': 0.710782241821289, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}, metrics={'val_total': 74, 'val_loss': 50.82978057861328, 'val_avg_loss': 0.6868889267380173, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541, 'test_total': 40, 'test_loss': 28.431289672851562, 'test_avg_loss': 0.710782241821289, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:40:25 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:40:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:40:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:40:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.149780, avg_loss=0.690749, seen=200, correct=103, accuracy=0.515000
2025-10-02 10:40:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:30 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 138.1497802734375, 'val_avg_loss': 0.6907489013671875, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-10-02 10:40:30 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:40:30 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 138.1497802734375, 'val_avg_loss': 0.6907489013671875, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-10-02 10:40:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.601503014564514, 'val_avg_loss': 0.6920300602912903, 'val_seen': 50, 'val_correct': 25, 'val_acc': 0.5}}
2025-10-02 10:40:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 138.1497802734375, 'val_avg_loss': 0.6907489013671875, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}}
2025-10-02 10:40:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:40:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.376122, avg_loss=0.659403, seen=40, correct=28, accuracy=0.700000
2025-10-02 10:40:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:33 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.376121520996094, 'test_avg_loss': 0.6594030380249023, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-10-02 10:40:33 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 138.1497802734375, 'val_avg_loss': 0.6907489013671875, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-10-02 10:40:33 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.376121520996094, 'test_avg_loss': 0.6594030380249023, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-10-02 10:40:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.197589039802551, 'test_avg_loss': 0.6197589039802551, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-10-02 10:40:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.376121520996094, 'test_avg_loss': 0.6594030380249023, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-10-02 10:40:33 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.376121520996094, 'test_avg_loss': 0.6594030380249023, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 200, 'val_loss': 138.1497802734375, 'val_avg_loss': 0.6907489013671875, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515, 'test_total': 40, 'test_loss': 26.376121520996094, 'test_avg_loss': 0.6594030380249023, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-10-02 10:40:33 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:40:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:40:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:40:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:40:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.899994, avg_loss=0.709500, seen=200, correct=97, accuracy=0.485000
2025-10-02 10:40:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:40 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 141.89999389648438, 'val_avg_loss': 0.7094999694824219, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-10-02 10:40:40 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:40:40 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 141.89999389648438, 'val_avg_loss': 0.7094999694824219, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-10-02 10:40:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.58379566669464, 'val_avg_loss': 0.7116759133338928, 'val_seen': 50, 'val_correct': 26, 'val_acc': 0.52}}
2025-10-02 10:40:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 141.89999389648438, 'val_avg_loss': 0.7094999694824219, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}}
2025-10-02 10:40:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:40:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.984726, avg_loss=0.724618, seen=40, correct=16, accuracy=0.400000
2025-10-02 10:40:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:43 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.984725952148438, 'test_avg_loss': 0.7246181488037109, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:40:43 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 141.89999389648438, 'val_avg_loss': 0.7094999694824219, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-10-02 10:40:43 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.984725952148438, 'test_avg_loss': 0.7246181488037109, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:40:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.874634504318237, 'test_avg_loss': 0.6874634504318238, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:40:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.984725952148438, 'test_avg_loss': 0.7246181488037109, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}}
2025-10-02 10:40:43 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.984725952148438, 'test_avg_loss': 0.7246181488037109, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}, metrics={'val_total': 200, 'val_loss': 141.89999389648438, 'val_avg_loss': 0.7094999694824219, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485, 'test_total': 40, 'test_loss': 28.984725952148438, 'test_avg_loss': 0.7246181488037109, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-10-02 10:40:43 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:40:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:40:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-02 10:40:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-02 10:40:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.950729, avg_loss=0.684273, seen=54, correct=29, accuracy=0.537037
2025-10-02 10:40:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:46 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 54, 'val_loss': 36.95072937011719, 'val_avg_loss': 0.6842727661132812, 'val_seen': 54, 'val_correct': 29, 'val_acc': 0.5370370370370371}
2025-10-02 10:40:46 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:40:46 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 54, 'val_loss': 36.95072937011719, 'val_avg_loss': 0.6842727661132812, 'val_seen': 54, 'val_correct': 29, 'val_acc': 0.5370370370370371}
2025-10-02 10:40:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 14, 'val_loss': 9.752775430679321, 'val_avg_loss': 0.6966268164770943, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}}
2025-10-02 10:40:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 54, 'val_loss': 36.95072937011719, 'val_avg_loss': 0.6842727661132812, 'val_seen': 54, 'val_correct': 29, 'val_acc': 0.5370370370370371}}
2025-10-02 10:40:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:40:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:40:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.631969, avg_loss=0.665799, seen=40, correct=25, accuracy=0.625000
2025-10-02 10:40:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:48 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.631969451904297, 'test_avg_loss': 0.6657992362976074, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-10-02 10:40:48 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 54, 'val_loss': 36.95072937011719, 'val_avg_loss': 0.6842727661132812, 'val_seen': 54, 'val_correct': 29, 'val_acc': 0.5370370370370371}
2025-10-02 10:40:48 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.631969451904297, 'test_avg_loss': 0.6657992362976074, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-10-02 10:40:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.972172260284424, 'test_avg_loss': 0.6972172260284424, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:40:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.631969451904297, 'test_avg_loss': 0.6657992362976074, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-10-02 10:40:48 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.631969451904297, 'test_avg_loss': 0.6657992362976074, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 54, 'val_loss': 36.95072937011719, 'val_avg_loss': 0.6842727661132812, 'val_seen': 54, 'val_correct': 29, 'val_acc': 0.5370370370370371, 'test_total': 40, 'test_loss': 26.631969451904297, 'test_avg_loss': 0.6657992362976074, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-10-02 10:40:48 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:40:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:40:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:40:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.145401, avg_loss=0.685727, seen=200, correct=111, accuracy=0.555000
2025-10-02 10:40:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:53 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 137.14540100097656, 'val_avg_loss': 0.6857270050048828, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-10-02 10:40:53 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:40:53 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 137.14540100097656, 'val_avg_loss': 0.6857270050048828, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-10-02 10:40:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.605981945991516, 'val_avg_loss': 0.7121196389198303, 'val_seen': 50, 'val_correct': 23, 'val_acc': 0.46}}
2025-10-02 10:40:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 137.14540100097656, 'val_avg_loss': 0.6857270050048828, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}}
2025-10-02 10:40:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:40:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:40:54 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.380522, avg_loss=0.684513, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:40:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:40:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:40:56 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.380521774291992, 'test_avg_loss': 0.6845130443572998, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:40:56 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 137.14540100097656, 'val_avg_loss': 0.6857270050048828, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-10-02 10:40:56 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.380521774291992, 'test_avg_loss': 0.6845130443572998, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:40:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.510902285575867, 'test_avg_loss': 0.6510902285575867, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-10-02 10:40:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.380521774291992, 'test_avg_loss': 0.6845130443572998, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-10-02 10:40:56 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.380521774291992, 'test_avg_loss': 0.6845130443572998, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 200, 'val_loss': 137.14540100097656, 'val_avg_loss': 0.6857270050048828, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555, 'test_total': 40, 'test_loss': 27.380521774291992, 'test_avg_loss': 0.6845130443572998, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:40:56 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:40:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:40:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:40:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:40:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:41:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.541107, avg_loss=0.697706, seen=200, correct=86, accuracy=0.430000
2025-10-02 10:41:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:02 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 139.54110717773438, 'val_avg_loss': 0.6977055358886719, 'val_seen': 200, 'val_correct': 86, 'val_acc': 0.43}
2025-10-02 10:41:02 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:41:02 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 139.54110717773438, 'val_avg_loss': 0.6977055358886719, 'val_seen': 200, 'val_correct': 86, 'val_acc': 0.43}
2025-10-02 10:41:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.84909242391586, 'val_avg_loss': 0.6969818484783172, 'val_seen': 50, 'val_correct': 24, 'val_acc': 0.48}}
2025-10-02 10:41:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 139.54110717773438, 'val_avg_loss': 0.6977055358886719, 'val_seen': 200, 'val_correct': 86, 'val_acc': 0.43}}
2025-10-02 10:41:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:41:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:41:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.172071, avg_loss=0.704302, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:41:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:05 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.17207145690918, 'test_avg_loss': 0.7043017864227294, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:41:05 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 139.54110717773438, 'val_avg_loss': 0.6977055358886719, 'val_seen': 200, 'val_correct': 86, 'val_acc': 0.43}
2025-10-02 10:41:05 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.17207145690918, 'test_avg_loss': 0.7043017864227294, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:41:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.967517971992493, 'test_avg_loss': 0.6967517971992493, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:41:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.17207145690918, 'test_avg_loss': 0.7043017864227294, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-10-02 10:41:05 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.17207145690918, 'test_avg_loss': 0.7043017864227294, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 200, 'val_loss': 139.54110717773438, 'val_avg_loss': 0.6977055358886719, 'val_seen': 200, 'val_correct': 86, 'val_acc': 0.43, 'test_total': 40, 'test_loss': 28.17207145690918, 'test_avg_loss': 0.7043017864227294, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:41:05 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:41:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:41:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 10:41:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 10:41:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.098022, avg_loss=0.712024, seen=83, correct=37, accuracy=0.445783
2025-10-02 10:41:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:09 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 83, 'val_loss': 59.0980224609375, 'val_avg_loss': 0.712024366999247, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205}
2025-10-02 10:41:09 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:41:09 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 83, 'val_loss': 59.0980224609375, 'val_avg_loss': 0.712024366999247, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205}
2025-10-02 10:41:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 21, 'val_loss': 15.633121013641357, 'val_avg_loss': 0.7444343339829218, 'val_seen': 21, 'val_correct': 9, 'val_acc': 0.42857142857142855}}
2025-10-02 10:41:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 83, 'val_loss': 59.0980224609375, 'val_avg_loss': 0.712024366999247, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205}}
2025-10-02 10:41:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:41:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:41:10 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.227146, avg_loss=0.730679, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:41:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:11 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.22714614868164, 'test_avg_loss': 0.730678653717041, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:41:11 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 83, 'val_loss': 59.0980224609375, 'val_avg_loss': 0.712024366999247, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205}
2025-10-02 10:41:11 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.22714614868164, 'test_avg_loss': 0.730678653717041, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:41:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.663819313049316, 'test_avg_loss': 0.7663819313049316, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-10-02 10:41:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.22714614868164, 'test_avg_loss': 0.730678653717041, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}}
2025-10-02 10:41:11 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.22714614868164, 'test_avg_loss': 0.730678653717041, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}, metrics={'val_total': 83, 'val_loss': 59.0980224609375, 'val_avg_loss': 0.712024366999247, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205, 'test_total': 40, 'test_loss': 29.22714614868164, 'test_avg_loss': 0.730678653717041, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:41:11 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:41:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:41:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:41:16 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.676208, avg_loss=0.678381, seen=200, correct=113, accuracy=0.565000
2025-10-02 10:41:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:17 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 135.67620849609375, 'val_avg_loss': 0.6783810424804687, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-10-02 10:41:17 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:41:17 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 135.67620849609375, 'val_avg_loss': 0.6783810424804687, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-10-02 10:41:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.69760584831238, 'val_avg_loss': 0.7139521169662476, 'val_seen': 50, 'val_correct': 22, 'val_acc': 0.44}}
2025-10-02 10:41:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 135.67620849609375, 'val_avg_loss': 0.6783810424804687, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}}
2025-10-02 10:41:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:41:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:41:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.116821, avg_loss=0.677921, seen=40, correct=24, accuracy=0.600000
2025-10-02 10:41:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:20 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.1168212890625, 'test_avg_loss': 0.6779205322265625, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-10-02 10:41:20 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 135.67620849609375, 'val_avg_loss': 0.6783810424804687, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-10-02 10:41:20 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.1168212890625, 'test_avg_loss': 0.6779205322265625, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-10-02 10:41:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.8667497634887695, 'test_avg_loss': 0.686674976348877, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:41:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.1168212890625, 'test_avg_loss': 0.6779205322265625, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-10-02 10:41:20 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.1168212890625, 'test_avg_loss': 0.6779205322265625, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 200, 'val_loss': 135.67620849609375, 'val_avg_loss': 0.6783810424804687, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565, 'test_total': 40, 'test_loss': 27.1168212890625, 'test_avg_loss': 0.6779205322265625, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-10-02 10:41:20 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:41:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:41:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-02 10:41:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-02 10:41:23 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=82.668503, avg_loss=0.694693, seen=119, correct=57, accuracy=0.478992
2025-10-02 10:41:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:25 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 119, 'val_loss': 82.66850280761719, 'val_avg_loss': 0.6946933009043461, 'val_seen': 119, 'val_correct': 57, 'val_acc': 0.4789915966386555}
2025-10-02 10:41:25 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:41:25 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 119, 'val_loss': 82.66850280761719, 'val_avg_loss': 0.6946933009043461, 'val_seen': 119, 'val_correct': 57, 'val_acc': 0.4789915966386555}
2025-10-02 10:41:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 30, 'val_loss': 20.748041033744812, 'val_avg_loss': 0.6916013677914937, 'val_seen': 30, 'val_correct': 17, 'val_acc': 0.5666666666666667}}
2025-10-02 10:41:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 119, 'val_loss': 82.66850280761719, 'val_avg_loss': 0.6946933009043461, 'val_seen': 119, 'val_correct': 57, 'val_acc': 0.4789915966386555}}
2025-10-02 10:41:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:41:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:41:26 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.115347, avg_loss=0.677884, seen=40, correct=23, accuracy=0.575000
2025-10-02 10:41:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:28 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.115346908569336, 'test_avg_loss': 0.6778836727142334, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:41:28 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 119, 'val_loss': 82.66850280761719, 'val_avg_loss': 0.6946933009043461, 'val_seen': 119, 'val_correct': 57, 'val_acc': 0.4789915966386555}
2025-10-02 10:41:28 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.115346908569336, 'test_avg_loss': 0.6778836727142334, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:41:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.59869909286499, 'test_avg_loss': 0.659869909286499, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:41:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.115346908569336, 'test_avg_loss': 0.6778836727142334, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-10-02 10:41:28 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.115346908569336, 'test_avg_loss': 0.6778836727142334, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 119, 'val_loss': 82.66850280761719, 'val_avg_loss': 0.6946933009043461, 'val_seen': 119, 'val_correct': 57, 'val_acc': 0.4789915966386555, 'test_total': 40, 'test_loss': 27.115346908569336, 'test_avg_loss': 0.6778836727142334, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-10-02 10:41:28 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:41:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:41:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:41:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:41:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.575607, avg_loss=0.697878, seen=200, correct=103, accuracy=0.515000
2025-10-02 10:41:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:34 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 139.5756072998047, 'val_avg_loss': 0.6978780364990235, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-10-02 10:41:34 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:41:34 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 139.5756072998047, 'val_avg_loss': 0.6978780364990235, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-10-02 10:41:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.59364366531372, 'val_avg_loss': 0.7118728733062745, 'val_seen': 50, 'val_correct': 23, 'val_acc': 0.46}}
2025-10-02 10:41:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 139.5756072998047, 'val_avg_loss': 0.6978780364990235, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}}
2025-10-02 10:41:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:41:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:41:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.575588, avg_loss=0.689390, seen=40, correct=21, accuracy=0.525000
2025-10-02 10:41:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:37 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.57558822631836, 'test_avg_loss': 0.689389705657959, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:41:37 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 139.5756072998047, 'val_avg_loss': 0.6978780364990235, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-10-02 10:41:37 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.57558822631836, 'test_avg_loss': 0.689389705657959, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:41:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.160728335380554, 'test_avg_loss': 0.7160728335380554, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:41:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.57558822631836, 'test_avg_loss': 0.689389705657959, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-10-02 10:41:37 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.57558822631836, 'test_avg_loss': 0.689389705657959, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 200, 'val_loss': 139.5756072998047, 'val_avg_loss': 0.6978780364990235, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515, 'test_total': 40, 'test_loss': 27.57558822631836, 'test_avg_loss': 0.689389705657959, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-10-02 10:41:37 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:41:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:41:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-02 10:41:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-02 10:41:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.865772, avg_loss=0.706357, seen=89, correct=44, accuracy=0.494382
2025-10-02 10:41:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:41 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 89, 'val_loss': 62.86577224731445, 'val_avg_loss': 0.706356991542859, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101}
2025-10-02 10:41:41 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:41:41 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 89, 'val_loss': 62.86577224731445, 'val_avg_loss': 0.706356991542859, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101}
2025-10-02 10:41:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 23, 'val_loss': 16.484446704387665, 'val_avg_loss': 0.7167150741038115, 'val_seen': 23, 'val_correct': 11, 'val_acc': 0.4782608695652174}}
2025-10-02 10:41:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 89, 'val_loss': 62.86577224731445, 'val_avg_loss': 0.706356991542859, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101}}
2025-10-02 10:41:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:41:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.010962, avg_loss=0.675274, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:41:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:43 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.010961532592773, 'test_avg_loss': 0.6752740383148194, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:41:43 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 89, 'val_loss': 62.86577224731445, 'val_avg_loss': 0.706356991542859, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101}
2025-10-02 10:41:43 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.010961532592773, 'test_avg_loss': 0.6752740383148194, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:41:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.45908522605896, 'test_avg_loss': 0.645908522605896, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-10-02 10:41:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.010961532592773, 'test_avg_loss': 0.6752740383148194, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-10-02 10:41:43 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.010961532592773, 'test_avg_loss': 0.6752740383148194, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 89, 'val_loss': 62.86577224731445, 'val_avg_loss': 0.706356991542859, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101, 'test_total': 40, 'test_loss': 27.010961532592773, 'test_avg_loss': 0.6752740383148194, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:41:43 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:41:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:41:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:41:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=141.445068, avg_loss=0.707225, seen=200, correct=89, accuracy=0.445000
2025-10-02 10:41:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:50 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 141.445068359375, 'val_avg_loss': 0.707225341796875, 'val_seen': 200, 'val_correct': 89, 'val_acc': 0.445}
2025-10-02 10:41:50 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:41:50 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 141.445068359375, 'val_avg_loss': 0.707225341796875, 'val_seen': 200, 'val_correct': 89, 'val_acc': 0.445}
2025-10-02 10:41:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.5643949508667, 'val_avg_loss': 0.691287899017334, 'val_seen': 50, 'val_correct': 26, 'val_acc': 0.52}}
2025-10-02 10:41:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 141.445068359375, 'val_avg_loss': 0.707225341796875, 'val_seen': 200, 'val_correct': 89, 'val_acc': 0.445}}
2025-10-02 10:41:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:41:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:41:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.308102, avg_loss=0.707703, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:41:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:53 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.308101654052734, 'test_avg_loss': 0.7077025413513184, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:41:53 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 141.445068359375, 'val_avg_loss': 0.707225341796875, 'val_seen': 200, 'val_correct': 89, 'val_acc': 0.445}
2025-10-02 10:41:53 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.308101654052734, 'test_avg_loss': 0.7077025413513184, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:41:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.51099157333374, 'test_avg_loss': 0.7510991573333741, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:41:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.308101654052734, 'test_avg_loss': 0.7077025413513184, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-10-02 10:41:53 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.308101654052734, 'test_avg_loss': 0.7077025413513184, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 200, 'val_loss': 141.445068359375, 'val_avg_loss': 0.707225341796875, 'val_seen': 200, 'val_correct': 89, 'val_acc': 0.445, 'test_total': 40, 'test_loss': 28.308101654052734, 'test_avg_loss': 0.7077025413513184, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:41:53 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:41:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:41:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-02 10:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-02 10:41:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=73.323936, avg_loss=0.733239, seen=100, correct=40, accuracy=0.400000
2025-10-02 10:41:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:41:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:41:58 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 100, 'val_loss': 73.32393646240234, 'val_avg_loss': 0.7332393646240234, 'val_seen': 100, 'val_correct': 40, 'val_acc': 0.4}
2025-10-02 10:41:58 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:41:58 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 100, 'val_loss': 73.32393646240234, 'val_avg_loss': 0.7332393646240234, 'val_seen': 100, 'val_correct': 40, 'val_acc': 0.4}
2025-10-02 10:41:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 25, 'val_loss': 17.63492715358734, 'val_avg_loss': 0.7053970861434936, 'val_seen': 25, 'val_correct': 13, 'val_acc': 0.52}}
2025-10-02 10:41:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 100, 'val_loss': 73.32393646240234, 'val_avg_loss': 0.7332393646240234, 'val_seen': 100, 'val_correct': 40, 'val_acc': 0.4}}
2025-10-02 10:41:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:41:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:41:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:41:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:41:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.885946, avg_loss=0.722149, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:41:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:41:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:01 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.88594627380371, 'test_avg_loss': 0.7221486568450928, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:42:01 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 100, 'val_loss': 73.32393646240234, 'val_avg_loss': 0.7332393646240234, 'val_seen': 100, 'val_correct': 40, 'val_acc': 0.4}
2025-10-02 10:42:01 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.88594627380371, 'test_avg_loss': 0.7221486568450928, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:42:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.011783957481384, 'test_avg_loss': 0.7011783957481384, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-10-02 10:42:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.88594627380371, 'test_avg_loss': 0.7221486568450928, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-10-02 10:42:01 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.88594627380371, 'test_avg_loss': 0.7221486568450928, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 100, 'val_loss': 73.32393646240234, 'val_avg_loss': 0.7332393646240234, 'val_seen': 100, 'val_correct': 40, 'val_acc': 0.4, 'test_total': 40, 'test_loss': 28.88594627380371, 'test_avg_loss': 0.7221486568450928, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:42:01 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:42:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:42:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-02 10:42:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-02 10:42:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.553986, avg_loss=0.686854, seen=110, correct=60, accuracy=0.545455
2025-10-02 10:42:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:05 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 110, 'val_loss': 75.55398559570312, 'val_avg_loss': 0.6868544145063921, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-10-02 10:42:05 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:42:05 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 110, 'val_loss': 75.55398559570312, 'val_avg_loss': 0.6868544145063921, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-10-02 10:42:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 19.013670921325684, 'val_avg_loss': 0.6790596757616315, 'val_seen': 28, 'val_correct': 17, 'val_acc': 0.6071428571428571}}
2025-10-02 10:42:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 110, 'val_loss': 75.55398559570312, 'val_avg_loss': 0.6868544145063921, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}}
2025-10-02 10:42:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:42:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.771481, avg_loss=0.719287, seen=40, correct=17, accuracy=0.425000
2025-10-02 10:42:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:08 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.771480560302734, 'test_avg_loss': 0.7192870140075683, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:42:08 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 110, 'val_loss': 75.55398559570312, 'val_avg_loss': 0.6868544145063921, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-10-02 10:42:08 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.771480560302734, 'test_avg_loss': 0.7192870140075683, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:42:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.250566840171814, 'test_avg_loss': 0.7250566840171814, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:42:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.771480560302734, 'test_avg_loss': 0.7192870140075683, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}}
2025-10-02 10:42:08 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.771480560302734, 'test_avg_loss': 0.7192870140075683, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}, metrics={'val_total': 110, 'val_loss': 75.55398559570312, 'val_avg_loss': 0.6868544145063921, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454, 'test_total': 40, 'test_loss': 28.771480560302734, 'test_avg_loss': 0.7192870140075683, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-10-02 10:42:08 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:42:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:42:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-02 10:42:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-02 10:42:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=104.269836, avg_loss=0.709319, seen=147, correct=73, accuracy=0.496599
2025-10-02 10:42:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:13 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 147, 'val_loss': 104.26983642578125, 'val_avg_loss': 0.7093186151413691, 'val_seen': 147, 'val_correct': 73, 'val_acc': 0.4965986394557823}
2025-10-02 10:42:13 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:42:13 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 147, 'val_loss': 104.26983642578125, 'val_avg_loss': 0.7093186151413691, 'val_seen': 147, 'val_correct': 73, 'val_acc': 0.4965986394557823}
2025-10-02 10:42:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 37, 'val_loss': 25.580150246620178, 'val_avg_loss': 0.6913554120708156, 'val_seen': 37, 'val_correct': 24, 'val_acc': 0.6486486486486487}}
2025-10-02 10:42:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 147, 'val_loss': 104.26983642578125, 'val_avg_loss': 0.7093186151413691, 'val_seen': 147, 'val_correct': 73, 'val_acc': 0.4965986394557823}}
2025-10-02 10:42:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:42:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.514975, avg_loss=0.687874, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:42:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:15 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.51497459411621, 'test_avg_loss': 0.6878743648529053, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:42:15 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 147, 'val_loss': 104.26983642578125, 'val_avg_loss': 0.7093186151413691, 'val_seen': 147, 'val_correct': 73, 'val_acc': 0.4965986394557823}
2025-10-02 10:42:15 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.51497459411621, 'test_avg_loss': 0.6878743648529053, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:42:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.764060735702515, 'test_avg_loss': 0.6764060735702515, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:42:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.51497459411621, 'test_avg_loss': 0.6878743648529053, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-10-02 10:42:15 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.51497459411621, 'test_avg_loss': 0.6878743648529053, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 147, 'val_loss': 104.26983642578125, 'val_avg_loss': 0.7093186151413691, 'val_seen': 147, 'val_correct': 73, 'val_acc': 0.4965986394557823, 'test_total': 40, 'test_loss': 27.51497459411621, 'test_avg_loss': 0.6878743648529053, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:42:15 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:42:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:42:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-02 10:42:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-02 10:42:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=31.823391, avg_loss=0.691813, seen=46, correct=23, accuracy=0.500000
2025-10-02 10:42:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:19 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 46, 'val_loss': 31.82339096069336, 'val_avg_loss': 0.6918128469715947, 'val_seen': 46, 'val_correct': 23, 'val_acc': 0.5}
2025-10-02 10:42:19 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:42:19 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 46, 'val_loss': 31.82339096069336, 'val_avg_loss': 0.6918128469715947, 'val_seen': 46, 'val_correct': 23, 'val_acc': 0.5}
2025-10-02 10:42:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 12, 'val_loss': 8.623493432998657, 'val_avg_loss': 0.7186244527498881, 'val_seen': 12, 'val_correct': 4, 'val_acc': 0.3333333333333333}}
2025-10-02 10:42:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 46, 'val_loss': 31.82339096069336, 'val_avg_loss': 0.6918128469715947, 'val_seen': 46, 'val_correct': 23, 'val_acc': 0.5}}
2025-10-02 10:42:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:42:20 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.055717, avg_loss=0.676393, seen=40, correct=20, accuracy=0.500000
2025-10-02 10:42:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:22 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.05571746826172, 'test_avg_loss': 0.676392936706543, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:42:22 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 46, 'val_loss': 31.82339096069336, 'val_avg_loss': 0.6918128469715947, 'val_seen': 46, 'val_correct': 23, 'val_acc': 0.5}
2025-10-02 10:42:22 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.05571746826172, 'test_avg_loss': 0.676392936706543, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:42:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.892346978187561, 'test_avg_loss': 0.6892346978187561, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:42:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.05571746826172, 'test_avg_loss': 0.676392936706543, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-10-02 10:42:22 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.05571746826172, 'test_avg_loss': 0.676392936706543, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 46, 'val_loss': 31.82339096069336, 'val_avg_loss': 0.6918128469715947, 'val_seen': 46, 'val_correct': 23, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 27.05571746826172, 'test_avg_loss': 0.676392936706543, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-10-02 10:42:22 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:42:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:42:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-02 10:42:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-02 10:42:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=89.629593, avg_loss=0.679012, seen=132, correct=75, accuracy=0.568182
2025-10-02 10:42:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:26 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 132, 'val_loss': 89.62959289550781, 'val_avg_loss': 0.6790120673902107, 'val_seen': 132, 'val_correct': 75, 'val_acc': 0.5681818181818182}
2025-10-02 10:42:26 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:42:26 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 132, 'val_loss': 89.62959289550781, 'val_avg_loss': 0.6790120673902107, 'val_seen': 132, 'val_correct': 75, 'val_acc': 0.5681818181818182}
2025-10-02 10:42:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 33, 'val_loss': 22.66197443008423, 'val_avg_loss': 0.6867264978813402, 'val_seen': 33, 'val_correct': 18, 'val_acc': 0.5454545454545454}}
2025-10-02 10:42:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 132, 'val_loss': 89.62959289550781, 'val_avg_loss': 0.6790120673902107, 'val_seen': 132, 'val_correct': 75, 'val_acc': 0.5681818181818182}}
2025-10-02 10:42:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:42:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.317837, avg_loss=0.732946, seen=40, correct=19, accuracy=0.475000
2025-10-02 10:42:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:29 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.31783676147461, 'test_avg_loss': 0.7329459190368652, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:42:29 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 132, 'val_loss': 89.62959289550781, 'val_avg_loss': 0.6790120673902107, 'val_seen': 132, 'val_correct': 75, 'val_acc': 0.5681818181818182}
2025-10-02 10:42:29 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.31783676147461, 'test_avg_loss': 0.7329459190368652, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:42:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.145511150360107, 'test_avg_loss': 0.7145511150360108, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:42:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.31783676147461, 'test_avg_loss': 0.7329459190368652, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-10-02 10:42:29 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.31783676147461, 'test_avg_loss': 0.7329459190368652, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 132, 'val_loss': 89.62959289550781, 'val_avg_loss': 0.6790120673902107, 'val_seen': 132, 'val_correct': 75, 'val_acc': 0.5681818181818182, 'test_total': 40, 'test_loss': 29.31783676147461, 'test_avg_loss': 0.7329459190368652, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-10-02 10:42:29 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:42:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:42:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-02 10:42:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-02 10:42:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=91.448601, avg_loss=0.687583, seen=133, correct=78, accuracy=0.586466
2025-10-02 10:42:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:34 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 133, 'val_loss': 91.44860076904297, 'val_avg_loss': 0.6875834644288945, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338}
2025-10-02 10:42:34 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:42:34 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 133, 'val_loss': 91.44860076904297, 'val_avg_loss': 0.6875834644288945, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338}
2025-10-02 10:42:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 22.84826695919037, 'val_avg_loss': 0.6720078517408932, 'val_seen': 34, 'val_correct': 23, 'val_acc': 0.6764705882352942}}
2025-10-02 10:42:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 133, 'val_loss': 91.44860076904297, 'val_avg_loss': 0.6875834644288945, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338}}
2025-10-02 10:42:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:42:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:42:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.922821, avg_loss=0.698071, seen=40, correct=22, accuracy=0.550000
2025-10-02 10:42:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:37 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.922821044921875, 'test_avg_loss': 0.6980705261230469, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:42:37 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 133, 'val_loss': 91.44860076904297, 'val_avg_loss': 0.6875834644288945, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338}
2025-10-02 10:42:37 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.922821044921875, 'test_avg_loss': 0.6980705261230469, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:42:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.521797299385071, 'test_avg_loss': 0.7521797299385071, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:42:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.922821044921875, 'test_avg_loss': 0.6980705261230469, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-10-02 10:42:37 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.922821044921875, 'test_avg_loss': 0.6980705261230469, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 133, 'val_loss': 91.44860076904297, 'val_avg_loss': 0.6875834644288945, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338, 'test_total': 40, 'test_loss': 27.922821044921875, 'test_avg_loss': 0.6980705261230469, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-10-02 10:42:37 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:42:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:42:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-02 10:42:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-02 10:42:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=57.466667, avg_loss=0.692369, seen=83, correct=41, accuracy=0.493976
2025-10-02 10:42:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:40 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 83, 'val_loss': 57.46666717529297, 'val_avg_loss': 0.6923694840396744, 'val_seen': 83, 'val_correct': 41, 'val_acc': 0.4939759036144578}
2025-10-02 10:42:40 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:42:40 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 83, 'val_loss': 57.46666717529297, 'val_avg_loss': 0.6923694840396744, 'val_seen': 83, 'val_correct': 41, 'val_acc': 0.4939759036144578}
2025-10-02 10:42:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 21, 'val_loss': 14.991001844406128, 'val_avg_loss': 0.713857230686006, 'val_seen': 21, 'val_correct': 10, 'val_acc': 0.47619047619047616}}
2025-10-02 10:42:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 83, 'val_loss': 57.46666717529297, 'val_avg_loss': 0.6923694840396744, 'val_seen': 83, 'val_correct': 41, 'val_acc': 0.4939759036144578}}
2025-10-02 10:42:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:42:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:42:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.808775, avg_loss=0.695219, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:42:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:43 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.808774948120117, 'test_avg_loss': 0.6952193737030029, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:42:43 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 83, 'val_loss': 57.46666717529297, 'val_avg_loss': 0.6923694840396744, 'val_seen': 83, 'val_correct': 41, 'val_acc': 0.4939759036144578}
2025-10-02 10:42:43 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.808774948120117, 'test_avg_loss': 0.6952193737030029, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:42:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.622284173965454, 'test_avg_loss': 0.6622284173965454, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-10-02 10:42:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.808774948120117, 'test_avg_loss': 0.6952193737030029, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-10-02 10:42:43 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.808774948120117, 'test_avg_loss': 0.6952193737030029, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 83, 'val_loss': 57.46666717529297, 'val_avg_loss': 0.6923694840396744, 'val_seen': 83, 'val_correct': 41, 'val_acc': 0.4939759036144578, 'test_total': 40, 'test_loss': 27.808774948120117, 'test_avg_loss': 0.6952193737030029, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:42:43 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:42:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:42:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-02 10:42:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-02 10:42:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=130.294693, avg_loss=0.693057, seen=188, correct=91, accuracy=0.484043
2025-10-02 10:42:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:49 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 188, 'val_loss': 130.29469299316406, 'val_avg_loss': 0.6930568776232131, 'val_seen': 188, 'val_correct': 91, 'val_acc': 0.48404255319148937}
2025-10-02 10:42:49 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:42:49 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 188, 'val_loss': 130.29469299316406, 'val_avg_loss': 0.6930568776232131, 'val_seen': 188, 'val_correct': 91, 'val_acc': 0.48404255319148937}
2025-10-02 10:42:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 47, 'val_loss': 32.65347611904144, 'val_avg_loss': 0.694754811043435, 'val_seen': 47, 'val_correct': 23, 'val_acc': 0.48936170212765956}}
2025-10-02 10:42:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 188, 'val_loss': 130.29469299316406, 'val_avg_loss': 0.6930568776232131, 'val_seen': 188, 'val_correct': 91, 'val_acc': 0.48404255319148937}}
2025-10-02 10:42:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:42:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:42:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.762753, avg_loss=0.694069, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:42:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:52 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.762752532958984, 'test_avg_loss': 0.6940688133239746, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:42:52 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 188, 'val_loss': 130.29469299316406, 'val_avg_loss': 0.6930568776232131, 'val_seen': 188, 'val_correct': 91, 'val_acc': 0.48404255319148937}
2025-10-02 10:42:52 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.762752532958984, 'test_avg_loss': 0.6940688133239746, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:42:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.037707805633545, 'test_avg_loss': 0.7037707805633545, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-10-02 10:42:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.762752532958984, 'test_avg_loss': 0.6940688133239746, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-10-02 10:42:52 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.762752532958984, 'test_avg_loss': 0.6940688133239746, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 188, 'val_loss': 130.29469299316406, 'val_avg_loss': 0.6930568776232131, 'val_seen': 188, 'val_correct': 91, 'val_acc': 0.48404255319148937, 'test_total': 40, 'test_loss': 27.762752532958984, 'test_avg_loss': 0.6940688133239746, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:42:52 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:42:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:42:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-02 10:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-02 10:42:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.288116, avg_loss=0.686441, seen=200, correct=111, accuracy=0.555000
2025-10-02 10:42:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:42:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:42:57 (federatedscope.core.workers.client:727) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 137.28811645507812, 'val_avg_loss': 0.6864405822753906, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-10-02 10:42:57 (federatedscope.core.workers.client:728) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-10-02 10:42:57 (federatedscope.core.workers.client:729) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 137.28811645507812, 'val_avg_loss': 0.6864405822753906, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-10-02 10:42:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.38771975040436, 'val_avg_loss': 0.7077543950080871, 'val_seen': 50, 'val_correct': 28, 'val_acc': 0.56}}
2025-10-02 10:42:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 137.28811645507812, 'val_avg_loss': 0.6864405822753906, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}}
2025-10-02 10:42:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-02 10:42:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=800, num_train_batch_last_epoch=1600, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-02 10:42:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-02 10:42:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.935987, avg_loss=0.698400, seen=40, correct=18, accuracy=0.450000
2025-10-02 10:42:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-02 10:42:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-02 10:42:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-02 10:43:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2686MB allocated=2649MB
2025-10-02 10:43:00 (federatedscope.core.workers.client:727) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.93598747253418, 'test_avg_loss': 0.6983996868133545, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:43:00 (federatedscope.core.workers.client:728) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 137.28811645507812, 'val_avg_loss': 0.6864405822753906, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-10-02 10:43:00 (federatedscope.core.workers.client:729) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.93598747253418, 'test_avg_loss': 0.6983996868133545, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:43:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.566998839378357, 'test_avg_loss': 0.7566998839378357, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-10-02 10:43:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.93598747253418, 'test_avg_loss': 0.6983996868133545, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-10-02 10:43:00 (federatedscope.core.workers.client:756) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.93598747253418, 'test_avg_loss': 0.6983996868133545, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 200, 'val_loss': 137.28811645507812, 'val_avg_loss': 0.6864405822753906, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555, 'test_total': 40, 'test_loss': 27.93598747253418, 'test_avg_loss': 0.6983996868133545, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-10-02 10:43:00 (federatedscope.core.workers.client:766) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-10-02 10:43:00 (federatedscope.core.workers.server:772) INFO: {'Role': 'Server #', 'Round': 0, 'Results_weighted_avg': {'val_total': 124.9622641509434, 'val_loss': 108.5644144527361, 'val_avg_loss': 0.6958555860604366, 'val_acc': 0.5186471387588706, 'test_total': 40.0, 'test_loss': 27.831162614642448, 'test_avg_loss': 0.6957790653660613, 'test_acc': 0.5084905660377359}, 'Results_avg': {'val_total': 124.9622641509434, 'val_loss': 86.9556895561938, 'val_avg_loss': 0.6969133940131321, 'val_acc': 0.5156879697605187, 'test_total': 40.0, 'test_loss': 27.831162614642448, 'test_avg_loss': 0.6957790653660612, 'test_acc': 0.5084905660377358}, 'Results_fairness': {'val_total': 124.9622641509434, 'test_total': 40.0, 'val_loss_std': 43.30656504984056, 'val_loss_bottom_decile': 22.50518226623535, 'val_loss_top_decile': 139.54110717773438, 'val_loss_min': 7.752235412597656, 'val_loss_max': 141.89999389648438, 'val_loss_bottom10%': 10.703056716918946, 'val_loss_top10%': 140.47562408447266, 'val_loss_cos1': 0.895131347647442, 'val_loss_entropy': 3.8224788196226753, 'val_avg_loss_std': 0.01735758215890446, 'val_avg_loss_bottom_decile': 0.6790120673902107, 'val_avg_loss_top_decile': 0.7094999694824219, 'val_avg_loss_min': 0.6426264899117606, 'val_avg_loss_max': 0.7615907842462714, 'val_avg_loss_bottom10%': 0.6698404995129625, 'val_avg_loss_top10%': 0.7286071689464183, 'val_avg_loss_cos1': 0.9996899806190243, 'val_avg_loss_entropy': 3.9699835345303187, 'val_acc_std': 0.05978728214810312, 'val_acc_bottom_decile': 0.4375, 'val_acc_top_decile': 0.583941605839416, 'val_acc_min': 0.36363636363636365, 'val_acc_max': 0.6428571428571429, 'val_acc_bottom10%': 0.39081962481962484, 'val_acc_top10%': 0.6049458019003905, 'val_acc_cos1': 0.9933463150224003, 'val_acc_entropy': 3.963384252586337, 'test_loss_std': 0.9360448606576222, 'test_loss_bottom_decile': 26.679269790649414, 'test_loss_top_decile': 29.168304443359375, 'test_loss_min': 25.01206398010254, 'test_loss_max': 29.679832458496094, 'test_loss_bottom10%': 26.169749450683593, 'test_loss_top10%': 29.33972422281901, 'test_loss_cos1': 0.9994348908025152, 'test_loss_entropy': 3.969724575050869, 'test_avg_loss_std': 0.023401121516440554, 'test_avg_loss_bottom_decile': 0.6669817447662354, 'test_avg_loss_top_decile': 0.7292076110839844, 'test_avg_loss_min': 0.6253015995025635, 'test_avg_loss_max': 0.7419958114624023, 'test_avg_loss_bottom10%': 0.6542437362670899, 'test_avg_loss_top10%': 0.7334931055704752, 'test_avg_loss_cos1': 0.9994348908025151, 'test_avg_loss_entropy': 3.9697245750524623, 'test_acc_std': 0.09032741756427305, 'test_acc_bottom_decile': 0.4, 'test_acc_top_decile': 0.625, 'test_acc_min': 0.35, 'test_acc_max': 0.775, 'test_acc_bottom10%': 0.37, 'test_acc_top10%': 0.6750000000000002, 'test_acc_cos1': 0.9845861587236883, 'test_acc_entropy': 3.9547714951464137}}
2025-10-02 10:43:00 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 7.752235412597656, 'val_total': 11.0, 'val_avg_loss': 0.6426264899117606, 'val_acc': 0.6428571428571429, 'test_total': 40.0, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_acc': 0.775}}
2025-10-02 10:43:00 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 7.752235412597656, 'val_total': 11.0, 'val_avg_loss': 0.6426264899117606, 'val_acc': 0.6428571428571429, 'test_total': 40.0, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_acc': 0.775}, 'client_summarized_weighted_avg': {'val_loss': 108.5644144527361, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6958555860604366, 'val_acc': 0.5186471387588706, 'test_total': 40.0, 'test_loss': 27.831162614642448, 'test_avg_loss': 0.6957790653660613, 'test_acc': 0.5084905660377359}}
2025-10-02 10:43:00 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 7.752235412597656, 'val_total': 11.0, 'val_avg_loss': 0.6426264899117606, 'val_acc': 0.6428571428571429, 'test_total': 40.0, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_acc': 0.775}, 'client_summarized_weighted_avg': {'val_loss': 108.5644144527361, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6958555860604366, 'val_acc': 0.5186471387588706, 'test_total': 40.0, 'test_loss': 27.831162614642448, 'test_avg_loss': 0.6957790653660613, 'test_acc': 0.5084905660377359}, 'client_summarized_avg': {'val_loss': 86.9556895561938, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6969133940131321, 'val_acc': 0.5156879697605187, 'test_total': 40.0, 'test_loss': 27.831162614642448, 'test_avg_loss': 0.6957790653660612, 'test_acc': 0.5084905660377358}}
2025-10-02 10:43:00 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 7.752235412597656, 'val_total': 11.0, 'val_avg_loss': 0.6426264899117606, 'val_acc': 0.6428571428571429, 'test_total': 40.0, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_acc': 0.775}, 'client_summarized_weighted_avg': {'val_loss': 108.5644144527361, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6958555860604366, 'val_acc': 0.5186471387588706, 'test_total': 40.0, 'test_loss': 27.831162614642448, 'test_avg_loss': 0.6957790653660613, 'test_acc': 0.5084905660377359}, 'client_summarized_avg': {'val_loss': 86.9556895561938, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6969133940131321, 'val_acc': 0.5156879697605187, 'test_total': 40.0, 'test_loss': 27.831162614642448, 'test_avg_loss': 0.6957790653660612, 'test_acc': 0.5084905660377358}, 'client_summarized_fairness': {'val_loss_entropy': 3.8224788196226753, 'val_loss_cos1': 0.895131347647442, 'val_loss_top10%': 140.47562408447266, 'val_loss_bottom10%': 10.703056716918946, 'val_loss_max': 141.89999389648438, 'val_loss_min': 7.752235412597656, 'val_loss_top_decile': 139.54110717773438, 'val_loss_bottom_decile': 22.50518226623535, 'val_loss_std': 43.30656504984056, 'val_total': 124.9622641509434, 'test_total': 40.0, 'val_avg_loss_std': 0.01735758215890446, 'val_avg_loss_bottom_decile': 0.6790120673902107, 'val_avg_loss_top_decile': 0.7094999694824219, 'val_avg_loss_min': 0.6426264899117606, 'val_avg_loss_max': 0.7615907842462714, 'val_avg_loss_bottom10%': 0.6698404995129625, 'val_avg_loss_top10%': 0.7286071689464183, 'val_avg_loss_cos1': 0.9996899806190243, 'val_avg_loss_entropy': 3.9699835345303187, 'val_acc_std': 0.05978728214810312, 'val_acc_bottom_decile': 0.4375, 'val_acc_top_decile': 0.583941605839416, 'val_acc_min': 0.36363636363636365, 'val_acc_max': 0.6428571428571429, 'val_acc_bottom10%': 0.39081962481962484, 'val_acc_top10%': 0.6049458019003905, 'val_acc_cos1': 0.9933463150224003, 'val_acc_entropy': 3.963384252586337, 'test_loss_std': 0.9360448606576222, 'test_loss_bottom_decile': 26.679269790649414, 'test_loss_top_decile': 29.168304443359375, 'test_loss_min': 25.01206398010254, 'test_loss_max': 29.679832458496094, 'test_loss_bottom10%': 26.169749450683593, 'test_loss_top10%': 29.33972422281901, 'test_loss_cos1': 0.9994348908025152, 'test_loss_entropy': 3.969724575050869, 'test_avg_loss_std': 0.023401121516440554, 'test_avg_loss_bottom_decile': 0.6669817447662354, 'test_avg_loss_top_decile': 0.7292076110839844, 'test_avg_loss_min': 0.6253015995025635, 'test_avg_loss_max': 0.7419958114624023, 'test_avg_loss_bottom10%': 0.6542437362670899, 'test_avg_loss_top10%': 0.7334931055704752, 'test_avg_loss_cos1': 0.9994348908025151, 'test_avg_loss_entropy': 3.9697245750524623, 'test_acc_std': 0.09032741756427305, 'test_acc_bottom_decile': 0.4, 'test_acc_top_decile': 0.625, 'test_acc_min': 0.35, 'test_acc_max': 0.775, 'test_acc_bottom10%': 0.37, 'test_acc_top10%': 0.6750000000000002, 'test_acc_cos1': 0.9845861587236883, 'test_acc_entropy': 3.9547714951464137}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:518) INFO: Server: Final evaluation is finished! Starting merging results.
2025-10-02 10:43:00 (federatedscope.core.workers.server:644) INFO: {'Role': 'Server #', 'Round': 'Final', 'Results_raw': {'client_best_individual': {'val_loss': 7.752235412597656, 'val_total': 11.0, 'val_avg_loss': 0.6426264899117606, 'val_acc': 0.6428571428571429, 'test_total': 40.0, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_acc': 0.775}, 'client_summarized_weighted_avg': {'val_loss': 108.5644144527361, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6958555860604366, 'val_acc': 0.5186471387588706, 'test_total': 40.0, 'test_loss': 27.831162614642448, 'test_avg_loss': 0.6957790653660613, 'test_acc': 0.5084905660377359}, 'client_summarized_avg': {'val_loss': 86.9556895561938, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6969133940131321, 'val_acc': 0.5156879697605187, 'test_total': 40.0, 'test_loss': 27.831162614642448, 'test_avg_loss': 0.6957790653660612, 'test_acc': 0.5084905660377358}, 'client_summarized_fairness': {'val_loss_entropy': 3.8224788196226753, 'val_loss_cos1': 0.895131347647442, 'val_loss_top10%': 140.47562408447266, 'val_loss_bottom10%': 10.703056716918946, 'val_loss_max': 141.89999389648438, 'val_loss_min': 7.752235412597656, 'val_loss_top_decile': 139.54110717773438, 'val_loss_bottom_decile': 22.50518226623535, 'val_loss_std': 43.30656504984056, 'val_total': 124.9622641509434, 'test_total': 40.0, 'val_avg_loss_std': 0.01735758215890446, 'val_avg_loss_bottom_decile': 0.6790120673902107, 'val_avg_loss_top_decile': 0.7094999694824219, 'val_avg_loss_min': 0.6426264899117606, 'val_avg_loss_max': 0.7615907842462714, 'val_avg_loss_bottom10%': 0.6698404995129625, 'val_avg_loss_top10%': 0.7286071689464183, 'val_avg_loss_cos1': 0.9996899806190243, 'val_avg_loss_entropy': 3.9699835345303187, 'val_acc_std': 0.05978728214810312, 'val_acc_bottom_decile': 0.4375, 'val_acc_top_decile': 0.583941605839416, 'val_acc_min': 0.36363636363636365, 'val_acc_max': 0.6428571428571429, 'val_acc_bottom10%': 0.39081962481962484, 'val_acc_top10%': 0.6049458019003905, 'val_acc_cos1': 0.9933463150224003, 'val_acc_entropy': 3.963384252586337, 'test_loss_std': 0.9360448606576222, 'test_loss_bottom_decile': 26.679269790649414, 'test_loss_top_decile': 29.168304443359375, 'test_loss_min': 25.01206398010254, 'test_loss_max': 29.679832458496094, 'test_loss_bottom10%': 26.169749450683593, 'test_loss_top10%': 29.33972422281901, 'test_loss_cos1': 0.9994348908025152, 'test_loss_entropy': 3.969724575050869, 'test_avg_loss_std': 0.023401121516440554, 'test_avg_loss_bottom_decile': 0.6669817447662354, 'test_avg_loss_top_decile': 0.7292076110839844, 'test_avg_loss_min': 0.6253015995025635, 'test_avg_loss_max': 0.7419958114624023, 'test_avg_loss_bottom10%': 0.6542437362670899, 'test_avg_loss_top10%': 0.7334931055704752, 'test_avg_loss_cos1': 0.9994348908025151, 'test_avg_loss_entropy': 3.9697245750524623, 'test_acc_std': 0.09032741756427305, 'test_acc_bottom_decile': 0.4, 'test_acc_top_decile': 0.625, 'test_acc_min': 0.35, 'test_acc_max': 0.775, 'test_acc_bottom10%': 0.37, 'test_acc_top10%': 0.6750000000000002, 'test_acc_cos1': 0.9845861587236883, 'test_acc_entropy': 3.9547714951464137}}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'val_total': 146, 'val_loss': 100.07649230957031, 'val_avg_loss': 0.6854554267778789, 'val_acc': 0.5616438356164384, 'test_total': 40, 'test_loss': 27.621728897094727, 'test_avg_loss': 0.6905432224273682, 'test_acc': 0.575}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 7.780364990234375, 'val_avg_loss': 0.707305908203125, 'val_acc': 0.36363636363636365, 'test_total': 40, 'test_loss': 28.85181427001953, 'test_avg_loss': 0.7212953567504883, 'test_acc': 0.35}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'val_total': 36, 'val_loss': 25.014095306396484, 'val_avg_loss': 0.6948359807332357, 'val_acc': 0.6388888888888888, 'test_total': 40, 'test_loss': 27.32720184326172, 'test_avg_loss': 0.6831800460815429, 'test_acc': 0.6}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 8.377498626708984, 'val_avg_loss': 0.7615907842462714, 'val_acc': 0.36363636363636365, 'test_total': 40, 'test_loss': 29.328142166137695, 'test_avg_loss': 0.7332035541534424, 'test_acc': 0.375}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'val_total': 14, 'val_loss': 8.996770858764648, 'val_avg_loss': 0.6426264899117606, 'val_acc': 0.6428571428571429, 'test_total': 40, 'test_loss': 28.097932815551758, 'test_avg_loss': 0.7024483203887939, 'test_acc': 0.55}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'val_total': 134, 'val_loss': 92.97224426269531, 'val_avg_loss': 0.6938227183783232, 'val_acc': 0.5223880597014925, 'test_total': 40, 'test_loss': 27.52861976623535, 'test_avg_loss': 0.6882154941558838, 'test_acc': 0.55}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'val_total': 57, 'val_loss': 38.517852783203125, 'val_avg_loss': 0.6757518032140899, 'val_acc': 0.543859649122807, 'test_total': 40, 'test_loss': 28.379833221435547, 'test_avg_loss': 0.7094958305358887, 'test_acc': 0.475}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'val_total': 69, 'val_loss': 49.22789001464844, 'val_avg_loss': 0.7134476813717164, 'val_acc': 0.5652173913043478, 'test_total': 40, 'test_loss': 27.291873931884766, 'test_avg_loss': 0.6822968482971191, 'test_acc': 0.5}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'val_total': 188, 'val_loss': 132.60011291503906, 'val_avg_loss': 0.7053197495480801, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 29.168304443359375, 'test_avg_loss': 0.7292076110839844, 'test_acc': 0.475}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'val_total': 63, 'val_loss': 46.7359733581543, 'val_avg_loss': 0.7418408469548301, 'val_acc': 0.3968253968253968, 'test_total': 40, 'test_loss': 29.03860092163086, 'test_avg_loss': 0.7259650230407715, 'test_acc': 0.525}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #11', 'Round': 1, 'Results_raw': {'val_total': 32, 'val_loss': 22.50518226623535, 'val_avg_loss': 0.7032869458198547, 'val_acc': 0.4375, 'test_total': 40, 'test_loss': 27.32354736328125, 'test_avg_loss': 0.6830886840820313, 'test_acc': 0.5}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #12', 'Round': 1, 'Results_raw': {'val_total': 137, 'val_loss': 95.20416259765625, 'val_avg_loss': 0.6949208948734032, 'val_acc': 0.583941605839416, 'test_total': 40, 'test_loss': 28.205326080322266, 'test_avg_loss': 0.7051331520080566, 'test_acc': 0.425}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #13', 'Round': 1, 'Results_raw': {'val_total': 72, 'val_loss': 50.332672119140625, 'val_avg_loss': 0.6990648905436198, 'val_acc': 0.5138888888888888, 'test_total': 40, 'test_loss': 28.08916473388672, 'test_avg_loss': 0.702229118347168, 'test_acc': 0.4}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #14', 'Round': 1, 'Results_raw': {'val_total': 160, 'val_loss': 113.35665130615234, 'val_avg_loss': 0.7084790706634522, 'val_acc': 0.49375, 'test_total': 40, 'test_loss': 27.55852508544922, 'test_avg_loss': 0.6889631271362304, 'test_acc': 0.475}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #15', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 138.17330932617188, 'val_avg_loss': 0.6908665466308593, 'val_acc': 0.535, 'test_total': 40, 'test_loss': 26.437728881835938, 'test_avg_loss': 0.6609432220458984, 'test_acc': 0.675}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #16', 'Round': 1, 'Results_raw': {'val_total': 136, 'val_loss': 93.35166931152344, 'val_avg_loss': 0.6864093331729665, 'val_acc': 0.5882352941176471, 'test_total': 40, 'test_loss': 27.548303604125977, 'test_avg_loss': 0.6887075901031494, 'test_acc': 0.575}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #17', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 136.96986389160156, 'val_avg_loss': 0.6848493194580079, 'val_acc': 0.58, 'test_total': 40, 'test_loss': 26.93454360961914, 'test_avg_loss': 0.6733635902404785, 'test_acc': 0.5}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #18', 'Round': 1, 'Results_raw': {'val_total': 135, 'val_loss': 95.59817504882812, 'val_avg_loss': 0.7081346299913195, 'val_acc': 0.4666666666666667, 'test_total': 40, 'test_loss': 28.568370819091797, 'test_avg_loss': 0.7142092704772949, 'test_acc': 0.375}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #19', 'Round': 1, 'Results_raw': {'val_total': 110, 'val_loss': 77.84930419921875, 'val_avg_loss': 0.707720947265625, 'val_acc': 0.509090909090909, 'test_total': 40, 'test_loss': 28.288684844970703, 'test_avg_loss': 0.7072171211242676, 'test_acc': 0.575}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #20', 'Round': 1, 'Results_raw': {'val_total': 126, 'val_loss': 86.62260437011719, 'val_avg_loss': 0.6874809870644222, 'val_acc': 0.5476190476190477, 'test_total': 40, 'test_loss': 25.01206398010254, 'test_avg_loss': 0.6253015995025635, 'test_acc': 0.775}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #21', 'Round': 1, 'Results_raw': {'val_total': 153, 'val_loss': 107.88156127929688, 'val_avg_loss': 0.7051082436555351, 'val_acc': 0.5098039215686274, 'test_total': 40, 'test_loss': 26.679269790649414, 'test_avg_loss': 0.6669817447662354, 'test_acc': 0.625}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #22', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 7.752235412597656, 'val_avg_loss': 0.7047486738725142, 'val_acc': 0.5454545454545454, 'test_total': 40, 'test_loss': 28.81939125061035, 'test_avg_loss': 0.7204847812652588, 'test_acc': 0.45}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #23', 'Round': 1, 'Results_raw': {'val_total': 30, 'val_loss': 20.608413696289062, 'val_avg_loss': 0.6869471232096355, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 27.08338165283203, 'test_avg_loss': 0.6770845413208008, 'test_acc': 0.625}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #24', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 138.65020751953125, 'val_avg_loss': 0.6932510375976563, 'val_acc': 0.55, 'test_total': 40, 'test_loss': 29.31708335876465, 'test_avg_loss': 0.7329270839691162, 'test_acc': 0.35}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #25', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 140.6211395263672, 'val_avg_loss': 0.703105697631836, 'val_acc': 0.49, 'test_total': 40, 'test_loss': 29.679832458496094, 'test_avg_loss': 0.7419958114624023, 'test_acc': 0.4}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #26', 'Round': 1, 'Results_raw': {'val_total': 161, 'val_loss': 108.43250274658203, 'val_avg_loss': 0.6734938058793914, 'val_acc': 0.5031055900621118, 'test_total': 40, 'test_loss': 26.3908634185791, 'test_avg_loss': 0.6597715854644776, 'test_acc': 0.65}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #27', 'Round': 1, 'Results_raw': {'val_total': 123, 'val_loss': 87.10189056396484, 'val_avg_loss': 0.7081454517395516, 'val_acc': 0.5040650406504065, 'test_total': 40, 'test_loss': 28.1495418548584, 'test_avg_loss': 0.7037385463714599, 'test_acc': 0.425}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #28', 'Round': 1, 'Results_raw': {'val_total': 75, 'val_loss': 51.366416931152344, 'val_avg_loss': 0.6848855590820313, 'val_acc': 0.5733333333333334, 'test_total': 40, 'test_loss': 27.659269332885742, 'test_avg_loss': 0.6914817333221436, 'test_acc': 0.475}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #29', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 139.7708282470703, 'val_avg_loss': 0.6988541412353516, 'val_acc': 0.47, 'test_total': 40, 'test_loss': 27.143476486206055, 'test_avg_loss': 0.6785869121551513, 'test_acc': 0.525}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #30', 'Round': 1, 'Results_raw': {'val_total': 170, 'val_loss': 117.85662841796875, 'val_avg_loss': 0.6932742848115809, 'val_acc': 0.5529411764705883, 'test_total': 40, 'test_loss': 27.592084884643555, 'test_avg_loss': 0.6898021221160888, 'test_acc': 0.5}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #31', 'Round': 1, 'Results_raw': {'val_total': 193, 'val_loss': 135.35971069335938, 'val_avg_loss': 0.7013456512609294, 'val_acc': 0.538860103626943, 'test_total': 40, 'test_loss': 26.766963958740234, 'test_avg_loss': 0.6691740989685059, 'test_acc': 0.6}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #32', 'Round': 1, 'Results_raw': {'val_total': 112, 'val_loss': 76.04232788085938, 'val_avg_loss': 0.6789493560791016, 'val_acc': 0.5892857142857143, 'test_total': 40, 'test_loss': 27.863191604614258, 'test_avg_loss': 0.6965797901153564, 'test_acc': 0.525}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #33', 'Round': 1, 'Results_raw': {'val_total': 74, 'val_loss': 50.82978057861328, 'val_avg_loss': 0.6868889267380173, 'val_acc': 0.5540540540540541, 'test_total': 40, 'test_loss': 28.431289672851562, 'test_avg_loss': 0.710782241821289, 'test_acc': 0.4}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #34', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 138.1497802734375, 'val_avg_loss': 0.6907489013671875, 'val_acc': 0.515, 'test_total': 40, 'test_loss': 26.376121520996094, 'test_avg_loss': 0.6594030380249023, 'test_acc': 0.7}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #35', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 141.89999389648438, 'val_avg_loss': 0.7094999694824219, 'val_acc': 0.485, 'test_total': 40, 'test_loss': 28.984725952148438, 'test_avg_loss': 0.7246181488037109, 'test_acc': 0.4}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #36', 'Round': 1, 'Results_raw': {'val_total': 54, 'val_loss': 36.95072937011719, 'val_avg_loss': 0.6842727661132812, 'val_acc': 0.5370370370370371, 'test_total': 40, 'test_loss': 26.631969451904297, 'test_avg_loss': 0.6657992362976074, 'test_acc': 0.625}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #37', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 137.14540100097656, 'val_avg_loss': 0.6857270050048828, 'val_acc': 0.555, 'test_total': 40, 'test_loss': 27.380521774291992, 'test_avg_loss': 0.6845130443572998, 'test_acc': 0.525}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #38', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 139.54110717773438, 'val_avg_loss': 0.6977055358886719, 'val_acc': 0.43, 'test_total': 40, 'test_loss': 28.17207145690918, 'test_avg_loss': 0.7043017864227294, 'test_acc': 0.5}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #39', 'Round': 1, 'Results_raw': {'val_total': 83, 'val_loss': 59.0980224609375, 'val_avg_loss': 0.712024366999247, 'val_acc': 0.4457831325301205, 'test_total': 40, 'test_loss': 29.22714614868164, 'test_avg_loss': 0.730678653717041, 'test_acc': 0.425}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #40', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 135.67620849609375, 'val_avg_loss': 0.6783810424804687, 'val_acc': 0.565, 'test_total': 40, 'test_loss': 27.1168212890625, 'test_avg_loss': 0.6779205322265625, 'test_acc': 0.6}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #41', 'Round': 1, 'Results_raw': {'val_total': 119, 'val_loss': 82.66850280761719, 'val_avg_loss': 0.6946933009043461, 'val_acc': 0.4789915966386555, 'test_total': 40, 'test_loss': 27.115346908569336, 'test_avg_loss': 0.6778836727142334, 'test_acc': 0.575}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #42', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 139.5756072998047, 'val_avg_loss': 0.6978780364990235, 'val_acc': 0.515, 'test_total': 40, 'test_loss': 27.57558822631836, 'test_avg_loss': 0.689389705657959, 'test_acc': 0.525}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #43', 'Round': 1, 'Results_raw': {'val_total': 89, 'val_loss': 62.86577224731445, 'val_avg_loss': 0.706356991542859, 'val_acc': 0.4943820224719101, 'test_total': 40, 'test_loss': 27.010961532592773, 'test_avg_loss': 0.6752740383148194, 'test_acc': 0.5}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #44', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 141.445068359375, 'val_avg_loss': 0.707225341796875, 'val_acc': 0.445, 'test_total': 40, 'test_loss': 28.308101654052734, 'test_avg_loss': 0.7077025413513184, 'test_acc': 0.45}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #45', 'Round': 1, 'Results_raw': {'val_total': 100, 'val_loss': 73.32393646240234, 'val_avg_loss': 0.7332393646240234, 'val_acc': 0.4, 'test_total': 40, 'test_loss': 28.88594627380371, 'test_avg_loss': 0.7221486568450928, 'test_acc': 0.475}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #46', 'Round': 1, 'Results_raw': {'val_total': 110, 'val_loss': 75.55398559570312, 'val_avg_loss': 0.6868544145063921, 'val_acc': 0.5454545454545454, 'test_total': 40, 'test_loss': 28.771480560302734, 'test_avg_loss': 0.7192870140075683, 'test_acc': 0.425}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #47', 'Round': 1, 'Results_raw': {'val_total': 147, 'val_loss': 104.26983642578125, 'val_avg_loss': 0.7093186151413691, 'val_acc': 0.4965986394557823, 'test_total': 40, 'test_loss': 27.51497459411621, 'test_avg_loss': 0.6878743648529053, 'test_acc': 0.55}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #48', 'Round': 1, 'Results_raw': {'val_total': 46, 'val_loss': 31.82339096069336, 'val_avg_loss': 0.6918128469715947, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 27.05571746826172, 'test_avg_loss': 0.676392936706543, 'test_acc': 0.5}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #49', 'Round': 1, 'Results_raw': {'val_total': 132, 'val_loss': 89.62959289550781, 'val_avg_loss': 0.6790120673902107, 'val_acc': 0.5681818181818182, 'test_total': 40, 'test_loss': 29.31783676147461, 'test_avg_loss': 0.7329459190368652, 'test_acc': 0.475}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #50', 'Round': 1, 'Results_raw': {'val_total': 133, 'val_loss': 91.44860076904297, 'val_avg_loss': 0.6875834644288945, 'val_acc': 0.5864661654135338, 'test_total': 40, 'test_loss': 27.922821044921875, 'test_avg_loss': 0.6980705261230469, 'test_acc': 0.55}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #51', 'Round': 1, 'Results_raw': {'val_total': 83, 'val_loss': 57.46666717529297, 'val_avg_loss': 0.6923694840396744, 'val_acc': 0.4939759036144578, 'test_total': 40, 'test_loss': 27.808774948120117, 'test_avg_loss': 0.6952193737030029, 'test_acc': 0.45}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #52', 'Round': 1, 'Results_raw': {'val_total': 188, 'val_loss': 130.29469299316406, 'val_avg_loss': 0.6930568776232131, 'val_acc': 0.48404255319148937, 'test_total': 40, 'test_loss': 27.762752532958984, 'test_avg_loss': 0.6940688133239746, 'test_acc': 0.45}}
2025-10-02 10:43:00 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #53', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 137.28811645507812, 'val_avg_loss': 0.6864405822753906, 'val_acc': 0.555, 'test_total': 40, 'test_loss': 27.93598747253418, 'test_avg_loss': 0.6983996868133545, 'test_acc': 0.45}}
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 237.35513596666667, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 64648, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 1 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 237.35540481666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 2 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 237.2964163, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 3 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 237.25679308333335, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 4 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 237.21757861666669, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 5 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 237.17858601666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 6 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 237.13907468333335, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 7 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 237.0998863, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 8 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 237.06046641666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 9 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 237.01821861666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 10 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 236.97664883333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 11 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 236.93675206666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 12 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 236.89672395, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 13 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 236.85684845, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 14 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 236.81722704999999, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 15 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 236.77717023333335, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 16 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 236.73730658333335, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 17 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 236.69720585, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 18 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 236.65789933333332, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 19 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 236.6177796, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 20 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 236.57427805, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 21 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 236.53207581666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 22 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 236.49169088333335, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 23 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 236.45099081666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 24 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 236.41063185, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 25 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 236.36997443333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 26 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 236.32943168333335, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 27 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 236.28903388333336, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 28 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 236.24842471666668, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 29 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 236.20762745, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 30 received finish message =================
2025-10-02 10:43:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:00 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 236.16504438333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:00 (federatedscope.core.workers.client:842) INFO: ================= client 31 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 236.12042195, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 32 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 236.07921885, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 33 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 236.03783915, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 34 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 235.99644961666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 35 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 235.9550006, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 36 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #36, the system-related metrics are: {'id': 36, 'fl_end_time_minutes': 235.91354238333332, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 37 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #37, the system-related metrics are: {'id': 37, 'fl_end_time_minutes': 235.87200435000003, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 38 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #38, the system-related metrics are: {'id': 38, 'fl_end_time_minutes': 235.8310166, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 39 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #39, the system-related metrics are: {'id': 39, 'fl_end_time_minutes': 235.79101926666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 40 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #40, the system-related metrics are: {'id': 40, 'fl_end_time_minutes': 235.75043486666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 41 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #41, the system-related metrics are: {'id': 41, 'fl_end_time_minutes': 235.70575691666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 42 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #42, the system-related metrics are: {'id': 42, 'fl_end_time_minutes': 235.66565601666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 43 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #43, the system-related metrics are: {'id': 43, 'fl_end_time_minutes': 235.62588801666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 44 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #44, the system-related metrics are: {'id': 44, 'fl_end_time_minutes': 235.58577325, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 45 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #45, the system-related metrics are: {'id': 45, 'fl_end_time_minutes': 235.54587256666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 46 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #46, the system-related metrics are: {'id': 46, 'fl_end_time_minutes': 235.50629421666665, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 47 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #47, the system-related metrics are: {'id': 47, 'fl_end_time_minutes': 235.46701171666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 48 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #48, the system-related metrics are: {'id': 48, 'fl_end_time_minutes': 235.42783938333332, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 49 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #49, the system-related metrics are: {'id': 49, 'fl_end_time_minutes': 235.38874116666665, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 50 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #50, the system-related metrics are: {'id': 50, 'fl_end_time_minutes': 235.32974656666664, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 51 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #51, the system-related metrics are: {'id': 51, 'fl_end_time_minutes': 235.2872296, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 52 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #52, the system-related metrics are: {'id': 52, 'fl_end_time_minutes': 235.24693825, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.workers.client:842) INFO: ================= client 53 received finish message =================
2025-10-02 10:43:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:268) INFO: In worker #53, the system-related metrics are: {'id': 53, 'fl_end_time_minutes': 235.2074382, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 225856, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:441) INFO: We will compress the file eval_results.raw into a .gz file, and delete the old one
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 236.3028788932099, 'sys_avg/total_model_size': '466.3M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '217.65K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-10-02 10:43:01 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.6379028713745841, 'sys_std/total_model_size': '64.05M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '21.22K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
