2025-11-11 15:44:09 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/base_1.0/exp_print.log
2025-11-11 15:44:09 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/base_1.0
2025-11-11 15:44:29 (federatedscope.core.data.base_translator:236) INFO: Main process: Completion file found. Skipping generation.
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:266) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=75955, Val=29029, Test=39603, Total=144587
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=503, Val=26, Test=40, Total=569
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=1378, Val=72, Test=40, Total=1490
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=1021, Val=53, Test=40, Total=1114
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=2655, Val=139, Test=40, Total=2834
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=3146, Val=165, Test=40, Total=3351
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=4423, Val=200, Test=40, Total=4663
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=2331, Val=122, Test=40, Total=2493
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=315, Val=16, Test=40, Total=371
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=601, Val=31, Test=40, Total=672
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=647, Val=34, Test=40, Total=721
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2893, Val=152, Test=40, Total=3085
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=18241, Val=200, Test=40, Total=18481
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=5557, Val=200, Test=40, Total=5797
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=3713, Val=195, Test=40, Total=3948
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=8035, Val=200, Test=40, Total=8275
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=2083, Val=109, Test=40, Total=2232
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=9535, Val=200, Test=40, Total=9775
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=3066, Val=161, Test=40, Total=3267
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=176, Val=9, Test=40, Total=225
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=9424, Val=200, Test=40, Total=9664
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=1724, Val=90, Test=40, Total=1854
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=825, Val=43, Test=40, Total=908
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=43, Val=2, Test=40, Total=85
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=9397, Val=200, Test=40, Total=9637
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=5211, Val=200, Test=40, Total=5451
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=4112, Val=200, Test=40, Total=4352
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=6572, Val=200, Test=40, Total=6812
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=8062, Val=200, Test=40, Total=8302
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=2723, Val=143, Test=40, Total=2906
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=229, Val=12, Test=40, Total=281
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=2594, Val=136, Test=40, Total=2770
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=3560, Val=187, Test=40, Total=3787
2025-11-11 15:45:00 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=7379, Val=200, Test=40, Total=7619
2025-11-11 15:45:02 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-11-11 15:45:02 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-11-11 15:45:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140560092598336 | out_emb=(Linear) num=151646 ptr=140560092598336 | lora_ptr=None
2025-11-11 15:45:07 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-11-11 15:45:08 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:09 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-11-11 15:45:09 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:10 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-11-11 15:45:11 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:12 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-11-11 15:45:12 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:13 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-11-11 15:45:14 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:15 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-11-11 15:45:15 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:16 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-11-11 15:45:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:18 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-11-11 15:45:18 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:19 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-11-11 15:45:20 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:21 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-11-11 15:45:21 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:23 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-11-11 15:45:23 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:24 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-11-11 15:45:24 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:25 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-11-11 15:45:26 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:27 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-11-11 15:45:27 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:28 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-11-11 15:45:29 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:30 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-11-11 15:45:30 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:32 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-11-11 15:45:32 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:33 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-11-11 15:45:33 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:35 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-11-11 15:45:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:36 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-11-11 15:45:36 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:37 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-11-11 15:45:38 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:39 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-11-11 15:45:39 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:40 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-11-11 15:45:41 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:42 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-11-11 15:45:42 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:43 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-11-11 15:45:43 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:45 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-11-11 15:45:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:46 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-11-11 15:45:46 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:48 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-11-11 15:45:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:49 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-11-11 15:45:50 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:51 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-11-11 15:45:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:52 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-11-11 15:45:53 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:54 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-11-11 15:45:54 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:55 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-11-11 15:45:55 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:57 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-11-11 15:45:57 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:45:58 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-11-11 15:45:58 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:46:00 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-11-11 15:46:00 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-11-11 15:46:00 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 336.
2025-11-11 15:46:00 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 626.
2025-11-11 15:46:00 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 336. 
Preserved para names in local update: {'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight'}.
2025-11-11 15:46:00 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-11-11 15:46:00 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-11-11 15:46:00 (federatedscope.core.workers.server:984) INFO: Waited all clients join, start now...
2025-11-11 15:46:00 (federatedscope.core.workers.server:993) INFO: ----------- Starting training (Round #0) -------------
2025-11-11 15:46:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 15:46:05 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 15:46:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:05 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 15:46:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:46:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:07 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 15:46:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:46:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:46:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=144.072144, avg_loss=0.720361, seen=200, correct=108, accuracy=0.540000
2025-11-11 15:46:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:46:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:46:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=988MB allocated=983MB
2025-11-11 15:46:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:46:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.548676, avg_loss=0.713717, seen=40, correct=20, accuracy=0.500000
2025-11-11 15:46:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:46:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=988MB allocated=983MB
2025-11-11 15:46:14 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 15:46:14 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 15:46:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2016, total=8062)
2025-11-11 15:46:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:14 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 15:46:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:14 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 15:46:14 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1008, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 15:46:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 15:46:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 15:46:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 15:46:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:46:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:46:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.109344, avg_loss=0.705547, seen=200, correct=103, accuracy=0.515000
2025-11-11 15:46:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:46:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:46:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:46:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:46:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:46:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.865475, avg_loss=0.696637, seen=40, correct=23, accuracy=0.575000
2025-11-11 15:46:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:46:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:46:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:46:27 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 15:46:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 15:46:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 15:46:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 15:46:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:46:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.883514, avg_loss=0.714418, seen=200, correct=94, accuracy=0.470000
2025-11-11 15:46:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:46:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:46:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:46:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.679352, avg_loss=0.716984, seen=40, correct=23, accuracy=0.575000
2025-11-11 15:46:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:46:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:46:40 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 15:46:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 15:46:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 15:46:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 15:46:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:46:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:46:50 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.136658, avg_loss=0.710683, seen=200, correct=93, accuracy=0.465000
2025-11-11 15:46:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:46:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:46:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:46:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:46:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:46:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.033554, avg_loss=0.700839, seen=40, correct=24, accuracy=0.600000
2025-11-11 15:46:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:46:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:46:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:46:53 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 15:47:00 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 15:47:00 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 15:47:00 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 15:47:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:47:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:47:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.100922, avg_loss=0.700505, seen=200, correct=102, accuracy=0.510000
2025-11-11 15:47:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:47:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:47:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.481632, avg_loss=0.687041, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:47:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-11-11 15:47:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 15:47:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 15:47:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 15:47:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:47:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:47:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=137.335297, avg_loss=0.686676, seen=200, correct=106, accuracy=0.530000
2025-11-11 15:47:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:47:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:47:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.884258, avg_loss=0.672106, seen=40, correct=23, accuracy=0.575000
2025-11-11 15:47:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:19 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.575000
2025-11-11 15:47:25 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 15:47:25 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 15:47:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 15:47:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:47:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:47:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.852356, avg_loss=0.684262, seen=200, correct=105, accuracy=0.525000
2025-11-11 15:47:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:47:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:47:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.859974, avg_loss=0.671499, seen=40, correct=25, accuracy=0.625000
2025-11-11 15:47:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:31 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 15:47:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 15:47:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 15:47:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 15:47:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:47:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:47:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.783569, avg_loss=0.678918, seen=200, correct=106, accuracy=0.530000
2025-11-11 15:47:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:47:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.687084, avg_loss=0.667177, seen=40, correct=25, accuracy=0.625000
2025-11-11 15:47:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:44 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 15:47:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 15:47:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 15:47:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 15:47:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:47:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.071213, avg_loss=0.680356, seen=200, correct=105, accuracy=0.525000
2025-11-11 15:47:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:47:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:47:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.566946, avg_loss=0.664174, seen=40, correct=25, accuracy=0.625000
2025-11-11 15:47:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:47:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:47:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:47:57 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 15:48:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 15:48:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 15:48:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 15:48:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:48:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:48:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.773575, avg_loss=0.683868, seen=200, correct=111, accuracy=0.555000
2025-11-11 15:48:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:48:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:48:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:48:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.915615, avg_loss=0.672890, seen=40, correct=27, accuracy=0.675000
2025-11-11 15:48:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:48:10 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.675000
2025-11-11 15:48:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 15:48:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 15:48:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 15:48:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:48:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:48:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=134.101227, avg_loss=0.670506, seen=200, correct=114, accuracy=0.570000
2025-11-11 15:48:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:48:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:48:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.262915, avg_loss=0.656573, seen=40, correct=24, accuracy=0.600000
2025-11-11 15:48:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:48:23 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.600000
2025-11-11 15:48:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 15:48:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 15:48:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:48:24 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 15:48:24 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {}}
2025-11-11 15:48:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 15:48:24 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 15:48:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:24 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 15:48:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:48:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 15:48:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:48:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:48:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=146.579391, avg_loss=0.732897, seen=200, correct=104, accuracy=0.520000
2025-11-11 15:48:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1030MB allocated=1008MB
2025-11-11 15:48:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:48:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:48:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.784618, avg_loss=0.694615, seen=40, correct=23, accuracy=0.575000
2025-11-11 15:48:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1030MB allocated=1008MB
2025-11-11 15:48:31 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 15:48:31 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 15:48:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2356, total=9424)
2025-11-11 15:48:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:31 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 15:48:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:31 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 15:48:31 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1178, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 15:48:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 15:48:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 15:48:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 15:48:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:48:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:48:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=147.158630, avg_loss=0.735793, seen=200, correct=99, accuracy=0.495000
2025-11-11 15:48:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:48:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:48:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.318180, avg_loss=0.707955, seen=40, correct=20, accuracy=0.500000
2025-11-11 15:48:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:48:44 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.500000
2025-11-11 15:48:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 15:48:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 15:48:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 15:48:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:48:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=152.870163, avg_loss=0.764351, seen=200, correct=87, accuracy=0.435000
2025-11-11 15:48:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:48:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:48:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:48:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.607725, avg_loss=0.740193, seen=40, correct=19, accuracy=0.475000
2025-11-11 15:48:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:48:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:48:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:48:57 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.475000
2025-11-11 15:49:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 15:49:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 15:49:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 15:49:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:49:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=144.601379, avg_loss=0.723007, seen=200, correct=94, accuracy=0.470000
2025-11-11 15:49:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:49:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:49:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:49:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:49:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.480682, avg_loss=0.687017, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:49:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:49:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:49:10 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.550000
2025-11-11 15:49:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 15:49:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 15:49:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 15:49:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:49:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:49:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:49:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.831955, avg_loss=0.709160, seen=200, correct=103, accuracy=0.515000
2025-11-11 15:49:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:49:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:49:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:49:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:49:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:49:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:49:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.072752, avg_loss=0.676819, seen=40, correct=25, accuracy=0.625000
2025-11-11 15:49:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:49:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:49:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:49:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 15:49:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 15:49:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 15:49:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 15:49:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:49:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:49:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.685471, avg_loss=0.703427, seen=200, correct=105, accuracy=0.525000
2025-11-11 15:49:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:49:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:49:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:49:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:49:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:49:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.209303, avg_loss=0.680233, seen=40, correct=24, accuracy=0.600000
2025-11-11 15:49:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:49:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:49:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:49:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-11-11 15:49:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 15:49:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 15:49:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 15:49:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:49:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:49:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:49:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.006424, avg_loss=0.705032, seen=200, correct=102, accuracy=0.510000
2025-11-11 15:49:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:49:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:49:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:49:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:49:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.844103, avg_loss=0.671103, seen=40, correct=24, accuracy=0.600000
2025-11-11 15:49:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:49:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:49:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:49:49 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.600000
2025-11-11 15:49:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 15:49:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 15:49:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 15:49:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:49:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:49:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:49:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:49:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.486755, avg_loss=0.707434, seen=200, correct=102, accuracy=0.510000
2025-11-11 15:49:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:50:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:50:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:50:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.396076, avg_loss=0.684902, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:50:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:50:02 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.625000, curr=0.550000
2025-11-11 15:50:08 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 15:50:08 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 15:50:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 15:50:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:50:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:50:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.841797, avg_loss=0.709209, seen=200, correct=100, accuracy=0.500000
2025-11-11 15:50:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:50:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:50:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:50:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.354614, avg_loss=0.683865, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:50:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:50:15 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.625000, curr=0.550000
2025-11-11 15:50:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 15:50:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 15:50:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 15:50:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:50:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:50:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.920013, avg_loss=0.699600, seen=200, correct=108, accuracy=0.540000
2025-11-11 15:50:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:50:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:50:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:50:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.051933, avg_loss=0.676298, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:50:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:50:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.625000, curr=0.550000
2025-11-11 15:50:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 15:50:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 15:50:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 15:50:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:50:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:50:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.508392, avg_loss=0.692542, seen=200, correct=113, accuracy=0.565000
2025-11-11 15:50:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:50:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:50:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:50:40 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.582972, avg_loss=0.664574, seen=40, correct=25, accuracy=0.625000
2025-11-11 15:50:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:50:41 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 15:50:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 15:50:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 15:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:50:42 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 15:50:42 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {}}
2025-11-11 15:50:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 15:50:42 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 15:50:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:42 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 15:50:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 15:50:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:50:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:50:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=82.583878, avg_loss=0.757650, seen=109, correct=49, accuracy=0.449541
2025-11-11 15:50:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1052MB allocated=1025MB
2025-11-11 15:50:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:50:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:50:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=31.364315, avg_loss=0.784108, seen=40, correct=18, accuracy=0.450000
2025-11-11 15:50:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1052MB allocated=1025MB
2025-11-11 15:50:48 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.450000
2025-11-11 15:50:48 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 15:50:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=521, total=2083)
2025-11-11 15:50:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:48 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 15:50:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:48 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 15:50:48 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=261, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 15:50:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 15:50:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 15:50:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 15:50:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:50:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=78.398712, avg_loss=0.719254, seen=109, correct=54, accuracy=0.495413
2025-11-11 15:50:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:50:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:50:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:50:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:50:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:50:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=31.068617, avg_loss=0.776715, seen=40, correct=12, accuracy=0.300000
2025-11-11 15:50:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:50:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:50:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:00 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.450000, curr=0.300000
2025-11-11 15:51:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 15:51:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 15:51:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 15:51:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:51:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:51:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=78.009697, avg_loss=0.715685, seen=109, correct=54, accuracy=0.495413
2025-11-11 15:51:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:51:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=31.148479, avg_loss=0.778712, seen=40, correct=13, accuracy=0.325000
2025-11-11 15:51:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:11 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.450000, curr=0.325000
2025-11-11 15:51:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 15:51:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 15:51:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 15:51:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:51:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:51:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=79.704124, avg_loss=0.731230, seen=109, correct=49, accuracy=0.449541
2025-11-11 15:51:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:51:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.498821, avg_loss=0.762471, seen=40, correct=15, accuracy=0.375000
2025-11-11 15:51:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:23 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.450000, curr=0.375000
2025-11-11 15:51:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 15:51:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 15:51:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 15:51:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:51:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:51:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=80.985001, avg_loss=0.742982, seen=109, correct=49, accuracy=0.449541
2025-11-11 15:51:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:51:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:51:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.196508, avg_loss=0.754913, seen=40, correct=19, accuracy=0.475000
2025-11-11 15:51:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 15:51:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 15:51:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 15:51:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 15:51:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:51:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:51:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=78.560402, avg_loss=0.720738, seen=109, correct=51, accuracy=0.467890
2025-11-11 15:51:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:51:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:51:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.012848, avg_loss=0.750321, seen=40, correct=16, accuracy=0.400000
2025-11-11 15:51:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:46 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.400000
2025-11-11 15:51:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 15:51:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 15:51:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 15:51:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:51:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:51:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=76.620186, avg_loss=0.702937, seen=109, correct=55, accuracy=0.504587
2025-11-11 15:51:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:51:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:51:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:51:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.672718, avg_loss=0.741818, seen=40, correct=13, accuracy=0.325000
2025-11-11 15:51:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:51:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:51:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:51:58 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.475000, curr=0.325000
2025-11-11 15:52:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 15:52:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 15:52:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 15:52:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:52:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:52:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=76.388168, avg_loss=0.700809, seen=109, correct=55, accuracy=0.504587
2025-11-11 15:52:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:52:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:52:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.831259, avg_loss=0.745781, seen=40, correct=14, accuracy=0.350000
2025-11-11 15:52:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:52:10 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.475000, curr=0.350000
2025-11-11 15:52:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 15:52:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 15:52:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 15:52:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:52:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:52:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=76.247459, avg_loss=0.699518, seen=109, correct=56, accuracy=0.513761
2025-11-11 15:52:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:52:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:52:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.569653, avg_loss=0.739241, seen=40, correct=13, accuracy=0.325000
2025-11-11 15:52:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:52:21 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.475000, curr=0.325000
2025-11-11 15:52:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 15:52:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 15:52:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 15:52:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:52:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:52:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=75.889313, avg_loss=0.696232, seen=109, correct=57, accuracy=0.522936
2025-11-11 15:52:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:52:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:52:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:52:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.065481, avg_loss=0.726637, seen=40, correct=16, accuracy=0.400000
2025-11-11 15:52:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:52:33 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.475000, curr=0.400000
2025-11-11 15:52:40 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 15:52:40 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 15:52:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 15:52:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 15:52:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 15:52:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=74.842613, avg_loss=0.686629, seen=109, correct=60, accuracy=0.550459
2025-11-11 15:52:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:52:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:52:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:52:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.098194, avg_loss=0.727455, seen=40, correct=15, accuracy=0.375000
2025-11-11 15:52:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:52:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.475000, curr=0.375000
2025-11-11 15:52:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 15:52:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 15:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 15:52:46 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 15:52:46 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {}}
2025-11-11 15:52:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 15:52:46 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 15:52:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:46 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 15:52:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:52:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 15:52:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:52:50 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=145.850601, avg_loss=0.775801, seen=188, correct=84, accuracy=0.446809
2025-11-11 15:52:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1074MB allocated=1041MB
2025-11-11 15:52:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:52:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.494049, avg_loss=0.737351, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:52:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:52:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:52:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1074MB allocated=1041MB
2025-11-11 15:52:53 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 15:52:53 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 15:52:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-11-11 15:52:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:52:53 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 15:52:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:52:53 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 15:52:53 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 15:53:00 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 15:53:00 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 15:53:00 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 15:53:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:53:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:53:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=145.467514, avg_loss=0.773763, seen=188, correct=86, accuracy=0.457447
2025-11-11 15:53:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:53:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:53:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.633995, avg_loss=0.740850, seen=40, correct=23, accuracy=0.575000
2025-11-11 15:53:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:06 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 15:53:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 15:53:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 15:53:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 15:53:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:53:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=136.736725, avg_loss=0.727323, seen=188, correct=91, accuracy=0.484043
2025-11-11 15:53:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:53:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.792667, avg_loss=0.719817, seen=40, correct=18, accuracy=0.450000
2025-11-11 15:53:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:19 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.450000
2025-11-11 15:53:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 15:53:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 15:53:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 15:53:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:53:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:53:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=134.959656, avg_loss=0.717871, seen=188, correct=94, accuracy=0.500000
2025-11-11 15:53:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:53:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:53:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.536116, avg_loss=0.738403, seen=40, correct=19, accuracy=0.475000
2025-11-11 15:53:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:33 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.475000
2025-11-11 15:53:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 15:53:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 15:53:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 15:53:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:53:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:53:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=133.698654, avg_loss=0.711163, seen=188, correct=98, accuracy=0.521277
2025-11-11 15:53:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:53:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:53:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.410366, avg_loss=0.710259, seen=40, correct=18, accuracy=0.450000
2025-11-11 15:53:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:46 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.450000
2025-11-11 15:53:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 15:53:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 15:53:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 15:53:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:53:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:53:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=135.609970, avg_loss=0.721330, seen=188, correct=82, accuracy=0.436170
2025-11-11 15:53:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:53:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:53:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:53:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.022667, avg_loss=0.700567, seen=40, correct=20, accuracy=0.500000
2025-11-11 15:53:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:53:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:53:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:53:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:53:59 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.500000
2025-11-11 15:54:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 15:54:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 15:54:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 15:54:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:54:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:54:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=138.951111, avg_loss=0.739102, seen=188, correct=87, accuracy=0.462766
2025-11-11 15:54:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:54:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:54:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:54:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:54:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:54:11 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.155249, avg_loss=0.703881, seen=40, correct=21, accuracy=0.525000
2025-11-11 15:54:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:54:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:54:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:54:12 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.575000, curr=0.525000
2025-11-11 15:54:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 15:54:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 15:54:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 15:54:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:54:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:54:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:54:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=138.179092, avg_loss=0.734995, seen=188, correct=85, accuracy=0.452128
2025-11-11 15:54:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:54:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:54:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:54:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:54:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:54:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.993729, avg_loss=0.699843, seen=40, correct=20, accuracy=0.500000
2025-11-11 15:54:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:54:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:54:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:54:25 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.575000, curr=0.500000
2025-11-11 15:54:31 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 15:54:31 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 15:54:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 15:54:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:54:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:54:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:54:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=132.829056, avg_loss=0.706538, seen=188, correct=92, accuracy=0.489362
2025-11-11 15:54:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:54:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:54:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:54:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:54:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.941204, avg_loss=0.698530, seen=40, correct=19, accuracy=0.475000
2025-11-11 15:54:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:54:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:54:37 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=7/25), best=0.575000, curr=0.475000
2025-11-11 15:54:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 15:54:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 15:54:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 15:54:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:54:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:54:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:54:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=132.913849, avg_loss=0.706989, seen=188, correct=87, accuracy=0.462766
2025-11-11 15:54:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:54:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:54:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:54:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:54:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:54:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:54:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.679968, avg_loss=0.691999, seen=40, correct=20, accuracy=0.500000
2025-11-11 15:54:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:54:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:54:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:54:50 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=8/25), best=0.575000, curr=0.500000
2025-11-11 15:54:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 15:54:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 15:54:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 15:54:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 15:54:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:54:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 15:55:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=134.653534, avg_loss=0.716242, seen=188, correct=77, accuracy=0.409574
2025-11-11 15:55:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:55:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:55:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.570181, avg_loss=0.689255, seen=40, correct=19, accuracy=0.475000
2025-11-11 15:55:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:55:03 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=9/25), best=0.575000, curr=0.475000
2025-11-11 15:55:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 15:55:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 15:55:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 15:55:04 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 15:55:04 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {}}
2025-11-11 15:55:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 15:55:04 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 15:55:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:04 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 15:55:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 15:55:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:55:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=31.485935, avg_loss=0.732231, seen=43, correct=24, accuracy=0.558140
2025-11-11 15:55:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1094MB allocated=1058MB
2025-11-11 15:55:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:55:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:55:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.101191, avg_loss=0.752530, seen=40, correct=19, accuracy=0.475000
2025-11-11 15:55:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1094MB allocated=1058MB
2025-11-11 15:55:09 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 15:55:09 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 15:55:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=207, total=825)
2025-11-11 15:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:09 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 15:55:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:09 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 15:55:09 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=104, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 15:55:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 15:55:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 15:55:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 15:55:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:55:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:55:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=31.355639, avg_loss=0.729201, seen=43, correct=21, accuracy=0.488372
2025-11-11 15:55:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:55:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:55:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:55:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.070141, avg_loss=0.751754, seen=40, correct=17, accuracy=0.425000
2025-11-11 15:55:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:55:20 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.425000
2025-11-11 15:55:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 15:55:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 15:55:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 15:55:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:55:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:55:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=31.104496, avg_loss=0.723360, seen=43, correct=21, accuracy=0.488372
2025-11-11 15:55:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:55:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:55:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:55:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.885677, avg_loss=0.747142, seen=40, correct=17, accuracy=0.425000
2025-11-11 15:55:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:55:31 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.475000, curr=0.425000
2025-11-11 15:55:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 15:55:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 15:55:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 15:55:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:55:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=31.101978, avg_loss=0.723302, seen=43, correct=22, accuracy=0.511628
2025-11-11 15:55:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:55:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:55:40 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.695908, avg_loss=0.742398, seen=40, correct=19, accuracy=0.475000
2025-11-11 15:55:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:55:41 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 15:55:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 15:55:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 15:55:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 15:55:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:55:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=30.971119, avg_loss=0.720259, seen=43, correct=22, accuracy=0.511628
2025-11-11 15:55:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:55:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:55:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.388264, avg_loss=0.734707, seen=40, correct=18, accuracy=0.450000
2025-11-11 15:55:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:55:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:55:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.450000
2025-11-11 15:55:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 15:55:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 15:55:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 15:55:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:55:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:55:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:55:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:55:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=30.924829, avg_loss=0.719182, seen=43, correct=23, accuracy=0.534884
2025-11-11 15:55:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:56:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.416565, avg_loss=0.735414, seen=40, correct=18, accuracy=0.450000
2025-11-11 15:56:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:02 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.475000, curr=0.450000
2025-11-11 15:56:08 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 15:56:08 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 15:56:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 15:56:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:56:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:56:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=30.843849, avg_loss=0.717299, seen=43, correct=21, accuracy=0.488372
2025-11-11 15:56:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:56:11 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.154373, avg_loss=0.728859, seen=40, correct=19, accuracy=0.475000
2025-11-11 15:56:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:12 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 15:56:19 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 15:56:19 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 15:56:19 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 15:56:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:56:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:56:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=30.527384, avg_loss=0.709939, seen=43, correct=22, accuracy=0.511628
2025-11-11 15:56:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:56:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.110186, avg_loss=0.727755, seen=40, correct=17, accuracy=0.425000
2025-11-11 15:56:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:23 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.425000
2025-11-11 15:56:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 15:56:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 15:56:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 15:56:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:56:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:56:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=30.844698, avg_loss=0.717319, seen=43, correct=26, accuracy=0.604651
2025-11-11 15:56:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:56:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.378826, avg_loss=0.734471, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:56:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:33 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 15:56:40 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 15:56:40 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 15:56:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 15:56:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:56:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:56:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=30.415218, avg_loss=0.707331, seen=43, correct=23, accuracy=0.534884
2025-11-11 15:56:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:56:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.601408, avg_loss=0.715035, seen=40, correct=20, accuracy=0.500000
2025-11-11 15:56:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:43 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.500000
2025-11-11 15:56:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 15:56:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 15:56:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 15:56:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 15:56:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 15:56:51 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=30.247240, avg_loss=0.703424, seen=43, correct=21, accuracy=0.488372
2025-11-11 15:56:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:56:53 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.674183, avg_loss=0.716855, seen=40, correct=16, accuracy=0.400000
2025-11-11 15:56:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:54 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.400000
2025-11-11 15:56:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 15:56:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 15:56:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 15:56:55 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 15:56:55 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {}}
2025-11-11 15:56:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 15:56:55 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 15:56:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:55 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 15:56:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 15:56:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:56:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=145.533356, avg_loss=0.727667, seen=200, correct=99, accuracy=0.495000
2025-11-11 15:56:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1094MB allocated=1075MB
2025-11-11 15:57:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.783844, avg_loss=0.694596, seen=40, correct=18, accuracy=0.450000
2025-11-11 15:57:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1094MB allocated=1075MB
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.450000
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2009, total=8035)
2025-11-11 15:57:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1005, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 15:57:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 15:57:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 15:57:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 15:57:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:57:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.665436, avg_loss=0.683327, seen=200, correct=112, accuracy=0.560000
2025-11-11 15:57:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:57:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.814205, avg_loss=0.670355, seen=40, correct=23, accuracy=0.575000
2025-11-11 15:57:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:57:15 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 15:57:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 15:57:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 15:57:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 15:57:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:57:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:57:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.019714, avg_loss=0.680099, seen=200, correct=114, accuracy=0.570000
2025-11-11 15:57:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:57:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.163973, avg_loss=0.679099, seen=40, correct=23, accuracy=0.575000
2025-11-11 15:57:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:57:27 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 15:57:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 15:57:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 15:57:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 15:57:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:57:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=137.250763, avg_loss=0.686254, seen=200, correct=110, accuracy=0.550000
2025-11-11 15:57:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:57:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.834925, avg_loss=0.670873, seen=40, correct=25, accuracy=0.625000
2025-11-11 15:57:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:57:40 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 15:57:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 15:57:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 15:57:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 15:57:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:57:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:57:50 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.673462, avg_loss=0.678367, seen=200, correct=120, accuracy=0.600000
2025-11-11 15:57:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:57:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.227253, avg_loss=0.680681, seen=40, correct=25, accuracy=0.625000
2025-11-11 15:57:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:57:53 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 15:58:00 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 15:58:00 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 15:58:00 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 15:58:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.125366, avg_loss=0.675627, seen=200, correct=110, accuracy=0.550000
2025-11-11 15:58:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.862396, avg_loss=0.671560, seen=40, correct=23, accuracy=0.575000
2025-11-11 15:58:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.575000
2025-11-11 15:58:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 15:58:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 15:58:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 15:58:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.452118, avg_loss=0.682261, seen=200, correct=105, accuracy=0.525000
2025-11-11 15:58:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.659897, avg_loss=0.666497, seen=40, correct=26, accuracy=0.650000
2025-11-11 15:58:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:19 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 15:58:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 15:58:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 15:58:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 15:58:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=137.018188, avg_loss=0.685091, seen=200, correct=106, accuracy=0.530000
2025-11-11 15:58:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.074741, avg_loss=0.676869, seen=40, correct=18, accuracy=0.450000
2025-11-11 15:58:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.450000
2025-11-11 15:58:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 15:58:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 15:58:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 15:58:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.230560, avg_loss=0.676153, seen=200, correct=111, accuracy=0.555000
2025-11-11 15:58:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.742203, avg_loss=0.668555, seen=40, correct=24, accuracy=0.600000
2025-11-11 15:58:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.600000
2025-11-11 15:58:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 15:58:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 15:58:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 15:58:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=134.528320, avg_loss=0.672642, seen=200, correct=114, accuracy=0.570000
2025-11-11 15:58:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.179495, avg_loss=0.679487, seen=40, correct=24, accuracy=0.600000
2025-11-11 15:58:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:58:58 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.650000, curr=0.600000
2025-11-11 15:59:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 15:59:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 15:59:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 15:59:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:59:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:59:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=134.537048, avg_loss=0.672685, seen=200, correct=116, accuracy=0.580000
2025-11-11 15:59:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:59:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.148043, avg_loss=0.678701, seen=40, correct=25, accuracy=0.625000
2025-11-11 15:59:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:59:11 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.650000, curr=0.625000
2025-11-11 15:59:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 15:59:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 15:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 15:59:12 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 15:59:12 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {}}
2025-11-11 15:59:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 15:59:12 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 15:59:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:12 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 15:59:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 15:59:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 15:59:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 15:59:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=26.086008, avg_loss=0.767236, seen=34, correct=12, accuracy=0.352941
2025-11-11 15:59:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1114MB allocated=1092MB
2025-11-11 15:59:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.452774, avg_loss=0.711319, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:59:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1114MB allocated=1092MB
2025-11-11 15:59:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 15:59:17 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 15:59:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=162, total=647)
2025-11-11 15:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:17 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 15:59:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:17 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 15:59:17 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=81, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 15:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=24.464792, avg_loss=0.719553, seen=34, correct=15, accuracy=0.441176
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 15:59:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.826471, avg_loss=0.720662, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:59:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 15:59:27 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 15:59:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 15:59:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 15:59:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 15:59:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 15:59:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 15:59:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=24.069563, avg_loss=0.707928, seen=34, correct=17, accuracy=0.500000
2025-11-11 15:59:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 15:59:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.074785, avg_loss=0.726870, seen=40, correct=20, accuracy=0.500000
2025-11-11 15:59:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 15:59:38 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.500000
2025-11-11 15:59:45 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 15:59:45 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 15:59:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 15:59:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 15:59:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 15:59:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=24.122839, avg_loss=0.709495, seen=34, correct=18, accuracy=0.529412
2025-11-11 15:59:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 15:59:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.124844, avg_loss=0.728121, seen=40, correct=20, accuracy=0.500000
2025-11-11 15:59:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 15:59:49 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.500000
2025-11-11 15:59:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 15:59:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 15:59:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 15:59:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 15:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 15:59:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=23.998960, avg_loss=0.705852, seen=34, correct=18, accuracy=0.529412
2025-11-11 15:59:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 15:59:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.083990, avg_loss=0.702100, seen=40, correct=22, accuracy=0.550000
2025-11-11 15:59:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 15:59:59 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:00:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:00:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:00:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:00:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:00:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:00:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=24.051115, avg_loss=0.707386, seen=34, correct=17, accuracy=0.500000
2025-11-11 16:00:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.175140, avg_loss=0.704379, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:00:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:09 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:00:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:00:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:00:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:00:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:00:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:00:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=23.795401, avg_loss=0.699865, seen=34, correct=17, accuracy=0.500000
2025-11-11 16:00:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.806366, avg_loss=0.695159, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:00:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:20 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.500000
2025-11-11 16:00:27 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:00:27 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:00:27 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:00:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:00:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:00:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=23.362707, avg_loss=0.687138, seen=34, correct=17, accuracy=0.500000
2025-11-11 16:00:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.053471, avg_loss=0.701337, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:00:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:31 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:00:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:00:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:00:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:00:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:00:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:00:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=23.403267, avg_loss=0.688331, seen=34, correct=16, accuracy=0.470588
2025-11-11 16:00:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:40 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.550377, avg_loss=0.688759, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:00:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:41 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:00:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:00:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:00:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:00:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:00:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:00:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=23.545650, avg_loss=0.692519, seen=34, correct=16, accuracy=0.470588
2025-11-11 16:00:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.488791, avg_loss=0.687220, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:00:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:00:51 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-11-11 16:00:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:00:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:00:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:00:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:00:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:00:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=23.206173, avg_loss=0.682534, seen=34, correct=16, accuracy=0.470588
2025-11-11 16:00:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:01:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.033432, avg_loss=0.675836, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:01:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:01:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:01:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:01:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:01:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:01:03 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:01:03 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:01:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:01:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:01:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=54.731560, avg_loss=0.760161, seen=72, correct=37, accuracy=0.513889
2025-11-11 16:01:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1134MB allocated=1109MB
2025-11-11 16:01:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.340519, avg_loss=0.733513, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:01:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1134MB allocated=1109MB
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=345, total=1378)
2025-11-11 16:01:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=173, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:01:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:01:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:01:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:01:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:01:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=51.602619, avg_loss=0.716703, seen=72, correct=38, accuracy=0.527778
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:01:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.473255, avg_loss=0.711831, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:01:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:01:19 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:01:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:01:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:01:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:01:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:01:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:01:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=51.644115, avg_loss=0.717279, seen=72, correct=38, accuracy=0.527778
2025-11-11 16:01:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:01:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.317577, avg_loss=0.707939, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:01:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:01:31 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:01:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:01:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:01:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:01:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:01:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:01:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=51.512672, avg_loss=0.715454, seen=72, correct=39, accuracy=0.541667
2025-11-11 16:01:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:01:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.469097, avg_loss=0.711727, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:01:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:01:42 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:01:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:01:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:01:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:01:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:01:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:01:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=52.239677, avg_loss=0.725551, seen=72, correct=40, accuracy=0.555556
2025-11-11 16:01:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:01:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.022850, avg_loss=0.700571, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:01:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:01:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.500000
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:01:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:02:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=51.748817, avg_loss=0.718734, seen=72, correct=38, accuracy=0.527778
2025-11-11 16:02:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.027323, avg_loss=0.700683, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:02:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.525000
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:02:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=50.951862, avg_loss=0.707665, seen=72, correct=37, accuracy=0.513889
2025-11-11 16:02:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.263872, avg_loss=0.706597, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:02:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:15 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.525000
2025-11-11 16:02:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:02:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:02:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:02:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:02:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=50.462940, avg_loss=0.700874, seen=72, correct=38, accuracy=0.527778
2025-11-11 16:02:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.887316, avg_loss=0.697183, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:02:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:26 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.575000
2025-11-11 16:02:33 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:02:33 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:02:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:02:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:02:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:02:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=50.479111, avg_loss=0.701099, seen=72, correct=39, accuracy=0.541667
2025-11-11 16:02:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.697292, avg_loss=0.692432, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:02:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:38 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 16:02:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:02:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:02:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:02:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:02:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:02:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=50.241249, avg_loss=0.697795, seen=72, correct=42, accuracy=0.583333
2025-11-11 16:02:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.718006, avg_loss=0.692950, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:02:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:48 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.600000
2025-11-11 16:02:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:02:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:02:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:02:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:02:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:02:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=49.986530, avg_loss=0.694257, seen=72, correct=41, accuracy=0.569444
2025-11-11 16:02:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:02:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.405249, avg_loss=0.685131, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:02:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:03:00 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.550000
2025-11-11 16:03:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:03:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:03:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:03:01 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:03:01 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:03:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:03:01 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:03:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:01 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:03:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:03:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:03:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=112.411629, avg_loss=0.739550, seen=152, correct=73, accuracy=0.480263
2025-11-11 16:03:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1154MB allocated=1125MB
2025-11-11 16:03:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.066832, avg_loss=0.726671, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:03:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1154MB allocated=1125MB
2025-11-11 16:03:07 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:03:07 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:03:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=724, total=2893)
2025-11-11 16:03:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:07 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:03:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:07 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:03:07 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=362, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:03:14 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:03:14 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:03:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:03:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:03:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=110.615509, avg_loss=0.727734, seen=152, correct=70, accuracy=0.460526
2025-11-11 16:03:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:03:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.946333, avg_loss=0.723658, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:03:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:03:20 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:03:27 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:03:27 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:03:27 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:03:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:03:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:03:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=108.779053, avg_loss=0.715652, seen=152, correct=70, accuracy=0.460526
2025-11-11 16:03:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:03:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.547388, avg_loss=0.713685, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:03:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:03:32 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:03:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:03:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:03:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:03:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:03:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:03:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=108.977257, avg_loss=0.716956, seen=152, correct=67, accuracy=0.440789
2025-11-11 16:03:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:03:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.371994, avg_loss=0.709300, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:03:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:03:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-11-11 16:03:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:03:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:03:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:03:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:03:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:03:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=107.287643, avg_loss=0.705840, seen=152, correct=75, accuracy=0.493421
2025-11-11 16:03:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:03:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.100414, avg_loss=0.702510, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:03:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:03:57 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.575000
2025-11-11 16:04:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:04:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:04:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:04:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:04:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:04:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=106.382568, avg_loss=0.699885, seen=152, correct=74, accuracy=0.486842
2025-11-11 16:04:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:04:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.895012, avg_loss=0.697375, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:04:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:04:10 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.550000
2025-11-11 16:04:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:04:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:04:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:04:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:04:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:04:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=105.223808, avg_loss=0.692262, seen=152, correct=85, accuracy=0.559211
2025-11-11 16:04:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:04:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.554108, avg_loss=0.688853, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:04:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:04:23 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.550000
2025-11-11 16:04:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:04:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:04:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:04:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:04:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=104.018867, avg_loss=0.684335, seen=152, correct=86, accuracy=0.565789
2025-11-11 16:04:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:04:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.290319, avg_loss=0.682258, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:04:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:04:35 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.550000
2025-11-11 16:04:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:04:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:04:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:04:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:04:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=103.554306, avg_loss=0.681278, seen=152, correct=81, accuracy=0.532895
2025-11-11 16:04:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:04:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.139084, avg_loss=0.678477, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:04:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:04:47 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:04:54 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:04:54 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:04:54 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:04:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:04:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=102.609261, avg_loss=0.675061, seen=152, correct=89, accuracy=0.585526
2025-11-11 16:04:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:04:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.885448, avg_loss=0.672136, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:04:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:05:00 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 16:05:07 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:05:07 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:05:07 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:05:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:05:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=102.213478, avg_loss=0.672457, seen=152, correct=92, accuracy=0.605263
2025-11-11 16:05:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:05:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.952925, avg_loss=0.673823, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:05:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:05:12 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.625000
2025-11-11 16:05:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:05:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:05:13 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:05:13 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:05:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:05:13 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:05:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:13 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:05:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:05:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:05:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:05:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=146.658752, avg_loss=0.733294, seen=200, correct=97, accuracy=0.485000
2025-11-11 16:05:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1174MB allocated=1142MB
2025-11-11 16:05:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=31.334568, avg_loss=0.783364, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:05:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1174MB allocated=1142MB
2025-11-11 16:05:20 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:05:20 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:05:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1643, total=6572)
2025-11-11 16:05:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:21 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:05:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:21 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:05:21 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=822, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:05:27 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:05:27 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:05:27 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:05:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:05:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:05:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=143.897858, avg_loss=0.719489, seen=200, correct=97, accuracy=0.485000
2025-11-11 16:05:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:05:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.387655, avg_loss=0.759691, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:05:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:05:33 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:05:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:05:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.442108, avg_loss=0.702211, seen=200, correct=101, accuracy=0.505000
2025-11-11 16:05:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:05:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.357483, avg_loss=0.733937, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:05:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:05:46 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.500000
2025-11-11 16:05:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:05:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:05:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:05:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:05:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:05:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.285446, avg_loss=0.681427, seen=200, correct=118, accuracy=0.590000
2025-11-11 16:05:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:05:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.456841, avg_loss=0.711421, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:05:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:05:59 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.475000
2025-11-11 16:06:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:06:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:06:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:06:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:06:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.830597, avg_loss=0.679153, seen=200, correct=113, accuracy=0.565000
2025-11-11 16:06:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:06:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.103670, avg_loss=0.702592, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:06:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:06:13 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.450000
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:06:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.795898, avg_loss=0.678979, seen=200, correct=112, accuracy=0.560000
2025-11-11 16:06:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:06:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.522552, avg_loss=0.713064, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:06:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:06:26 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.525000
2025-11-11 16:06:33 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:06:33 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:06:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:06:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:06:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:06:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.348602, avg_loss=0.681743, seen=200, correct=111, accuracy=0.555000
2025-11-11 16:06:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:06:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.274776, avg_loss=0.706869, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:06:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:06:39 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.575000, curr=0.500000
2025-11-11 16:06:45 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:06:45 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:06:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:06:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:06:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:06:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=133.382416, avg_loss=0.666912, seen=200, correct=122, accuracy=0.610000
2025-11-11 16:06:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:06:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.473686, avg_loss=0.686842, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:06:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:06:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.575000, curr=0.550000
2025-11-11 16:06:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:06:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:06:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:06:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:07:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=131.911606, avg_loss=0.659558, seen=200, correct=127, accuracy=0.635000
2025-11-11 16:07:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:07:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:04 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.998695, avg_loss=0.674967, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:07:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:07:05 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=7/25), best=0.575000, curr=0.525000
2025-11-11 16:07:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:07:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:07:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:07:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:07:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:07:15 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=131.443542, avg_loss=0.657218, seen=200, correct=123, accuracy=0.615000
2025-11-11 16:07:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:07:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:17 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.046360, avg_loss=0.676159, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:07:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:07:18 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=8/25), best=0.575000, curr=0.500000
2025-11-11 16:07:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:07:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:07:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:07:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:07:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=130.822144, avg_loss=0.654111, seen=200, correct=127, accuracy=0.635000
2025-11-11 16:07:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:07:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.991501, avg_loss=0.674788, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:07:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:07:31 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=9/25), best=0.575000, curr=0.550000
2025-11-11 16:07:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:07:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:07:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:07:32 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:07:32 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:07:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:07:32 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:07:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:32 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:07:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:07:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.189453, avg_loss=0.710947, seen=200, correct=114, accuracy=0.570000
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1194MB allocated=1159MB
2025-11-11 16:07:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=33.048138, avg_loss=0.826203, seen=40, correct=15, accuracy=0.375000
2025-11-11 16:07:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1194MB allocated=1159MB
2025-11-11 16:07:39 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.375000
2025-11-11 16:07:39 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:07:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1303, total=5211)
2025-11-11 16:07:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:39 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:07:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:39 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:07:39 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=652, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:07:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:07:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:07:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:07:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:07:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=146.975815, avg_loss=0.734879, seen=200, correct=91, accuracy=0.455000
2025-11-11 16:07:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:07:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.216822, avg_loss=0.705421, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:07:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:07:52 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:07:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:07:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:07:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:07:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:08:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.594620, avg_loss=0.712973, seen=200, correct=102, accuracy=0.510000
2025-11-11 16:08:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:04 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.130714, avg_loss=0.728268, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:08:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:05 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:08:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:08:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:08:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:08:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:08:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:08:15 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.259460, avg_loss=0.696297, seen=200, correct=114, accuracy=0.570000
2025-11-11 16:08:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.262049, avg_loss=0.756551, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:08:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:18 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.475000
2025-11-11 16:08:25 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:08:25 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:08:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:08:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:08:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.092957, avg_loss=0.695465, seen=200, correct=115, accuracy=0.575000
2025-11-11 16:08:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.159489, avg_loss=0.728987, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:08:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.500000
2025-11-11 16:08:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:08:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:08:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:08:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:08:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:08:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.938736, avg_loss=0.704694, seen=200, correct=98, accuracy=0.490000
2025-11-11 16:08:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.635868, avg_loss=0.690897, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:08:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:08:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:08:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:08:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:08:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:08:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.050827, avg_loss=0.690254, seen=200, correct=112, accuracy=0.560000
2025-11-11 16:08:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.911554, avg_loss=0.697789, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:08:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:08:58 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.525000
2025-11-11 16:09:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:09:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:09:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:09:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:09:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:09:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.428864, avg_loss=0.677144, seen=200, correct=121, accuracy=0.605000
2025-11-11 16:09:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:09:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.588154, avg_loss=0.739704, seen=40, correct=17, accuracy=0.425000
2025-11-11 16:09:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:09:11 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.425000
2025-11-11 16:09:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:09:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:09:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:09:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:09:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:09:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.405640, avg_loss=0.677028, seen=200, correct=119, accuracy=0.595000
2025-11-11 16:09:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:09:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.508129, avg_loss=0.712703, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:09:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:09:24 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.550000, curr=0.525000
2025-11-11 16:09:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:09:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:09:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:09:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:09:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.048920, avg_loss=0.690245, seen=200, correct=100, accuracy=0.500000
2025-11-11 16:09:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:09:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.734890, avg_loss=0.668372, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:09:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:09:36 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:09:43 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:09:43 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:09:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:09:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:09:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.848541, avg_loss=0.694243, seen=200, correct=100, accuracy=0.500000
2025-11-11 16:09:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:09:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.919636, avg_loss=0.647991, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:09:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:09:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:09:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:09:50 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:09:50 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:09:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:09:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:51 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:09:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:09:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:09:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=147.469131, avg_loss=0.737346, seen=200, correct=97, accuracy=0.485000
2025-11-11 16:09:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1194MB allocated=1176MB
2025-11-11 16:09:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.735912, avg_loss=0.718398, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:09:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1194MB allocated=1176MB
2025-11-11 16:09:58 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:09:58 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:09:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1845, total=7379)
2025-11-11 16:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:58 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:09:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:58 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:09:58 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=923, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:10:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:10:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:10:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:10:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:10:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.953003, avg_loss=0.714765, seen=200, correct=97, accuracy=0.485000
2025-11-11 16:10:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:10:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.947203, avg_loss=0.723680, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:10:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:10:11 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.450000
2025-11-11 16:10:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:10:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:10:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:10:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:10:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:10:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=143.660248, avg_loss=0.718301, seen=200, correct=105, accuracy=0.525000
2025-11-11 16:10:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:10:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.368038, avg_loss=0.759201, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:10:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:10:24 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.525000
2025-11-11 16:10:31 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:10:31 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:10:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:10:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:10:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.548737, avg_loss=0.702744, seen=200, correct=102, accuracy=0.510000
2025-11-11 16:10:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:10:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.937883, avg_loss=0.723447, seen=40, correct=14, accuracy=0.350000
2025-11-11 16:10:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:10:37 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.350000
2025-11-11 16:10:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:10:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:10:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:10:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:10:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.931656, avg_loss=0.694658, seen=200, correct=106, accuracy=0.530000
2025-11-11 16:10:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:10:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.124825, avg_loss=0.703121, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:10:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:10:50 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.500000
2025-11-11 16:10:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:10:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:10:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:10:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:10:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:11:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.064743, avg_loss=0.695324, seen=200, correct=107, accuracy=0.535000
2025-11-11 16:11:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.993061, avg_loss=0.699827, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:11:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:03 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.575000, curr=0.500000
2025-11-11 16:11:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:11:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:11:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:11:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:11:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.856400, avg_loss=0.694282, seen=200, correct=105, accuracy=0.525000
2025-11-11 16:11:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.971670, avg_loss=0.724292, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:11:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.575000, curr=0.475000
2025-11-11 16:11:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:11:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:11:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:11:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:11:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:11:26 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=137.597870, avg_loss=0.687989, seen=200, correct=113, accuracy=0.565000
2025-11-11 16:11:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.443375, avg_loss=0.711084, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:11:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:29 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:11:36 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:11:36 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:11:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:11:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:11:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.961319, avg_loss=0.679807, seen=200, correct=119, accuracy=0.595000
2025-11-11 16:11:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.551834, avg_loss=0.688796, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.500000
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:11:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.724823, avg_loss=0.678624, seen=200, correct=114, accuracy=0.570000
2025-11-11 16:11:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.337811, avg_loss=0.683445, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:11:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:11:55 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:12:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:12:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:12:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:12:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:12:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.499451, avg_loss=0.677497, seen=200, correct=114, accuracy=0.570000
2025-11-11 16:12:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:12:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.599268, avg_loss=0.664982, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:12:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:12:09 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 16:12:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:12:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:12:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:12:09 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:12:09 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:12:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:12:09 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:12:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:10 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:12:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:12:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:12:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:12:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=105.994247, avg_loss=0.779369, seen=136, correct=56, accuracy=0.411765
2025-11-11 16:12:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1214MB allocated=1193MB
2025-11-11 16:12:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.452282, avg_loss=0.686307, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:12:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1214MB allocated=1193MB
2025-11-11 16:12:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:12:16 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:12:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=649, total=2594)
2025-11-11 16:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:16 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:12:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:16 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:12:16 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=325, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:12:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:12:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:12:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:12:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:12:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:12:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=103.171944, avg_loss=0.758617, seen=136, correct=58, accuracy=0.426471
2025-11-11 16:12:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:12:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.170895, avg_loss=0.679272, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:12:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:12:28 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:12:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:12:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:12:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:12:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:12:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=101.295731, avg_loss=0.744822, seen=136, correct=58, accuracy=0.426471
2025-11-11 16:12:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:12:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.113045, avg_loss=0.677826, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:12:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:12:40 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.550000
2025-11-11 16:12:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:12:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:12:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:12:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:12:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=96.272148, avg_loss=0.707883, seen=136, correct=69, accuracy=0.507353
2025-11-11 16:12:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:12:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.613941, avg_loss=0.690349, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:12:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:12:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-11-11 16:12:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:12:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:12:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:12:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:12:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:13:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=95.269623, avg_loss=0.700512, seen=136, correct=81, accuracy=0.595588
2025-11-11 16:13:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.257280, avg_loss=0.731432, seen=40, correct=16, accuracy=0.400000
2025-11-11 16:13:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.400000
2025-11-11 16:13:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:13:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:13:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:13:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:13:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:13:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=95.155487, avg_loss=0.699673, seen=136, correct=76, accuracy=0.558824
2025-11-11 16:13:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.269278, avg_loss=0.731732, seen=40, correct=15, accuracy=0.375000
2025-11-11 16:13:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.375000
2025-11-11 16:13:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:13:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:13:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:13:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:13:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:13:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=94.597900, avg_loss=0.695573, seen=136, correct=74, accuracy=0.544118
2025-11-11 16:13:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.745232, avg_loss=0.693631, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:13:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.500000
2025-11-11 16:13:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:13:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:13:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:13:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:13:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=95.576447, avg_loss=0.702768, seen=136, correct=66, accuracy=0.485294
2025-11-11 16:13:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.484116, avg_loss=0.687103, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:13:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:40 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:13:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:13:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:13:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:13:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:13:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=95.267044, avg_loss=0.700493, seen=136, correct=65, accuracy=0.477941
2025-11-11 16:13:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.301268, avg_loss=0.682532, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:13:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:13:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-11-11 16:13:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:13:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:13:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:13:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:14:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=95.087105, avg_loss=0.699170, seen=136, correct=63, accuracy=0.463235
2025-11-11 16:14:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:14:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.204872, avg_loss=0.680122, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:14:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:14:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.575000
2025-11-11 16:14:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:14:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:14:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:14:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:14:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=94.564659, avg_loss=0.695328, seen=136, correct=65, accuracy=0.477941
2025-11-11 16:14:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:14:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.248516, avg_loss=0.681213, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:14:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:14:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.550000
2025-11-11 16:14:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:14:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:14:17 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:14:17 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:14:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:14:17 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:14:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:17 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:14:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:14:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:14:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:14:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=148.708023, avg_loss=0.743540, seen=200, correct=101, accuracy=0.505000
2025-11-11 16:14:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1234MB allocated=1209MB
2025-11-11 16:14:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=31.067139, avg_loss=0.776678, seen=40, correct=14, accuracy=0.350000
2025-11-11 16:14:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1234MB allocated=1209MB
2025-11-11 16:14:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.350000
2025-11-11 16:14:24 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:14:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1028, total=4112)
2025-11-11 16:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:25 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:14:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:25 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:14:25 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=514, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:14:31 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:14:31 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:14:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:14:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:14:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:14:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=144.858856, avg_loss=0.724294, seen=200, correct=99, accuracy=0.495000
2025-11-11 16:14:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:14:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.074696, avg_loss=0.751867, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:14:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:14:38 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.450000
2025-11-11 16:14:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:14:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:14:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:14:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:14:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=148.670883, avg_loss=0.743354, seen=200, correct=90, accuracy=0.450000
2025-11-11 16:14:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:14:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.413113, avg_loss=0.760328, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:14:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:14:51 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:15:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=144.351868, avg_loss=0.721759, seen=200, correct=94, accuracy=0.470000
2025-11-11 16:15:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.442392, avg_loss=0.736060, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:15:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.475000
2025-11-11 16:15:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:15:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:15:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:15:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:15:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.994019, avg_loss=0.714970, seen=200, correct=97, accuracy=0.485000
2025-11-11 16:15:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.334156, avg_loss=0.733354, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:15:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:15:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:15:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:15:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:15:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:15:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:15:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.952866, avg_loss=0.714764, seen=200, correct=98, accuracy=0.490000
2025-11-11 16:15:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.438343, avg_loss=0.735959, seen=40, correct=17, accuracy=0.425000
2025-11-11 16:15:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:30 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.425000
2025-11-11 16:15:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:15:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:15:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:15:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:15:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:15:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.649139, avg_loss=0.708246, seen=200, correct=100, accuracy=0.500000
2025-11-11 16:15:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.030731, avg_loss=0.725768, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:15:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:43 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.450000
2025-11-11 16:15:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:15:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:15:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:15:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:15:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.400192, avg_loss=0.697001, seen=200, correct=103, accuracy=0.515000
2025-11-11 16:15:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.039021, avg_loss=0.725976, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:15:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:15:56 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.500000, curr=0.450000
2025-11-11 16:16:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:16:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:16:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:16:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:16:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.497925, avg_loss=0.697490, seen=200, correct=102, accuracy=0.510000
2025-11-11 16:16:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:16:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.944454, avg_loss=0.723611, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:16:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:16:09 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.500000, curr=0.475000
2025-11-11 16:16:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:16:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:16:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:16:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:16:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:16:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.317368, avg_loss=0.701587, seen=200, correct=103, accuracy=0.515000
2025-11-11 16:16:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:16:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.422682, avg_loss=0.710567, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:16:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:16:22 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 16:16:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:16:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:16:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:16:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:16:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:16:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.596039, avg_loss=0.697980, seen=200, correct=107, accuracy=0.535000
2025-11-11 16:16:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:16:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.954967, avg_loss=0.698874, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:16:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:16:35 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.600000
2025-11-11 16:16:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:16:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:16:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:16:36 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:16:36 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:16:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:16:36 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:16:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:36 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:16:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:16:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:16:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:16:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=101.844330, avg_loss=0.712198, seen=143, correct=76, accuracy=0.531469
2025-11-11 16:16:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1254MB allocated=1226MB
2025-11-11 16:16:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.647724, avg_loss=0.716193, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:16:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1254MB allocated=1226MB
2025-11-11 16:16:43 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:16:43 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:16:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=681, total=2723)
2025-11-11 16:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:43 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:16:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:43 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:16:43 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=341, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:16:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:16:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:16:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:16:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:16:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:16:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=102.523643, avg_loss=0.716949, seen=143, correct=69, accuracy=0.482517
2025-11-11 16:16:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:16:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.796024, avg_loss=0.719901, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:16:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:16:55 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.475000
2025-11-11 16:17:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:17:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:17:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:17:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:17:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:17:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=103.919441, avg_loss=0.726709, seen=143, correct=67, accuracy=0.468531
2025-11-11 16:17:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:17:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.512787, avg_loss=0.737820, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:17:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:08 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.450000
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:17:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=101.516571, avg_loss=0.709906, seen=143, correct=71, accuracy=0.496503
2025-11-11 16:17:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:17:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.630676, avg_loss=0.715767, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:17:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:21 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.525000, curr=0.500000
2025-11-11 16:17:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:17:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:17:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:17:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:17:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:17:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=99.939072, avg_loss=0.698875, seen=143, correct=73, accuracy=0.510490
2025-11-11 16:17:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:17:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:17:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.209747, avg_loss=0.705244, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:17:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:34 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.525000, curr=0.500000
2025-11-11 16:17:40 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:17:40 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:17:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:17:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:17:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:17:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=99.476593, avg_loss=0.695641, seen=143, correct=79, accuracy=0.552448
2025-11-11 16:17:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:17:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:17:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.044388, avg_loss=0.701110, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:17:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:46 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.525000, curr=0.475000
2025-11-11 16:17:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:17:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:17:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:17:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:17:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:17:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=99.889694, avg_loss=0.698529, seen=143, correct=70, accuracy=0.489510
2025-11-11 16:17:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:17:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.465548, avg_loss=0.711639, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:17:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:17:58 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:18:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:18:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:18:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:18:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:18:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:18:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=98.143082, avg_loss=0.686315, seen=143, correct=91, accuracy=0.636364
2025-11-11 16:18:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:18:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.594143, avg_loss=0.689854, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:18:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:18:11 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.475000
2025-11-11 16:18:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:18:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:18:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:18:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:18:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:18:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=96.869957, avg_loss=0.677412, seen=143, correct=88, accuracy=0.615385
2025-11-11 16:18:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:18:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.516850, avg_loss=0.687921, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:18:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:18:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:18:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:18:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:18:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:18:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:18:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=96.452469, avg_loss=0.674493, seen=143, correct=91, accuracy=0.636364
2025-11-11 16:18:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:18:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.138597, avg_loss=0.678465, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:18:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:18:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.500000
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:18:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=95.961533, avg_loss=0.671060, seen=143, correct=85, accuracy=0.594406
2025-11-11 16:18:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:18:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.074646, avg_loss=0.676866, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:18:49 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:18:49 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:18:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:18:49 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:18:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:49 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:18:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:18:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:18:51 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=10.281507, avg_loss=0.642594, seen=16, correct=10, accuracy=0.625000
2025-11-11 16:18:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1274MB allocated=1243MB
2025-11-11 16:18:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:53 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.948456, avg_loss=0.723711, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:18:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1274MB allocated=1243MB
2025-11-11 16:18:54 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 16:18:54 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:18:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=79, total=315)
2025-11-11 16:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:54 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:18:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:54 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:18:54 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=40, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=11.884993, avg_loss=0.742812, seen=16, correct=6, accuracy=0.375000
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.782171, avg_loss=0.694554, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:19:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:04 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:19:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:19:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:19:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:19:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:19:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=12.089478, avg_loss=0.755592, seen=16, correct=6, accuracy=0.375000
2025-11-11 16:19:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.601915, avg_loss=0.690048, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:19:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:15 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.525000
2025-11-11 16:19:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:19:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:19:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:19:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:19:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=11.473476, avg_loss=0.717092, seen=16, correct=6, accuracy=0.375000
2025-11-11 16:19:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.432766, avg_loss=0.685819, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:19:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:25 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.525000
2025-11-11 16:19:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:19:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:19:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:19:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:19:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=11.381117, avg_loss=0.711320, seen=16, correct=6, accuracy=0.375000
2025-11-11 16:19:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.462965, avg_loss=0.686574, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:19:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:35 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.525000
2025-11-11 16:19:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:19:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:19:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:19:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:19:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=11.345556, avg_loss=0.709097, seen=16, correct=6, accuracy=0.375000
2025-11-11 16:19:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.497438, avg_loss=0.687436, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:19:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:46 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.525000
2025-11-11 16:19:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:19:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:19:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:19:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:19:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:19:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=11.180001, avg_loss=0.698750, seen=16, correct=6, accuracy=0.375000
2025-11-11 16:19:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.162258, avg_loss=0.679056, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:19:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:19:56 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.575000, curr=0.525000
2025-11-11 16:20:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:20:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:20:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:20:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:20:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=11.125796, avg_loss=0.695362, seen=16, correct=8, accuracy=0.500000
2025-11-11 16:20:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:20:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.849815, avg_loss=0.671245, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:20:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:20:07 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:20:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:20:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:20:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:20:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:20:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=11.141716, avg_loss=0.696357, seen=16, correct=7, accuracy=0.437500
2025-11-11 16:20:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:20:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.886467, avg_loss=0.672162, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:20:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:20:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:20:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:20:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:20:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:20:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:20:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=10.998659, avg_loss=0.687416, seen=16, correct=7, accuracy=0.437500
2025-11-11 16:20:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:20:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.261087, avg_loss=0.656527, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:20:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:20:27 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=10.652824, avg_loss=0.665802, seen=16, correct=8, accuracy=0.500000
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:20:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.364925, avg_loss=0.659123, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:20:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:20:38 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.525000
2025-11-11 16:20:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:20:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:20:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:20:38 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:20:38 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:20:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:20:39 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:20:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:39 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:20:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:20:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=143.526672, avg_loss=0.717633, seen=200, correct=101, accuracy=0.505000
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1294MB allocated=1260MB
2025-11-11 16:20:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.741140, avg_loss=0.743529, seen=40, correct=17, accuracy=0.425000
2025-11-11 16:20:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1294MB allocated=1260MB
2025-11-11 16:20:46 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.425000
2025-11-11 16:20:46 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:20:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1390, total=5557)
2025-11-11 16:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:46 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:20:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:46 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:20:46 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=695, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:20:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:20:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:20:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:20:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:20:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:20:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.823120, avg_loss=0.709116, seen=200, correct=101, accuracy=0.505000
2025-11-11 16:20:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:20:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.290285, avg_loss=0.732257, seen=40, correct=17, accuracy=0.425000
2025-11-11 16:20:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:20:59 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.425000
2025-11-11 16:21:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:21:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:21:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:21:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:21:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.431854, avg_loss=0.702159, seen=200, correct=113, accuracy=0.565000
2025-11-11 16:21:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:21:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:21:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.654217, avg_loss=0.666355, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:21:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:21:13 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:21:19 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:21:19 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:21:19 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:21:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:21:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.674591, avg_loss=0.713373, seen=200, correct=99, accuracy=0.495000
2025-11-11 16:21:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:21:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:21:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:21:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.614111, avg_loss=0.640353, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:21:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:21:26 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.700000
2025-11-11 16:21:33 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:21:33 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:21:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:21:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:21:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.606796, avg_loss=0.693034, seen=200, correct=107, accuracy=0.535000
2025-11-11 16:21:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:21:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:21:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:21:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.064331, avg_loss=0.676608, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:21:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:21:39 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.550000
2025-11-11 16:21:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:21:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:21:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:21:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:21:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=137.794083, avg_loss=0.688970, seen=200, correct=111, accuracy=0.555000
2025-11-11 16:21:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:21:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:21:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.150465, avg_loss=0.653762, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:21:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:21:53 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.625000
2025-11-11 16:21:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:21:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:21:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:21:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.183014, avg_loss=0.690915, seen=200, correct=114, accuracy=0.570000
2025-11-11 16:22:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.802391, avg_loss=0.645060, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:22:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.700000, curr=0.650000
2025-11-11 16:22:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:22:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:22:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:22:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=137.535828, avg_loss=0.687679, seen=200, correct=110, accuracy=0.550000
2025-11-11 16:22:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.735970, avg_loss=0.643399, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:22:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:19 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.700000, curr=0.650000
2025-11-11 16:22:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:22:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:22:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:22:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.668915, avg_loss=0.678345, seen=200, correct=117, accuracy=0.585000
2025-11-11 16:22:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.237925, avg_loss=0.655948, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:22:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.700000, curr=0.625000
2025-11-11 16:22:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:22:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:22:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:22:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.766754, avg_loss=0.678834, seen=200, correct=113, accuracy=0.565000
2025-11-11 16:22:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.802750, avg_loss=0.670069, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:22:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.700000, curr=0.575000
2025-11-11 16:22:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:22:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:22:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:22:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:22:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=133.906494, avg_loss=0.669532, seen=200, correct=118, accuracy=0.590000
2025-11-11 16:22:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.929937, avg_loss=0.648248, seen=40, correct=27, accuracy=0.675000
2025-11-11 16:22:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:22:59 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=7/25), best=0.700000, curr=0.675000
2025-11-11 16:22:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:22:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:22:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:23:00 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:23:00 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:23:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:23:00 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:23:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:00 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:23:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:01 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:23:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:23:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:23:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=112.522484, avg_loss=0.681954, seen=165, correct=90, accuracy=0.545455
2025-11-11 16:23:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1294MB allocated=1276MB
2025-11-11 16:23:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=32.845146, avg_loss=0.821129, seen=40, correct=15, accuracy=0.375000
2025-11-11 16:23:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1294MB allocated=1276MB
2025-11-11 16:23:06 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.375000
2025-11-11 16:23:06 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:23:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=787, total=3146)
2025-11-11 16:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:06 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:23:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:06 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:23:06 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=394, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:23:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:23:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:23:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:23:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:23:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=114.926331, avg_loss=0.696523, seen=165, correct=97, accuracy=0.587879
2025-11-11 16:23:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:23:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.404581, avg_loss=0.710115, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:23:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:23:19 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:23:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:23:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:23:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:23:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:23:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=118.951736, avg_loss=0.720920, seen=165, correct=86, accuracy=0.521212
2025-11-11 16:23:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:23:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.924866, avg_loss=0.698122, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:23:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:23:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.525000
2025-11-11 16:23:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:23:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:23:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:23:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:23:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=111.621849, avg_loss=0.676496, seen=165, correct=94, accuracy=0.569697
2025-11-11 16:23:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:23:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.463718, avg_loss=0.736593, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:23:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:23:44 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.550000, curr=0.450000
2025-11-11 16:23:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:23:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:23:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:23:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:23:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:23:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=111.139412, avg_loss=0.673572, seen=165, correct=97, accuracy=0.587879
2025-11-11 16:23:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:23:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.434599, avg_loss=0.735865, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:23:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:23:57 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.550000, curr=0.450000
2025-11-11 16:24:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:24:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:24:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:24:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:24:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:24:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=112.013283, avg_loss=0.678868, seen=165, correct=96, accuracy=0.581818
2025-11-11 16:24:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:24:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.354710, avg_loss=0.708868, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:24:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:24:09 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.550000, curr=0.525000
2025-11-11 16:24:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:24:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:24:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:24:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:24:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:24:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=111.752625, avg_loss=0.677289, seen=165, correct=96, accuracy=0.581818
2025-11-11 16:24:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:24:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.216341, avg_loss=0.705409, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:24:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:24:22 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.550000, curr=0.525000
2025-11-11 16:24:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:24:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:24:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:24:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:24:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:24:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=110.374985, avg_loss=0.668939, seen=165, correct=100, accuracy=0.606061
2025-11-11 16:24:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:24:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.712866, avg_loss=0.717822, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:24:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:24:34 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.550000, curr=0.475000
2025-11-11 16:24:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:24:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:24:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:24:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:24:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=111.041931, avg_loss=0.672981, seen=165, correct=97, accuracy=0.587879
2025-11-11 16:24:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:24:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.305485, avg_loss=0.707637, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:24:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:24:47 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=7/25), best=0.550000, curr=0.525000
2025-11-11 16:24:54 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:24:54 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:24:54 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:24:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:24:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:24:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=115.991531, avg_loss=0.702979, seen=165, correct=88, accuracy=0.533333
2025-11-11 16:24:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:24:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.635155, avg_loss=0.690879, seen=40, correct=27, accuracy=0.675000
2025-11-11 16:24:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:25:00 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.675000
2025-11-11 16:25:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:25:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:25:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:25:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:25:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=114.827972, avg_loss=0.695927, seen=165, correct=93, accuracy=0.563636
2025-11-11 16:25:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:25:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:11 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.616066, avg_loss=0.690402, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:25:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.575000
2025-11-11 16:25:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:25:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:25:13 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:25:13 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:25:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:25:13 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:25:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:13 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:25:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:25:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:25:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:25:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=150.232727, avg_loss=0.751164, seen=200, correct=89, accuracy=0.445000
2025-11-11 16:25:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1314MB allocated=1293MB
2025-11-11 16:25:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=31.836542, avg_loss=0.795914, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:25:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1314MB allocated=1293MB
2025-11-11 16:25:21 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 16:25:21 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:25:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2384, total=9535)
2025-11-11 16:25:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:21 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:25:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:21 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:25:21 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1192, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:25:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:25:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:25:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:25:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:25:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:25:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=144.691055, avg_loss=0.723455, seen=200, correct=88, accuracy=0.440000
2025-11-11 16:25:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:25:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.952190, avg_loss=0.748805, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:25:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:25:34 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.475000, curr=0.450000
2025-11-11 16:25:40 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:25:40 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:25:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:25:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:25:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:25:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=143.116440, avg_loss=0.715582, seen=200, correct=94, accuracy=0.470000
2025-11-11 16:25:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:25:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.542690, avg_loss=0.738567, seen=40, correct=15, accuracy=0.375000
2025-11-11 16:25:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:25:47 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.475000, curr=0.375000
2025-11-11 16:25:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:25:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:25:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:25:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:25:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:25:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=143.315491, avg_loss=0.716577, seen=200, correct=89, accuracy=0.445000
2025-11-11 16:25:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:25:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.672974, avg_loss=0.741824, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:25:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:26:00 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.475000, curr=0.450000
2025-11-11 16:26:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:26:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:26:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:26:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:26:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:26:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.577484, avg_loss=0.712887, seen=200, correct=91, accuracy=0.455000
2025-11-11 16:26:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:26:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:26:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:26:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.356274, avg_loss=0.733907, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:26:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:26:13 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.475000, curr=0.450000
2025-11-11 16:26:19 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:26:19 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:26:19 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:26:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:26:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:26:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.869598, avg_loss=0.709348, seen=200, correct=90, accuracy=0.450000
2025-11-11 16:26:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:26:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:26:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:26:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.119892, avg_loss=0.727997, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:26:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:26:26 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.475000, curr=0.450000
2025-11-11 16:26:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:26:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:26:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:26:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:26:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:26:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.139847, avg_loss=0.700699, seen=200, correct=101, accuracy=0.505000
2025-11-11 16:26:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:26:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:26:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:26:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.537697, avg_loss=0.713442, seen=40, correct=17, accuracy=0.425000
2025-11-11 16:26:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:26:39 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.475000, curr=0.425000
2025-11-11 16:26:45 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:26:45 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:26:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:26:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:26:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:26:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.765259, avg_loss=0.703826, seen=200, correct=93, accuracy=0.465000
2025-11-11 16:26:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:26:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:26:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:26:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.781580, avg_loss=0.719539, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:26:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:26:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=7/25), best=0.475000, curr=0.450000
2025-11-11 16:26:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:26:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:26:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:26:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:26:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:27:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.572678, avg_loss=0.712863, seen=200, correct=95, accuracy=0.475000
2025-11-11 16:27:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:27:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:04 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.072887, avg_loss=0.751822, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:27:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:27:05 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 16:27:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:27:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:27:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:27:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:27:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:27:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.467896, avg_loss=0.707339, seen=200, correct=95, accuracy=0.475000
2025-11-11 16:27:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:27:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:17 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.676096, avg_loss=0.741902, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:27:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:27:18 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:27:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:27:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:27:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:27:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:27:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:27:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.247955, avg_loss=0.691240, seen=200, correct=98, accuracy=0.490000
2025-11-11 16:27:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:27:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.391464, avg_loss=0.709787, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:27:31 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:27:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:27:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:27:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:27:32 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:27:32 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:27:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:27:32 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:27:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:32 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:27:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:27:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:27:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:27:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=142.821823, avg_loss=0.732420, seen=195, correct=100, accuracy=0.512821
2025-11-11 16:27:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1334MB allocated=1310MB
2025-11-11 16:27:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.008919, avg_loss=0.700223, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:27:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1334MB allocated=1310MB
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=929, total=3713)
2025-11-11 16:27:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=465, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:27:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:27:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:27:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:27:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:27:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:27:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=137.415329, avg_loss=0.704694, seen=195, correct=96, accuracy=0.492308
2025-11-11 16:27:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:27:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.210175, avg_loss=0.705254, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:27:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:27:53 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:27:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:27:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:27:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:28:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:28:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:28:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=140.352753, avg_loss=0.719758, seen=195, correct=99, accuracy=0.507692
2025-11-11 16:28:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.646740, avg_loss=0.741168, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:28:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.475000
2025-11-11 16:28:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:28:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:28:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:28:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:28:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:28:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=137.595490, avg_loss=0.705618, seen=195, correct=107, accuracy=0.548718
2025-11-11 16:28:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.267422, avg_loss=0.681686, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:28:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:19 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.550000
2025-11-11 16:28:25 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:28:25 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:28:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:28:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:28:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:28:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=140.931808, avg_loss=0.722727, seen=195, correct=101, accuracy=0.517949
2025-11-11 16:28:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.309200, avg_loss=0.682730, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:28:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.525000
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:28:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:28:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=138.677216, avg_loss=0.711165, seen=195, correct=103, accuracy=0.528205
2025-11-11 16:28:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.210135, avg_loss=0.680253, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:28:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.575000, curr=0.525000
2025-11-11 16:28:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:28:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:28:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:28:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:28:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:28:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=136.375214, avg_loss=0.699360, seen=195, correct=110, accuracy=0.564103
2025-11-11 16:28:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.033009, avg_loss=0.675825, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:28:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:28:59 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.575000, curr=0.550000
2025-11-11 16:29:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:29:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:29:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:29:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:29:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:29:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=135.623383, avg_loss=0.695505, seen=195, correct=100, accuracy=0.512821
2025-11-11 16:29:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:29:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:11 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.001030, avg_loss=0.675026, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:29:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:29:12 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 16:29:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:29:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:29:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:29:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:29:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:29:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=135.372498, avg_loss=0.694218, seen=195, correct=101, accuracy=0.517949
2025-11-11 16:29:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:29:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.891315, avg_loss=0.672283, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:29:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:29:25 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.650000, curr=0.625000
2025-11-11 16:29:31 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:29:31 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:29:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:29:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:29:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:29:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=135.890945, avg_loss=0.696877, seen=195, correct=103, accuracy=0.528205
2025-11-11 16:29:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:29:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.586832, avg_loss=0.664671, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:29:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:29:38 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.650000, curr=0.575000
2025-11-11 16:29:45 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:29:45 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:29:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:29:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:29:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:29:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=134.958069, avg_loss=0.692093, seen=195, correct=103, accuracy=0.528205
2025-11-11 16:29:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:29:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.938740, avg_loss=0.673468, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:29:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:29:51 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 16:29:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:29:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:29:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:29:52 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:29:52 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:29:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:29:52 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:29:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:52 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:29:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:29:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:29:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:29:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.244644, avg_loss=0.687054, seen=12, correct=7, accuracy=0.583333
2025-11-11 16:29:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1354MB allocated=1327MB
2025-11-11 16:29:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.941557, avg_loss=0.723539, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:29:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1354MB allocated=1327MB
2025-11-11 16:29:56 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:29:56 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=58, total=229)
2025-11-11 16:29:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=29, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:30:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:30:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:30:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:30:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:30:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:30:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.415342, avg_loss=0.701279, seen=12, correct=6, accuracy=0.500000
2025-11-11 16:30:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.235769, avg_loss=0.705894, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:30:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:07 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.475000
2025-11-11 16:30:14 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:30:14 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:30:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:30:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:30:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:30:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.615779, avg_loss=0.717982, seen=12, correct=6, accuracy=0.500000
2025-11-11 16:30:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.572136, avg_loss=0.689303, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:30:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:30:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:30:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:30:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:30:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:30:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:30:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.633228, avg_loss=0.719436, seen=12, correct=6, accuracy=0.500000
2025-11-11 16:30:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.849354, avg_loss=0.696234, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:30:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:27 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.475000
2025-11-11 16:30:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:30:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:30:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:30:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:30:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:30:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.374950, avg_loss=0.697913, seen=12, correct=6, accuracy=0.500000
2025-11-11 16:30:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.601624, avg_loss=0.690041, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:30:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:37 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.500000
2025-11-11 16:30:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:30:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:30:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:30:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:30:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:30:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.470530, avg_loss=0.705877, seen=12, correct=5, accuracy=0.416667
2025-11-11 16:30:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.946175, avg_loss=0.698654, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:30:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:48 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.525000, curr=0.475000
2025-11-11 16:30:54 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:30:54 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:30:54 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:30:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:30:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:30:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.614643, avg_loss=0.717887, seen=12, correct=5, accuracy=0.416667
2025-11-11 16:30:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.596191, avg_loss=0.689905, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:30:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:30:58 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.525000, curr=0.450000
2025-11-11 16:31:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:31:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:31:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:31:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:31:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:31:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.472128, avg_loss=0.706011, seen=12, correct=7, accuracy=0.583333
2025-11-11 16:31:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:31:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.560219, avg_loss=0.689005, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:31:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:31:08 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.525000, curr=0.500000
2025-11-11 16:31:14 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:31:14 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:31:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:31:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:31:15 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.430528, avg_loss=0.702544, seen=12, correct=6, accuracy=0.500000
2025-11-11 16:31:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:31:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:17 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.481155, avg_loss=0.687029, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:31:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:31:18 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:31:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.627478, avg_loss=0.718956, seen=12, correct=6, accuracy=0.500000
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:31:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.013151, avg_loss=0.675329, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:31:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:31:28 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:31:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:31:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:31:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:31:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:31:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:31:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=8.538834, avg_loss=0.711569, seen=12, correct=6, accuracy=0.500000
2025-11-11 16:31:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:31:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.061668, avg_loss=0.676542, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:31:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:31:38 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:31:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:31:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:31:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:31:39 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:31:39 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:31:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:31:39 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:31:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:39 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:31:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:31:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:31:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:31:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=159.017395, avg_loss=0.795087, seen=200, correct=84, accuracy=0.420000
2025-11-11 16:31:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1374MB allocated=1344MB
2025-11-11 16:31:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.743408, avg_loss=0.768585, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:31:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1374MB allocated=1344MB
2025-11-11 16:31:46 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.450000
2025-11-11 16:31:46 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:31:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2350, total=9397)
2025-11-11 16:31:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:47 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:31:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:47 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:31:47 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1175, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:31:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:31:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:31:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:31:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:31:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=145.446442, avg_loss=0.727232, seen=200, correct=95, accuracy=0.475000
2025-11-11 16:31:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:31:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.255030, avg_loss=0.681376, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:31:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:32:00 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 16:32:07 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:32:07 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:32:07 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:32:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:32:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=143.387268, avg_loss=0.716936, seen=200, correct=100, accuracy=0.500000
2025-11-11 16:32:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:32:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:32:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:32:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.342022, avg_loss=0.683551, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:32:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:32:13 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:32:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:32:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:32:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:32:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:32:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=143.463501, avg_loss=0.717318, seen=200, correct=93, accuracy=0.465000
2025-11-11 16:32:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:32:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:32:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:32:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.743443, avg_loss=0.668586, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:32:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:32:26 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-11-11 16:32:33 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:32:33 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:32:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:32:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:32:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=143.769180, avg_loss=0.718846, seen=200, correct=92, accuracy=0.460000
2025-11-11 16:32:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:32:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:32:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:32:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.061613, avg_loss=0.676540, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:32:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:32:39 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.525000
2025-11-11 16:32:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:32:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:32:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:32:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:32:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.896454, avg_loss=0.709482, seen=200, correct=93, accuracy=0.465000
2025-11-11 16:32:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:32:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:32:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:32:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.640856, avg_loss=0.666021, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:32:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:32:52 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 16:32:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:32:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:32:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:32:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:33:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=144.823181, avg_loss=0.724116, seen=200, correct=86, accuracy=0.430000
2025-11-11 16:33:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.373230, avg_loss=0.684331, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:33:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.625000, curr=0.550000
2025-11-11 16:33:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:33:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:33:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:33:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:33:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:33:15 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.866440, avg_loss=0.709332, seen=200, correct=95, accuracy=0.475000
2025-11-11 16:33:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.613005, avg_loss=0.665325, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:33:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:19 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.625000, curr=0.600000
2025-11-11 16:33:25 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:33:25 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:33:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:33:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:33:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.623917, avg_loss=0.693120, seen=200, correct=99, accuracy=0.495000
2025-11-11 16:33:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.232798, avg_loss=0.655820, seen=40, correct=27, accuracy=0.675000
2025-11-11 16:33:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:32 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.675000
2025-11-11 16:33:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:33:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:33:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:33:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:33:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:33:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=137.837250, avg_loss=0.689186, seen=200, correct=106, accuracy=0.530000
2025-11-11 16:33:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.934898, avg_loss=0.648372, seen=40, correct=27, accuracy=0.675000
2025-11-11 16:33:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.675000
2025-11-11 16:33:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:33:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:33:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:33:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:33:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:33:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.258560, avg_loss=0.701293, seen=200, correct=95, accuracy=0.475000
2025-11-11 16:33:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.404804, avg_loss=0.660120, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:33:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:58 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.675000, curr=0.550000
2025-11-11 16:33:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:33:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:33:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:33:59 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:33:59 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:33:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:33:59 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:33:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:59 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:33:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:00 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:34:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:34:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:34:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.527992, avg_loss=0.712615, seen=26, correct=15, accuracy=0.576923
2025-11-11 16:34:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1394MB allocated=1360MB
2025-11-11 16:34:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.562033, avg_loss=0.764051, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:34:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1394MB allocated=1360MB
2025-11-11 16:34:03 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 16:34:03 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:34:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=126, total=503)
2025-11-11 16:34:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:04 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:34:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:04 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:34:04 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=63, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.312763, avg_loss=0.704337, seen=26, correct=10, accuracy=0.384615
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.848679, avg_loss=0.721217, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:34:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:14 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 16:34:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:34:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:34:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:34:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:34:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:34:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.734070, avg_loss=0.720541, seen=26, correct=11, accuracy=0.423077
2025-11-11 16:34:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.528284, avg_loss=0.713207, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:34:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:25 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:34:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:34:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:34:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:34:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:34:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:34:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.588869, avg_loss=0.714957, seen=26, correct=11, accuracy=0.423077
2025-11-11 16:34:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.583017, avg_loss=0.714575, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:34:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:35 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-11-11 16:34:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:34:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:34:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:34:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:34:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:34:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.440449, avg_loss=0.709248, seen=26, correct=11, accuracy=0.423077
2025-11-11 16:34:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.383686, avg_loss=0.709592, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:34:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.550000
2025-11-11 16:34:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:34:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:34:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:34:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:34:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:34:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.104065, avg_loss=0.696310, seen=26, correct=14, accuracy=0.538462
2025-11-11 16:34:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.237535, avg_loss=0.705938, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:34:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:34:56 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.550000
2025-11-11 16:35:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:35:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:35:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:35:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:35:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:35:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.100376, avg_loss=0.696168, seen=26, correct=11, accuracy=0.423077
2025-11-11 16:35:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.891674, avg_loss=0.697292, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:35:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.525000
2025-11-11 16:35:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:35:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:35:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:35:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:35:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:35:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.471420, avg_loss=0.710439, seen=26, correct=10, accuracy=0.384615
2025-11-11 16:35:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.802374, avg_loss=0.695059, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:35:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:17 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.550000
2025-11-11 16:35:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:35:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:35:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:35:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:35:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:35:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=17.899246, avg_loss=0.688433, seen=26, correct=15, accuracy=0.576923
2025-11-11 16:35:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.693302, avg_loss=0.692333, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:35:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:27 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:35:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:35:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:35:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:35:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:35:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:35:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.086555, avg_loss=0.695637, seen=26, correct=11, accuracy=0.423077
2025-11-11 16:35:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.235235, avg_loss=0.680881, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:35:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:37 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:35:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:35:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:35:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:35:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:35:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:35:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=18.207396, avg_loss=0.700284, seen=26, correct=12, accuracy=0.461538
2025-11-11 16:35:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.089020, avg_loss=0.677225, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:35:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:48 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 16:35:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:35:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:35:49 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:35:49 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:35:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:35:49 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:35:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:49 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:35:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:35:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:35:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:35:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=146.514786, avg_loss=0.732574, seen=200, correct=102, accuracy=0.510000
2025-11-11 16:35:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1394MB allocated=1377MB
2025-11-11 16:35:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.988865, avg_loss=0.749722, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:35:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1394MB allocated=1377MB
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4561, total=18241)
2025-11-11 16:35:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=2281, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:36:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:36:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:36:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:36:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=142.837708, avg_loss=0.714189, seen=200, correct=94, accuracy=0.470000
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:36:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:36:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:36:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.572729, avg_loss=0.689318, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:36:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:36:10 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:36:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:36:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:36:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:36:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=141.455353, avg_loss=0.707277, seen=200, correct=100, accuracy=0.500000
2025-11-11 16:36:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:36:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:36:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:36:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.653091, avg_loss=0.691327, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:36:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:36:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:36:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:36:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:36:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:36:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.679749, avg_loss=0.703399, seen=200, correct=98, accuracy=0.490000
2025-11-11 16:36:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:36:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:36:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:36:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.442148, avg_loss=0.686054, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:36:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:36:36 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:36:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:36:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:36:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:36:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=140.785721, avg_loss=0.703929, seen=200, correct=107, accuracy=0.535000
2025-11-11 16:36:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:36:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:36:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:36:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.651411, avg_loss=0.691285, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:36:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:36:49 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:36:56 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:36:56 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:36:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:36:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.707184, avg_loss=0.698536, seen=200, correct=104, accuracy=0.520000
2025-11-11 16:36:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:37:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:37:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.801388, avg_loss=0.670035, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:37:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:37:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:37:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:37:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:37:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:37:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:37:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.735565, avg_loss=0.693678, seen=200, correct=103, accuracy=0.515000
2025-11-11 16:37:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:37:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:37:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.655308, avg_loss=0.666383, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:37:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:15 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:37:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:37:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:37:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:37:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:37:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:37:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.824112, avg_loss=0.699121, seen=200, correct=105, accuracy=0.525000
2025-11-11 16:37:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:37:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:37:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.285007, avg_loss=0.657125, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:37:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.575000
2025-11-11 16:37:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:37:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:37:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:37:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:37:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.009964, avg_loss=0.680050, seen=200, correct=118, accuracy=0.590000
2025-11-11 16:37:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:37:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:37:40 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.277458, avg_loss=0.656936, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:37:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:41 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 16:37:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:37:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:37:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:37:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:37:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:37:51 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=134.751190, avg_loss=0.673756, seen=200, correct=116, accuracy=0.580000
2025-11-11 16:37:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:37:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:37:53 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.847120, avg_loss=0.646178, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:37:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:37:54 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 16:38:01 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:38:01 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:38:01 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:38:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:38:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:38:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=132.656860, avg_loss=0.663284, seen=200, correct=124, accuracy=0.620000
2025-11-11 16:38:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:38:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.337811, avg_loss=0.658445, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:38:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:38:08 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 16:38:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:38:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:38:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:38:08 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:38:08 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:38:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:38:08 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:38:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:09 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:38:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:38:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:38:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=143.570038, avg_loss=0.717850, seen=200, correct=106, accuracy=0.530000
2025-11-11 16:38:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1416MB allocated=1394MB
2025-11-11 16:38:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=31.886379, avg_loss=0.797159, seen=40, correct=17, accuracy=0.425000
2025-11-11 16:38:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1416MB allocated=1394MB
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.425000
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1106, total=4423)
2025-11-11 16:38:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=553, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:38:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:38:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:38:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:38:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:38:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:38:26 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=139.461044, avg_loss=0.697305, seen=200, correct=101, accuracy=0.505000
2025-11-11 16:38:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:38:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.225061, avg_loss=0.705627, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:38:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:38:29 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:38:36 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:38:36 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:38:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:38:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:38:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:38:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.011902, avg_loss=0.690060, seen=200, correct=100, accuracy=0.500000
2025-11-11 16:38:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:38:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.249371, avg_loss=0.706234, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:38:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:38:42 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:38:49 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:38:49 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:38:49 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:38:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:38:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:38:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.324493, avg_loss=0.691622, seen=200, correct=102, accuracy=0.510000
2025-11-11 16:38:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:38:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.311478, avg_loss=0.732787, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:38:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:38:55 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:39:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:39:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:39:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:39:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:39:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:39:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=137.402527, avg_loss=0.687013, seen=200, correct=100, accuracy=0.500000
2025-11-11 16:39:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:39:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:39:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.779638, avg_loss=0.694491, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:39:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:39:08 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.500000
2025-11-11 16:39:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:39:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:39:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:39:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:39:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=136.485550, avg_loss=0.682428, seen=200, correct=102, accuracy=0.510000
2025-11-11 16:39:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:39:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:39:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.935757, avg_loss=0.698394, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:39:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:39:22 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.500000
2025-11-11 16:39:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:39:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:39:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:39:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:39:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=135.846848, avg_loss=0.679234, seen=200, correct=104, accuracy=0.520000
2025-11-11 16:39:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:39:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:39:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.226536, avg_loss=0.655663, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:39:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:39:35 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:39:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:39:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:39:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:39:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:39:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:39:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=134.865295, avg_loss=0.674326, seen=200, correct=106, accuracy=0.530000
2025-11-11 16:39:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:39:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:39:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.254639, avg_loss=0.681366, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:39:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:39:48 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:39:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:39:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:39:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:39:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:39:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:39:58 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=134.169769, avg_loss=0.670849, seen=200, correct=112, accuracy=0.560000
2025-11-11 16:39:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:39:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.206196, avg_loss=0.705155, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:40:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:40:01 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.500000
2025-11-11 16:40:08 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:40:08 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:40:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:40:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:40:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:40:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=138.484802, avg_loss=0.692424, seen=200, correct=111, accuracy=0.555000
2025-11-11 16:40:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:40:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=31.333139, avg_loss=0.783328, seen=40, correct=16, accuracy=0.400000
2025-11-11 16:40:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:40:14 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.400000
2025-11-11 16:40:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:40:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:40:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:40:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:40:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:40:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=131.827469, avg_loss=0.659137, seen=200, correct=120, accuracy=0.600000
2025-11-11 16:40:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:40:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.304129, avg_loss=0.682603, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:40:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:40:27 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.550000
2025-11-11 16:40:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:40:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:40:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:40:28 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:40:28 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:40:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:40:28 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:40:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=22, num_train_batch_last_epoch=12, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:28 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:40:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:40:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=22, num_train_batch_last_epoch=12, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:40:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=1.562492, avg_loss=0.781246, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:40:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1436MB allocated=1411MB
2025-11-11 16:40:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=22, num_train_batch_last_epoch=12, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=32.172028, avg_loss=0.804301, seen=40, correct=17, accuracy=0.425000
2025-11-11 16:40:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1436MB allocated=1411MB
2025-11-11 16:40:32 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.425000
2025-11-11 16:40:32 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:40:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:40:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:32 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:40:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=22, num_train_batch_last_epoch=12, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:32 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:40:33 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=6, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:40:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=1.519087, avg_loss=0.759543, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:40:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.612013, avg_loss=0.765300, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:40:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:40:43 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.475000
2025-11-11 16:40:49 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:40:49 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:40:49 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:40:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:40:50 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=1.418033, avg_loss=0.709016, seen=2, correct=0, accuracy=0.000000
2025-11-11 16:40:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:40:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.337639, avg_loss=0.758441, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:40:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:40:53 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:41:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:41:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:41:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=1.328671, avg_loss=0.664335, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:41:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.427753, avg_loss=0.735694, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:41:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:03 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:41:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:41:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:41:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:41:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:41:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=1.219986, avg_loss=0.609993, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:41:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.774841, avg_loss=0.719371, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:41:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:13 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:41:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=1.175823, avg_loss=0.587911, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.534214, avg_loss=0.688355, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:41:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.625000
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=1.055344, avg_loss=0.527672, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.591362, avg_loss=0.664784, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:41:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.917823, avg_loss=0.458912, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.334898, avg_loss=0.658372, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:41:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 16:41:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:41:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:41:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:41:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:41:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.791245, avg_loss=0.395622, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:41:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.639898, avg_loss=0.690997, seen=40, correct=29, accuracy=0.725000
2025-11-11 16:41:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:41:55 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.725000
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.760404, avg_loss=0.380202, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:42:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=32.137188, avg_loss=0.803430, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:42:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:42:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-11-11 16:42:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:42:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:42:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:42:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:42:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=1.137581, avg_loss=0.568790, seen=2, correct=1, accuracy=0.500000
2025-11-11 16:42:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:42:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=37.896084, avg_loss=0.947402, seen=40, correct=27, accuracy=0.675000
2025-11-11 16:42:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:42:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.675000
2025-11-11 16:42:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:42:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:42:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:42:17 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:42:17 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:42:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:42:17 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:42:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:17 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:42:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:42:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:42:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:42:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=102.599373, avg_loss=0.738125, seen=139, correct=73, accuracy=0.525180
2025-11-11 16:42:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1456MB allocated=1428MB
2025-11-11 16:42:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.289005, avg_loss=0.732225, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1456MB allocated=1428MB
2025-11-11 16:42:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:42:24 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:42:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=664, total=2655)
2025-11-11 16:42:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:24 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:42:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:24 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:42:24 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=332, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:42:31 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:42:31 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:42:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:42:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:42:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:42:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=100.147842, avg_loss=0.720488, seen=139, correct=71, accuracy=0.510791
2025-11-11 16:42:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.235529, avg_loss=0.730888, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:42:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.450000
2025-11-11 16:42:43 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:42:43 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:42:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:42:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:42:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:42:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=103.513763, avg_loss=0.744703, seen=139, correct=66, accuracy=0.474820
2025-11-11 16:42:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:42:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=33.806168, avg_loss=0.845154, seen=40, correct=15, accuracy=0.375000
2025-11-11 16:42:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:42:49 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.375000
2025-11-11 16:42:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:42:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:42:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:42:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:42:58 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=99.092323, avg_loss=0.712894, seen=139, correct=69, accuracy=0.496403
2025-11-11 16:42:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:42:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.849174, avg_loss=0.746229, seen=40, correct=16, accuracy=0.400000
2025-11-11 16:43:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:43:01 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.500000, curr=0.400000
2025-11-11 16:43:07 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:43:07 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:43:07 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:43:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:43:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:43:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=99.140686, avg_loss=0.713242, seen=139, correct=67, accuracy=0.482014
2025-11-11 16:43:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:43:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.134785, avg_loss=0.728370, seen=40, correct=16, accuracy=0.400000
2025-11-11 16:43:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:43:13 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.500000, curr=0.400000
2025-11-11 16:43:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:43:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:43:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:43:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:43:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:43:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=99.027092, avg_loss=0.712425, seen=139, correct=71, accuracy=0.510791
2025-11-11 16:43:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:43:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:43:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.600170, avg_loss=0.715004, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:43:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:43:25 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:43:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:43:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:43:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:43:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:43:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:43:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=98.980621, avg_loss=0.712091, seen=139, correct=70, accuracy=0.503597
2025-11-11 16:43:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:43:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:43:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.949505, avg_loss=0.748738, seen=40, correct=17, accuracy=0.425000
2025-11-11 16:43:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:43:37 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.425000
2025-11-11 16:43:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:43:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:43:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:43:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:43:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:43:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=98.980515, avg_loss=0.712090, seen=139, correct=63, accuracy=0.453237
2025-11-11 16:43:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:43:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:43:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.252823, avg_loss=0.756321, seen=40, correct=16, accuracy=0.400000
2025-11-11 16:43:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:43:50 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.400000
2025-11-11 16:43:56 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:43:56 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:43:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:43:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:43:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:43:58 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=97.549393, avg_loss=0.701794, seen=139, correct=67, accuracy=0.482014
2025-11-11 16:43:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:44:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.217997, avg_loss=0.730450, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:44:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:44:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:44:08 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:44:08 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:44:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:44:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:44:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:44:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=96.783966, avg_loss=0.696288, seen=139, correct=72, accuracy=0.517986
2025-11-11 16:44:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:44:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.915947, avg_loss=0.697899, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:44:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:44:14 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.550000, curr=0.525000
2025-11-11 16:44:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:44:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:44:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:44:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:44:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:44:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=96.137100, avg_loss=0.691634, seen=139, correct=78, accuracy=0.561151
2025-11-11 16:44:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:44:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.758228, avg_loss=0.668956, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:44:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:44:26 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:44:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:44:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:44:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:44:27 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:44:27 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:44:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:44:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:44:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:44:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=115.295090, avg_loss=0.716119, seen=161, correct=79, accuracy=0.490683
2025-11-11 16:44:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1476MB allocated=1444MB
2025-11-11 16:44:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.208652, avg_loss=0.705216, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:44:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1476MB allocated=1444MB
2025-11-11 16:44:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:44:34 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:44:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=767, total=3066)
2025-11-11 16:44:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:34 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:44:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:34 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:44:34 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=384, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:44:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:44:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:44:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:44:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:44:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:44:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=117.806206, avg_loss=0.731716, seen=161, correct=77, accuracy=0.478261
2025-11-11 16:44:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:44:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.344130, avg_loss=0.708603, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:44:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:44:47 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.450000
2025-11-11 16:44:54 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:44:54 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:44:54 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:44:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:44:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:44:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=118.694427, avg_loss=0.737232, seen=161, correct=78, accuracy=0.484472
2025-11-11 16:44:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:44:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.792435, avg_loss=0.719811, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:45:00 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.450000
2025-11-11 16:45:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:45:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:45:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:45:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:45:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=113.464012, avg_loss=0.704745, seen=161, correct=82, accuracy=0.509317
2025-11-11 16:45:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:45:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:11 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.715292, avg_loss=0.692882, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:45:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:45:12 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:45:19 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:45:19 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:45:19 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:45:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:45:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:45:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=111.774307, avg_loss=0.694250, seen=161, correct=89, accuracy=0.552795
2025-11-11 16:45:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:45:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.642111, avg_loss=0.691053, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:45:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:45:25 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:45:31 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:45:31 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:45:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:45:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:45:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=112.044167, avg_loss=0.695927, seen=161, correct=82, accuracy=0.509317
2025-11-11 16:45:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:45:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.725918, avg_loss=0.693148, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:45:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:45:37 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.550000
2025-11-11 16:45:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:45:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:45:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:45:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:45:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:45:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=112.440224, avg_loss=0.698386, seen=161, correct=82, accuracy=0.509317
2025-11-11 16:45:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:45:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.823896, avg_loss=0.695597, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:45:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:45:50 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.575000, curr=0.500000
2025-11-11 16:45:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:45:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:45:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:45:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:45:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:45:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=111.637695, avg_loss=0.693402, seen=161, correct=81, accuracy=0.503106
2025-11-11 16:45:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:46:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.562202, avg_loss=0.689055, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:46:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.575000, curr=0.475000
2025-11-11 16:46:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:46:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:46:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:46:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:46:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:46:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=109.797012, avg_loss=0.681969, seen=161, correct=90, accuracy=0.559006
2025-11-11 16:46:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:46:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.427382, avg_loss=0.685685, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:46:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:46:15 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:46:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:46:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:46:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:46:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:46:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:46:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=108.734848, avg_loss=0.675372, seen=161, correct=91, accuracy=0.565217
2025-11-11 16:46:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:46:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.084015, avg_loss=0.677100, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:46:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:46:28 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:46:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:46:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:46:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:46:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:46:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:46:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=108.740334, avg_loss=0.675406, seen=161, correct=93, accuracy=0.577640
2025-11-11 16:46:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:46:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.990545, avg_loss=0.674764, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:46:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:46:40 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:46:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:46:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:46:41 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:46:41 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:46:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:46:41 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:46:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:41 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:46:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:46:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:46:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:46:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=69.813011, avg_loss=0.775700, seen=90, correct=44, accuracy=0.488889
2025-11-11 16:46:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1496MB allocated=1461MB
2025-11-11 16:46:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.195652, avg_loss=0.654891, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1496MB allocated=1461MB
2025-11-11 16:46:47 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:46:47 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:46:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=431, total=1724)
2025-11-11 16:46:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:47 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:46:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:47 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:46:47 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=216, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:46:54 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:46:54 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:46:54 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:46:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:46:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:46:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=65.318604, avg_loss=0.725762, seen=90, correct=42, accuracy=0.466667
2025-11-11 16:46:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:46:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.601326, avg_loss=0.665033, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:46:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:46:59 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.700000
2025-11-11 16:47:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:47:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:47:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:47:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:47:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:47:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=64.891823, avg_loss=0.721020, seen=90, correct=45, accuracy=0.500000
2025-11-11 16:47:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:47:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:47:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.593754, avg_loss=0.689844, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:47:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:10 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.525000
2025-11-11 16:47:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:47:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:47:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:47:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:47:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:47:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=63.738853, avg_loss=0.708209, seen=90, correct=42, accuracy=0.466667
2025-11-11 16:47:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:47:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:47:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.650723, avg_loss=0.666268, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:47:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:22 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.650000
2025-11-11 16:47:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:47:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:47:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:47:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:47:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=63.589066, avg_loss=0.706545, seen=90, correct=42, accuracy=0.466667
2025-11-11 16:47:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:47:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:47:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.377811, avg_loss=0.659445, seen=40, correct=29, accuracy=0.725000
2025-11-11 16:47:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:33 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.725000
2025-11-11 16:47:40 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:47:40 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:47:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:47:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:47:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=63.023949, avg_loss=0.700266, seen=90, correct=45, accuracy=0.500000
2025-11-11 16:47:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:47:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.324924, avg_loss=0.658123, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:47:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:47:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=62.982475, avg_loss=0.699805, seen=90, correct=47, accuracy=0.522222
2025-11-11 16:47:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:47:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:47:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.309837, avg_loss=0.657746, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:47:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:47:56 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-11-11 16:48:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:48:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:48:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:48:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:48:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:48:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=62.137314, avg_loss=0.690415, seen=90, correct=44, accuracy=0.488889
2025-11-11 16:48:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:48:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.383615, avg_loss=0.659590, seen=40, correct=29, accuracy=0.725000
2025-11-11 16:48:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:48:07 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.725000
2025-11-11 16:48:14 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:48:14 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:48:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:48:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:48:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:48:15 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=61.251259, avg_loss=0.680570, seen=90, correct=48, accuracy=0.533333
2025-11-11 16:48:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:48:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.320353, avg_loss=0.658009, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:48:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:48:19 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.700000
2025-11-11 16:48:25 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:48:25 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:48:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:48:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:48:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:48:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=61.103531, avg_loss=0.678928, seen=90, correct=52, accuracy=0.577778
2025-11-11 16:48:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:48:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.244305, avg_loss=0.656108, seen=40, correct=31, accuracy=0.775000
2025-11-11 16:48:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:48:30 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.775000
2025-11-11 16:48:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:48:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:48:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:48:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:48:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=61.096214, avg_loss=0.678847, seen=90, correct=52, accuracy=0.577778
2025-11-11 16:48:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:48:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:40 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.723206, avg_loss=0.668080, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:48:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:48:41 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.775000, curr=0.575000
2025-11-11 16:48:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:48:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:48:42 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:48:42 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:48:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:48:42 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:48:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=88, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:42 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:48:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:48:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=88, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:48:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=5.453534, avg_loss=0.605948, seen=9, correct=7, accuracy=0.777778
2025-11-11 16:48:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1496MB allocated=1478MB
2025-11-11 16:48:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=88, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=32.769169, avg_loss=0.819229, seen=40, correct=15, accuracy=0.375000
2025-11-11 16:48:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1496MB allocated=1478MB
2025-11-11 16:48:46 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.375000
2025-11-11 16:48:46 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:48:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=44, total=176)
2025-11-11 16:48:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:47 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:48:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=88, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:47 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:48:47 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=22, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:48:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=5.579712, avg_loss=0.619968, seen=9, correct=7, accuracy=0.777778
2025-11-11 16:48:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:48:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=31.269276, avg_loss=0.781732, seen=40, correct=16, accuracy=0.400000
2025-11-11 16:48:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:48:57 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.400000
2025-11-11 16:49:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:49:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:49:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:49:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:49:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=6.238328, avg_loss=0.693148, seen=9, correct=4, accuracy=0.444444
2025-11-11 16:49:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.225380, avg_loss=0.730634, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:49:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:07 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:49:14 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:49:14 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:49:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:49:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:49:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=6.051661, avg_loss=0.672407, seen=9, correct=5, accuracy=0.555556
2025-11-11 16:49:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.973642, avg_loss=0.724341, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:49:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:17 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.450000
2025-11-11 16:49:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:49:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:49:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:49:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:49:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:49:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=5.787953, avg_loss=0.643106, seen=9, correct=8, accuracy=0.888889
2025-11-11 16:49:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.686346, avg_loss=0.717159, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:49:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:27 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.475000
2025-11-11 16:49:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:49:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:49:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:49:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:49:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:49:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=5.622547, avg_loss=0.624727, seen=9, correct=8, accuracy=0.888889
2025-11-11 16:49:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.807482, avg_loss=0.720187, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:49:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:38 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.500000, curr=0.450000
2025-11-11 16:49:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:49:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:49:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:49:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:49:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=5.723705, avg_loss=0.635967, seen=9, correct=8, accuracy=0.888889
2025-11-11 16:49:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.095146, avg_loss=0.702379, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:49:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:48 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:49:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:49:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:49:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:49:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:49:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:49:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=5.608377, avg_loss=0.623153, seen=9, correct=8, accuracy=0.888889
2025-11-11 16:49:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.659597, avg_loss=0.691490, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:49:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:49:58 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:50:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:50:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:50:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:50:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:50:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:50:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=5.605472, avg_loss=0.622830, seen=9, correct=8, accuracy=0.888889
2025-11-11 16:50:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.361000, avg_loss=0.684025, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:50:08 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:50:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:50:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:50:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:50:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:50:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:50:15 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=5.669919, avg_loss=0.629991, seen=9, correct=7, accuracy=0.777778
2025-11-11 16:50:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:50:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.665104, avg_loss=0.666628, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:50:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:50:19 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:50:25 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:50:25 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:50:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:50:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:50:26 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=5.570784, avg_loss=0.618976, seen=9, correct=7, accuracy=0.777778
2025-11-11 16:50:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:50:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.169735, avg_loss=0.654243, seen=40, correct=27, accuracy=0.675000
2025-11-11 16:50:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:50:29 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.675000
2025-11-11 16:50:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:50:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:50:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:50:30 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:50:30 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:50:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:50:30 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:50:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:30 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:50:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:50:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:50:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:50:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=22.950480, avg_loss=0.740338, seen=31, correct=14, accuracy=0.451613
2025-11-11 16:50:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1516MB allocated=1495MB
2025-11-11 16:50:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.182343, avg_loss=0.754559, seen=40, correct=14, accuracy=0.350000
2025-11-11 16:50:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1516MB allocated=1495MB
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.350000
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=151, total=601)
2025-11-11 16:50:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=76, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:50:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:50:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:50:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:50:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:50:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=20.771399, avg_loss=0.670045, seen=31, correct=17, accuracy=0.548387
2025-11-11 16:50:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:50:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.955967, avg_loss=0.648899, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:50:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:50:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 16:50:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:50:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:50:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:50:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:50:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:50:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=20.598101, avg_loss=0.664455, seen=31, correct=17, accuracy=0.548387
2025-11-11 16:50:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:50:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.423977, avg_loss=0.635599, seen=40, correct=27, accuracy=0.675000
2025-11-11 16:50:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:50:56 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.675000
2025-11-11 16:51:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:51:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:51:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:51:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:51:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:51:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=20.380169, avg_loss=0.657425, seen=31, correct=16, accuracy=0.516129
2025-11-11 16:51:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=24.856493, avg_loss=0.621412, seen=40, correct=29, accuracy=0.725000
2025-11-11 16:51:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:06 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.725000
2025-11-11 16:51:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:51:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:51:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:51:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:51:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:51:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=20.827625, avg_loss=0.671859, seen=31, correct=18, accuracy=0.580645
2025-11-11 16:51:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.988785, avg_loss=0.649720, seen=40, correct=27, accuracy=0.675000
2025-11-11 16:51:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:17 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.675000
2025-11-11 16:51:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:51:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:51:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:51:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:51:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:51:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=21.108646, avg_loss=0.680924, seen=31, correct=18, accuracy=0.580645
2025-11-11 16:51:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.375582, avg_loss=0.659390, seen=40, correct=25, accuracy=0.625000
2025-11-11 16:51:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.625000
2025-11-11 16:51:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:51:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:51:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:51:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:51:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:51:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=20.838680, avg_loss=0.672215, seen=31, correct=16, accuracy=0.516129
2025-11-11 16:51:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.798115, avg_loss=0.644953, seen=40, correct=27, accuracy=0.675000
2025-11-11 16:51:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:38 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.725000, curr=0.675000
2025-11-11 16:51:45 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:51:45 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:51:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:51:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:51:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:51:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=20.559166, avg_loss=0.663199, seen=31, correct=17, accuracy=0.548387
2025-11-11 16:51:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=24.891722, avg_loss=0.622293, seen=40, correct=32, accuracy=0.800000
2025-11-11 16:51:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:49 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.800000
2025-11-11 16:51:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:51:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:51:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:51:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:51:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:51:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=20.981413, avg_loss=0.676820, seen=31, correct=14, accuracy=0.451613
2025-11-11 16:51:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.522388, avg_loss=0.638060, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:51:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:51:59 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.800000, curr=0.700000
2025-11-11 16:52:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:52:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:52:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:52:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:52:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:52:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=20.977310, avg_loss=0.676687, seen=31, correct=17, accuracy=0.548387
2025-11-11 16:52:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:52:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.743622, avg_loss=0.643591, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:52:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:52:10 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.800000, curr=0.650000
2025-11-11 16:52:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:52:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:52:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:52:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 16:52:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 16:52:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=20.658878, avg_loss=0.666415, seen=31, correct=15, accuracy=0.483871
2025-11-11 16:52:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:52:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=24.690781, avg_loss=0.617270, seen=40, correct=30, accuracy=0.750000
2025-11-11 16:52:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:52:20 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.800000, curr=0.750000
2025-11-11 16:52:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:52:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 16:52:21 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:52:21 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:52:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:52:21 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:52:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:21 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:52:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:52:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:52:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=87.100677, avg_loss=0.713940, seen=122, correct=71, accuracy=0.581967
2025-11-11 16:52:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1536MB allocated=1511MB
2025-11-11 16:52:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.921345, avg_loss=0.723034, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:52:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1536MB allocated=1511MB
2025-11-11 16:52:27 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:52:27 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:52:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=583, total=2331)
2025-11-11 16:52:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:27 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:52:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:27 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:52:27 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=292, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:52:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:52:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:52:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:52:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:52:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=84.193710, avg_loss=0.690112, seen=122, correct=69, accuracy=0.565574
2025-11-11 16:52:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:52:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.249950, avg_loss=0.731249, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:52:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:52:39 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.450000
2025-11-11 16:52:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:52:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:52:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:52:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:52:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:52:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=86.795982, avg_loss=0.711442, seen=122, correct=65, accuracy=0.532787
2025-11-11 16:52:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:52:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.381763, avg_loss=0.759544, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:52:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:52:51 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.475000
2025-11-11 16:52:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:52:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:52:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:52:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:52:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:52:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=84.513786, avg_loss=0.692736, seen=122, correct=66, accuracy=0.540984
2025-11-11 16:52:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.099894, avg_loss=0.727497, seen=40, correct=17, accuracy=0.425000
2025-11-11 16:53:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.500000, curr=0.425000
2025-11-11 16:53:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:53:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:53:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:53:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:53:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=83.846512, avg_loss=0.687266, seen=122, correct=68, accuracy=0.557377
2025-11-11 16:53:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.726757, avg_loss=0.718169, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:14 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:53:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:53:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:53:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:53:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:53:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:53:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=83.685402, avg_loss=0.685946, seen=122, correct=73, accuracy=0.598361
2025-11-11 16:53:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.018160, avg_loss=0.700454, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:53:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:26 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:53:33 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:53:33 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:53:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:53:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:53:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:53:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=82.736771, avg_loss=0.678170, seen=122, correct=75, accuracy=0.614754
2025-11-11 16:53:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.173374, avg_loss=0.704334, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:53:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:38 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:53:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:53:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:53:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:53:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:53:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:53:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=82.941689, avg_loss=0.679850, seen=122, correct=74, accuracy=0.606557
2025-11-11 16:53:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.690742, avg_loss=0.692269, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:53:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:53:50 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:53:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:53:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=82.200836, avg_loss=0.673777, seen=122, correct=70, accuracy=0.573770
2025-11-11 16:53:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:54:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.055397, avg_loss=0.701385, seen=40, correct=23, accuracy=0.575000
2025-11-11 16:54:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:54:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.575000
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:54:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:54:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=82.145126, avg_loss=0.673321, seen=122, correct=74, accuracy=0.606557
2025-11-11 16:54:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:54:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.132473, avg_loss=0.703312, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:54:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:54:14 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.575000, curr=0.525000
2025-11-11 16:54:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:54:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:54:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:54:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 16:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 16:54:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=80.730354, avg_loss=0.661724, seen=122, correct=75, accuracy=0.614754
2025-11-11 16:54:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:54:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.639832, avg_loss=0.690996, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:54:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:54:26 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:54:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:54:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:54:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 16:54:27 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:54:27 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:54:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:54:27 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:54:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:27 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:54:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:54:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:54:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:54:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=138.836334, avg_loss=0.742440, seen=187, correct=90, accuracy=0.481283
2025-11-11 16:54:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1556MB allocated=1528MB
2025-11-11 16:54:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=33.390434, avg_loss=0.834761, seen=40, correct=14, accuracy=0.350000
2025-11-11 16:54:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1556MB allocated=1528MB
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.350000
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=890, total=3560)
2025-11-11 16:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=445, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:54:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:54:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:54:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:54:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:54:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=131.904877, avg_loss=0.705374, seen=187, correct=94, accuracy=0.502674
2025-11-11 16:54:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:54:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.096748, avg_loss=0.727419, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:54:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:54:47 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:54:54 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:54:54 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:54:54 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:54:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:54:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=131.100693, avg_loss=0.701073, seen=187, correct=102, accuracy=0.545455
2025-11-11 16:54:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:54:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.073570, avg_loss=0.701839, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:54:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:55:00 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.475000
2025-11-11 16:55:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:55:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:55:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:55:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:55:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=131.125977, avg_loss=0.701208, seen=187, correct=98, accuracy=0.524064
2025-11-11 16:55:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:55:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:55:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.460327, avg_loss=0.711508, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:55:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:55:12 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:55:19 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:55:19 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:55:19 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:55:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:55:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:55:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=131.408737, avg_loss=0.702721, seen=187, correct=97, accuracy=0.518717
2025-11-11 16:55:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:55:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:55:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.734219, avg_loss=0.743355, seen=40, correct=18, accuracy=0.450000
2025-11-11 16:55:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:55:25 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.500000, curr=0.450000
2025-11-11 16:55:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:55:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:55:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:55:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:55:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:55:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=130.519226, avg_loss=0.697964, seen=187, correct=93, accuracy=0.497326
2025-11-11 16:55:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:55:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:55:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.146372, avg_loss=0.728659, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:55:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:55:38 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.500000, curr=0.475000
2025-11-11 16:55:45 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:55:45 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:55:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:55:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:55:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:55:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=129.830170, avg_loss=0.694279, seen=187, correct=99, accuracy=0.529412
2025-11-11 16:55:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:55:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:55:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.365917, avg_loss=0.709148, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:55:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:55:51 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.500000
2025-11-11 16:55:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:55:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:55:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:55:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:56:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=130.035233, avg_loss=0.695376, seen=187, correct=96, accuracy=0.513369
2025-11-11 16:56:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:56:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.597441, avg_loss=0.714936, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:56:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:56:03 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.525000
2025-11-11 16:56:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:56:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:56:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:56:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:56:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:56:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=131.489899, avg_loss=0.703155, seen=187, correct=92, accuracy=0.491979
2025-11-11 16:56:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:56:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=30.198563, avg_loss=0.754964, seen=40, correct=14, accuracy=0.350000
2025-11-11 16:56:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:56:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.525000, curr=0.350000
2025-11-11 16:56:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:56:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:56:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:56:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:56:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:56:26 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=129.967209, avg_loss=0.695012, seen=187, correct=101, accuracy=0.540107
2025-11-11 16:56:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:56:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.181618, avg_loss=0.729540, seen=40, correct=15, accuracy=0.375000
2025-11-11 16:56:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:56:29 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.525000, curr=0.375000
2025-11-11 16:56:36 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:56:36 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:56:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:56:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 16:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:56:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=128.917389, avg_loss=0.689398, seen=187, correct=102, accuracy=0.545455
2025-11-11 16:56:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:56:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.995024, avg_loss=0.674876, seen=40, correct=22, accuracy=0.550000
2025-11-11 16:56:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:56:42 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.550000
2025-11-11 16:56:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:56:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 16:56:43 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:56:43 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:56:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:56:43 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:56:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:43 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:56:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:56:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:56:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:56:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=37.123795, avg_loss=0.700449, seen=53, correct=29, accuracy=0.547170
2025-11-11 16:56:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1578MB allocated=1545MB
2025-11-11 16:56:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.973907, avg_loss=0.674348, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:56:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1578MB allocated=1545MB
2025-11-11 16:56:48 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:56:48 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:56:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=256, total=1021)
2025-11-11 16:56:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:48 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:56:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:48 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:56:48 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=128, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:56:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:56:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:56:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:56:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:56:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:56:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=36.079472, avg_loss=0.680745, seen=53, correct=28, accuracy=0.528302
2025-11-11 16:56:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:56:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.402149, avg_loss=0.660054, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:56:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:56:59 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.650000
2025-11-11 16:57:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:57:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:57:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:57:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:57:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:57:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=35.839043, avg_loss=0.676208, seen=53, correct=30, accuracy=0.566038
2025-11-11 16:57:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.137638, avg_loss=0.653441, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:57:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:10 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.700000
2025-11-11 16:57:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:57:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:57:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:57:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:57:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:57:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=35.972546, avg_loss=0.678727, seen=53, correct=31, accuracy=0.584906
2025-11-11 16:57:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.403309, avg_loss=0.660083, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:57:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:21 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.700000
2025-11-11 16:57:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:57:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:57:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:57:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:57:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=35.941212, avg_loss=0.678136, seen=53, correct=29, accuracy=0.547170
2025-11-11 16:57:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.209858, avg_loss=0.655246, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:57:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:32 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.700000
2025-11-11 16:57:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:57:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:57:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:57:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:57:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=35.554401, avg_loss=0.670838, seen=53, correct=32, accuracy=0.603774
2025-11-11 16:57:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.923203, avg_loss=0.648080, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:57:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:43 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.700000
2025-11-11 16:57:49 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:57:49 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:57:49 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:57:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:57:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:57:50 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=36.035782, avg_loss=0.679920, seen=53, correct=26, accuracy=0.490566
2025-11-11 16:57:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.102716, avg_loss=0.652568, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:57:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:57:53 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.700000, curr=0.650000
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:58:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:58:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=35.367641, avg_loss=0.667314, seen=53, correct=32, accuracy=0.603774
2025-11-11 16:58:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:58:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.833492, avg_loss=0.645837, seen=40, correct=26, accuracy=0.650000
2025-11-11 16:58:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:58:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.700000, curr=0.650000
2025-11-11 16:58:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:58:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:58:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:58:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:58:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:58:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=35.542339, avg_loss=0.670610, seen=53, correct=33, accuracy=0.622642
2025-11-11 16:58:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:58:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.601255, avg_loss=0.640031, seen=40, correct=29, accuracy=0.725000
2025-11-11 16:58:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:58:15 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.725000
2025-11-11 16:58:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:58:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:58:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:58:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:58:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=35.549503, avg_loss=0.670745, seen=53, correct=31, accuracy=0.584906
2025-11-11 16:58:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:58:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.993380, avg_loss=0.649834, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:58:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:58:26 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.725000, curr=0.600000
2025-11-11 16:58:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:58:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:58:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:58:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 16:58:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 16:58:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=35.187050, avg_loss=0.663907, seen=53, correct=35, accuracy=0.660377
2025-11-11 16:58:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.358204, avg_loss=0.633955, seen=40, correct=28, accuracy=0.700000
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:58:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.725000, curr=0.700000
2025-11-11 16:58:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:58:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 16:58:37 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:58:37 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:58:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:58:37 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:58:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:37 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:58:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:58:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 16:58:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 16:58:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=100.345184, avg_loss=0.687296, seen=146, correct=87, accuracy=0.595890
2025-11-11 16:58:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1598MB allocated=1562MB
2025-11-11 16:58:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.608755, avg_loss=0.690219, seen=40, correct=24, accuracy=0.600000
2025-11-11 16:58:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1598MB allocated=1562MB
2025-11-11 16:58:43 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.600000
2025-11-11 16:58:43 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:58:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-11-11 16:58:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:44 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:58:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:44 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:58:44 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:58:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:58:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:58:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:58:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 16:58:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 16:58:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=99.842697, avg_loss=0.683854, seen=146, correct=78, accuracy=0.534247
2025-11-11 16:58:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:58:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.151497, avg_loss=0.703787, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:58:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:58:56 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.600000, curr=0.500000
2025-11-11 16:59:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:59:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:59:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:59:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 16:59:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 16:59:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=99.715271, avg_loss=0.682981, seen=146, correct=81, accuracy=0.554795
2025-11-11 16:59:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.762388, avg_loss=0.694060, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:59:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:08 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.600000, curr=0.500000
2025-11-11 16:59:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:59:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:59:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:59:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 16:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 16:59:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=99.848633, avg_loss=0.683895, seen=146, correct=80, accuracy=0.547945
2025-11-11 16:59:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.140961, avg_loss=0.703524, seen=40, correct=19, accuracy=0.475000
2025-11-11 16:59:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:20 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.600000, curr=0.475000
2025-11-11 16:59:27 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:59:27 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:59:27 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:59:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 16:59:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 16:59:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=98.904633, avg_loss=0.677429, seen=146, correct=79, accuracy=0.541096
2025-11-11 16:59:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.652531, avg_loss=0.691313, seen=40, correct=20, accuracy=0.500000
2025-11-11 16:59:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.600000, curr=0.500000
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 16:59:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 16:59:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=98.463318, avg_loss=0.674406, seen=146, correct=84, accuracy=0.575342
2025-11-11 16:59:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.747421, avg_loss=0.693686, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:59:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:44 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.600000, curr=0.525000
2025-11-11 16:59:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:59:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:59:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:59:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 16:59:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 16:59:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=97.827316, avg_loss=0.670050, seen=146, correct=88, accuracy=0.602740
2025-11-11 16:59:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.817570, avg_loss=0.695439, seen=40, correct=21, accuracy=0.525000
2025-11-11 16:59:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 16:59:56 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.600000, curr=0.525000
2025-11-11 17:00:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 17:00:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 17:00:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 17:00:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:00:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:00:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=96.500481, avg_loss=0.660962, seen=146, correct=91, accuracy=0.623288
2025-11-11 17:00:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:00:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.157787, avg_loss=0.678945, seen=40, correct=23, accuracy=0.575000
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:00:08 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=7/25), best=0.600000, curr=0.575000
2025-11-11 17:00:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 17:00:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 17:00:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 17:00:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:00:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=96.885727, avg_loss=0.663601, seen=146, correct=91, accuracy=0.623288
2025-11-11 17:00:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:00:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:00:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:00:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.400711, avg_loss=0.685018, seen=40, correct=23, accuracy=0.575000
2025-11-11 17:00:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:00:20 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=8/25), best=0.600000, curr=0.575000
2025-11-11 17:00:27 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 17:00:27 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 17:00:27 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 17:00:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:00:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:00:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=95.626160, avg_loss=0.654974, seen=146, correct=90, accuracy=0.616438
2025-11-11 17:00:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:00:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:00:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.178545, avg_loss=0.679464, seen=40, correct=22, accuracy=0.550000
2025-11-11 17:00:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:00:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=9/25), best=0.600000, curr=0.550000
2025-11-11 17:00:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 17:00:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 17:00:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 17:00:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:00:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:00:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=97.020401, avg_loss=0.664523, seen=146, correct=93, accuracy=0.636986
2025-11-11 17:00:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:00:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:00:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:00:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.792860, avg_loss=0.694822, seen=40, correct=21, accuracy=0.525000
2025-11-11 17:00:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:00:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=10/25), best=0.600000, curr=0.525000
2025-11-11 17:00:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 17:00:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 17:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:00:45 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 17:00:45 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {}}
2025-11-11 17:00:45 (federatedscope.core.workers.server:550) INFO: [in-place aggregation ] round=0
2025-11-11 17:00:46 (federatedscope.core.workers.server:434) INFO: Server: Training is finished! (skip final evaluation)
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 75.65410443333333, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 3336096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 1 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 75.65577218333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 2 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 75.62039556666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 3 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 75.59627226666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 4 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 75.5725067, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 5 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 75.54894161666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 6 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 75.52534456666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 7 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 75.50142475, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 8 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 75.47566321666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 9 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 75.4495273, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 10 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 75.42354511666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 11 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 75.39786408333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 12 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 75.37294551666668, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 13 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 75.34923215, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 14 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 75.32560981666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 15 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 75.30010366666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 16 received finish message =================
2025-11-11 17:00:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:46 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 75.27327586666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:46 (federatedscope.core.workers.client:836) INFO: ================= client 17 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 75.24691058333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 18 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 75.22298716666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 19 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 75.19910033333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 20 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 75.17541991666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 21 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 75.15148105, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 22 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 75.1278692, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 23 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 75.10413098333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 24 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 75.08031925, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 25 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 75.05653688333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 26 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 75.03229818333332, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 27 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 75.00672181666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 28 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 74.98086998333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 29 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 74.95600325000001, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 30 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 74.93118786666668, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 31 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 74.90668758333332, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 32 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 74.88278226666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 33 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 74.85920618333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 34 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 74.83570171666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.workers.client:836) INFO: ================= client 35 received finish message =================
2025-11-11 17:00:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 74.81206503333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 75.23918911296296, 'sys_avg/total_model_size': '461.9M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '197.72K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-11-11 17:00:47 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.25531318102325096, 'sys_std/total_model_size': '78.07M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '517.27K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
