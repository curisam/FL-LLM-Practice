2025-10-09 08:48:16 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/gfl/fedbis_oracle_u3_1.0/exp_print.log
2025-10-09 08:48:16 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/gfl/fedbis_oracle_u3_1.0
2025-10-09 08:48:40 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-10-09 08:49:22 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-10-09 08:49:23 (federatedscope.core.configs.config:256) INFO: the used configs are: 
adapter:
  use: False
aggregator:
  BFT_args:
    
  byzantine_node_num: 0
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
  robust_rule: fedavg
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.9637]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.1592]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: True
  drop_last: False
  file_path: 
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  load_splits: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  save_splits: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.9, 0.09, 0.01]
  splits_path: ./final_data_splits
  splitter: meta
  splitter_args: []
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: reddit-tldr-comparison-choice@llm
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 2
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 0
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: False
  freq: 500
  metrics: ['loss', 'acc']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['val', 'test']
expname: 
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_idx_for_local_train: 0
  client_num: 53
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  master_addr: 127.0.0.1
  master_port: 29500
  merge_test_data: False
  merge_val_data: False
  method: FedAvg
  mode: standalone
  online_aggr: False
  process_num: 1
  resource_info_file: 
  restore_from: 
  sample_client_num: 5
  sample_client_rate: -1.0
  sampler: cluster
  save_client_model: False
  save_freq: 100
  save_to: checkpoints_1.0_oracle/tldr_choice_qwen_fedbis_oracle_u3.ckpt
  share_local_model: True
  total_round_num: 201
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
fedswa:
  use: False
finetune:
  batch_or_epoch: epoch
  before_eval: False
  epoch_linear: 10
  freeze_param: 
  local_param: []
  local_update_steps: 1
  lr_linear: 0.005
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
  simple_tuning: False
  weight_decay: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  fts:
    M: 100
    M_target: 200
    allow_load_existing_info: True
    diff: False
    fed_bo_max_iter: 50
    g_var: 1e-06
    gp_opt_schedule: 1
    local_bo_epochs: 50
    local_bo_max_iter: 50
    ls: 1.0
    obs_noise: 1e-06
    ss: 
    target_clients: []
    use: False
    v_kernel: 1.0
    var: 0.1
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  pfedhpo:
    discrete: False
    ss: 
    target_fl_total_round: 1000
    train_anchor: False
    train_fl: False
    use: False
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  trial_index: 0
  working_folder: hpo
llm:
  accelerator:
    config: 
    use: True
  adapter:
    args: [{'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}]
    balance: True
    boundaries: []
    cluster_runtime: {'schedule_file': 'exp/tldr/choice_qwen/gfl/fedbis_oracle_u3_1.0/cluster_schedule/cluster_schedule_u3.json'}
    clusters: [[1, 6, 10, 13, 15, 16, 19, 20, 22, 26, 28, 29, 34, 39, 40, 43, 45, 46, 47, 48, 51], [2, 4, 7, 8, 9, 11, 12, 14, 17, 18, 23, 24, 25, 27, 30, 33, 35, 38, 42, 44, 49, 52, 53], [3, 5, 21, 31, 32, 36, 37, 41, 50]]
    clusters_file: fedbiscuit_script/tldr/clusters_u3_53.json
    count: 3
    grouping:
      round: 50
      use: False
    local_only: False
    mv_to_cpu: False
    per_client_target: 18.867924528301888
    round_budget: 200
    round_ends: [80, 167, 201]
    round_physical: []
    sample_num_per_adapter: [5, 5, 5]
    target_per_round: 5
    use: True
    warmup:
      round: 0
      use: True
  cache:
    model: 
  chat:
    max_history_len: 10
    max_len: 1024
  deepspeed:
    ds_config: 
    use: False
  fedrlhf:
    config_file: 
    frequency: 100
    pretrained: False
    train:
      batch_or_epoch: batch
      local_update_steps: 10
    use: False
  grad_accum_step: 2
  max_new_token: 60
  num_completions: 2
  offsite_tuning:
    emu_align:
      data:
        root: data
        splits: [0.8, 0.1, 0.1]
        type: alpaca@llm
      exit_after_align: False
      init_enable_ground_truth: False
      initial_only: True
      kl_divergence: raw
      layerwise_distill: False
      restore_from: 
      save_to: 
      sim_loss: l2
      train:
        batch_or_epoch: batch
        enable_ground_truth: False
        initial_update_rounds: 50
        kd_loss_weight: 0.9
        lm_loss_weight: 0.1
        local_update_steps: 10
        optimizer:
          lr: 0.01
          type: SGD
      use: False
    emu_l: 1
    emu_r: 10
    eval_type: emu
    kwargs: [{}]
    llm_generated:
      ratio: 0.1
      use: False
    save_full_model: False
    strategy: drop_layer
    use: False
  retry_on_nan_loss: False
  reward_coeff: 0.1
  rlhf: False
  tok_len: 1024
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.5
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 256
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  llm_kwargs: [{}]
  llm_type: CausalLM
  load_from_local_pretrained_fs_config: 
  load_from_local_pretrained_model_path: 
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 1
  pretrain_tasks: []
  stage: 
  task: node
  type: Qwen/Qwen2-0.5B@huggingface_llm
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/tldr/choice_qwen/gfl/fedbis_oracle_u3_1.0
personalization:
  K: 5
  beta: 1.0
  epoch_feature: 1
  epoch_linear: 2
  local_param: []
  local_update_steps: 30
  lr: 1e-05
  lr_feature: 0.1
  lr_linear: 0.1
  regular_weight: 0.1
  share_non_trainable_para: False
  weight_decay: 0.0
print_decimal_digits: 6
quantization:
  method: none
  nbits: 8
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  data_para_dids: []
  is_enable_half: True
  local_update_steps: 30
  optimizer:
    betas: (0.9, 0.95)
    lr: 1e-05
    type: AdamW
  scheduler:
    gamma: 1.0
    milestones: [75, 125]
    type: 
    warmup_ratio: 0.0
trainer:
  choices: ['A', 'B']
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.0001
    gamma: 0.03
    inc_factor: 1.0
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: llmrewardchoicetrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2025-10-09 08:49:25 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-10-09 08:49:25 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-10-09 08:49:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140137376604224 | out_emb=(Linear) num=151646 ptr=140137376604224 | lora_ptr=None
2025-10-09 08:49:39 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-10-09 08:49:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:49:42 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-10-09 08:49:42 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:49:45 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-10-09 08:49:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:49:47 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-10-09 08:49:47 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:49:50 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-10-09 08:49:50 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:49:52 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-10-09 08:49:52 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:49:54 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-10-09 08:49:55 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:49:57 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-10-09 08:49:57 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:00 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-10-09 08:50:00 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:02 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-10-09 08:50:02 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:05 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-10-09 08:50:05 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:07 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-10-09 08:50:08 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:10 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-10-09 08:50:10 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:13 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-10-09 08:50:13 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:15 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-10-09 08:50:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:18 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-10-09 08:50:18 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:21 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-10-09 08:50:21 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:23 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-10-09 08:50:23 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:26 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-10-09 08:50:26 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:30 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-10-09 08:50:30 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:32 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-10-09 08:50:32 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:35 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-10-09 08:50:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:37 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-10-09 08:50:37 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:40 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-10-09 08:50:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:42 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-10-09 08:50:42 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:45 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-10-09 08:50:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:47 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-10-09 08:50:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:50 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-10-09 08:50:50 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:52 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-10-09 08:50:53 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:55 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-10-09 08:50:55 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:50:58 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-10-09 08:50:58 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:00 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-10-09 08:51:00 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:03 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-10-09 08:51:03 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:05 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-10-09 08:51:06 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:08 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-10-09 08:51:08 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:10 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-10-09 08:51:11 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:13 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-10-09 08:51:13 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:15 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-10-09 08:51:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:18 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-10-09 08:51:18 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:22 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-10-09 08:51:22 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:24 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-10-09 08:51:25 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:27 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-10-09 08:51:27 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:29 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-10-09 08:51:30 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:32 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-10-09 08:51:32 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:34 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-10-09 08:51:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:37 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-10-09 08:51:37 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:40 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-10-09 08:51:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:42 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-10-09 08:51:42 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:45 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-10-09 08:51:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:47 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-10-09 08:51:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:50 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-10-09 08:51:50 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:52 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-10-09 08:51:53 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:55 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-10-09 08:51:55 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-09 08:51:58 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-10-09 08:51:58 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-10-09 08:51:58 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 1344.
2025-10-09 08:51:58 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 1634.
2025-10-09 08:51:58 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 1344. 
Preserved para names in local update: {'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight'}.
2025-10-09 08:51:58 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-10-09 08:51:58 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-10-09 08:51:58 (federatedscope.llm.llm_local.server:147) INFO: Waited all clients join, start now...
2025-10-09 08:51:58 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=0 aidx=0 | s=5 (candidates=21)
2025-10-09 08:51:58 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[22, 39, 51, 6, 29] (from 21)
2025-10-09 08:51:58 (federatedscope.llm.llm_local.server:161) INFO: ----------- Starting training (Round #0) -------------
2025-10-09 08:52:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:52:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:52:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-09 08:52:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 08:52:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140137376604224 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:52:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:52:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:52:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:52:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:52:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:52:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=339.052704, avg_loss=0.706360, seen=480, correct=260, accuracy=0.541667
2025-10-09 08:52:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:52:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:52:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:52:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=1793MB
2025-10-09 08:52:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.15169250965118, 'train_avg_loss': 0.6845974375804266, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 08:52:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 339.0527038574219, 'train_avg_loss': 0.7063597997029623, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 08:52:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 339.0527038574219, 'train_avg_loss': 0.7063597997029623, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 08:52:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:52:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:52:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-09 08:52:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 08:52:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:52:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:52:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:52:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:52:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:53:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:53:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=350.085815, avg_loss=0.729345, seen=480, correct=235, accuracy=0.489583
2025-10-09 08:53:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:53:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:53:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:53:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2094MB allocated=1802MB
2025-10-09 08:53:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.85536938905716, 'train_avg_loss': 0.7237947449088097, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 08:53:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 350.0858154296875, 'train_avg_loss': 0.7293454488118489, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 08:53:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 350.0858154296875, 'train_avg_loss': 0.7293454488118489, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 08:53:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:53:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:53:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-09 08:53:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 08:53:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:53:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:53:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:53:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:53:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:54:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:54:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.669220, avg_loss=0.703478, seen=480, correct=261, accuracy=0.543750
2025-10-09 08:54:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:54:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:54:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:54:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2154MB allocated=1810MB
2025-10-09 08:54:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.50469720363617, 'train_avg_loss': 0.7125391433636348, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 08:54:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.6692199707031, 'train_avg_loss': 0.7034775416056315, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 08:54:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 337.6692199707031, 'train_avg_loss': 0.7034775416056315, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 08:54:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:54:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:54:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-09 08:54:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 08:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:54:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:54:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:54:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:54:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:54:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:54:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=341.691101, avg_loss=0.711856, seen=480, correct=240, accuracy=0.500000
2025-10-09 08:54:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:54:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:54:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:54:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2104MB allocated=1819MB
2025-10-09 08:54:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.36175906658173, 'train_avg_loss': 0.7113479922215143, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 08:54:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 341.69110107421875, 'train_avg_loss': 0.711856460571289, 'train_seen': 480, 'train_correct': 240, 'train_acc': 0.5}}
2025-10-09 08:54:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 341.69110107421875, 'train_avg_loss': 0.711856460571289, 'train_seen': 480, 'train_correct': 240, 'train_acc': 0.5}}
2025-10-09 08:54:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:54:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:54:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-09 08:54:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 08:54:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:54:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:54:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:54:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:54:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:55:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:55:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=344.678284, avg_loss=0.718080, seen=480, correct=237, accuracy=0.493750
2025-10-09 08:55:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:55:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:55:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2138MB allocated=1827MB
2025-10-09 08:55:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.51551938056946, 'train_avg_loss': 0.7126293281714121, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 08:55:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 344.67828369140625, 'train_avg_loss': 0.7180797576904296, 'train_seen': 480, 'train_correct': 237, 'train_acc': 0.49375}}
2025-10-09 08:55:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 344.67828369140625, 'train_avg_loss': 0.7180797576904296, 'train_seen': 480, 'train_correct': 237, 'train_acc': 0.49375}}
2025-10-09 08:55:27 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #1) -------------
2025-10-09 08:55:28 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=1 aidx=0 | s=5 (candidates=21)
2025-10-09 08:55:28 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[19, 51, 45, 28, 20] (from 21)
2025-10-09 08:55:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:55:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:55:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-05
2025-10-09 08:55:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 08:55:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:55:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:55:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:55:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:55:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:56:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:56:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.965454, avg_loss=0.699928, seen=480, correct=242, accuracy=0.504167
2025-10-09 08:56:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:56:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:56:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:56:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=1 reserved=2212MB allocated=1869MB
2025-10-09 08:56:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.28731840848923, 'train_avg_loss': 0.6940609867374102, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 08:56:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.9654541015625, 'train_avg_loss': 0.6999280293782552, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 08:56:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 335.9654541015625, 'train_avg_loss': 0.6999280293782552, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 08:56:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:56:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:56:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-05
2025-10-09 08:56:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 08:56:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:56:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:56:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:56:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:56:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:56:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:56:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.745483, avg_loss=0.693220, seen=480, correct=265, accuracy=0.552083
2025-10-09 08:56:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:56:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:56:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:56:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=1 reserved=2104MB allocated=1844MB
2025-10-09 08:56:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.89741307497025, 'train_avg_loss': 0.6991451089580853, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 08:56:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.7454833984375, 'train_avg_loss': 0.6932197570800781, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 08:56:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 332.7454833984375, 'train_avg_loss': 0.6932197570800781, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 08:56:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:56:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:56:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-05
2025-10-09 08:56:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 08:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:56:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:56:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:56:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:56:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:57:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:57:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.541565, avg_loss=0.692795, seen=480, correct=258, accuracy=0.537500
2025-10-09 08:57:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:57:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:57:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:57:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=1 reserved=2142MB allocated=1878MB
2025-10-09 08:57:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.41479009389877, 'train_avg_loss': 0.7034565841158231, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 08:57:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.54156494140625, 'train_avg_loss': 0.692794926961263, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 08:57:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 332.54156494140625, 'train_avg_loss': 0.692794926961263, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 08:57:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:57:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:57:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-05
2025-10-09 08:57:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 08:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:57:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:57:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:57:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:57:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:58:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:58:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.236725, avg_loss=0.700493, seen=480, correct=249, accuracy=0.518750
2025-10-09 08:58:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:58:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:58:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=1 reserved=2128MB allocated=1886MB
2025-10-09 08:58:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.02260887622833, 'train_avg_loss': 0.7001884073019028, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 08:58:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.2367248535156, 'train_avg_loss': 0.7004931767781576, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 08:58:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 336.2367248535156, 'train_avg_loss': 0.7004931767781576, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 08:58:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:58:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:58:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-05
2025-10-09 08:58:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 08:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:58:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:58:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:58:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:58:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:58:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:58:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.715576, avg_loss=0.691074, seen=480, correct=263, accuracy=0.547917
2025-10-09 08:58:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:58:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:58:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:58:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=1 reserved=2176MB allocated=1895MB
2025-10-09 08:58:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 1, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.35414028167725, 'train_avg_loss': 0.702951169013977, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 08:58:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 1, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.715576171875, 'train_avg_loss': 0.6910741170247395, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 08:58:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 1, 'Results_raw': {'train_total': 480, 'train_loss': 331.715576171875, 'train_avg_loss': 0.6910741170247395, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 08:59:00 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #2) -------------
2025-10-09 08:59:00 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=2 aidx=0 | s=5 (candidates=21)
2025-10-09 08:59:00 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[39, 47, 48, 19, 45] (from 21)
2025-10-09 08:59:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:59:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:59:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-05
2025-10-09 08:59:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 08:59:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:59:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:59:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:59:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:59:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 08:59:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 08:59:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=340.171021, avg_loss=0.708690, seen=480, correct=260, accuracy=0.541667
2025-10-09 08:59:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 08:59:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:59:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 08:59:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=2 reserved=2104MB allocated=1869MB
2025-10-09 08:59:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.09638118743896, 'train_avg_loss': 0.7091365098953247, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 08:59:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.1710205078125, 'train_avg_loss': 0.7086896260579427, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 08:59:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 340.1710205078125, 'train_avg_loss': 0.7086896260579427, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 08:59:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 08:59:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 08:59:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-05
2025-10-09 08:59:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 08:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 08:59:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 08:59:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 08:59:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 08:59:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:00:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:00:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.787170, avg_loss=0.687057, seen=480, correct=270, accuracy=0.562500
2025-10-09 09:00:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:00:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:00:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:00:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=2 reserved=2130MB allocated=1903MB
2025-10-09 09:00:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.43083518743515, 'train_avg_loss': 0.7119236265619596, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 09:00:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.78717041015625, 'train_avg_loss': 0.6870566050211588, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 09:00:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 329.78717041015625, 'train_avg_loss': 0.6870566050211588, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 09:00:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:00:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:00:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-05
2025-10-09 09:00:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 09:00:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:00:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:00:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:00:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:00:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:01:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:01:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=339.055756, avg_loss=0.706366, seen=480, correct=243, accuracy=0.506250
2025-10-09 09:01:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:01:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:01:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=2 reserved=2146MB allocated=1912MB
2025-10-09 09:01:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.55842757225037, 'train_avg_loss': 0.7213202297687531, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 09:01:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 339.0557556152344, 'train_avg_loss': 0.7063661575317383, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 09:01:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 339.0557556152344, 'train_avg_loss': 0.7063661575317383, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 09:01:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:01:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:01:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-05
2025-10-09 09:01:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 09:01:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:01:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:01:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:01:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:01:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:01:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:01:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.582214, avg_loss=0.697046, seen=480, correct=242, accuracy=0.504167
2025-10-09 09:01:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:01:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:01:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:01:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=2 reserved=2104MB allocated=1887MB
2025-10-09 09:01:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.93014639616013, 'train_avg_loss': 0.6910845533013343, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 09:01:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.58221435546875, 'train_avg_loss': 0.6970462799072266, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 09:01:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 334.58221435546875, 'train_avg_loss': 0.6970462799072266, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 09:01:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:01:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:01:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #2, planning to set LR to 1.00e-05
2025-10-09 09:01:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 09:01:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:01:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:01:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:01:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:01:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:02:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:02:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.958038, avg_loss=0.691579, seen=480, correct=259, accuracy=0.539583
2025-10-09 09:02:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:02:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:02:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:02:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=2 reserved=2104MB allocated=1887MB
2025-10-09 09:02:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 2, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.00915598869324, 'train_avg_loss': 0.700076299905777, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 09:02:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 2, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.9580383300781, 'train_avg_loss': 0.6915792465209961, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:02:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 2, 'Results_raw': {'train_total': 480, 'train_loss': 331.9580383300781, 'train_avg_loss': 0.6915792465209961, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:02:30 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #3) -------------
2025-10-09 09:02:31 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=3 aidx=0 | s=5 (candidates=21)
2025-10-09 09:02:31 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[47, 22, 39, 45, 51] (from 21)
2025-10-09 09:02:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:02:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:02:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-05
2025-10-09 09:02:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 09:02:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:02:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:02:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:02:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:02:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:03:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:03:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.304321, avg_loss=0.688134, seen=480, correct=260, accuracy=0.541667
2025-10-09 09:03:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:03:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:03:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=3 reserved=2080MB allocated=1861MB
2025-10-09 09:03:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.88065671920776, 'train_avg_loss': 0.7156721393267313, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 09:03:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.3043212890625, 'train_avg_loss': 0.6881340026855469, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:03:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 330.3043212890625, 'train_avg_loss': 0.6881340026855469, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:03:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:03:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:03:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-05
2025-10-09 09:03:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 09:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:03:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:03:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:03:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:03:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:03:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:03:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.757843, avg_loss=0.695329, seen=480, correct=259, accuracy=0.539583
2025-10-09 09:03:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:03:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:03:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:03:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=3 reserved=2088MB allocated=1861MB
2025-10-09 09:03:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.33074033260345, 'train_avg_loss': 0.6860895027716954, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 09:03:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.7578430175781, 'train_avg_loss': 0.6953288396199544, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:03:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 333.7578430175781, 'train_avg_loss': 0.6953288396199544, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:03:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:03:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:03:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-05
2025-10-09 09:03:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 09:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:03:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:03:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:03:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:03:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:04:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:04:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=338.959412, avg_loss=0.706165, seen=480, correct=255, accuracy=0.531250
2025-10-09 09:04:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:04:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:04:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:04:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=3 reserved=2078MB allocated=1861MB
2025-10-09 09:04:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.32975459098816, 'train_avg_loss': 0.7027479549249013, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 09:04:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.95941162109375, 'train_avg_loss': 0.7061654408772786, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 09:04:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 338.95941162109375, 'train_avg_loss': 0.7061654408772786, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 09:04:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:04:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:04:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-05
2025-10-09 09:04:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 09:04:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:04:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:04:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:04:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:04:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:05:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:05:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.344574, avg_loss=0.684051, seen=480, correct=277, accuracy=0.577083
2025-10-09 09:05:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:05:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:05:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:05:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=3 reserved=2078MB allocated=1861MB
2025-10-09 09:05:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.26178646087646, 'train_avg_loss': 0.6938482205073039, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 09:05:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.3445739746094, 'train_avg_loss': 0.6840511957804362, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 09:05:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 328.3445739746094, 'train_avg_loss': 0.6840511957804362, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 09:05:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:05:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:05:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #3, planning to set LR to 1.00e-05
2025-10-09 09:05:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 09:05:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:05:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:05:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:05:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:05:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:05:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:05:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.641876, avg_loss=0.688837, seen=480, correct=259, accuracy=0.539583
2025-10-09 09:05:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:05:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:05:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=3 reserved=2094MB allocated=1861MB
2025-10-09 09:05:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 3, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.93363356590271, 'train_avg_loss': 0.6994469463825226, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 09:05:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 3, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.6418762207031, 'train_avg_loss': 0.6888372421264648, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:05:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 3, 'Results_raw': {'train_total': 480, 'train_loss': 330.6418762207031, 'train_avg_loss': 0.6888372421264648, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:05:59 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #4) -------------
2025-10-09 09:05:59 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=4 aidx=0 | s=5 (candidates=21)
2025-10-09 09:05:59 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[26, 47, 20, 6, 1] (from 21)
2025-10-09 09:05:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:06:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:06:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-05
2025-10-09 09:06:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 09:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:06:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:06:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:06:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:06:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:06:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:06:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=338.837738, avg_loss=0.705912, seen=480, correct=232, accuracy=0.483333
2025-10-09 09:06:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:06:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:06:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=4 reserved=2102MB allocated=1895MB
2025-10-09 09:06:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.7220653295517, 'train_avg_loss': 0.6893505444129308, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 09:06:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.8377380371094, 'train_avg_loss': 0.7059119542439779, 'train_seen': 480, 'train_correct': 232, 'train_acc': 0.48333333333333334}}
2025-10-09 09:06:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 338.8377380371094, 'train_avg_loss': 0.7059119542439779, 'train_seen': 480, 'train_correct': 232, 'train_acc': 0.48333333333333334}}
2025-10-09 09:06:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:06:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:06:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-05
2025-10-09 09:06:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 09:06:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:06:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:06:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:06:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:06:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:07:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:07:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.383789, avg_loss=0.686216, seen=480, correct=266, accuracy=0.554167
2025-10-09 09:07:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:07:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:07:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=4 reserved=2080MB allocated=1870MB
2025-10-09 09:07:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.92661553621292, 'train_avg_loss': 0.6993884628017744, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 09:07:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.3837890625, 'train_avg_loss': 0.6862162272135417, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:07:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 329.3837890625, 'train_avg_loss': 0.6862162272135417, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:07:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:07:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:07:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-05
2025-10-09 09:07:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 09:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:07:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:07:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:07:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:07:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:08:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:08:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.852783, avg_loss=0.680943, seen=480, correct=271, accuracy=0.564583
2025-10-09 09:08:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:08:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:08:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=4 reserved=2078MB allocated=1870MB
2025-10-09 09:08:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.08724689483643, 'train_avg_loss': 0.6923937241236369, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 09:08:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.852783203125, 'train_avg_loss': 0.6809432983398438, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 09:08:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 326.852783203125, 'train_avg_loss': 0.6809432983398438, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 09:08:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:08:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:08:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-05
2025-10-09 09:08:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 09:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:08:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:08:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:08:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:08:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:08:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:08:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.871613, avg_loss=0.701816, seen=480, correct=247, accuracy=0.514583
2025-10-09 09:08:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:08:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:08:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:08:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=4 reserved=2084MB allocated=1870MB
2025-10-09 09:08:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.38843476772308, 'train_avg_loss': 0.7115702897310257, 'train_seen': 120, 'train_correct': 56, 'train_acc': 0.4666666666666667}}
2025-10-09 09:08:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.8716125488281, 'train_avg_loss': 0.7018158594767253, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 09:08:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 336.8716125488281, 'train_avg_loss': 0.7018158594767253, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 09:08:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:08:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:08:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #4, planning to set LR to 1.00e-05
2025-10-09 09:08:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 09:08:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:08:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:08:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:08:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:08:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:09:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:09:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=340.867371, avg_loss=0.710140, seen=480, correct=240, accuracy=0.500000
2025-10-09 09:09:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:09:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:09:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:09:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=4 reserved=2104MB allocated=1904MB
2025-10-09 09:09:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 4, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.79022371768951, 'train_avg_loss': 0.7065851976474126, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 09:09:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 4, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.86737060546875, 'train_avg_loss': 0.7101403554280599, 'train_seen': 480, 'train_correct': 240, 'train_acc': 0.5}}
2025-10-09 09:09:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 4, 'Results_raw': {'train_total': 480, 'train_loss': 340.86737060546875, 'train_avg_loss': 0.7101403554280599, 'train_seen': 480, 'train_correct': 240, 'train_acc': 0.5}}
2025-10-09 09:09:29 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #5) -------------
2025-10-09 09:09:29 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=5 aidx=0 | s=5 (candidates=21)
2025-10-09 09:09:29 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[22, 19, 34, 45, 29] (from 21)
2025-10-09 09:09:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:09:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:09:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-05
2025-10-09 09:09:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 09:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:09:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:09:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:09:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:09:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:10:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:10:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.243530, avg_loss=0.690091, seen=480, correct=259, accuracy=0.539583
2025-10-09 09:10:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:10:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:10:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=5 reserved=2110MB allocated=1904MB
2025-10-09 09:10:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.18711805343628, 'train_avg_loss': 0.6848926504453023, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 09:10:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.2435302734375, 'train_avg_loss': 0.6900906880696615, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:10:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 331.2435302734375, 'train_avg_loss': 0.6900906880696615, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:10:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:10:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:10:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-05
2025-10-09 09:10:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 09:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:10:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:10:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:10:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:10:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:10:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:10:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.250244, avg_loss=0.692188, seen=480, correct=259, accuracy=0.539583
2025-10-09 09:10:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:10:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:10:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=5 reserved=2210MB allocated=1904MB
2025-10-09 09:10:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.53003215789795, 'train_avg_loss': 0.687750267982483, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 09:10:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.250244140625, 'train_avg_loss': 0.692188008626302, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:10:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 332.250244140625, 'train_avg_loss': 0.692188008626302, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:10:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:10:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:10:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-05
2025-10-09 09:10:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 09:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:10:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:10:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:10:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:10:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:11:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:11:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.564087, avg_loss=0.701175, seen=480, correct=236, accuracy=0.491667
2025-10-09 09:11:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:11:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:11:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:11:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=5 reserved=2134MB allocated=1937MB
2025-10-09 09:11:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.58929634094238, 'train_avg_loss': 0.6965774695078532, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 09:11:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.5640869140625, 'train_avg_loss': 0.7011751810709635, 'train_seen': 480, 'train_correct': 236, 'train_acc': 0.49166666666666664}}
2025-10-09 09:11:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 336.5640869140625, 'train_avg_loss': 0.7011751810709635, 'train_seen': 480, 'train_correct': 236, 'train_acc': 0.49166666666666664}}
2025-10-09 09:11:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:11:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:11:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-05
2025-10-09 09:11:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 09:11:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:11:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:11:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:11:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:11:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:12:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:12:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.377899, avg_loss=0.682037, seen=480, correct=266, accuracy=0.554167
2025-10-09 09:12:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:12:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:12:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=5 reserved=2104MB allocated=1912MB
2025-10-09 09:12:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.32125973701477, 'train_avg_loss': 0.6860104978084565, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 09:12:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.3778991699219, 'train_avg_loss': 0.6820372899373373, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:12:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 327.3778991699219, 'train_avg_loss': 0.6820372899373373, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:12:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:12:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:12:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #5, planning to set LR to 1.00e-05
2025-10-09 09:12:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 09:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:12:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:12:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:12:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:12:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:12:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:12:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.312805, avg_loss=0.692318, seen=480, correct=262, accuracy=0.545833
2025-10-09 09:12:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:12:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:12:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=5 reserved=2110MB allocated=1912MB
2025-10-09 09:12:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 5, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.46615558862686, 'train_avg_loss': 0.7038846299052238, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 09:12:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 5, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.31280517578125, 'train_avg_loss': 0.692318344116211, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 09:12:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 5, 'Results_raw': {'train_total': 480, 'train_loss': 332.31280517578125, 'train_avg_loss': 0.692318344116211, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 09:12:59 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #6) -------------
2025-10-09 09:13:00 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=6 aidx=0 | s=5 (candidates=21)
2025-10-09 09:13:00 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[22, 28, 20, 16, 1] (from 21)
2025-10-09 09:13:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:13:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:13:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-05
2025-10-09 09:13:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 09:13:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:13:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:13:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:13:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:13:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:13:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:13:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.234375, avg_loss=0.690072, seen=480, correct=259, accuracy=0.539583
2025-10-09 09:13:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:13:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:13:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=6 reserved=2088MB allocated=1887MB
2025-10-09 09:13:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.2863941192627, 'train_avg_loss': 0.6857199509938557, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 09:13:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.234375, 'train_avg_loss': 0.6900716145833333, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:13:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 331.234375, 'train_avg_loss': 0.6900716145833333, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:13:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:13:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:13:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-05
2025-10-09 09:13:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 09:13:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:13:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:13:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:13:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:13:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:14:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:14:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.337708, avg_loss=0.694454, seen=480, correct=244, accuracy=0.508333
2025-10-09 09:14:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:14:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:14:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=6 reserved=2078MB allocated=1887MB
2025-10-09 09:14:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.7485533952713, 'train_avg_loss': 0.6895712782939275, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 09:14:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.33770751953125, 'train_avg_loss': 0.6944535573323568, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 09:14:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 333.33770751953125, 'train_avg_loss': 0.6944535573323568, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 09:14:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:14:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:14:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-05
2025-10-09 09:14:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 09:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:14:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:14:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:14:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:14:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:15:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:15:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.617523, avg_loss=0.678370, seen=480, correct=272, accuracy=0.566667
2025-10-09 09:15:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:15:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:15:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=6 reserved=2078MB allocated=1887MB
2025-10-09 09:15:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.86926877498627, 'train_avg_loss': 0.6822439064582189, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 09:15:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.6175231933594, 'train_avg_loss': 0.6783698399861654, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 09:15:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 325.6175231933594, 'train_avg_loss': 0.6783698399861654, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 09:15:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:15:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:15:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-05
2025-10-09 09:15:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 09:15:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:15:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:15:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:15:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:15:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:15:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:15:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.818237, avg_loss=0.691288, seen=480, correct=260, accuracy=0.541667
2025-10-09 09:15:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:15:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:15:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=6 reserved=2170MB allocated=1920MB
2025-10-09 09:15:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.44959926605225, 'train_avg_loss': 0.670413327217102, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 09:15:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.8182373046875, 'train_avg_loss': 0.6912879943847656, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:15:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 331.8182373046875, 'train_avg_loss': 0.6912879943847656, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:15:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:15:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:15:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #6, planning to set LR to 1.00e-05
2025-10-09 09:15:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 09:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:15:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:15:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:15:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:15:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:16:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:16:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.529419, avg_loss=0.696936, seen=480, correct=251, accuracy=0.522917
2025-10-09 09:16:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:16:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:16:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:16:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=6 reserved=2080MB allocated=1895MB
2025-10-09 09:16:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 6, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.69737374782562, 'train_avg_loss': 0.6891447812318802, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 09:16:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 6, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.5294189453125, 'train_avg_loss': 0.696936289469401, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 09:16:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 6, 'Results_raw': {'train_total': 480, 'train_loss': 334.5294189453125, 'train_avg_loss': 0.696936289469401, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 09:16:31 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #7) -------------
2025-10-09 09:16:32 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=7 aidx=0 | s=5 (candidates=21)
2025-10-09 09:16:32 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[20, 6, 16, 15, 47] (from 21)
2025-10-09 09:16:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:16:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:16:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-05
2025-10-09 09:16:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 09:16:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:16:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:16:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:16:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:16:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:17:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:17:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.933441, avg_loss=0.674861, seen=480, correct=278, accuracy=0.579167
2025-10-09 09:17:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:17:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:17:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:17:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=7 reserved=2078MB allocated=1895MB
2025-10-09 09:17:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 7, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.29846692085266, 'train_avg_loss': 0.6858205576737721, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 09:17:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 7, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.9334411621094, 'train_avg_loss': 0.6748613357543946, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 09:17:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 7, 'Results_raw': {'train_total': 480, 'train_loss': 323.9334411621094, 'train_avg_loss': 0.6748613357543946, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 09:17:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:17:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:17:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-05
2025-10-09 09:17:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 09:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:17:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:17:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:17:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:17:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:17:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:17:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.779327, avg_loss=0.695374, seen=480, correct=248, accuracy=0.516667
2025-10-09 09:17:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:17:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:17:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:17:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=7 reserved=2084MB allocated=1895MB
2025-10-09 09:17:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 7, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.41482067108154, 'train_avg_loss': 0.7034568389256796, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 09:17:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 7, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.7793273925781, 'train_avg_loss': 0.6953735987345377, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 09:17:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 7, 'Results_raw': {'train_total': 480, 'train_loss': 333.7793273925781, 'train_avg_loss': 0.6953735987345377, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 09:17:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:17:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:17:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-05
2025-10-09 09:17:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 09:17:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:17:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:17:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:17:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:17:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:18:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:18:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.613098, avg_loss=0.692944, seen=480, correct=249, accuracy=0.518750
2025-10-09 09:18:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:18:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:18:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:18:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=7 reserved=2106MB allocated=1895MB
2025-10-09 09:18:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 7, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.20495218038559, 'train_avg_loss': 0.6767079348365466, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 09:18:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 7, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.61309814453125, 'train_avg_loss': 0.6929439544677735, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 09:18:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 7, 'Results_raw': {'train_total': 480, 'train_loss': 332.61309814453125, 'train_avg_loss': 0.6929439544677735, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 09:18:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:18:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:18:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-05
2025-10-09 09:18:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 09:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:18:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:18:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:18:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:18:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:19:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:19:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.964478, avg_loss=0.687426, seen=480, correct=253, accuracy=0.527083
2025-10-09 09:19:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:19:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:19:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:19:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=7 reserved=2104MB allocated=1929MB
2025-10-09 09:19:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 7, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.26775622367859, 'train_avg_loss': 0.7022313018639882, 'train_seen': 120, 'train_correct': 55, 'train_acc': 0.4583333333333333}}
2025-10-09 09:19:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 7, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.9644775390625, 'train_avg_loss': 0.6874259948730469, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 09:19:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 7, 'Results_raw': {'train_total': 480, 'train_loss': 329.9644775390625, 'train_avg_loss': 0.6874259948730469, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 09:19:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:19:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:19:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #7, planning to set LR to 1.00e-05
2025-10-09 09:19:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 09:19:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:19:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:19:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:19:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:19:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:20:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.320526, avg_loss=0.684001, seen=480, correct=259, accuracy=0.539583
2025-10-09 09:20:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:20:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:20:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=7 reserved=2082MB allocated=1904MB
2025-10-09 09:20:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 7, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.34216296672821, 'train_avg_loss': 0.694518024722735, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 09:20:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 7, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.3205261230469, 'train_avg_loss': 0.684001096089681, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:20:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 7, 'Results_raw': {'train_total': 480, 'train_loss': 328.3205261230469, 'train_avg_loss': 0.684001096089681, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 09:20:06 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #8) -------------
2025-10-09 09:20:06 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=8 aidx=0 | s=5 (candidates=21)
2025-10-09 09:20:06 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[48, 45, 39, 20, 6] (from 21)
2025-10-09 09:20:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:20:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:20:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #8, planning to set LR to 1.00e-05
2025-10-09 09:20:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 09:20:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:20:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:20:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:20:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:20:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:20:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.033295, avg_loss=0.695903, seen=480, correct=244, accuracy=0.508333
2025-10-09 09:20:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:20:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:20:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=8 reserved=2078MB allocated=1904MB
2025-10-09 09:20:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 8, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.01452767848969, 'train_avg_loss': 0.7084543973207473, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 09:20:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 8, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.0332946777344, 'train_avg_loss': 0.69590269724528, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 09:20:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 8, 'Results_raw': {'train_total': 480, 'train_loss': 334.0332946777344, 'train_avg_loss': 0.69590269724528, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 09:20:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:20:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:20:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #8, planning to set LR to 1.00e-05
2025-10-09 09:20:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 09:20:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:20:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:20:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:20:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:20:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:21:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:21:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.606506, avg_loss=0.682514, seen=480, correct=266, accuracy=0.554167
2025-10-09 09:21:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:21:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:21:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=8 reserved=2078MB allocated=1904MB
2025-10-09 09:21:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 8, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.1287043094635, 'train_avg_loss': 0.6844058692455292, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 09:21:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 8, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.60650634765625, 'train_avg_loss': 0.6825135548909506, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:21:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 8, 'Results_raw': {'train_total': 480, 'train_loss': 327.60650634765625, 'train_avg_loss': 0.6825135548909506, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:21:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:21:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:21:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #8, planning to set LR to 1.00e-05
2025-10-09 09:21:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 09:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:21:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:21:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:21:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:21:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:22:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:22:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.628357, avg_loss=0.695059, seen=480, correct=266, accuracy=0.554167
2025-10-09 09:22:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:22:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:22:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:22:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=8 reserved=2078MB allocated=1904MB
2025-10-09 09:22:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 8, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.7469391822815, 'train_avg_loss': 0.6895578265190124, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 09:22:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 8, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.62835693359375, 'train_avg_loss': 0.695059076944987, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:22:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 8, 'Results_raw': {'train_total': 480, 'train_loss': 333.62835693359375, 'train_avg_loss': 0.695059076944987, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:22:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:22:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:22:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #8, planning to set LR to 1.00e-05
2025-10-09 09:22:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 09:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:22:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:22:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:22:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:22:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:22:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:22:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.302612, avg_loss=0.671464, seen=480, correct=286, accuracy=0.595833
2025-10-09 09:22:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:22:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:22:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:22:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=8 reserved=2078MB allocated=1904MB
2025-10-09 09:22:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 8, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.15252459049225, 'train_avg_loss': 0.6846043715874354, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 09:22:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 8, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.3026123046875, 'train_avg_loss': 0.6714637756347657, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 09:22:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 8, 'Results_raw': {'train_total': 480, 'train_loss': 322.3026123046875, 'train_avg_loss': 0.6714637756347657, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 09:22:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:22:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:22:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #8, planning to set LR to 1.00e-05
2025-10-09 09:22:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 09:22:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:22:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:22:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:22:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:22:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:23:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:23:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.238861, avg_loss=0.692164, seen=480, correct=250, accuracy=0.520833
2025-10-09 09:23:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:23:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:23:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:23:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=8 reserved=2082MB allocated=1904MB
2025-10-09 09:23:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 8, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.69658422470093, 'train_avg_loss': 0.697471535205841, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 09:23:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 8, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.2388610839844, 'train_avg_loss': 0.6921642939249675, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 09:23:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 8, 'Results_raw': {'train_total': 480, 'train_loss': 332.2388610839844, 'train_avg_loss': 0.6921642939249675, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 09:23:37 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #9) -------------
2025-10-09 09:23:37 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=9 aidx=0 | s=5 (candidates=21)
2025-10-09 09:23:37 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[45, 34, 13, 40, 46] (from 21)
2025-10-09 09:23:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:23:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:23:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #9, planning to set LR to 1.00e-05
2025-10-09 09:23:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 09:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:23:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:23:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:23:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:23:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:24:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:24:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.644714, avg_loss=0.682593, seen=480, correct=266, accuracy=0.554167
2025-10-09 09:24:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:24:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:24:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:24:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=9 reserved=2078MB allocated=1904MB
2025-10-09 09:24:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 9, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.35382014513016, 'train_avg_loss': 0.6862818345427513, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 09:24:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 9, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.64471435546875, 'train_avg_loss': 0.6825931549072266, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:24:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 9, 'Results_raw': {'train_total': 480, 'train_loss': 327.64471435546875, 'train_avg_loss': 0.6825931549072266, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 09:24:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:24:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:24:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #9, planning to set LR to 1.00e-05
2025-10-09 09:24:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 09:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:24:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:24:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:24:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:24:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:25:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.911743, avg_loss=0.699816, seen=480, correct=243, accuracy=0.506250
2025-10-09 09:25:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:25:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:25:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=9 reserved=2078MB allocated=1904MB
2025-10-09 09:25:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 9, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.47572433948517, 'train_avg_loss': 0.6956310361623764, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 09:25:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 9, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.9117431640625, 'train_avg_loss': 0.6998161315917969, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 09:25:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 9, 'Results_raw': {'train_total': 480, 'train_loss': 335.9117431640625, 'train_avg_loss': 0.6998161315917969, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 09:25:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:25:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:25:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #9, planning to set LR to 1.00e-05
2025-10-09 09:25:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 09:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:25:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:25:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:25:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:25:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:25:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:25:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.900085, avg_loss=0.685209, seen=480, correct=256, accuracy=0.533333
2025-10-09 09:25:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:25:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:25:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:25:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=9 reserved=2104MB allocated=1937MB
2025-10-09 09:25:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 9, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.06490409374237, 'train_avg_loss': 0.6922075341145197, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 09:25:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 9, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.90008544921875, 'train_avg_loss': 0.685208511352539, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 09:25:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 9, 'Results_raw': {'train_total': 480, 'train_loss': 328.90008544921875, 'train_avg_loss': 0.685208511352539, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 09:25:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:25:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:25:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #9, planning to set LR to 1.00e-05
2025-10-09 09:25:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 09:25:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:25:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:25:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:25:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:25:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:26:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:26:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.694702, avg_loss=0.697281, seen=480, correct=234, accuracy=0.487500
2025-10-09 09:26:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:26:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:26:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:26:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=9 reserved=2104MB allocated=1946MB
2025-10-09 09:26:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 9, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.08890044689178, 'train_avg_loss': 0.7174075037240982, 'train_seen': 120, 'train_correct': 56, 'train_acc': 0.4666666666666667}}
2025-10-09 09:26:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 9, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.6947021484375, 'train_avg_loss': 0.6972806294759114, 'train_seen': 480, 'train_correct': 234, 'train_acc': 0.4875}}
2025-10-09 09:26:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 9, 'Results_raw': {'train_total': 480, 'train_loss': 334.6947021484375, 'train_avg_loss': 0.6972806294759114, 'train_seen': 480, 'train_correct': 234, 'train_acc': 0.4875}}
2025-10-09 09:26:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:26:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:26:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #9, planning to set LR to 1.00e-05
2025-10-09 09:26:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 09:26:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:26:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:26:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:26:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:26:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:27:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:27:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.898987, avg_loss=0.695623, seen=480, correct=246, accuracy=0.512500
2025-10-09 09:27:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:27:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:27:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=9 reserved=2128MB allocated=1954MB
2025-10-09 09:27:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 9, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.96681928634644, 'train_avg_loss': 0.7163901607195536, 'train_seen': 120, 'train_correct': 55, 'train_acc': 0.4583333333333333}}
2025-10-09 09:27:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 9, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.89898681640625, 'train_avg_loss': 0.6956228892008464, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 09:27:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 9, 'Results_raw': {'train_total': 480, 'train_loss': 333.89898681640625, 'train_avg_loss': 0.6956228892008464, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 09:27:07 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #10) -------------
2025-10-09 09:27:07 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=10 aidx=0 | s=5 (candidates=21)
2025-10-09 09:27:07 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[29, 26, 51, 46, 10] (from 21)
2025-10-09 09:27:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:27:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:27:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #10, planning to set LR to 1.00e-05
2025-10-09 09:27:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 09:27:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:27:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:27:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:27:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:27:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:27:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:27:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.031677, avg_loss=0.683399, seen=480, correct=260, accuracy=0.541667
2025-10-09 09:27:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:27:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:27:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:27:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=10 reserved=2108MB allocated=1954MB
2025-10-09 09:27:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 10, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.16869765520096, 'train_avg_loss': 0.6847391471266746, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 09:27:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 10, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.03167724609375, 'train_avg_loss': 0.6833993275960286, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:27:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 10, 'Results_raw': {'train_total': 480, 'train_loss': 328.03167724609375, 'train_avg_loss': 0.6833993275960286, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:27:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:27:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:27:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #10, planning to set LR to 1.00e-05
2025-10-09 09:27:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 09:27:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:27:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:27:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:27:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:27:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:28:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:28:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.577362, avg_loss=0.692870, seen=480, correct=253, accuracy=0.527083
2025-10-09 09:28:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:28:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:28:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:28:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=10 reserved=2104MB allocated=1954MB
2025-10-09 09:28:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 10, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.23038899898529, 'train_avg_loss': 0.6769199083248775, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 09:28:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 10, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.5773620605469, 'train_avg_loss': 0.692869504292806, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 09:28:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 10, 'Results_raw': {'train_total': 480, 'train_loss': 332.5773620605469, 'train_avg_loss': 0.692869504292806, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 09:28:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:28:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:28:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #10, planning to set LR to 1.00e-05
2025-10-09 09:28:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 09:28:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:28:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:28:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:28:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:28:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:29:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:29:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.367493, avg_loss=0.675766, seen=480, correct=273, accuracy=0.568750
2025-10-09 09:29:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:29:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:29:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=10 reserved=2114MB allocated=1954MB
2025-10-09 09:29:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 10, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.849165558815, 'train_avg_loss': 0.690409712990125, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 09:29:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 10, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.36749267578125, 'train_avg_loss': 0.6757656097412109, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 09:29:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 10, 'Results_raw': {'train_total': 480, 'train_loss': 324.36749267578125, 'train_avg_loss': 0.6757656097412109, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 09:29:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:29:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:29:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #10, planning to set LR to 1.00e-05
2025-10-09 09:29:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 09:29:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:29:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:29:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:29:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:29:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:29:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:29:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.417053, avg_loss=0.690452, seen=480, correct=271, accuracy=0.564583
2025-10-09 09:29:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:29:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:29:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:29:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=10 reserved=2128MB allocated=1954MB
2025-10-09 09:29:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 10, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.95306599140167, 'train_avg_loss': 0.6996088832616806, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 09:29:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 10, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.41705322265625, 'train_avg_loss': 0.6904521942138672, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 09:29:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 10, 'Results_raw': {'train_total': 480, 'train_loss': 331.41705322265625, 'train_avg_loss': 0.6904521942138672, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 09:29:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:29:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:29:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #10, planning to set LR to 1.00e-05
2025-10-09 09:29:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 09:29:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:29:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:29:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:29:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:29:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:30:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:30:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.660889, avg_loss=0.678460, seen=480, correct=268, accuracy=0.558333
2025-10-09 09:30:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:30:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:30:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:30:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=10 reserved=2156MB allocated=1988MB
2025-10-09 09:30:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 10, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.96343421936035, 'train_avg_loss': 0.6746952851613363, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 09:30:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 10, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.660888671875, 'train_avg_loss': 0.6784601847330729, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 09:30:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 10, 'Results_raw': {'train_total': 480, 'train_loss': 325.660888671875, 'train_avg_loss': 0.6784601847330729, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 09:30:39 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #11) -------------
2025-10-09 09:30:39 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=11 aidx=0 | s=5 (candidates=21)
2025-10-09 09:30:39 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[1, 28, 16, 19, 48] (from 21)
2025-10-09 09:30:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:30:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:30:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #11, planning to set LR to 1.00e-05
2025-10-09 09:30:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 09:30:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:30:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:30:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:30:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:30:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:31:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:31:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.972076, avg_loss=0.691608, seen=480, correct=244, accuracy=0.508333
2025-10-09 09:31:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:31:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:31:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:31:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=11 reserved=2104MB allocated=1963MB
2025-10-09 09:31:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 11, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.43961608409882, 'train_avg_loss': 0.6869968007008235, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 09:31:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 11, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.9720764160156, 'train_avg_loss': 0.6916084925333659, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 09:31:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 11, 'Results_raw': {'train_total': 480, 'train_loss': 331.9720764160156, 'train_avg_loss': 0.6916084925333659, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 09:31:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:31:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:31:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #11, planning to set LR to 1.00e-05
2025-10-09 09:31:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 09:31:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:31:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:31:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:31:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:31:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:32:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:32:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.258179, avg_loss=0.688038, seen=480, correct=233, accuracy=0.485417
2025-10-09 09:32:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:32:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:32:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:32:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=11 reserved=2104MB allocated=1963MB
2025-10-09 09:32:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 11, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.48449689149857, 'train_avg_loss': 0.6873708074291547, 'train_seen': 120, 'train_correct': 55, 'train_acc': 0.4583333333333333}}
2025-10-09 09:32:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 11, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.2581787109375, 'train_avg_loss': 0.6880378723144531, 'train_seen': 480, 'train_correct': 233, 'train_acc': 0.48541666666666666}}
2025-10-09 09:32:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 11, 'Results_raw': {'train_total': 480, 'train_loss': 330.2581787109375, 'train_avg_loss': 0.6880378723144531, 'train_seen': 480, 'train_correct': 233, 'train_acc': 0.48541666666666666}}
2025-10-09 09:32:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:32:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:32:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #11, planning to set LR to 1.00e-05
2025-10-09 09:32:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 09:32:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:32:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:32:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:32:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:32:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:32:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:32:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.166626, avg_loss=0.685764, seen=480, correct=271, accuracy=0.564583
2025-10-09 09:32:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:32:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:32:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:32:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=11 reserved=2130MB allocated=1963MB
2025-10-09 09:32:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 11, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.25137686729431, 'train_avg_loss': 0.6687614738941192, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 09:32:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 11, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.1666259765625, 'train_avg_loss': 0.6857638041178385, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 09:32:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 11, 'Results_raw': {'train_total': 480, 'train_loss': 329.1666259765625, 'train_avg_loss': 0.6857638041178385, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 09:32:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:32:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:32:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #11, planning to set LR to 1.00e-05
2025-10-09 09:32:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 09:32:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:32:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:32:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:32:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:32:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:33:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:33:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.582764, avg_loss=0.682464, seen=480, correct=268, accuracy=0.558333
2025-10-09 09:33:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:33:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:33:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:33:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=11 reserved=2214MB allocated=1963MB
2025-10-09 09:33:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 11, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.9067211151123, 'train_avg_loss': 0.6658893426259359, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 09:33:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 11, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.582763671875, 'train_avg_loss': 0.6824640909830729, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 09:33:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 11, 'Results_raw': {'train_total': 480, 'train_loss': 327.582763671875, 'train_avg_loss': 0.6824640909830729, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 09:33:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:33:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:33:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #11, planning to set LR to 1.00e-05
2025-10-09 09:33:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 09:33:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:33:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:33:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:33:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:33:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:34:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:34:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.093567, avg_loss=0.691862, seen=480, correct=261, accuracy=0.543750
2025-10-09 09:34:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:34:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:34:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:34:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=11 reserved=2104MB allocated=1963MB
2025-10-09 09:34:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 11, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.48110568523407, 'train_avg_loss': 0.7123425473769506, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 09:34:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 11, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.09356689453125, 'train_avg_loss': 0.6918615976969401, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 09:34:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 11, 'Results_raw': {'train_total': 480, 'train_loss': 332.09356689453125, 'train_avg_loss': 0.6918615976969401, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 09:34:12 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #12) -------------
2025-10-09 09:34:12 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=12 aidx=0 | s=5 (candidates=21)
2025-10-09 09:34:12 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[29, 26, 13, 40, 51] (from 21)
2025-10-09 09:34:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:34:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:34:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #12, planning to set LR to 1.00e-05
2025-10-09 09:34:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 09:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:34:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:34:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:34:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:34:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:34:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:34:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.123260, avg_loss=0.677340, seen=480, correct=275, accuracy=0.572917
2025-10-09 09:34:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:34:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:34:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:34:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=12 reserved=2084MB allocated=1938MB
2025-10-09 09:34:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 12, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.87639689445496, 'train_avg_loss': 0.6823033074537913, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 09:34:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 12, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.1232604980469, 'train_avg_loss': 0.6773401260375976, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 09:34:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 12, 'Results_raw': {'train_total': 480, 'train_loss': 325.1232604980469, 'train_avg_loss': 0.6773401260375976, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 09:34:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:34:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:34:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #12, planning to set LR to 1.00e-05
2025-10-09 09:34:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 09:34:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:34:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:34:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:34:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:34:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:35:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:35:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.043243, avg_loss=0.689673, seen=480, correct=258, accuracy=0.537500
2025-10-09 09:35:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:35:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:35:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:35:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=12 reserved=2078MB allocated=1938MB
2025-10-09 09:35:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 12, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.86498641967773, 'train_avg_loss': 0.6738748868306478, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 09:35:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 12, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.0432434082031, 'train_avg_loss': 0.6896734237670898, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 09:35:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 12, 'Results_raw': {'train_total': 480, 'train_loss': 331.0432434082031, 'train_avg_loss': 0.6896734237670898, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 09:35:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:35:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:35:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #12, planning to set LR to 1.00e-05
2025-10-09 09:35:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 09:35:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:35:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:35:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:35:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:35:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:36:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:36:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.473541, avg_loss=0.673903, seen=480, correct=283, accuracy=0.589583
2025-10-09 09:36:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:36:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:36:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:36:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=12 reserved=2078MB allocated=1938MB
2025-10-09 09:36:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 12, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.51562631130219, 'train_avg_loss': 0.6792968859275182, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 09:36:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 12, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.4735412597656, 'train_avg_loss': 0.673903210957845, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 09:36:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 12, 'Results_raw': {'train_total': 480, 'train_loss': 323.4735412597656, 'train_avg_loss': 0.673903210957845, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 09:36:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:36:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:36:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #12, planning to set LR to 1.00e-05
2025-10-09 09:36:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 09:36:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:36:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:36:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:36:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:36:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:36:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:36:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.988281, avg_loss=0.695809, seen=480, correct=246, accuracy=0.512500
2025-10-09 09:36:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:37:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:37:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=12 reserved=2078MB allocated=1938MB
2025-10-09 09:37:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 12, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.58065605163574, 'train_avg_loss': 0.7131721337636312, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 09:37:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 12, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.98828125, 'train_avg_loss': 0.6958089192708333, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 09:37:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 12, 'Results_raw': {'train_total': 480, 'train_loss': 333.98828125, 'train_avg_loss': 0.6958089192708333, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 09:37:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:37:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:37:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #12, planning to set LR to 1.00e-05
2025-10-09 09:37:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 09:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:37:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:37:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:37:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:37:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:37:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:37:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.223572, avg_loss=0.673382, seen=480, correct=284, accuracy=0.591667
2025-10-09 09:37:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:37:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:37:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:37:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=12 reserved=2090MB allocated=1938MB
2025-10-09 09:37:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 12, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.8624757528305, 'train_avg_loss': 0.6905206312735875, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 09:37:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 12, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.22357177734375, 'train_avg_loss': 0.6733824412027994, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 09:37:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 12, 'Results_raw': {'train_total': 480, 'train_loss': 323.22357177734375, 'train_avg_loss': 0.6733824412027994, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 09:37:44 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #13) -------------
2025-10-09 09:37:45 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=13 aidx=0 | s=5 (candidates=21)
2025-10-09 09:37:45 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[1, 29, 39, 19, 46] (from 21)
2025-10-09 09:37:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:37:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:37:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #13, planning to set LR to 1.00e-05
2025-10-09 09:37:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 09:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:37:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:37:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:37:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:37:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:38:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:38:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.870941, avg_loss=0.691398, seen=480, correct=253, accuracy=0.527083
2025-10-09 09:38:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:38:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:38:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:38:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=13 reserved=2078MB allocated=1938MB
2025-10-09 09:38:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 13, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.20006704330444, 'train_avg_loss': 0.6850005586942037, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 09:38:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 13, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.8709411621094, 'train_avg_loss': 0.6913977940877278, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 09:38:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 13, 'Results_raw': {'train_total': 480, 'train_loss': 331.8709411621094, 'train_avg_loss': 0.6913977940877278, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 09:38:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:38:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:38:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #13, planning to set LR to 1.00e-05
2025-10-09 09:38:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 09:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:38:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:38:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:38:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:38:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:39:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:39:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.757874, avg_loss=0.676579, seen=480, correct=269, accuracy=0.560417
2025-10-09 09:39:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:39:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:39:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:39:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=13 reserved=2084MB allocated=1938MB
2025-10-09 09:39:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 13, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.52347975969315, 'train_avg_loss': 0.6876956646641096, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 09:39:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 13, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.75787353515625, 'train_avg_loss': 0.6765789031982422, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 09:39:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 13, 'Results_raw': {'train_total': 480, 'train_loss': 324.75787353515625, 'train_avg_loss': 0.6765789031982422, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 09:39:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:39:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:39:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #13, planning to set LR to 1.00e-05
2025-10-09 09:39:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 09:39:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:39:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:39:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:39:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:39:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:39:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:39:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.236298, avg_loss=0.690076, seen=480, correct=261, accuracy=0.543750
2025-10-09 09:39:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:39:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:39:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=13 reserved=2078MB allocated=1938MB
2025-10-09 09:39:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 13, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.90137529373169, 'train_avg_loss': 0.6825114607810974, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 09:39:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 13, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.2362976074219, 'train_avg_loss': 0.6900756200154622, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 09:39:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 13, 'Results_raw': {'train_total': 480, 'train_loss': 331.2362976074219, 'train_avg_loss': 0.6900756200154622, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 09:39:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:39:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:39:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #13, planning to set LR to 1.00e-05
2025-10-09 09:39:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 09:39:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:39:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:39:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:39:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:39:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:40:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:40:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.136444, avg_loss=0.681534, seen=480, correct=265, accuracy=0.552083
2025-10-09 09:40:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:40:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:40:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=13 reserved=2188MB allocated=1938MB
2025-10-09 09:40:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 13, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.83179849386215, 'train_avg_loss': 0.6735983207821846, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 09:40:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 13, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.1364440917969, 'train_avg_loss': 0.6815342585245768, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 09:40:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 13, 'Results_raw': {'train_total': 480, 'train_loss': 327.1364440917969, 'train_avg_loss': 0.6815342585245768, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 09:40:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:40:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:40:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #13, planning to set LR to 1.00e-05
2025-10-09 09:40:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 09:40:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:40:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:40:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:40:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:40:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:41:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:41:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.485138, avg_loss=0.690594, seen=480, correct=263, accuracy=0.547917
2025-10-09 09:41:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:41:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:41:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:41:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=13 reserved=2104MB allocated=1938MB
2025-10-09 09:41:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 13, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.80462920665741, 'train_avg_loss': 0.7067052433888118, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 09:41:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 13, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.4851379394531, 'train_avg_loss': 0.6905940373738607, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 09:41:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 13, 'Results_raw': {'train_total': 480, 'train_loss': 331.4851379394531, 'train_avg_loss': 0.6905940373738607, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 09:41:11 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #14) -------------
2025-10-09 09:41:12 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=14 aidx=0 | s=5 (candidates=21)
2025-10-09 09:41:12 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[1, 22, 26, 40, 13] (from 21)
2025-10-09 09:41:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:41:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:41:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #14, planning to set LR to 1.00e-05
2025-10-09 09:41:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 09:41:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:41:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:41:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:41:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:41:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:41:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:41:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.371521, avg_loss=0.690357, seen=480, correct=252, accuracy=0.525000
2025-10-09 09:41:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:41:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:41:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:41:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=14 reserved=2078MB allocated=1938MB
2025-10-09 09:41:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 14, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.06220781803131, 'train_avg_loss': 0.6838517318169276, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 09:41:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 14, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.37152099609375, 'train_avg_loss': 0.6903573354085286, 'train_seen': 480, 'train_correct': 252, 'train_acc': 0.525}}
2025-10-09 09:41:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 14, 'Results_raw': {'train_total': 480, 'train_loss': 331.37152099609375, 'train_avg_loss': 0.6903573354085286, 'train_seen': 480, 'train_correct': 252, 'train_acc': 0.525}}
2025-10-09 09:41:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:41:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:41:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #14, planning to set LR to 1.00e-05
2025-10-09 09:41:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 09:41:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:41:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:41:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:41:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:41:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:42:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:42:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.288757, avg_loss=0.675602, seen=480, correct=276, accuracy=0.575000
2025-10-09 09:42:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:42:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:42:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:42:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=14 reserved=2086MB allocated=1938MB
2025-10-09 09:42:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 14, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.62929010391235, 'train_avg_loss': 0.6802440841992696, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 09:42:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 14, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.28875732421875, 'train_avg_loss': 0.675601577758789, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 09:42:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 14, 'Results_raw': {'train_total': 480, 'train_loss': 324.28875732421875, 'train_avg_loss': 0.675601577758789, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 09:42:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:42:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:42:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #14, planning to set LR to 1.00e-05
2025-10-09 09:42:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 09:42:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:42:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:42:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:42:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:42:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:43:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:43:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.580353, avg_loss=0.688709, seen=480, correct=262, accuracy=0.545833
2025-10-09 09:43:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:43:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:43:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:43:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=14 reserved=2078MB allocated=1938MB
2025-10-09 09:43:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 14, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.68207502365112, 'train_avg_loss': 0.6723506251970927, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 09:43:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 14, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.5803527832031, 'train_avg_loss': 0.6887090682983399, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 09:43:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 14, 'Results_raw': {'train_total': 480, 'train_loss': 330.5803527832031, 'train_avg_loss': 0.6887090682983399, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 09:43:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:43:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:43:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #14, planning to set LR to 1.00e-05
2025-10-09 09:43:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 09:43:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:43:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:43:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:43:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:43:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:44:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:44:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.313690, avg_loss=0.694404, seen=480, correct=248, accuracy=0.516667
2025-10-09 09:44:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:44:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:44:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=14 reserved=2078MB allocated=1938MB
2025-10-09 09:44:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 14, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.21926975250244, 'train_avg_loss': 0.718493914604187, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 09:44:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 14, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.3136901855469, 'train_avg_loss': 0.6944035212198894, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 09:44:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 14, 'Results_raw': {'train_total': 480, 'train_loss': 333.3136901855469, 'train_avg_loss': 0.6944035212198894, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 09:44:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:44:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:44:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #14, planning to set LR to 1.00e-05
2025-10-09 09:44:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 09:44:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:44:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:44:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:44:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:44:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:44:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:44:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.357086, avg_loss=0.667411, seen=480, correct=288, accuracy=0.600000
2025-10-09 09:44:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:44:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:44:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:44:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=14 reserved=2078MB allocated=1938MB
2025-10-09 09:44:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 14, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.14471763372421, 'train_avg_loss': 0.6762059802810351, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 09:44:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 14, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.3570861816406, 'train_avg_loss': 0.6674105962117513, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 09:44:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 14, 'Results_raw': {'train_total': 480, 'train_loss': 320.3570861816406, 'train_avg_loss': 0.6674105962117513, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 09:44:47 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #15) -------------
2025-10-09 09:44:47 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=15 aidx=0 | s=5 (candidates=21)
2025-10-09 09:44:47 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[39, 20, 16, 13, 43] (from 21)
2025-10-09 09:44:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:44:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:44:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #15, planning to set LR to 1.00e-05
2025-10-09 09:44:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 09:44:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:44:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:44:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:44:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:44:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:45:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:45:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.290009, avg_loss=0.688104, seen=480, correct=260, accuracy=0.541667
2025-10-09 09:45:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:45:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:45:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:45:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=15 reserved=2078MB allocated=1938MB
2025-10-09 09:45:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 15, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.52943974733353, 'train_avg_loss': 0.6794119978944461, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 09:45:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 15, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.2900085449219, 'train_avg_loss': 0.6881041844685872, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:45:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 15, 'Results_raw': {'train_total': 480, 'train_loss': 330.2900085449219, 'train_avg_loss': 0.6881041844685872, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:45:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:45:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:45:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #15, planning to set LR to 1.00e-05
2025-10-09 09:45:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 09:45:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:45:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:45:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:45:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:45:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:46:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:46:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.552002, avg_loss=0.659483, seen=480, correct=294, accuracy=0.612500
2025-10-09 09:46:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:46:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:46:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:46:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=15 reserved=2078MB allocated=1938MB
2025-10-09 09:46:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 15, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.11643648147583, 'train_avg_loss': 0.6759703040122986, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 09:46:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 15, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.552001953125, 'train_avg_loss': 0.6594833374023438, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 09:46:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 15, 'Results_raw': {'train_total': 480, 'train_loss': 316.552001953125, 'train_avg_loss': 0.6594833374023438, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 09:46:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:46:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:46:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #15, planning to set LR to 1.00e-05
2025-10-09 09:46:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 09:46:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:46:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:46:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:46:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:46:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:46:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:46:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.564178, avg_loss=0.682425, seen=480, correct=267, accuracy=0.556250
2025-10-09 09:46:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:46:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:46:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:46:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=15 reserved=2104MB allocated=1938MB
2025-10-09 09:46:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 15, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.34969013929367, 'train_avg_loss': 0.6612474178274472, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 09:46:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 15, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.5641784667969, 'train_avg_loss': 0.6824253718058269, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 09:46:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 15, 'Results_raw': {'train_total': 480, 'train_loss': 327.5641784667969, 'train_avg_loss': 0.6824253718058269, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 09:46:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:46:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:46:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #15, planning to set LR to 1.00e-05
2025-10-09 09:46:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 09:46:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:46:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:46:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:46:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:46:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:47:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:47:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.569458, avg_loss=0.659520, seen=480, correct=295, accuracy=0.614583
2025-10-09 09:47:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:47:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:47:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:47:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=15 reserved=2078MB allocated=1938MB
2025-10-09 09:47:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 15, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.13851124048233, 'train_avg_loss': 0.6678209270040194, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 09:47:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 15, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.5694580078125, 'train_avg_loss': 0.6595197041829427, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 09:47:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 15, 'Results_raw': {'train_total': 480, 'train_loss': 316.5694580078125, 'train_avg_loss': 0.6595197041829427, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 09:47:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:47:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:47:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #15, planning to set LR to 1.00e-05
2025-10-09 09:47:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 09:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:47:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:47:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:47:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:47:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:48:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:48:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.394165, avg_loss=0.671655, seen=480, correct=288, accuracy=0.600000
2025-10-09 09:48:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:48:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:48:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=15 reserved=2180MB allocated=1971MB
2025-10-09 09:48:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 15, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.25461333990097, 'train_avg_loss': 0.6937884444991748, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 09:48:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 15, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.3941650390625, 'train_avg_loss': 0.6716545104980469, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 09:48:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 15, 'Results_raw': {'train_total': 480, 'train_loss': 322.3941650390625, 'train_avg_loss': 0.6716545104980469, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 09:48:24 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #16) -------------
2025-10-09 09:48:25 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=16 aidx=0 | s=5 (candidates=21)
2025-10-09 09:48:25 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[51, 13, 45, 29, 16] (from 21)
2025-10-09 09:48:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:48:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:48:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #16, planning to set LR to 1.00e-05
2025-10-09 09:48:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 09:48:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:48:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:48:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:48:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:48:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:49:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:49:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.607178, avg_loss=0.655432, seen=480, correct=292, accuracy=0.608333
2025-10-09 09:49:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:49:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:49:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:49:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=16 reserved=2112MB allocated=1971MB
2025-10-09 09:49:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 16, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.51962959766388, 'train_avg_loss': 0.679330246647199, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 09:49:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 16, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.607177734375, 'train_avg_loss': 0.655431620279948, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 09:49:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 16, 'Results_raw': {'train_total': 480, 'train_loss': 314.607177734375, 'train_avg_loss': 0.655431620279948, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 09:49:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:49:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:49:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #16, planning to set LR to 1.00e-05
2025-10-09 09:49:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 09:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:49:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:49:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:49:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:49:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:49:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:49:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.124207, avg_loss=0.656509, seen=480, correct=301, accuracy=0.627083
2025-10-09 09:49:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:49:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:49:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=16 reserved=2102MB allocated=1971MB
2025-10-09 09:49:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 16, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.63576185703278, 'train_avg_loss': 0.6636313488086064, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 09:49:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 16, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.12420654296875, 'train_avg_loss': 0.6565087636311849, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 09:49:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 16, 'Results_raw': {'train_total': 480, 'train_loss': 315.12420654296875, 'train_avg_loss': 0.6565087636311849, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 09:49:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:49:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:49:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #16, planning to set LR to 1.00e-05
2025-10-09 09:49:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 09:49:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:49:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:49:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:49:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:49:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:50:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:50:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.613251, avg_loss=0.674194, seen=480, correct=275, accuracy=0.572917
2025-10-09 09:50:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:50:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:50:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:50:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=16 reserved=2104MB allocated=1971MB
2025-10-09 09:50:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 16, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.07466787099838, 'train_avg_loss': 0.6756222322583199, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 09:50:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 16, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.6132507324219, 'train_avg_loss': 0.6741942723592123, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 09:50:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 16, 'Results_raw': {'train_total': 480, 'train_loss': 323.6132507324219, 'train_avg_loss': 0.6741942723592123, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 09:50:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:50:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:50:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #16, planning to set LR to 1.00e-05
2025-10-09 09:50:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 09:50:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:50:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:50:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:50:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:50:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:51:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:51:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.347504, avg_loss=0.669474, seen=480, correct=278, accuracy=0.579167
2025-10-09 09:51:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:51:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:51:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=16 reserved=2108MB allocated=1971MB
2025-10-09 09:51:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 16, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.55368340015411, 'train_avg_loss': 0.6796140283346176, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 09:51:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 16, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.3475036621094, 'train_avg_loss': 0.6694739659627279, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 09:51:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 16, 'Results_raw': {'train_total': 480, 'train_loss': 321.3475036621094, 'train_avg_loss': 0.6694739659627279, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 09:51:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:51:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:51:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #16, planning to set LR to 1.00e-05
2025-10-09 09:51:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 09:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:51:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:51:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:51:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:51:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:51:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:51:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.945984, avg_loss=0.676971, seen=480, correct=271, accuracy=0.564583
2025-10-09 09:51:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:51:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:51:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:51:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=16 reserved=2132MB allocated=1971MB
2025-10-09 09:51:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 16, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.31709206104279, 'train_avg_loss': 0.6526424338420233, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 09:51:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 16, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.94598388671875, 'train_avg_loss': 0.6769707997639974, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 09:51:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 16, 'Results_raw': {'train_total': 480, 'train_loss': 324.94598388671875, 'train_avg_loss': 0.6769707997639974, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 09:51:58 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #17) -------------
2025-10-09 09:51:58 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=17 aidx=0 | s=5 (candidates=21)
2025-10-09 09:51:58 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[10, 40, 26, 45, 34] (from 21)
2025-10-09 09:51:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:51:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:51:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #17, planning to set LR to 1.00e-05
2025-10-09 09:51:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 09:51:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:51:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:51:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:51:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:51:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:52:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:52:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.186554, avg_loss=0.656639, seen=480, correct=285, accuracy=0.593750
2025-10-09 09:52:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:52:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:52:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:52:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=17 reserved=2078MB allocated=1946MB
2025-10-09 09:52:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 17, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.47004514932632, 'train_avg_loss': 0.6539170429110527, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 09:52:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 17, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.1865539550781, 'train_avg_loss': 0.6566386540730794, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 09:52:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 17, 'Results_raw': {'train_total': 480, 'train_loss': 315.1865539550781, 'train_avg_loss': 0.6566386540730794, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 09:52:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:52:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:52:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #17, planning to set LR to 1.00e-05
2025-10-09 09:52:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 09:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:52:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:52:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:52:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:52:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:53:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:53:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.577789, avg_loss=0.688704, seen=480, correct=260, accuracy=0.541667
2025-10-09 09:53:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:53:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:53:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:53:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=17 reserved=2078MB allocated=1946MB
2025-10-09 09:53:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 17, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.78329229354858, 'train_avg_loss': 0.7148607691129049, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 09:53:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 17, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.5777893066406, 'train_avg_loss': 0.6887037277221679, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:53:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 17, 'Results_raw': {'train_total': 480, 'train_loss': 330.5777893066406, 'train_avg_loss': 0.6887037277221679, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:53:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:53:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:53:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #17, planning to set LR to 1.00e-05
2025-10-09 09:53:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 09:53:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:53:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:53:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:53:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:53:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:54:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:54:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.511414, avg_loss=0.688565, seen=480, correct=261, accuracy=0.543750
2025-10-09 09:54:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:54:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:54:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:54:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=17 reserved=2078MB allocated=1946MB
2025-10-09 09:54:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 17, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.24873334169388, 'train_avg_loss': 0.6687394445141156, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 09:54:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 17, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.51141357421875, 'train_avg_loss': 0.688565444946289, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 09:54:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 17, 'Results_raw': {'train_total': 480, 'train_loss': 330.51141357421875, 'train_avg_loss': 0.688565444946289, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 09:54:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:54:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:54:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #17, planning to set LR to 1.00e-05
2025-10-09 09:54:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 09:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:54:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:54:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:54:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:54:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:54:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:54:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.597412, avg_loss=0.678328, seen=480, correct=267, accuracy=0.556250
2025-10-09 09:54:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:54:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:54:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:54:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=17 reserved=2078MB allocated=1946MB
2025-10-09 09:54:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 17, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.00298899412155, 'train_avg_loss': 0.6750249082843462, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 09:54:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 17, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.597412109375, 'train_avg_loss': 0.6783279418945313, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 09:54:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 17, 'Results_raw': {'train_total': 480, 'train_loss': 325.597412109375, 'train_avg_loss': 0.6783279418945313, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 09:54:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:54:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:54:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #17, planning to set LR to 1.00e-05
2025-10-09 09:54:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 09:54:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:54:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:54:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:54:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:54:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:55:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:55:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.721954, avg_loss=0.697337, seen=480, correct=261, accuracy=0.543750
2025-10-09 09:55:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:55:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:55:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=17 reserved=2078MB allocated=1946MB
2025-10-09 09:55:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 17, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.89354997873306, 'train_avg_loss': 0.6991129164894422, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 09:55:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 17, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.7219543457031, 'train_avg_loss': 0.6973374048868816, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 09:55:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 17, 'Results_raw': {'train_total': 480, 'train_loss': 334.7219543457031, 'train_avg_loss': 0.6973374048868816, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 09:55:28 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #18) -------------
2025-10-09 09:55:28 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=18 aidx=0 | s=5 (candidates=21)
2025-10-09 09:55:28 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[40, 46, 26, 20, 43] (from 21)
2025-10-09 09:55:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:55:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:55:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #18, planning to set LR to 1.00e-05
2025-10-09 09:55:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 09:55:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:55:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:55:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:55:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:55:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:56:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:56:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.503418, avg_loss=0.690632, seen=480, correct=257, accuracy=0.535417
2025-10-09 09:56:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:56:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:56:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:56:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=18 reserved=2078MB allocated=1946MB
2025-10-09 09:56:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 18, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.72967457771301, 'train_avg_loss': 0.714413954814275, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 09:56:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 18, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.50341796875, 'train_avg_loss': 0.6906321207682292, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 09:56:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 18, 'Results_raw': {'train_total': 480, 'train_loss': 331.50341796875, 'train_avg_loss': 0.6906321207682292, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 09:56:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:56:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:56:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #18, planning to set LR to 1.00e-05
2025-10-09 09:56:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 09:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:56:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:56:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:56:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:56:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:56:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:56:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.584717, avg_loss=0.680385, seen=480, correct=275, accuracy=0.572917
2025-10-09 09:56:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:56:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:56:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=18 reserved=2102MB allocated=1946MB
2025-10-09 09:56:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 18, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.28937995433807, 'train_avg_loss': 0.6940781662861506, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 09:56:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 18, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.584716796875, 'train_avg_loss': 0.6803848266601562, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 09:56:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 18, 'Results_raw': {'train_total': 480, 'train_loss': 326.584716796875, 'train_avg_loss': 0.6803848266601562, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 09:56:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:56:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:56:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #18, planning to set LR to 1.00e-05
2025-10-09 09:56:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 09:56:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:56:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:56:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:56:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:56:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:57:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:57:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.037842, avg_loss=0.691746, seen=480, correct=260, accuracy=0.541667
2025-10-09 09:57:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:57:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:57:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:57:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=18 reserved=2078MB allocated=1946MB
2025-10-09 09:57:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 18, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.64976048469543, 'train_avg_loss': 0.672081337372462, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 09:57:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 18, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.037841796875, 'train_avg_loss': 0.6917455037434895, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:57:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 18, 'Results_raw': {'train_total': 480, 'train_loss': 332.037841796875, 'train_avg_loss': 0.6917455037434895, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 09:57:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:57:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:57:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #18, planning to set LR to 1.00e-05
2025-10-09 09:57:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 09:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:57:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:57:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:57:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:57:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:58:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:58:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.057495, avg_loss=0.654286, seen=480, correct=292, accuracy=0.608333
2025-10-09 09:58:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:58:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:58:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=18 reserved=2078MB allocated=1946MB
2025-10-09 09:58:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 18, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.65327388048172, 'train_avg_loss': 0.672110615670681, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 09:58:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 18, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.0574951171875, 'train_avg_loss': 0.6542864481608073, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 09:58:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 18, 'Results_raw': {'train_total': 480, 'train_loss': 314.0574951171875, 'train_avg_loss': 0.6542864481608073, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 09:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:58:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:58:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #18, planning to set LR to 1.00e-05
2025-10-09 09:58:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 09:58:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:58:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:58:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:58:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:58:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:59:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:59:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.494263, avg_loss=0.659363, seen=480, correct=293, accuracy=0.610417
2025-10-09 09:59:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:59:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:59:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:59:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=18 reserved=2078MB allocated=1946MB
2025-10-09 09:59:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 18, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.62194013595581, 'train_avg_loss': 0.6801828344662985, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 09:59:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 18, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.4942626953125, 'train_avg_loss': 0.6593630472819011, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 09:59:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 18, 'Results_raw': {'train_total': 480, 'train_loss': 316.4942626953125, 'train_avg_loss': 0.6593630472819011, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 09:59:04 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #19) -------------
2025-10-09 09:59:04 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=19 aidx=0 | s=5 (candidates=21)
2025-10-09 09:59:04 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[48, 28, 16, 29, 40] (from 21)
2025-10-09 09:59:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:59:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:59:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #19, planning to set LR to 1.00e-05
2025-10-09 09:59:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 09:59:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:59:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:59:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:59:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:59:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 09:59:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 09:59:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.853790, avg_loss=0.683029, seen=480, correct=273, accuracy=0.568750
2025-10-09 09:59:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 09:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:59:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 09:59:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=19 reserved=2078MB allocated=1946MB
2025-10-09 09:59:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 19, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.18402540683746, 'train_avg_loss': 0.7098668783903122, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 09:59:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 19, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.8537902832031, 'train_avg_loss': 0.6830287297566732, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 09:59:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 19, 'Results_raw': {'train_total': 480, 'train_loss': 327.8537902832031, 'train_avg_loss': 0.6830287297566732, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 09:59:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 09:59:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 09:59:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #19, planning to set LR to 1.00e-05
2025-10-09 09:59:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 09:59:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 09:59:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 09:59:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 09:59:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 09:59:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:00:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:00:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.955353, avg_loss=0.674907, seen=480, correct=275, accuracy=0.572917
2025-10-09 10:00:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:00:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:00:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:00:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=19 reserved=2078MB allocated=1946MB
2025-10-09 10:00:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 19, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.89968430995941, 'train_avg_loss': 0.6824973692496618, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 10:00:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 19, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.9553527832031, 'train_avg_loss': 0.6749069849650066, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 10:00:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 19, 'Results_raw': {'train_total': 480, 'train_loss': 323.9553527832031, 'train_avg_loss': 0.6749069849650066, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 10:00:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:00:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:00:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #19, planning to set LR to 1.00e-05
2025-10-09 10:00:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 10:00:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:00:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:00:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:00:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:00:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:01:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:01:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.659485, avg_loss=0.668041, seen=480, correct=269, accuracy=0.560417
2025-10-09 10:01:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:01:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:01:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:01:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=19 reserved=2108MB allocated=1946MB
2025-10-09 10:01:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 19, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.04686111211777, 'train_avg_loss': 0.6337238426009814, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 10:01:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 19, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.65948486328125, 'train_avg_loss': 0.6680405934651693, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 10:01:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 19, 'Results_raw': {'train_total': 480, 'train_loss': 320.65948486328125, 'train_avg_loss': 0.6680405934651693, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 10:01:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:01:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:01:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #19, planning to set LR to 1.00e-05
2025-10-09 10:01:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 10:01:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:01:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:01:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:01:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:01:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:01:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:01:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.283478, avg_loss=0.669341, seen=480, correct=270, accuracy=0.562500
2025-10-09 10:01:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:01:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:01:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:01:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=19 reserved=2086MB allocated=1946MB
2025-10-09 10:01:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 19, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.56222891807556, 'train_avg_loss': 0.679685240983963, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 10:01:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 19, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.2834777832031, 'train_avg_loss': 0.6693405787150065, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 10:01:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 19, 'Results_raw': {'train_total': 480, 'train_loss': 321.2834777832031, 'train_avg_loss': 0.6693405787150065, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 10:01:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:01:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:01:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #19, planning to set LR to 1.00e-05
2025-10-09 10:01:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 10:01:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:01:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:01:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:01:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:01:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:02:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:02:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.972046, avg_loss=0.691608, seen=480, correct=265, accuracy=0.552083
2025-10-09 10:02:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:02:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:02:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:02:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=19 reserved=2078MB allocated=1946MB
2025-10-09 10:02:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 19, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.30807286500931, 'train_avg_loss': 0.7109006072084109, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 10:02:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 19, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.9720458984375, 'train_avg_loss': 0.6916084289550781, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 10:02:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 19, 'Results_raw': {'train_total': 480, 'train_loss': 331.9720458984375, 'train_avg_loss': 0.6916084289550781, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 10:02:36 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #20) -------------
2025-10-09 10:02:36 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=20 aidx=0 | s=5 (candidates=21)
2025-10-09 10:02:36 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[22, 1, 10, 51, 28] (from 21)
2025-10-09 10:02:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:02:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:02:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #20, planning to set LR to 1.00e-05
2025-10-09 10:02:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 10:02:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:02:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:02:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:02:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:02:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:03:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:03:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.882446, avg_loss=0.664338, seen=480, correct=284, accuracy=0.591667
2025-10-09 10:03:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:03:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:03:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=20 reserved=2086MB allocated=1946MB
2025-10-09 10:03:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 20, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.15377539396286, 'train_avg_loss': 0.6762814616163572, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 10:03:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 20, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.8824462890625, 'train_avg_loss': 0.6643384297688802, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 10:03:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 20, 'Results_raw': {'train_total': 480, 'train_loss': 318.8824462890625, 'train_avg_loss': 0.6643384297688802, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 10:03:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:03:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:03:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #20, planning to set LR to 1.00e-05
2025-10-09 10:03:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 10:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:03:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:03:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:03:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:03:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:03:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:03:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.804535, avg_loss=0.687093, seen=480, correct=264, accuracy=0.550000
2025-10-09 10:03:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:03:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:04:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:04:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=20 reserved=2078MB allocated=1946MB
2025-10-09 10:04:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 20, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.74063301086426, 'train_avg_loss': 0.6811719417572022, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 10:04:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 20, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.8045349121094, 'train_avg_loss': 0.6870927810668945, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 10:04:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 20, 'Results_raw': {'train_total': 480, 'train_loss': 329.8045349121094, 'train_avg_loss': 0.6870927810668945, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 10:04:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:04:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:04:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #20, planning to set LR to 1.00e-05
2025-10-09 10:04:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 10:04:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:04:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:04:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:04:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:04:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:04:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:04:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.857819, avg_loss=0.647620, seen=480, correct=298, accuracy=0.620833
2025-10-09 10:04:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:04:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:04:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:04:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=20 reserved=2078MB allocated=1946MB
2025-10-09 10:04:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 20, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.81438398361206, 'train_avg_loss': 0.6484531998634339, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 10:04:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 20, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.8578186035156, 'train_avg_loss': 0.6476204554239909, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 10:04:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 20, 'Results_raw': {'train_total': 480, 'train_loss': 310.8578186035156, 'train_avg_loss': 0.6476204554239909, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 10:04:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:04:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:04:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #20, planning to set LR to 1.00e-05
2025-10-09 10:04:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 10:04:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:04:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:04:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:04:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:04:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:05:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:05:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.148621, avg_loss=0.652393, seen=480, correct=286, accuracy=0.595833
2025-10-09 10:05:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:05:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:05:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=20 reserved=2088MB allocated=1946MB
2025-10-09 10:05:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 20, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.84559553861618, 'train_avg_loss': 0.6820466294884682, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 10:05:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 20, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.14862060546875, 'train_avg_loss': 0.6523929595947265, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 10:05:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 20, 'Results_raw': {'train_total': 480, 'train_loss': 313.14862060546875, 'train_avg_loss': 0.6523929595947265, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 10:05:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:05:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:05:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #20, planning to set LR to 1.00e-05
2025-10-09 10:05:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 10:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:05:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:05:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:05:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:05:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:06:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:06:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.348938, avg_loss=0.673644, seen=480, correct=279, accuracy=0.581250
2025-10-09 10:06:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:06:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:06:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=20 reserved=2078MB allocated=1946MB
2025-10-09 10:06:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 20, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.90434682369232, 'train_avg_loss': 0.6742028901974361, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 10:06:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 20, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.34893798828125, 'train_avg_loss': 0.6736436208089193, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 10:06:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 20, 'Results_raw': {'train_total': 480, 'train_loss': 323.34893798828125, 'train_avg_loss': 0.6736436208089193, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 10:06:02 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #21) -------------
2025-10-09 10:06:03 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=21 aidx=0 | s=5 (candidates=21)
2025-10-09 10:06:03 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[39, 47, 43, 19, 22] (from 21)
2025-10-09 10:06:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:06:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:06:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #21, planning to set LR to 1.00e-05
2025-10-09 10:06:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 10:06:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:06:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:06:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:06:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:06:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:06:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:06:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.723328, avg_loss=0.678590, seen=480, correct=284, accuracy=0.591667
2025-10-09 10:06:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:06:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:06:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=21 reserved=2078MB allocated=1946MB
2025-10-09 10:06:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 21, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.05104672908783, 'train_avg_loss': 0.667092056075732, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 10:06:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 21, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.72332763671875, 'train_avg_loss': 0.6785902659098307, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 10:06:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 21, 'Results_raw': {'train_total': 480, 'train_loss': 325.72332763671875, 'train_avg_loss': 0.6785902659098307, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 10:06:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:06:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:06:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #21, planning to set LR to 1.00e-05
2025-10-09 10:06:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 10:06:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:06:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:06:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:06:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:06:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:07:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:07:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.754120, avg_loss=0.668238, seen=480, correct=288, accuracy=0.600000
2025-10-09 10:07:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:07:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:07:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=21 reserved=2078MB allocated=1946MB
2025-10-09 10:07:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 21, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.13864636421204, 'train_avg_loss': 0.6594887197017669, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 10:07:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 21, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.7541198730469, 'train_avg_loss': 0.6682377497355143, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 10:07:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 21, 'Results_raw': {'train_total': 480, 'train_loss': 320.7541198730469, 'train_avg_loss': 0.6682377497355143, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 10:07:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:07:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:07:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #21, planning to set LR to 1.00e-05
2025-10-09 10:07:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 10:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:07:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:07:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:07:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:07:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:08:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:08:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.446930, avg_loss=0.642598, seen=480, correct=308, accuracy=0.641667
2025-10-09 10:08:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:08:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:08:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:08:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=21 reserved=2078MB allocated=1946MB
2025-10-09 10:08:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 21, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.82383239269257, 'train_avg_loss': 0.6735319366057714, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 10:08:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 21, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.4469299316406, 'train_avg_loss': 0.642597770690918, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 10:08:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 21, 'Results_raw': {'train_total': 480, 'train_loss': 308.4469299316406, 'train_avg_loss': 0.642597770690918, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 10:08:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:08:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:08:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #21, planning to set LR to 1.00e-05
2025-10-09 10:08:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 10:08:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:08:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:08:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:08:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:08:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:08:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:08:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.254303, avg_loss=0.663030, seen=480, correct=288, accuracy=0.600000
2025-10-09 10:08:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:08:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:08:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=21 reserved=2188MB allocated=1946MB
2025-10-09 10:08:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 21, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.57770639657974, 'train_avg_loss': 0.6464808866381645, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 10:08:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 21, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.2543029785156, 'train_avg_loss': 0.6630297978719075, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 10:08:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 21, 'Results_raw': {'train_total': 480, 'train_loss': 318.2543029785156, 'train_avg_loss': 0.6630297978719075, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 10:08:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:08:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:08:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #21, planning to set LR to 1.00e-05
2025-10-09 10:08:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 10:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:08:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:08:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:08:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:08:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:09:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:09:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.154633, avg_loss=0.662822, seen=480, correct=279, accuracy=0.581250
2025-10-09 10:09:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:09:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:09:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=21 reserved=2086MB allocated=1946MB
2025-10-09 10:09:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 21, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.07968664169312, 'train_avg_loss': 0.6756640553474427, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 10:09:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 21, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.1546325683594, 'train_avg_loss': 0.662822151184082, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 10:09:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 21, 'Results_raw': {'train_total': 480, 'train_loss': 318.1546325683594, 'train_avg_loss': 0.662822151184082, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 10:09:33 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #22) -------------
2025-10-09 10:09:34 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=22 aidx=0 | s=5 (candidates=21)
2025-10-09 10:09:34 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[16, 19, 22, 46, 39] (from 21)
2025-10-09 10:09:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:09:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:09:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #22, planning to set LR to 1.00e-05
2025-10-09 10:09:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 10:09:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:09:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:09:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:09:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:09:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:10:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:10:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.922791, avg_loss=0.660256, seen=480, correct=282, accuracy=0.587500
2025-10-09 10:10:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:10:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:10:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=22 reserved=2106MB allocated=1946MB
2025-10-09 10:10:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 22, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.55778884887695, 'train_avg_loss': 0.6213149070739746, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 10:10:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 22, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.92279052734375, 'train_avg_loss': 0.6602558135986328, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 10:10:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 22, 'Results_raw': {'train_total': 480, 'train_loss': 316.92279052734375, 'train_avg_loss': 0.6602558135986328, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 10:10:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:10:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:10:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #22, planning to set LR to 1.00e-05
2025-10-09 10:10:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 10:10:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:10:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:10:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:10:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:10:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:10:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:10:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.977295, avg_loss=0.662453, seen=480, correct=291, accuracy=0.606250
2025-10-09 10:10:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:10:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:10:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:10:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=22 reserved=2188MB allocated=1946MB
2025-10-09 10:10:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 22, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.42573356628418, 'train_avg_loss': 0.6452144463857015, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 10:10:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 22, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.977294921875, 'train_avg_loss': 0.6624526977539062, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 10:10:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 22, 'Results_raw': {'train_total': 480, 'train_loss': 317.977294921875, 'train_avg_loss': 0.6624526977539062, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 10:10:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:10:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:10:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #22, planning to set LR to 1.00e-05
2025-10-09 10:10:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 10:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:10:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:10:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:10:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:10:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:11:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:11:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.044250, avg_loss=0.631342, seen=480, correct=311, accuracy=0.647917
2025-10-09 10:11:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:11:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:11:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=22 reserved=2086MB allocated=1946MB
2025-10-09 10:11:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 22, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.9590476155281, 'train_avg_loss': 0.6413253967960676, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 10:11:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 22, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.04425048828125, 'train_avg_loss': 0.6313421885172527, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 10:11:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 22, 'Results_raw': {'train_total': 480, 'train_loss': 303.04425048828125, 'train_avg_loss': 0.6313421885172527, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 10:11:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:11:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:11:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #22, planning to set LR to 1.00e-05
2025-10-09 10:11:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 10:11:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:11:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:11:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:11:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:11:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:12:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:12:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.408752, avg_loss=0.677935, seen=480, correct=272, accuracy=0.566667
2025-10-09 10:12:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:12:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:12:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:12:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=22 reserved=2102MB allocated=1946MB
2025-10-09 10:12:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 22, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.2588261961937, 'train_avg_loss': 0.6771568849682807, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 10:12:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 22, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.40875244140625, 'train_avg_loss': 0.6779349009195964, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 10:12:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 22, 'Results_raw': {'train_total': 480, 'train_loss': 325.40875244140625, 'train_avg_loss': 0.6779349009195964, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 10:12:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:12:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:12:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #22, planning to set LR to 1.00e-05
2025-10-09 10:12:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 10:12:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:12:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:12:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:12:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:12:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:13:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:13:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.421570, avg_loss=0.680045, seen=480, correct=284, accuracy=0.591667
2025-10-09 10:13:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:13:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:13:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:13:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=22 reserved=2078MB allocated=1946MB
2025-10-09 10:13:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 22, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.78989958763123, 'train_avg_loss': 0.6649158298969269, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 10:13:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 22, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.42156982421875, 'train_avg_loss': 0.6800449371337891, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 10:13:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 22, 'Results_raw': {'train_total': 480, 'train_loss': 326.42156982421875, 'train_avg_loss': 0.6800449371337891, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 10:13:04 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #23) -------------
2025-10-09 10:13:05 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=23 aidx=0 | s=5 (candidates=21)
2025-10-09 10:13:05 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[19, 48, 39, 34, 45] (from 21)
2025-10-09 10:13:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:13:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:13:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #23, planning to set LR to 1.00e-05
2025-10-09 10:13:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 10:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:13:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:13:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:13:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:13:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:13:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:13:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.007843, avg_loss=0.660433, seen=480, correct=291, accuracy=0.606250
2025-10-09 10:13:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:13:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:13:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=23 reserved=2188MB allocated=1946MB
2025-10-09 10:13:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 23, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.48523545265198, 'train_avg_loss': 0.6373769621054332, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 10:13:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 23, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.0078430175781, 'train_avg_loss': 0.660433006286621, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 10:13:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 23, 'Results_raw': {'train_total': 480, 'train_loss': 317.0078430175781, 'train_avg_loss': 0.660433006286621, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 10:13:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:13:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:13:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #23, planning to set LR to 1.00e-05
2025-10-09 10:13:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 10:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:13:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:13:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:13:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:13:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:14:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.687195, avg_loss=0.676432, seen=480, correct=270, accuracy=0.562500
2025-10-09 10:14:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:14:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:14:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=23 reserved=2078MB allocated=1946MB
2025-10-09 10:14:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 23, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.99091047048569, 'train_avg_loss': 0.7165909205873807, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 10:14:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 23, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.68719482421875, 'train_avg_loss': 0.6764316558837891, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 10:14:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 23, 'Results_raw': {'train_total': 480, 'train_loss': 324.68719482421875, 'train_avg_loss': 0.6764316558837891, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 10:14:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:14:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:14:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #23, planning to set LR to 1.00e-05
2025-10-09 10:14:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 10:14:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:14:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:14:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:14:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:14:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:15:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:15:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.944214, avg_loss=0.670717, seen=480, correct=293, accuracy=0.610417
2025-10-09 10:15:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:15:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:15:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:15:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=23 reserved=2078MB allocated=1946MB
2025-10-09 10:15:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 23, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.72291773557663, 'train_avg_loss': 0.6560243144631386, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 10:15:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 23, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.9442138671875, 'train_avg_loss': 0.6707171122233073, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 10:15:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 23, 'Results_raw': {'train_total': 480, 'train_loss': 321.9442138671875, 'train_avg_loss': 0.6707171122233073, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 10:15:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:15:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:15:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #23, planning to set LR to 1.00e-05
2025-10-09 10:15:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 10:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:15:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:15:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:15:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:15:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:15:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:15:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.448364, avg_loss=0.682184, seen=480, correct=273, accuracy=0.568750
2025-10-09 10:15:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:15:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:15:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:15:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=23 reserved=2078MB allocated=1946MB
2025-10-09 10:15:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 23, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.16263836622238, 'train_avg_loss': 0.6930219863851865, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 10:15:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 23, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.4483642578125, 'train_avg_loss': 0.6821840922037761, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 10:15:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 23, 'Results_raw': {'train_total': 480, 'train_loss': 327.4483642578125, 'train_avg_loss': 0.6821840922037761, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 10:15:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:15:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:15:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #23, planning to set LR to 1.00e-05
2025-10-09 10:15:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 10:15:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:15:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:15:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:15:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:15:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:16:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:16:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.831055, avg_loss=0.670481, seen=480, correct=280, accuracy=0.583333
2025-10-09 10:16:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:16:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:16:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:16:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=23 reserved=2078MB allocated=1946MB
2025-10-09 10:16:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 23, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.50709354877472, 'train_avg_loss': 0.662559112906456, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 10:16:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 23, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.8310546875, 'train_avg_loss': 0.6704813639322916, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 10:16:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 23, 'Results_raw': {'train_total': 480, 'train_loss': 321.8310546875, 'train_avg_loss': 0.6704813639322916, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 10:16:42 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #24) -------------
2025-10-09 10:16:43 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=24 aidx=0 | s=5 (candidates=21)
2025-10-09 10:16:43 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[22, 6, 10, 28, 47] (from 21)
2025-10-09 10:16:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:16:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:16:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #24, planning to set LR to 1.00e-05
2025-10-09 10:16:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 10:16:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:16:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:16:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:16:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:16:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:17:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:17:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.175537, avg_loss=0.635782, seen=480, correct=312, accuracy=0.650000
2025-10-09 10:17:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:17:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:17:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:17:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=24 reserved=2086MB allocated=1946MB
2025-10-09 10:17:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 24, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.20233917236328, 'train_avg_loss': 0.651686159769694, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 10:17:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 24, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.175537109375, 'train_avg_loss': 0.6357823689778646, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 10:17:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 24, 'Results_raw': {'train_total': 480, 'train_loss': 305.175537109375, 'train_avg_loss': 0.6357823689778646, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 10:17:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:17:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:17:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #24, planning to set LR to 1.00e-05
2025-10-09 10:17:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 10:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:17:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:17:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:17:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:17:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:18:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:18:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.876007, avg_loss=0.662242, seen=480, correct=282, accuracy=0.587500
2025-10-09 10:18:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:18:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:18:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:18:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=24 reserved=2086MB allocated=1946MB
2025-10-09 10:18:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 24, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.26598089933395, 'train_avg_loss': 0.6522165074944496, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 10:18:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 24, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.8760070800781, 'train_avg_loss': 0.6622416814168294, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 10:18:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 24, 'Results_raw': {'train_total': 480, 'train_loss': 317.8760070800781, 'train_avg_loss': 0.6622416814168294, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 10:18:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:18:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:18:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #24, planning to set LR to 1.00e-05
2025-10-09 10:18:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 10:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:18:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:18:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:18:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:18:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:18:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:18:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.964233, avg_loss=0.637425, seen=480, correct=302, accuracy=0.629167
2025-10-09 10:18:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:18:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:18:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=24 reserved=2078MB allocated=1946MB
2025-10-09 10:18:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 24, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.27882426977158, 'train_avg_loss': 0.6273235355814298, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 10:18:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 24, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.9642333984375, 'train_avg_loss': 0.6374254862467448, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 10:18:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 24, 'Results_raw': {'train_total': 480, 'train_loss': 305.9642333984375, 'train_avg_loss': 0.6374254862467448, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 10:18:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:18:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:18:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #24, planning to set LR to 1.00e-05
2025-10-09 10:18:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 10:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:18:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:18:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:18:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:18:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:19:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:19:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.439972, avg_loss=0.661333, seen=480, correct=293, accuracy=0.610417
2025-10-09 10:19:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:19:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:19:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=24 reserved=2078MB allocated=1946MB
2025-10-09 10:19:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 24, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.0924162864685, 'train_avg_loss': 0.6591034690539043, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 10:19:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 24, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.4399719238281, 'train_avg_loss': 0.6613332748413085, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 10:19:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 24, 'Results_raw': {'train_total': 480, 'train_loss': 317.4399719238281, 'train_avg_loss': 0.6613332748413085, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 10:19:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:19:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:19:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #24, planning to set LR to 1.00e-05
2025-10-09 10:19:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 10:19:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:19:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:19:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:19:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:19:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:20:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:20:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.719910, avg_loss=0.666083, seen=480, correct=293, accuracy=0.610417
2025-10-09 10:20:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:20:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:20:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=24 reserved=2078MB allocated=1946MB
2025-10-09 10:20:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 24, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.08433938026428, 'train_avg_loss': 0.6673694948355356, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 10:20:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 24, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.71990966796875, 'train_avg_loss': 0.6660831451416016, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 10:20:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 24, 'Results_raw': {'train_total': 480, 'train_loss': 319.71990966796875, 'train_avg_loss': 0.6660831451416016, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 10:20:18 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #25) -------------
2025-10-09 10:20:18 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=25 aidx=0 | s=5 (candidates=21)
2025-10-09 10:20:18 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[22, 43, 51, 39, 20] (from 21)
2025-10-09 10:20:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:20:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:20:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #25, planning to set LR to 1.00e-05
2025-10-09 10:20:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 10:20:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:20:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:20:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:20:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:20:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:21:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:21:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.524963, avg_loss=0.634427, seen=480, correct=309, accuracy=0.643750
2025-10-09 10:21:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:21:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:21:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=25 reserved=2086MB allocated=1946MB
2025-10-09 10:21:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 25, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.30776739120483, 'train_avg_loss': 0.6525647282600403, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 10:21:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 25, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.52496337890625, 'train_avg_loss': 0.634427007039388, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 10:21:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 25, 'Results_raw': {'train_total': 480, 'train_loss': 304.52496337890625, 'train_avg_loss': 0.634427007039388, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 10:21:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:21:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:21:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #25, planning to set LR to 1.00e-05
2025-10-09 10:21:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 10:21:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:21:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:21:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:21:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:21:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:21:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:21:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.923950, avg_loss=0.641508, seen=480, correct=310, accuracy=0.645833
2025-10-09 10:21:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:21:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:21:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=25 reserved=2078MB allocated=1946MB
2025-10-09 10:21:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 25, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.89356702566147, 'train_avg_loss': 0.674113058547179, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 10:21:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 25, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.9239501953125, 'train_avg_loss': 0.6415082295735677, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 10:21:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 25, 'Results_raw': {'train_total': 480, 'train_loss': 307.9239501953125, 'train_avg_loss': 0.6415082295735677, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 10:21:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:21:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:21:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #25, planning to set LR to 1.00e-05
2025-10-09 10:21:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 10:21:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:21:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:21:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:21:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:21:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:22:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:22:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.138306, avg_loss=0.650288, seen=480, correct=284, accuracy=0.591667
2025-10-09 10:22:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:22:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:22:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:22:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=25 reserved=2088MB allocated=1946MB
2025-10-09 10:22:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 25, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.7967654466629, 'train_avg_loss': 0.6899730453888575, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 10:22:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 25, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.1383056640625, 'train_avg_loss': 0.6502881368001302, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 10:22:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 25, 'Results_raw': {'train_total': 480, 'train_loss': 312.1383056640625, 'train_avg_loss': 0.6502881368001302, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 10:22:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:22:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:22:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #25, planning to set LR to 1.00e-05
2025-10-09 10:22:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 10:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:22:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:22:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:22:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:22:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:23:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:23:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.376892, avg_loss=0.669535, seen=480, correct=294, accuracy=0.612500
2025-10-09 10:23:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:23:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:23:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:23:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=25 reserved=2078MB allocated=1946MB
2025-10-09 10:23:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 25, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.8974946141243, 'train_avg_loss': 0.6574791217843692, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 10:23:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 25, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.37689208984375, 'train_avg_loss': 0.6695351918538411, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 10:23:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 25, 'Results_raw': {'train_total': 480, 'train_loss': 321.37689208984375, 'train_avg_loss': 0.6695351918538411, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 10:23:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:23:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:23:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #25, planning to set LR to 1.00e-05
2025-10-09 10:23:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 10:23:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:23:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:23:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:23:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:23:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:23:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:23:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.869019, avg_loss=0.633060, seen=480, correct=306, accuracy=0.637500
2025-10-09 10:23:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:23:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:23:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=25 reserved=2078MB allocated=1946MB
2025-10-09 10:23:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 25, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.87166130542755, 'train_avg_loss': 0.6489305108785629, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 10:23:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 25, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.8690185546875, 'train_avg_loss': 0.6330604553222656, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 10:23:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 25, 'Results_raw': {'train_total': 480, 'train_loss': 303.8690185546875, 'train_avg_loss': 0.6330604553222656, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 10:23:59 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #26) -------------
2025-10-09 10:24:00 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=26 aidx=0 | s=5 (candidates=21)
2025-10-09 10:24:00 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[48, 1, 28, 16, 22] (from 21)
2025-10-09 10:24:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:24:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:24:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #26, planning to set LR to 1.00e-05
2025-10-09 10:24:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 10:24:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:24:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:24:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:24:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:24:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:24:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:24:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.437592, avg_loss=0.673828, seen=480, correct=275, accuracy=0.572917
2025-10-09 10:24:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:24:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:24:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=26 reserved=2078MB allocated=1946MB
2025-10-09 10:24:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 26, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.5509387254715, 'train_avg_loss': 0.7295911560455958, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 10:24:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 26, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.4375915527344, 'train_avg_loss': 0.6738283157348632, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 10:24:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 26, 'Results_raw': {'train_total': 480, 'train_loss': 323.4375915527344, 'train_avg_loss': 0.6738283157348632, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 10:24:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:24:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:24:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #26, planning to set LR to 1.00e-05
2025-10-09 10:24:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 10:24:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:24:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:24:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:24:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:24:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:25:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:25:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.088745, avg_loss=0.691852, seen=480, correct=264, accuracy=0.550000
2025-10-09 10:25:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:25:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:25:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:25:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=26 reserved=2078MB allocated=1946MB
2025-10-09 10:25:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 26, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.94049113988876, 'train_avg_loss': 0.6828374261657397, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 10:25:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 26, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.0887451171875, 'train_avg_loss': 0.691851552327474, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 10:25:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 26, 'Results_raw': {'train_total': 480, 'train_loss': 332.0887451171875, 'train_avg_loss': 0.691851552327474, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 10:25:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:25:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:25:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #26, planning to set LR to 1.00e-05
2025-10-09 10:25:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 10:25:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:25:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:25:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:25:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:25:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:26:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:26:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.563904, avg_loss=0.657425, seen=480, correct=298, accuracy=0.620833
2025-10-09 10:26:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:26:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:26:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:26:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=26 reserved=2078MB allocated=1946MB
2025-10-09 10:26:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 26, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.90505623817444, 'train_avg_loss': 0.6575421353181203, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 10:26:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 26, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.56390380859375, 'train_avg_loss': 0.657424799601237, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 10:26:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 26, 'Results_raw': {'train_total': 480, 'train_loss': 315.56390380859375, 'train_avg_loss': 0.657424799601237, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 10:26:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:26:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:26:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #26, planning to set LR to 1.00e-05
2025-10-09 10:26:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 10:26:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:26:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:26:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:26:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:26:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:26:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:26:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.119843, avg_loss=0.654416, seen=480, correct=292, accuracy=0.608333
2025-10-09 10:26:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:26:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:26:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:26:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=26 reserved=2106MB allocated=1946MB
2025-10-09 10:26:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 26, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.72337889671326, 'train_avg_loss': 0.6143614908059438, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 10:26:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 26, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.1198425292969, 'train_avg_loss': 0.6544163386027019, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 10:26:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 26, 'Results_raw': {'train_total': 480, 'train_loss': 314.1198425292969, 'train_avg_loss': 0.6544163386027019, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 10:26:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:26:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:26:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #26, planning to set LR to 1.00e-05
2025-10-09 10:26:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 10:26:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:26:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:26:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:26:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:26:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:27:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:27:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.319611, avg_loss=0.631916, seen=480, correct=312, accuracy=0.650000
2025-10-09 10:27:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:27:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:27:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:27:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=26 reserved=2086MB allocated=1946MB
2025-10-09 10:27:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 26, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.29950100183487, 'train_avg_loss': 0.6608291750152906, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 10:27:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 26, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.3196105957031, 'train_avg_loss': 0.6319158554077149, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 10:27:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 26, 'Results_raw': {'train_total': 480, 'train_loss': 303.3196105957031, 'train_avg_loss': 0.6319158554077149, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 10:27:38 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #27) -------------
2025-10-09 10:27:39 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=27 aidx=0 | s=5 (candidates=21)
2025-10-09 10:27:39 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[40, 46, 51, 47, 6] (from 21)
2025-10-09 10:27:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:27:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:27:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #27, planning to set LR to 1.00e-05
2025-10-09 10:27:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 10:27:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:27:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:27:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:27:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:27:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:28:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:28:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.613586, avg_loss=0.682528, seen=480, correct=277, accuracy=0.577083
2025-10-09 10:28:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:28:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:28:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:28:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=27 reserved=2078MB allocated=1946MB
2025-10-09 10:28:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 27, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.65162706375122, 'train_avg_loss': 0.7137635588645935, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 10:28:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 27, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.61358642578125, 'train_avg_loss': 0.682528305053711, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 10:28:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 27, 'Results_raw': {'train_total': 480, 'train_loss': 327.61358642578125, 'train_avg_loss': 0.682528305053711, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 10:28:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:28:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:28:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #27, planning to set LR to 1.00e-05
2025-10-09 10:28:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 10:28:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:28:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:28:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:28:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:28:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:29:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:29:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.005737, avg_loss=0.689595, seen=480, correct=268, accuracy=0.558333
2025-10-09 10:29:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:29:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:29:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:29:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=27 reserved=2100MB allocated=1946MB
2025-10-09 10:29:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 27, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.23494040966034, 'train_avg_loss': 0.6769578367471695, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 10:29:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 27, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.0057373046875, 'train_avg_loss': 0.6895952860514323, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 10:29:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 27, 'Results_raw': {'train_total': 480, 'train_loss': 331.0057373046875, 'train_avg_loss': 0.6895952860514323, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 10:29:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:29:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:29:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #27, planning to set LR to 1.00e-05
2025-10-09 10:29:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 10:29:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:29:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:29:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:29:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:29:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:29:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:29:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.308289, avg_loss=0.642309, seen=480, correct=296, accuracy=0.616667
2025-10-09 10:29:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:29:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:29:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:29:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=27 reserved=2088MB allocated=1946MB
2025-10-09 10:29:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 27, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.85168653726578, 'train_avg_loss': 0.6904307211438815, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 10:29:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 27, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.30828857421875, 'train_avg_loss': 0.6423089345296223, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 10:29:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 27, 'Results_raw': {'train_total': 480, 'train_loss': 308.30828857421875, 'train_avg_loss': 0.6423089345296223, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 10:29:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:29:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:29:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #27, planning to set LR to 1.00e-05
2025-10-09 10:29:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 10:29:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:29:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:29:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:29:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:29:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:30:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:30:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.836151, avg_loss=0.655909, seen=480, correct=296, accuracy=0.616667
2025-10-09 10:30:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:30:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:30:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:30:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=27 reserved=2078MB allocated=1946MB
2025-10-09 10:30:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 27, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.90809160470963, 'train_avg_loss': 0.6575674300392469, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 10:30:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 27, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.8361511230469, 'train_avg_loss': 0.6559086481730143, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 10:30:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 27, 'Results_raw': {'train_total': 480, 'train_loss': 314.8361511230469, 'train_avg_loss': 0.6559086481730143, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 10:30:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:30:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:30:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #27, planning to set LR to 1.00e-05
2025-10-09 10:30:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 10:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:30:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:30:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:30:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:30:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:31:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:31:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.700836, avg_loss=0.666043, seen=480, correct=285, accuracy=0.593750
2025-10-09 10:31:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:31:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:31:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:31:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=27 reserved=2086MB allocated=1946MB
2025-10-09 10:31:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 27, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.66714042425156, 'train_avg_loss': 0.6472261702020963, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 10:31:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 27, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.7008361816406, 'train_avg_loss': 0.6660434087117513, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 10:31:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 27, 'Results_raw': {'train_total': 480, 'train_loss': 319.7008361816406, 'train_avg_loss': 0.6660434087117513, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 10:31:19 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #28) -------------
2025-10-09 10:31:19 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=28 aidx=0 | s=5 (candidates=21)
2025-10-09 10:31:19 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[20, 19, 47, 45, 48] (from 21)
2025-10-09 10:31:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:31:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:31:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #28, planning to set LR to 1.00e-05
2025-10-09 10:31:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 10:31:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:31:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:31:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:31:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:31:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:32:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:32:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=298.368469, avg_loss=0.621601, seen=480, correct=310, accuracy=0.645833
2025-10-09 10:32:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:32:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:32:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:32:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=28 reserved=2078MB allocated=1946MB
2025-10-09 10:32:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 28, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.67010712623596, 'train_avg_loss': 0.6389175593852997, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 10:32:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 28, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 298.36846923828125, 'train_avg_loss': 0.6216009775797526, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 10:32:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 28, 'Results_raw': {'train_total': 480, 'train_loss': 298.36846923828125, 'train_avg_loss': 0.6216009775797526, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 10:32:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:32:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:32:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #28, planning to set LR to 1.00e-05
2025-10-09 10:32:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 10:32:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:32:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:32:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:32:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:32:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:32:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:32:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.418732, avg_loss=0.648789, seen=480, correct=302, accuracy=0.629167
2025-10-09 10:32:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:32:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:32:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:32:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=28 reserved=2188MB allocated=1946MB
2025-10-09 10:32:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 28, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.15873473882675, 'train_avg_loss': 0.6346561228235562, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 10:32:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 28, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.4187316894531, 'train_avg_loss': 0.6487890243530273, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 10:32:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 28, 'Results_raw': {'train_total': 480, 'train_loss': 311.4187316894531, 'train_avg_loss': 0.6487890243530273, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 10:32:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:32:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:32:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #28, planning to set LR to 1.00e-05
2025-10-09 10:32:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 10:32:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:32:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:32:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:32:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:32:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:33:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:33:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.006561, avg_loss=0.658347, seen=480, correct=294, accuracy=0.612500
2025-10-09 10:33:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:33:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:33:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=28 reserved=2078MB allocated=1946MB
2025-10-09 10:33:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 28, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.44178026914597, 'train_avg_loss': 0.6453481689095497, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 10:33:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 28, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.0065612792969, 'train_avg_loss': 0.6583470026652019, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 10:33:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 28, 'Results_raw': {'train_total': 480, 'train_loss': 316.0065612792969, 'train_avg_loss': 0.6583470026652019, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 10:33:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:33:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:33:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #28, planning to set LR to 1.00e-05
2025-10-09 10:33:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 10:33:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:33:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:33:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:33:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:33:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:34:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:34:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.595642, avg_loss=0.665824, seen=480, correct=286, accuracy=0.595833
2025-10-09 10:34:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:34:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:34:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:34:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=28 reserved=2078MB allocated=1946MB
2025-10-09 10:34:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 28, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.1467677950859, 'train_avg_loss': 0.6595563982923826, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 10:34:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 28, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.59564208984375, 'train_avg_loss': 0.6658242543538412, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 10:34:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 28, 'Results_raw': {'train_total': 480, 'train_loss': 319.59564208984375, 'train_avg_loss': 0.6658242543538412, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 10:34:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:34:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:34:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #28, planning to set LR to 1.00e-05
2025-10-09 10:34:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 10:34:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:34:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:34:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:34:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:34:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:35:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:35:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.764252, avg_loss=0.670342, seen=480, correct=277, accuracy=0.577083
2025-10-09 10:35:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:35:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:35:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:35:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=28 reserved=2078MB allocated=1946MB
2025-10-09 10:35:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 28, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.26004099845886, 'train_avg_loss': 0.7188336749871572, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 10:35:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 28, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.7642517089844, 'train_avg_loss': 0.6703421910603841, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 10:35:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 28, 'Results_raw': {'train_total': 480, 'train_loss': 321.7642517089844, 'train_avg_loss': 0.6703421910603841, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 10:35:07 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #29) -------------
2025-10-09 10:35:07 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=29 aidx=0 | s=5 (candidates=21)
2025-10-09 10:35:07 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[1, 20, 45, 6, 26] (from 21)
2025-10-09 10:35:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:35:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:35:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #29, planning to set LR to 1.00e-05
2025-10-09 10:35:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 10:35:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:35:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:35:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:35:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:35:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:35:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:35:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.461273, avg_loss=0.686378, seen=480, correct=270, accuracy=0.562500
2025-10-09 10:35:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:35:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:35:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:35:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=29 reserved=2078MB allocated=1946MB
2025-10-09 10:35:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 29, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.05174285173416, 'train_avg_loss': 0.675431190431118, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 10:35:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 29, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.4612731933594, 'train_avg_loss': 0.6863776524861653, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 10:35:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 29, 'Results_raw': {'train_total': 480, 'train_loss': 329.4612731933594, 'train_avg_loss': 0.6863776524861653, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 10:35:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:35:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:35:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #29, planning to set LR to 1.00e-05
2025-10-09 10:35:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 10:35:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:35:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:35:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:35:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:35:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:36:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:36:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=299.557251, avg_loss=0.624078, seen=480, correct=315, accuracy=0.656250
2025-10-09 10:36:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:36:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:36:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=29 reserved=2078MB allocated=1946MB
2025-10-09 10:36:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 29, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.73030090332031, 'train_avg_loss': 0.6394191741943359, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 10:36:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 29, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 299.5572509765625, 'train_avg_loss': 0.6240776062011719, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 10:36:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 29, 'Results_raw': {'train_total': 480, 'train_loss': 299.5572509765625, 'train_avg_loss': 0.6240776062011719, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 10:36:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:36:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:36:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #29, planning to set LR to 1.00e-05
2025-10-09 10:36:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 10:36:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:36:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:36:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:36:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:36:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:37:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:37:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.302124, avg_loss=0.656879, seen=480, correct=287, accuracy=0.597917
2025-10-09 10:37:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:37:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:37:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:37:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=29 reserved=2078MB allocated=1946MB
2025-10-09 10:37:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 29, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.89297980070114, 'train_avg_loss': 0.6574414983391762, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 10:37:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 29, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.3021240234375, 'train_avg_loss': 0.6568794250488281, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 10:37:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 29, 'Results_raw': {'train_total': 480, 'train_loss': 315.3021240234375, 'train_avg_loss': 0.6568794250488281, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 10:37:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:37:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:37:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #29, planning to set LR to 1.00e-05
2025-10-09 10:37:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 10:37:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:37:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:37:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:37:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:37:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:38:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:38:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.398254, avg_loss=0.654996, seen=480, correct=289, accuracy=0.602083
2025-10-09 10:38:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:38:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:38:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:38:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=29 reserved=2086MB allocated=1946MB
2025-10-09 10:38:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 29, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.19210785627365, 'train_avg_loss': 0.6432675654689471, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 10:38:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 29, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.39825439453125, 'train_avg_loss': 0.6549963633219401, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 10:38:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 29, 'Results_raw': {'train_total': 480, 'train_loss': 314.39825439453125, 'train_avg_loss': 0.6549963633219401, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 10:38:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:38:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:38:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #29, planning to set LR to 1.00e-05
2025-10-09 10:38:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 10:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:38:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:38:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:38:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:38:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:38:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:38:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.815674, avg_loss=0.676699, seen=480, correct=269, accuracy=0.560417
2025-10-09 10:38:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:38:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:38:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:38:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=29 reserved=2078MB allocated=1946MB
2025-10-09 10:38:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 29, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.78260135650635, 'train_avg_loss': 0.6648550113042195, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 10:38:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 29, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.815673828125, 'train_avg_loss': 0.6766993204752604, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 10:38:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 29, 'Results_raw': {'train_total': 480, 'train_loss': 324.815673828125, 'train_avg_loss': 0.6766993204752604, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 10:38:49 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #30) -------------
2025-10-09 10:38:50 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=30 aidx=0 | s=5 (candidates=21)
2025-10-09 10:38:50 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[43, 45, 10, 51, 20] (from 21)
2025-10-09 10:38:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:38:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:38:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #30, planning to set LR to 1.00e-05
2025-10-09 10:38:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 10:38:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:38:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:38:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:38:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:38:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:39:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:39:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.978424, avg_loss=0.639538, seen=480, correct=304, accuracy=0.633333
2025-10-09 10:39:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:39:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:39:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:39:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=30 reserved=2078MB allocated=1946MB
2025-10-09 10:39:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 30, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.68375754356384, 'train_avg_loss': 0.6640313128630321, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 10:39:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 30, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.9784240722656, 'train_avg_loss': 0.6395383834838867, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 10:39:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 30, 'Results_raw': {'train_total': 480, 'train_loss': 306.9784240722656, 'train_avg_loss': 0.6395383834838867, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 10:39:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:39:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:39:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #30, planning to set LR to 1.00e-05
2025-10-09 10:39:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 10:39:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:39:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:39:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:39:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:39:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:40:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:40:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.005188, avg_loss=0.656261, seen=480, correct=288, accuracy=0.600000
2025-10-09 10:40:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:40:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:40:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:40:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=30 reserved=2078MB allocated=1946MB
2025-10-09 10:40:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 30, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.8148467540741, 'train_avg_loss': 0.6567903896172841, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 10:40:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 30, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.00518798828125, 'train_avg_loss': 0.6562608083089193, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 10:40:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 30, 'Results_raw': {'train_total': 480, 'train_loss': 315.00518798828125, 'train_avg_loss': 0.6562608083089193, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 10:40:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:40:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:40:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #30, planning to set LR to 1.00e-05
2025-10-09 10:40:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 10:40:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:40:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:40:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:40:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:40:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:40:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:40:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.855957, avg_loss=0.626783, seen=480, correct=303, accuracy=0.631250
2025-10-09 10:40:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:40:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:41:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:41:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=30 reserved=2078MB allocated=1946MB
2025-10-09 10:41:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 30, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.21574348211288, 'train_avg_loss': 0.610131195684274, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 10:41:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 30, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.85595703125, 'train_avg_loss': 0.6267832438151042, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 10:41:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 30, 'Results_raw': {'train_total': 480, 'train_loss': 300.85595703125, 'train_avg_loss': 0.6267832438151042, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 10:41:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:41:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:41:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #30, planning to set LR to 1.00e-05
2025-10-09 10:41:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 10:41:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:41:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:41:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:41:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:41:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:41:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:41:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.241760, avg_loss=0.648420, seen=480, correct=289, accuracy=0.602083
2025-10-09 10:41:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:41:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:41:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=30 reserved=2088MB allocated=1946MB
2025-10-09 10:41:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 30, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.19583308696747, 'train_avg_loss': 0.6849652757247289, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 10:41:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 30, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.24176025390625, 'train_avg_loss': 0.6484203338623047, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 10:41:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 30, 'Results_raw': {'train_total': 480, 'train_loss': 311.24176025390625, 'train_avg_loss': 0.6484203338623047, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 10:41:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:41:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:41:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #30, planning to set LR to 1.00e-05
2025-10-09 10:41:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 10:41:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:41:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:41:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:41:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:41:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:42:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:42:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.579773, avg_loss=0.626208, seen=480, correct=309, accuracy=0.643750
2025-10-09 10:42:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:42:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:42:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=30 reserved=2078MB allocated=1946MB
2025-10-09 10:42:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 30, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.13437533378601, 'train_avg_loss': 0.6427864611148835, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 10:42:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 30, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.57977294921875, 'train_avg_loss': 0.6262078603108724, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 10:42:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 30, 'Results_raw': {'train_total': 480, 'train_loss': 300.57977294921875, 'train_avg_loss': 0.6262078603108724, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 10:42:29 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #31) -------------
2025-10-09 10:42:30 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=31 aidx=0 | s=5 (candidates=21)
2025-10-09 10:42:30 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[47, 15, 6, 1, 46] (from 21)
2025-10-09 10:42:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:42:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:42:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #31, planning to set LR to 1.00e-05
2025-10-09 10:42:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 10:42:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:42:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:42:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:42:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:42:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:43:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:43:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.256409, avg_loss=0.654701, seen=480, correct=302, accuracy=0.629167
2025-10-09 10:43:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:43:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:43:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:43:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=31 reserved=2078MB allocated=1946MB
2025-10-09 10:43:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 31, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.96483051776886, 'train_avg_loss': 0.6413735876480738, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 10:43:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 31, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.25640869140625, 'train_avg_loss': 0.6547008514404297, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 10:43:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 31, 'Results_raw': {'train_total': 480, 'train_loss': 314.25640869140625, 'train_avg_loss': 0.6547008514404297, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 10:43:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:43:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:43:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #31, planning to set LR to 1.00e-05
2025-10-09 10:43:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 10:43:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:43:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:43:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:43:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:43:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:43:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:43:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.857147, avg_loss=0.668452, seen=480, correct=291, accuracy=0.606250
2025-10-09 10:43:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:43:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:43:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:43:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=31 reserved=2078MB allocated=1946MB
2025-10-09 10:43:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 31, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.3858430981636, 'train_avg_loss': 0.69488202581803, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 10:43:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 31, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.8571472167969, 'train_avg_loss': 0.6684523900349935, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 10:43:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 31, 'Results_raw': {'train_total': 480, 'train_loss': 320.8571472167969, 'train_avg_loss': 0.6684523900349935, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 10:43:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:44:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:44:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #31, planning to set LR to 1.00e-05
2025-10-09 10:44:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 10:44:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:44:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:44:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:44:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:44:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:44:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:44:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.409241, avg_loss=0.661269, seen=480, correct=282, accuracy=0.587500
2025-10-09 10:44:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:44:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:44:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:44:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=31 reserved=2086MB allocated=1946MB
2025-10-09 10:44:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 31, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.56164544820786, 'train_avg_loss': 0.6463470454017322, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 10:44:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 31, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.40924072265625, 'train_avg_loss': 0.6612692515055338, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 10:44:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 31, 'Results_raw': {'train_total': 480, 'train_loss': 317.40924072265625, 'train_avg_loss': 0.6612692515055338, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 10:44:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:44:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:44:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #31, planning to set LR to 1.00e-05
2025-10-09 10:44:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 10:44:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:44:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:44:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:44:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:44:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:45:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:45:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.926727, avg_loss=0.689431, seen=480, correct=268, accuracy=0.558333
2025-10-09 10:45:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:45:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:45:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:45:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=31 reserved=2078MB allocated=1946MB
2025-10-09 10:45:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 31, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.78356146812439, 'train_avg_loss': 0.6815296789010366, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 10:45:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 31, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.9267272949219, 'train_avg_loss': 0.6894306818644206, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 10:45:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 31, 'Results_raw': {'train_total': 480, 'train_loss': 330.9267272949219, 'train_avg_loss': 0.6894306818644206, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 10:45:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:45:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:45:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #31, planning to set LR to 1.00e-05
2025-10-09 10:45:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 10:45:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:45:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:45:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:45:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:45:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:46:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:46:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.654114, avg_loss=0.678446, seen=480, correct=278, accuracy=0.579167
2025-10-09 10:46:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:46:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:46:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:46:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=31 reserved=2106MB allocated=1946MB
2025-10-09 10:46:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 31, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.3641928434372, 'train_avg_loss': 0.6613682736953099, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 10:46:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 31, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.65411376953125, 'train_avg_loss': 0.6784460703531902, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 10:46:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 31, 'Results_raw': {'train_total': 480, 'train_loss': 325.65411376953125, 'train_avg_loss': 0.6784460703531902, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 10:46:09 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #32) -------------
2025-10-09 10:46:10 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=32 aidx=0 | s=5 (candidates=21)
2025-10-09 10:46:10 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[51, 34, 19, 43, 28] (from 21)
2025-10-09 10:46:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:46:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:46:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #32, planning to set LR to 1.00e-05
2025-10-09 10:46:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 10:46:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:46:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:46:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:46:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:46:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:46:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:46:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.183777, avg_loss=0.646216, seen=480, correct=285, accuracy=0.593750
2025-10-09 10:46:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:46:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:46:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:46:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=32 reserved=2088MB allocated=1946MB
2025-10-09 10:46:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 32, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.38478076457977, 'train_avg_loss': 0.6865398397048315, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 10:46:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 32, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.18377685546875, 'train_avg_loss': 0.6462162017822266, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 10:46:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 32, 'Results_raw': {'train_total': 480, 'train_loss': 310.18377685546875, 'train_avg_loss': 0.6462162017822266, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 10:46:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:46:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:46:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #32, planning to set LR to 1.00e-05
2025-10-09 10:46:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 10:46:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:46:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:46:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:46:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:46:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:47:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:47:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.162048, avg_loss=0.677421, seen=480, correct=287, accuracy=0.597917
2025-10-09 10:47:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:47:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:47:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:47:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=32 reserved=2078MB allocated=1946MB
2025-10-09 10:47:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 32, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.0206179022789, 'train_avg_loss': 0.6918384825189908, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 10:47:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 32, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.16204833984375, 'train_avg_loss': 0.6774209340413412, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 10:47:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 32, 'Results_raw': {'train_total': 480, 'train_loss': 325.16204833984375, 'train_avg_loss': 0.6774209340413412, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 10:47:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:47:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:47:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #32, planning to set LR to 1.00e-05
2025-10-09 10:47:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 10:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:47:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:47:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:47:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:47:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:48:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:48:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.294373, avg_loss=0.638113, seen=480, correct=298, accuracy=0.620833
2025-10-09 10:48:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:48:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:48:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:48:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=32 reserved=2188MB allocated=1946MB
2025-10-09 10:48:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 32, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.67128229141235, 'train_avg_loss': 0.6139273524284363, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 10:48:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 32, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.29437255859375, 'train_avg_loss': 0.638113276163737, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 10:48:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 32, 'Results_raw': {'train_total': 480, 'train_loss': 306.29437255859375, 'train_avg_loss': 0.638113276163737, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 10:48:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:48:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:48:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #32, planning to set LR to 1.00e-05
2025-10-09 10:48:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 10:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:48:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:48:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:48:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:48:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:49:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:49:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.609650, avg_loss=0.632520, seen=480, correct=308, accuracy=0.641667
2025-10-09 10:49:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:49:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:49:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:49:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=32 reserved=2078MB allocated=1946MB
2025-10-09 10:49:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 32, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.69514900445938, 'train_avg_loss': 0.6641262417038282, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 10:49:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 32, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.6096496582031, 'train_avg_loss': 0.6325201034545899, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 10:49:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 32, 'Results_raw': {'train_total': 480, 'train_loss': 303.6096496582031, 'train_avg_loss': 0.6325201034545899, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 10:49:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:49:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:49:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #32, planning to set LR to 1.00e-05
2025-10-09 10:49:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 10:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:49:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:49:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:49:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:49:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:49:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:49:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.995911, avg_loss=0.652075, seen=480, correct=296, accuracy=0.616667
2025-10-09 10:49:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:49:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:49:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:49:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=32 reserved=2078MB allocated=1946MB
2025-10-09 10:49:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 32, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.88808935880661, 'train_avg_loss': 0.6490674113233884, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 10:49:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 32, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.99591064453125, 'train_avg_loss': 0.6520748138427734, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 10:49:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 32, 'Results_raw': {'train_total': 480, 'train_loss': 312.99591064453125, 'train_avg_loss': 0.6520748138427734, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 10:49:51 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #33) -------------
2025-10-09 10:49:52 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=33 aidx=0 | s=5 (candidates=21)
2025-10-09 10:49:52 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[48, 22, 45, 6, 39] (from 21)
2025-10-09 10:49:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:49:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:49:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #33, planning to set LR to 1.00e-05
2025-10-09 10:49:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 10:49:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:49:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:49:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:49:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:49:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:50:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:50:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.267670, avg_loss=0.652641, seen=480, correct=290, accuracy=0.604167
2025-10-09 10:50:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:50:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:50:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:50:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=33 reserved=2078MB allocated=1946MB
2025-10-09 10:50:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 33, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.21789103746414, 'train_avg_loss': 0.7184824253122012, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 10:50:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 33, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.2676696777344, 'train_avg_loss': 0.6526409784952799, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 10:50:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 33, 'Results_raw': {'train_total': 480, 'train_loss': 313.2676696777344, 'train_avg_loss': 0.6526409784952799, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 10:50:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:50:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:50:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #33, planning to set LR to 1.00e-05
2025-10-09 10:50:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 10:50:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:50:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:50:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:50:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:50:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:51:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:51:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=289.541779, avg_loss=0.603212, seen=480, correct=330, accuracy=0.687500
2025-10-09 10:51:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:51:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:51:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=33 reserved=2086MB allocated=1946MB
2025-10-09 10:51:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 33, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.8851210474968, 'train_avg_loss': 0.6407093420624733, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 10:51:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 33, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 289.5417785644531, 'train_avg_loss': 0.603212038675944, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 10:51:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 33, 'Results_raw': {'train_total': 480, 'train_loss': 289.5417785644531, 'train_avg_loss': 0.603212038675944, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 10:51:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:51:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:51:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #33, planning to set LR to 1.00e-05
2025-10-09 10:51:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 10:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:51:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:51:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:51:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:51:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:52:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:52:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.508179, avg_loss=0.657309, seen=480, correct=294, accuracy=0.612500
2025-10-09 10:52:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:52:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:52:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:52:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=33 reserved=2078MB allocated=1946MB
2025-10-09 10:52:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 33, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.52314001321793, 'train_avg_loss': 0.6543595001101494, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 10:52:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 33, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.5081787109375, 'train_avg_loss': 0.6573087056477864, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 10:52:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 33, 'Results_raw': {'train_total': 480, 'train_loss': 315.5081787109375, 'train_avg_loss': 0.6573087056477864, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 10:52:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:52:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:52:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #33, planning to set LR to 1.00e-05
2025-10-09 10:52:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 10:52:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:52:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:52:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:52:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:52:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:52:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:52:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.190369, avg_loss=0.658730, seen=480, correct=285, accuracy=0.593750
2025-10-09 10:52:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:52:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:52:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:52:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=33 reserved=2086MB allocated=1946MB
2025-10-09 10:52:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 33, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.98482573032379, 'train_avg_loss': 0.6415402144193649, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 10:52:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 33, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.19036865234375, 'train_avg_loss': 0.6587299346923828, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 10:52:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 33, 'Results_raw': {'train_total': 480, 'train_loss': 316.19036865234375, 'train_avg_loss': 0.6587299346923828, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 10:52:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:52:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:52:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #33, planning to set LR to 1.00e-05
2025-10-09 10:52:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 10:52:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:52:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:52:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:52:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:52:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:53:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:53:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.093781, avg_loss=0.668945, seen=480, correct=294, accuracy=0.612500
2025-10-09 10:53:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:53:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:53:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:53:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=33 reserved=2078MB allocated=1946MB
2025-10-09 10:53:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 33, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.92141342163086, 'train_avg_loss': 0.6576784451802572, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 10:53:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 33, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.0937805175781, 'train_avg_loss': 0.6689453760782877, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 10:53:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 33, 'Results_raw': {'train_total': 480, 'train_loss': 321.0937805175781, 'train_avg_loss': 0.6689453760782877, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 10:53:28 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #34) -------------
2025-10-09 10:53:29 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=34 aidx=0 | s=5 (candidates=21)
2025-10-09 10:53:29 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[29, 45, 26, 16, 22] (from 21)
2025-10-09 10:53:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:53:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:53:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #34, planning to set LR to 1.00e-05
2025-10-09 10:53:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 10:53:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:53:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:53:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:53:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:53:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:54:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:54:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.133514, avg_loss=0.658611, seen=480, correct=277, accuracy=0.577083
2025-10-09 10:54:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:54:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:54:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:54:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=34 reserved=2084MB allocated=1946MB
2025-10-09 10:54:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 34, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.76703536510468, 'train_avg_loss': 0.6563919613758723, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 10:54:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 34, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.1335144042969, 'train_avg_loss': 0.6586114883422851, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 10:54:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 34, 'Results_raw': {'train_total': 480, 'train_loss': 316.1335144042969, 'train_avg_loss': 0.6586114883422851, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 10:54:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:54:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:54:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #34, planning to set LR to 1.00e-05
2025-10-09 10:54:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 10:54:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:54:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:54:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:54:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:54:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:54:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:54:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.877777, avg_loss=0.658079, seen=480, correct=285, accuracy=0.593750
2025-10-09 10:54:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:54:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:54:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:54:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=34 reserved=2078MB allocated=1946MB
2025-10-09 10:54:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 34, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.54522097110748, 'train_avg_loss': 0.6628768414258956, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 10:54:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 34, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.8777770996094, 'train_avg_loss': 0.6580787022908529, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 10:54:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 34, 'Results_raw': {'train_total': 480, 'train_loss': 315.8777770996094, 'train_avg_loss': 0.6580787022908529, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 10:54:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:55:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:55:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #34, planning to set LR to 1.00e-05
2025-10-09 10:55:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 10:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:55:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:55:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:55:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:55:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:55:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:55:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.975494, avg_loss=0.664532, seen=480, correct=293, accuracy=0.610417
2025-10-09 10:55:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:55:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:55:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:55:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=34 reserved=2078MB allocated=1946MB
2025-10-09 10:55:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 34, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.57859036326408, 'train_avg_loss': 0.6464882530272007, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 10:55:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 34, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.9754943847656, 'train_avg_loss': 0.6645322799682617, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 10:55:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 34, 'Results_raw': {'train_total': 480, 'train_loss': 318.9754943847656, 'train_avg_loss': 0.6645322799682617, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 10:55:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:55:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:55:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #34, planning to set LR to 1.00e-05
2025-10-09 10:55:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 10:55:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:55:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:55:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:55:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:55:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:56:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:56:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.498840, avg_loss=0.644789, seen=480, correct=307, accuracy=0.639583
2025-10-09 10:56:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:56:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:56:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:56:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=34 reserved=2106MB allocated=1946MB
2025-10-09 10:56:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 34, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.68001478910446, 'train_avg_loss': 0.6056667899092039, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 10:56:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 34, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.49884033203125, 'train_avg_loss': 0.6447892506917318, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 10:56:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 34, 'Results_raw': {'train_total': 480, 'train_loss': 309.49884033203125, 'train_avg_loss': 0.6447892506917318, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 10:56:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:56:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:56:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #34, planning to set LR to 1.00e-05
2025-10-09 10:56:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 10:56:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:56:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:56:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:56:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:56:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:57:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:57:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=289.224548, avg_loss=0.602551, seen=480, correct=334, accuracy=0.695833
2025-10-09 10:57:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:57:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:57:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:57:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=34 reserved=2086MB allocated=1946MB
2025-10-09 10:57:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 34, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.32331997156143, 'train_avg_loss': 0.6360276664296786, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 10:57:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 34, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 289.22454833984375, 'train_avg_loss': 0.6025511423746744, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 10:57:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 34, 'Results_raw': {'train_total': 480, 'train_loss': 289.22454833984375, 'train_avg_loss': 0.6025511423746744, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 10:57:06 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #35) -------------
2025-10-09 10:57:07 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=35 aidx=0 | s=5 (candidates=21)
2025-10-09 10:57:07 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[48, 13, 40, 20, 46] (from 21)
2025-10-09 10:57:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:57:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:57:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #35, planning to set LR to 1.00e-05
2025-10-09 10:57:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 10:57:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:57:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:57:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:57:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:57:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:57:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:57:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.991364, avg_loss=0.660399, seen=480, correct=290, accuracy=0.604167
2025-10-09 10:57:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:57:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:57:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:57:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=35 reserved=2078MB allocated=1946MB
2025-10-09 10:57:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 35, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.76195615530014, 'train_avg_loss': 0.7313496346275011, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 10:57:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 35, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.9913635253906, 'train_avg_loss': 0.6603986740112304, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 10:57:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 35, 'Results_raw': {'train_total': 480, 'train_loss': 316.9913635253906, 'train_avg_loss': 0.6603986740112304, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 10:57:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:57:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:57:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #35, planning to set LR to 1.00e-05
2025-10-09 10:57:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 10:57:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:57:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:57:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:57:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:57:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:58:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:58:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=299.388306, avg_loss=0.623726, seen=480, correct=313, accuracy=0.652083
2025-10-09 10:58:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:58:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:58:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=35 reserved=2078MB allocated=1946MB
2025-10-09 10:58:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 35, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.26056325435638, 'train_avg_loss': 0.6355046937863033, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 10:58:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 35, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 299.3883056640625, 'train_avg_loss': 0.6237256368001302, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 10:58:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 35, 'Results_raw': {'train_total': 480, 'train_loss': 299.3883056640625, 'train_avg_loss': 0.6237256368001302, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 10:58:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:58:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:58:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #35, planning to set LR to 1.00e-05
2025-10-09 10:58:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 10:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:58:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:58:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:58:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:58:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:59:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:59:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.479736, avg_loss=0.678083, seen=480, correct=282, accuracy=0.587500
2025-10-09 10:59:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:59:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:59:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 10:59:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=35 reserved=2078MB allocated=1946MB
2025-10-09 10:59:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 35, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.16706734895706, 'train_avg_loss': 0.7097255612413088, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 10:59:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 35, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.479736328125, 'train_avg_loss': 0.6780827840169271, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 10:59:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 35, 'Results_raw': {'train_total': 480, 'train_loss': 325.479736328125, 'train_avg_loss': 0.6780827840169271, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 10:59:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 10:59:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 10:59:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #35, planning to set LR to 1.00e-05
2025-10-09 10:59:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 10:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 10:59:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 10:59:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 10:59:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 10:59:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 10:59:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 10:59:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=291.583740, avg_loss=0.607466, seen=480, correct=317, accuracy=0.660417
2025-10-09 10:59:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 10:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:00:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:00:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=35 reserved=2078MB allocated=1946MB
2025-10-09 11:00:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 35, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.0181959271431, 'train_avg_loss': 0.6251516327261925, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 11:00:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 35, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 291.583740234375, 'train_avg_loss': 0.6074661254882813, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 11:00:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 35, 'Results_raw': {'train_total': 480, 'train_loss': 291.583740234375, 'train_avg_loss': 0.6074661254882813, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 11:00:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:00:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:00:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #35, planning to set LR to 1.00e-05
2025-10-09 11:00:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 11:00:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:00:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:00:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:00:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:00:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:00:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:00:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.811523, avg_loss=0.680857, seen=480, correct=275, accuracy=0.572917
2025-10-09 11:00:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:00:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:00:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:00:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=35 reserved=2104MB allocated=1946MB
2025-10-09 11:00:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 35, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.08163750171661, 'train_avg_loss': 0.6506803125143051, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 11:00:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 35, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.8115234375, 'train_avg_loss': 0.6808573404947916, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 11:00:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 35, 'Results_raw': {'train_total': 480, 'train_loss': 326.8115234375, 'train_avg_loss': 0.6808573404947916, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 11:00:45 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #36) -------------
2025-10-09 11:00:45 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=36 aidx=0 | s=5 (candidates=21)
2025-10-09 11:00:45 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[39, 15, 46, 6, 16] (from 21)
2025-10-09 11:00:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:00:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:00:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #36, planning to set LR to 1.00e-05
2025-10-09 11:00:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 11:00:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:00:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:00:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:00:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:00:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:01:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:01:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.163818, avg_loss=0.662841, seen=480, correct=303, accuracy=0.631250
2025-10-09 11:01:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:01:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:01:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=36 reserved=2078MB allocated=1946MB
2025-10-09 11:01:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 36, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.308758020401, 'train_avg_loss': 0.6525729835033417, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 11:01:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 36, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.163818359375, 'train_avg_loss': 0.6628412882486979, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 11:01:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 36, 'Results_raw': {'train_total': 480, 'train_loss': 318.163818359375, 'train_avg_loss': 0.6628412882486979, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 11:01:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:01:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:01:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #36, planning to set LR to 1.00e-05
2025-10-09 11:01:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 11:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:01:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:01:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:01:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:01:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:02:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:02:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.958374, avg_loss=0.668663, seen=480, correct=294, accuracy=0.612500
2025-10-09 11:02:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:02:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:02:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:02:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=36 reserved=2078MB allocated=1946MB
2025-10-09 11:02:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 36, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.25115102529526, 'train_avg_loss': 0.6937595918774605, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 11:02:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 36, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.9583740234375, 'train_avg_loss': 0.6686632792154948, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 11:02:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 36, 'Results_raw': {'train_total': 480, 'train_loss': 320.9583740234375, 'train_avg_loss': 0.6686632792154948, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 11:02:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:02:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:02:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #36, planning to set LR to 1.00e-05
2025-10-09 11:02:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 11:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:02:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:02:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:02:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:02:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:02:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:02:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.663910, avg_loss=0.663883, seen=480, correct=284, accuracy=0.591667
2025-10-09 11:02:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:02:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:02:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:02:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=36 reserved=2106MB allocated=1946MB
2025-10-09 11:02:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 36, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.3220525085926, 'train_avg_loss': 0.6360171042382717, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:02:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 36, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.6639099121094, 'train_avg_loss': 0.6638831456502279, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:02:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 36, 'Results_raw': {'train_total': 480, 'train_loss': 318.6639099121094, 'train_avg_loss': 0.6638831456502279, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:02:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:02:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:02:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #36, planning to set LR to 1.00e-05
2025-10-09 11:02:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 11:02:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:02:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:02:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:02:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:02:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:03:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:03:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.374359, avg_loss=0.654947, seen=480, correct=290, accuracy=0.604167
2025-10-09 11:03:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:03:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:03:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:03:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=36 reserved=2086MB allocated=1946MB
2025-10-09 11:03:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 36, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.16043734550476, 'train_avg_loss': 0.6346703112125397, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 11:03:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 36, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.3743591308594, 'train_avg_loss': 0.6549465815226237, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 11:03:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 36, 'Results_raw': {'train_total': 480, 'train_loss': 314.3743591308594, 'train_avg_loss': 0.6549465815226237, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 11:03:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:03:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:03:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #36, planning to set LR to 1.00e-05
2025-10-09 11:03:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 11:03:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:03:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:03:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:03:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:03:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:04:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:04:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.106567, avg_loss=0.643972, seen=480, correct=303, accuracy=0.631250
2025-10-09 11:04:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:04:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:04:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:04:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=36 reserved=2108MB allocated=1946MB
2025-10-09 11:04:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 36, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.71688222885132, 'train_avg_loss': 0.605974018573761, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 11:04:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 36, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.1065673828125, 'train_avg_loss': 0.6439720153808594, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 11:04:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 36, 'Results_raw': {'train_total': 480, 'train_loss': 309.1065673828125, 'train_avg_loss': 0.6439720153808594, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 11:04:15 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #37) -------------
2025-10-09 11:04:16 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=37 aidx=0 | s=5 (candidates=21)
2025-10-09 11:04:16 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[40, 34, 13, 45, 22] (from 21)
2025-10-09 11:04:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:04:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:04:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #37, planning to set LR to 1.00e-05
2025-10-09 11:04:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 11:04:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:04:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:04:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:04:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:04:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:04:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:04:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.333862, avg_loss=0.665279, seen=480, correct=284, accuracy=0.591667
2025-10-09 11:04:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:04:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:04:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=37 reserved=2078MB allocated=1946MB
2025-10-09 11:04:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 37, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.65961265563965, 'train_avg_loss': 0.6888301054636637, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 11:04:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 37, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.3338623046875, 'train_avg_loss': 0.6652788798014323, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:04:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 37, 'Results_raw': {'train_total': 480, 'train_loss': 319.3338623046875, 'train_avg_loss': 0.6652788798014323, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:04:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:05:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:05:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #37, planning to set LR to 1.00e-05
2025-10-09 11:05:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 11:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:05:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:05:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:05:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:05:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:05:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:05:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.545685, avg_loss=0.682387, seen=480, correct=283, accuracy=0.589583
2025-10-09 11:05:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:05:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:05:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=37 reserved=2078MB allocated=1946MB
2025-10-09 11:05:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 37, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.04579728841782, 'train_avg_loss': 0.6920483107368152, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 11:05:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 37, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.5456848144531, 'train_avg_loss': 0.682386843363444, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 11:05:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 37, 'Results_raw': {'train_total': 480, 'train_loss': 327.5456848144531, 'train_avg_loss': 0.682386843363444, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 11:05:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:05:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:05:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #37, planning to set LR to 1.00e-05
2025-10-09 11:05:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 11:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:05:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:05:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:05:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:05:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:06:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.956451, avg_loss=0.626993, seen=480, correct=309, accuracy=0.643750
2025-10-09 11:06:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:06:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:06:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=37 reserved=2078MB allocated=1946MB
2025-10-09 11:06:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 37, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.51016587018967, 'train_avg_loss': 0.6375847155849139, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 11:06:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 37, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.9564514160156, 'train_avg_loss': 0.6269926071166992, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 11:06:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 37, 'Results_raw': {'train_total': 480, 'train_loss': 300.9564514160156, 'train_avg_loss': 0.6269926071166992, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 11:06:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:06:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:06:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #37, planning to set LR to 1.00e-05
2025-10-09 11:06:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 11:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:06:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:06:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:06:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:06:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:07:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:07:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.492279, avg_loss=0.661442, seen=480, correct=284, accuracy=0.591667
2025-10-09 11:07:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:07:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:07:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=37 reserved=2078MB allocated=1946MB
2025-10-09 11:07:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 37, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.94097715616226, 'train_avg_loss': 0.6578414763013521, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 11:07:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 37, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.4922790527344, 'train_avg_loss': 0.66144224802653, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:07:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 37, 'Results_raw': {'train_total': 480, 'train_loss': 317.4922790527344, 'train_avg_loss': 0.66144224802653, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:07:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:07:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:07:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #37, planning to set LR to 1.00e-05
2025-10-09 11:07:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 11:07:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:07:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:07:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:07:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:07:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:07:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:07:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=277.565430, avg_loss=0.578261, seen=480, correct=345, accuracy=0.718750
2025-10-09 11:07:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:07:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:07:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=37 reserved=2086MB allocated=1946MB
2025-10-09 11:07:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 37, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.9546012878418, 'train_avg_loss': 0.6162883440653483, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 11:07:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 37, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 277.5654296875, 'train_avg_loss': 0.5782613118489583, 'train_seen': 480, 'train_correct': 345, 'train_acc': 0.71875}}
2025-10-09 11:07:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 37, 'Results_raw': {'train_total': 480, 'train_loss': 277.5654296875, 'train_avg_loss': 0.5782613118489583, 'train_seen': 480, 'train_correct': 345, 'train_acc': 0.71875}}
2025-10-09 11:07:51 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #38) -------------
2025-10-09 11:07:52 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=38 aidx=0 | s=5 (candidates=21)
2025-10-09 11:07:52 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[45, 47, 1, 46, 40] (from 21)
2025-10-09 11:07:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:07:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:07:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #38, planning to set LR to 1.00e-05
2025-10-09 11:07:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 11:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:07:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:07:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:07:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:07:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:08:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:08:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.002411, avg_loss=0.664588, seen=480, correct=286, accuracy=0.595833
2025-10-09 11:08:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:08:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:08:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:08:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=38 reserved=2078MB allocated=1946MB
2025-10-09 11:08:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 38, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.61569601297379, 'train_avg_loss': 0.6551308001081149, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 11:08:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 38, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.0024108886719, 'train_avg_loss': 0.6645883560180664, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 11:08:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 38, 'Results_raw': {'train_total': 480, 'train_loss': 319.0024108886719, 'train_avg_loss': 0.6645883560180664, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 11:08:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:08:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:08:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #38, planning to set LR to 1.00e-05
2025-10-09 11:08:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 11:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:08:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:08:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:08:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:08:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:09:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:09:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.911682, avg_loss=0.660233, seen=480, correct=294, accuracy=0.612500
2025-10-09 11:09:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:09:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:09:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=38 reserved=2078MB allocated=1946MB
2025-10-09 11:09:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 38, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.73569992184639, 'train_avg_loss': 0.6477974993487199, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 11:09:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 38, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.91168212890625, 'train_avg_loss': 0.660232671101888, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 11:09:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 38, 'Results_raw': {'train_total': 480, 'train_loss': 316.91168212890625, 'train_avg_loss': 0.660232671101888, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 11:09:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:09:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:09:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #38, planning to set LR to 1.00e-05
2025-10-09 11:09:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 11:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:09:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:09:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:09:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:09:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:10:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:10:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.187805, avg_loss=0.698308, seen=480, correct=272, accuracy=0.566667
2025-10-09 11:10:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:10:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:10:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:10:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=38 reserved=2078MB allocated=1946MB
2025-10-09 11:10:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 38, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.66080838441849, 'train_avg_loss': 0.6971734032034874, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 11:10:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 38, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.18780517578125, 'train_avg_loss': 0.6983079274495443, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 11:10:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 38, 'Results_raw': {'train_total': 480, 'train_loss': 335.18780517578125, 'train_avg_loss': 0.6983079274495443, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 11:10:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:10:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:10:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #38, planning to set LR to 1.00e-05
2025-10-09 11:10:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 11:10:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:10:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:10:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:10:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:10:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:10:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:10:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.175873, avg_loss=0.673283, seen=480, correct=284, accuracy=0.591667
2025-10-09 11:10:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:10:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:10:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:10:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=38 reserved=2106MB allocated=1946MB
2025-10-09 11:10:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 38, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.20752793550491, 'train_avg_loss': 0.6350627327958743, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 11:10:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 38, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.1758728027344, 'train_avg_loss': 0.67328306833903, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:10:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 38, 'Results_raw': {'train_total': 480, 'train_loss': 323.1758728027344, 'train_avg_loss': 0.67328306833903, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:10:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:10:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:10:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #38, planning to set LR to 1.00e-05
2025-10-09 11:10:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 11:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:10:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:10:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:10:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:10:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:11:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:11:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.059174, avg_loss=0.679290, seen=480, correct=282, accuracy=0.587500
2025-10-09 11:11:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:11:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:11:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=38 reserved=2078MB allocated=1946MB
2025-10-09 11:11:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 38, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.05806535482407, 'train_avg_loss': 0.7171505446235339, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 11:11:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 38, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.0591735839844, 'train_avg_loss': 0.6792899449666341, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 11:11:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 38, 'Results_raw': {'train_total': 480, 'train_loss': 326.0591735839844, 'train_avg_loss': 0.6792899449666341, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 11:11:29 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #39) -------------
2025-10-09 11:11:29 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=39 aidx=0 | s=5 (candidates=21)
2025-10-09 11:11:29 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[48, 13, 10, 43, 46] (from 21)
2025-10-09 11:11:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:11:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:11:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #39, planning to set LR to 1.00e-05
2025-10-09 11:11:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 11:11:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:11:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:11:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:11:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:11:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:12:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:12:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.034698, avg_loss=0.654239, seen=480, correct=287, accuracy=0.597917
2025-10-09 11:12:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:12:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:12:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:12:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=39 reserved=2078MB allocated=1946MB
2025-10-09 11:12:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 39, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.80304509401321, 'train_avg_loss': 0.7316920424501101, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 11:12:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 39, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.0346984863281, 'train_avg_loss': 0.6542389551798503, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 11:12:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 39, 'Results_raw': {'train_total': 480, 'train_loss': 314.0346984863281, 'train_avg_loss': 0.6542389551798503, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 11:12:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:12:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:12:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #39, planning to set LR to 1.00e-05
2025-10-09 11:12:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 11:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:12:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:12:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:12:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:12:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:12:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.613159, avg_loss=0.628361, seen=480, correct=308, accuracy=0.641667
2025-10-09 11:12:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:12:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:12:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=39 reserved=2078MB allocated=1946MB
2025-10-09 11:12:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 39, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.49317193031311, 'train_avg_loss': 0.6457764327526092, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 11:12:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 39, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.6131591796875, 'train_avg_loss': 0.6283607482910156, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:12:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 39, 'Results_raw': {'train_total': 480, 'train_loss': 301.6131591796875, 'train_avg_loss': 0.6283607482910156, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:12:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:12:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:12:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #39, planning to set LR to 1.00e-05
2025-10-09 11:12:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 11:12:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:12:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:12:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:12:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:12:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:13:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:13:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=297.538452, avg_loss=0.619872, seen=480, correct=307, accuracy=0.639583
2025-10-09 11:13:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:13:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:13:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=39 reserved=2078MB allocated=1946MB
2025-10-09 11:13:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 39, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.07162255048752, 'train_avg_loss': 0.600596854587396, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 11:13:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 39, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 297.5384521484375, 'train_avg_loss': 0.6198717753092448, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 11:13:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 39, 'Results_raw': {'train_total': 480, 'train_loss': 297.5384521484375, 'train_avg_loss': 0.6198717753092448, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 11:13:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:13:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:13:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #39, planning to set LR to 1.00e-05
2025-10-09 11:13:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 11:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:13:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:13:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:13:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:13:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:14:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:14:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.977295, avg_loss=0.633286, seen=480, correct=310, accuracy=0.645833
2025-10-09 11:14:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:14:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:14:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=39 reserved=2078MB allocated=1946MB
2025-10-09 11:14:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 39, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.62356960773468, 'train_avg_loss': 0.6635297467311223, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 11:14:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 39, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.977294921875, 'train_avg_loss': 0.6332860310872396, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 11:14:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 39, 'Results_raw': {'train_total': 480, 'train_loss': 303.977294921875, 'train_avg_loss': 0.6332860310872396, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 11:14:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:14:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:14:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #39, planning to set LR to 1.00e-05
2025-10-09 11:14:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 11:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:14:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:14:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:14:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:14:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:15:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:15:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.144226, avg_loss=0.660717, seen=480, correct=287, accuracy=0.597917
2025-10-09 11:15:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:15:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:15:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:15:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=39 reserved=2106MB allocated=1946MB
2025-10-09 11:15:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 39, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.55072003602982, 'train_avg_loss': 0.6379226669669151, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 11:15:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 39, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.14422607421875, 'train_avg_loss': 0.6607171376546224, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 11:15:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 39, 'Results_raw': {'train_total': 480, 'train_loss': 317.14422607421875, 'train_avg_loss': 0.6607171376546224, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 11:15:03 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #40) -------------
2025-10-09 11:15:03 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=40 aidx=0 | s=5 (candidates=21)
2025-10-09 11:15:03 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[29, 22, 10, 26, 39] (from 21)
2025-10-09 11:15:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:15:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:15:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #40, planning to set LR to 1.00e-05
2025-10-09 11:15:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 11:15:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:15:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:15:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:15:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:15:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:15:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:15:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.714172, avg_loss=0.659821, seen=480, correct=284, accuracy=0.591667
2025-10-09 11:15:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:15:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:15:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=40 reserved=2084MB allocated=1946MB
2025-10-09 11:15:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 40, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.3721125125885, 'train_avg_loss': 0.6447676042715709, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 11:15:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 40, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.71417236328125, 'train_avg_loss': 0.6598211924235026, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:15:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 40, 'Results_raw': {'train_total': 480, 'train_loss': 316.71417236328125, 'train_avg_loss': 0.6598211924235026, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 11:15:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:15:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:15:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #40, planning to set LR to 1.00e-05
2025-10-09 11:15:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 11:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:15:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:15:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:15:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:15:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:16:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:16:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=263.716553, avg_loss=0.549409, seen=480, correct=357, accuracy=0.743750
2025-10-09 11:16:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:16:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:16:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=40 reserved=2086MB allocated=1946MB
2025-10-09 11:16:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 40, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.57081747055054, 'train_avg_loss': 0.5797568122545879, 'train_seen': 120, 'train_correct': 88, 'train_acc': 0.7333333333333333}}
2025-10-09 11:16:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 40, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 263.716552734375, 'train_avg_loss': 0.5494094848632812, 'train_seen': 480, 'train_correct': 357, 'train_acc': 0.74375}}
2025-10-09 11:16:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 40, 'Results_raw': {'train_total': 480, 'train_loss': 263.716552734375, 'train_avg_loss': 0.5494094848632812, 'train_seen': 480, 'train_correct': 357, 'train_acc': 0.74375}}
2025-10-09 11:16:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:16:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:16:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #40, planning to set LR to 1.00e-05
2025-10-09 11:16:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 11:16:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:16:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:16:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:16:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:16:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:17:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:17:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=295.844604, avg_loss=0.616343, seen=480, correct=308, accuracy=0.641667
2025-10-09 11:17:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:17:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:17:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:17:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=40 reserved=2078MB allocated=1946MB
2025-10-09 11:17:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 40, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.89326393604279, 'train_avg_loss': 0.5991105328003565, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 11:17:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 40, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 295.8446044921875, 'train_avg_loss': 0.6163429260253906, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:17:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 40, 'Results_raw': {'train_total': 480, 'train_loss': 295.8446044921875, 'train_avg_loss': 0.6163429260253906, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:17:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:17:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:17:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #40, planning to set LR to 1.00e-05
2025-10-09 11:17:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 11:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:17:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:17:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:17:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:17:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:17:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:17:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.951416, avg_loss=0.664482, seen=480, correct=290, accuracy=0.604167
2025-10-09 11:17:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:17:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:17:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:17:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=40 reserved=2078MB allocated=1946MB
2025-10-09 11:17:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 40, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.0004460811615, 'train_avg_loss': 0.6416703840096791, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 11:17:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 40, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.951416015625, 'train_avg_loss': 0.6644821166992188, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 11:17:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 40, 'Results_raw': {'train_total': 480, 'train_loss': 318.951416015625, 'train_avg_loss': 0.6644821166992188, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 11:17:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:17:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:17:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #40, planning to set LR to 1.00e-05
2025-10-09 11:17:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 11:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:17:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:17:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:17:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:17:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:18:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:18:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.621277, avg_loss=0.661711, seen=480, correct=306, accuracy=0.637500
2025-10-09 11:18:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:18:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:18:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=40 reserved=2078MB allocated=1946MB
2025-10-09 11:18:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 40, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.58606541156769, 'train_avg_loss': 0.6548838784297307, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 11:18:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 40, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.62127685546875, 'train_avg_loss': 0.6617109934488933, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 11:18:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 40, 'Results_raw': {'train_total': 480, 'train_loss': 317.62127685546875, 'train_avg_loss': 0.6617109934488933, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 11:18:39 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #41) -------------
2025-10-09 11:18:40 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=41 aidx=0 | s=5 (candidates=21)
2025-10-09 11:18:40 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[29, 10, 48, 28, 19] (from 21)
2025-10-09 11:18:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:18:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:18:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #41, planning to set LR to 1.00e-05
2025-10-09 11:18:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 11:18:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:18:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:18:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:18:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:18:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:19:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:19:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.549164, avg_loss=0.655311, seen=480, correct=289, accuracy=0.602083
2025-10-09 11:19:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:19:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:19:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=41 reserved=2084MB allocated=1946MB
2025-10-09 11:19:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 41, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.73525017499924, 'train_avg_loss': 0.647793751458327, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 11:19:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 41, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.5491638183594, 'train_avg_loss': 0.6553107579549153, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 11:19:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 41, 'Results_raw': {'train_total': 480, 'train_loss': 314.5491638183594, 'train_avg_loss': 0.6553107579549153, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 11:19:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:19:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:19:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #41, planning to set LR to 1.00e-05
2025-10-09 11:19:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 11:19:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:19:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:19:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:19:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:19:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:20:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:20:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=295.766876, avg_loss=0.616181, seen=480, correct=308, accuracy=0.641667
2025-10-09 11:20:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:20:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:20:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=41 reserved=2078MB allocated=1946MB
2025-10-09 11:20:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 41, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.13848829269409, 'train_avg_loss': 0.6011540691057841, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 11:20:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 41, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 295.7668762207031, 'train_avg_loss': 0.6161809921264648, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:20:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 41, 'Results_raw': {'train_total': 480, 'train_loss': 295.7668762207031, 'train_avg_loss': 0.6161809921264648, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:20:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:20:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:20:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #41, planning to set LR to 1.00e-05
2025-10-09 11:20:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 11:20:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:20:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:20:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:20:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:20:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:20:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:20:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.486328, avg_loss=0.653097, seen=480, correct=286, accuracy=0.595833
2025-10-09 11:20:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:20:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:20:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=41 reserved=2078MB allocated=1946MB
2025-10-09 11:20:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 41, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.05921733379364, 'train_avg_loss': 0.7254934777816137, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 11:20:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 41, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.486328125, 'train_avg_loss': 0.6530965169270834, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 11:20:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 41, 'Results_raw': {'train_total': 480, 'train_loss': 313.486328125, 'train_avg_loss': 0.6530965169270834, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 11:20:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:20:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:20:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #41, planning to set LR to 1.00e-05
2025-10-09 11:20:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 11:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:20:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:20:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:20:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:20:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:21:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:21:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.715393, avg_loss=0.636907, seen=480, correct=308, accuracy=0.641667
2025-10-09 11:21:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:21:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:21:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=41 reserved=2078MB allocated=1946MB
2025-10-09 11:21:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 41, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.82364165782928, 'train_avg_loss': 0.6401970138152441, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 11:21:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 41, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.71539306640625, 'train_avg_loss': 0.6369070688883464, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:21:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 41, 'Results_raw': {'train_total': 480, 'train_loss': 305.71539306640625, 'train_avg_loss': 0.6369070688883464, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:21:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:21:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:21:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #41, planning to set LR to 1.00e-05
2025-10-09 11:21:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 11:21:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:21:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:21:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:21:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:21:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:22:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:22:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.056763, avg_loss=0.633452, seen=480, correct=307, accuracy=0.639583
2025-10-09 11:22:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:22:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:22:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:22:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=41 reserved=2188MB allocated=1946MB
2025-10-09 11:22:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 41, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.23760968446732, 'train_avg_loss': 0.6103134140372276, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 11:22:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 41, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.0567626953125, 'train_avg_loss': 0.6334515889485677, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 11:22:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 41, 'Results_raw': {'train_total': 480, 'train_loss': 304.0567626953125, 'train_avg_loss': 0.6334515889485677, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 11:22:17 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #42) -------------
2025-10-09 11:22:17 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=42 aidx=0 | s=5 (candidates=21)
2025-10-09 11:22:17 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[26, 15, 39, 1, 22] (from 21)
2025-10-09 11:22:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:22:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:22:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #42, planning to set LR to 1.00e-05
2025-10-09 11:22:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 11:22:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:22:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:22:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:22:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:22:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:22:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:22:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.205994, avg_loss=0.658762, seen=480, correct=296, accuracy=0.616667
2025-10-09 11:22:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:23:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:23:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=42 reserved=2078MB allocated=1946MB
2025-10-09 11:23:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 42, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.72938027977943, 'train_avg_loss': 0.6394115023314952, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 11:23:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 42, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.20599365234375, 'train_avg_loss': 0.6587624867757161, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:23:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 42, 'Results_raw': {'train_total': 480, 'train_loss': 316.20599365234375, 'train_avg_loss': 0.6587624867757161, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:23:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:23:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:23:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #42, planning to set LR to 1.00e-05
2025-10-09 11:23:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 11:23:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:23:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:23:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:23:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:23:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:23:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:23:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.025574, avg_loss=0.666720, seen=480, correct=296, accuracy=0.616667
2025-10-09 11:23:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:23:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:23:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=42 reserved=2078MB allocated=1946MB
2025-10-09 11:23:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 42, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.6404384970665, 'train_avg_loss': 0.6886703208088875, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 11:23:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 42, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.02557373046875, 'train_avg_loss': 0.6667199452718099, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:23:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 42, 'Results_raw': {'train_total': 480, 'train_loss': 320.02557373046875, 'train_avg_loss': 0.6667199452718099, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:23:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:23:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:23:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #42, planning to set LR to 1.00e-05
2025-10-09 11:23:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 11:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:23:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:23:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:23:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:23:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:24:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:24:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.139771, avg_loss=0.650291, seen=480, correct=313, accuracy=0.652083
2025-10-09 11:24:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:24:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:24:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=42 reserved=2078MB allocated=1946MB
2025-10-09 11:24:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 42, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.04960858821869, 'train_avg_loss': 0.6420800715684891, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 11:24:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 42, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.1397705078125, 'train_avg_loss': 0.6502911885579427, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 11:24:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 42, 'Results_raw': {'train_total': 480, 'train_loss': 312.1397705078125, 'train_avg_loss': 0.6502911885579427, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 11:24:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:24:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:24:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #42, planning to set LR to 1.00e-05
2025-10-09 11:24:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 11:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:24:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:24:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:24:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:24:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:25:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:25:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.310303, avg_loss=0.683980, seen=480, correct=277, accuracy=0.577083
2025-10-09 11:25:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:25:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:25:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=42 reserved=2078MB allocated=1946MB
2025-10-09 11:25:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 42, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.09990483522415, 'train_avg_loss': 0.684165873626868, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 11:25:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 42, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.310302734375, 'train_avg_loss': 0.6839797973632813, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 11:25:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 42, 'Results_raw': {'train_total': 480, 'train_loss': 328.310302734375, 'train_avg_loss': 0.6839797973632813, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 11:25:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:25:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:25:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #42, planning to set LR to 1.00e-05
2025-10-09 11:25:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 11:25:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:25:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:25:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:25:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:25:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:25:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:25:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=262.937134, avg_loss=0.547786, seen=480, correct=357, accuracy=0.743750
2025-10-09 11:25:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:25:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:25:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:25:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=42 reserved=2086MB allocated=1946MB
2025-10-09 11:25:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 42, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.13777327537537, 'train_avg_loss': 0.5844814439614614, 'train_seen': 120, 'train_correct': 89, 'train_acc': 0.7416666666666667}}
2025-10-09 11:25:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 42, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 262.9371337890625, 'train_avg_loss': 0.5477856953938802, 'train_seen': 480, 'train_correct': 357, 'train_acc': 0.74375}}
2025-10-09 11:25:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 42, 'Results_raw': {'train_total': 480, 'train_loss': 262.9371337890625, 'train_avg_loss': 0.5477856953938802, 'train_seen': 480, 'train_correct': 357, 'train_acc': 0.74375}}
2025-10-09 11:25:54 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #43) -------------
2025-10-09 11:25:55 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=43 aidx=0 | s=5 (candidates=21)
2025-10-09 11:25:55 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[47, 28, 10, 46, 51] (from 21)
2025-10-09 11:25:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:25:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:25:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #43, planning to set LR to 1.00e-05
2025-10-09 11:25:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 11:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:25:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:25:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:25:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:25:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:26:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:26:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.736938, avg_loss=0.661952, seen=480, correct=297, accuracy=0.618750
2025-10-09 11:26:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:26:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:26:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:26:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=43 reserved=2078MB allocated=1946MB
2025-10-09 11:26:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 43, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.24652695655823, 'train_avg_loss': 0.6437210579713185, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:26:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 43, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.7369384765625, 'train_avg_loss': 0.6619519551595052, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 11:26:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 43, 'Results_raw': {'train_total': 480, 'train_loss': 317.7369384765625, 'train_avg_loss': 0.6619519551595052, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 11:26:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:26:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:26:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #43, planning to set LR to 1.00e-05
2025-10-09 11:26:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 11:26:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:26:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:26:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:26:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:26:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:27:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:27:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.829773, avg_loss=0.639229, seen=480, correct=309, accuracy=0.643750
2025-10-09 11:27:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:27:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:27:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=43 reserved=2078MB allocated=1946MB
2025-10-09 11:27:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 43, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.25825268030167, 'train_avg_loss': 0.6521521056691806, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 11:27:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 43, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.82977294921875, 'train_avg_loss': 0.6392286936442058, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 11:27:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 43, 'Results_raw': {'train_total': 480, 'train_loss': 306.82977294921875, 'train_avg_loss': 0.6392286936442058, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 11:27:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:27:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:27:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #43, planning to set LR to 1.00e-05
2025-10-09 11:27:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 11:27:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:27:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:27:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:27:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:27:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:28:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:28:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=296.167053, avg_loss=0.617015, seen=480, correct=312, accuracy=0.650000
2025-10-09 11:28:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:28:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:28:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:28:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=43 reserved=2078MB allocated=1946MB
2025-10-09 11:28:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 43, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.95381680130959, 'train_avg_loss': 0.5912818066775799, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 11:28:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 43, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 296.16705322265625, 'train_avg_loss': 0.6170146942138672, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 11:28:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 43, 'Results_raw': {'train_total': 480, 'train_loss': 296.16705322265625, 'train_avg_loss': 0.6170146942138672, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 11:28:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:28:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:28:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #43, planning to set LR to 1.00e-05
2025-10-09 11:28:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 11:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:28:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:28:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:28:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:28:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:28:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:28:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.108063, avg_loss=0.666892, seen=480, correct=293, accuracy=0.610417
2025-10-09 11:28:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:28:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:28:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:28:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=43 reserved=2104MB allocated=1946MB
2025-10-09 11:28:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 43, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.65592110157013, 'train_avg_loss': 0.6137993425130844, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 11:28:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 43, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.1080627441406, 'train_avg_loss': 0.6668917973836263, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 11:28:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 43, 'Results_raw': {'train_total': 480, 'train_loss': 320.1080627441406, 'train_avg_loss': 0.6668917973836263, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 11:28:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:28:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:28:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #43, planning to set LR to 1.00e-05
2025-10-09 11:28:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 11:28:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:28:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:28:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:28:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:28:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:29:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:29:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.820282, avg_loss=0.647542, seen=480, correct=291, accuracy=0.606250
2025-10-09 11:29:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:29:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:29:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:29:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=43 reserved=2088MB allocated=1946MB
2025-10-09 11:29:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 43, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.8810938000679, 'train_avg_loss': 0.7073424483338991, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 11:29:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 43, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.8202819824219, 'train_avg_loss': 0.6475422541300456, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 11:29:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 43, 'Results_raw': {'train_total': 480, 'train_loss': 310.8202819824219, 'train_avg_loss': 0.6475422541300456, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 11:29:25 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #44) -------------
2025-10-09 11:29:25 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=44 aidx=0 | s=5 (candidates=21)
2025-10-09 11:29:25 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[19, 10, 47, 40, 34] (from 21)
2025-10-09 11:29:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:29:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:29:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #44, planning to set LR to 1.00e-05
2025-10-09 11:29:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 11:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:29:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:29:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:29:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:29:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:30:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:30:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=296.962830, avg_loss=0.618673, seen=480, correct=314, accuracy=0.654167
2025-10-09 11:30:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:30:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:30:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:30:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=44 reserved=2188MB allocated=1946MB
2025-10-09 11:30:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 44, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.81862837076187, 'train_avg_loss': 0.6068219030896823, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 11:30:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 44, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 296.96282958984375, 'train_avg_loss': 0.6186725616455078, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 11:30:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 44, 'Results_raw': {'train_total': 480, 'train_loss': 296.96282958984375, 'train_avg_loss': 0.6186725616455078, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 11:30:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:30:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:30:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #44, planning to set LR to 1.00e-05
2025-10-09 11:30:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 11:30:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:30:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:30:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:30:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:30:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:30:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:30:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=297.529114, avg_loss=0.619852, seen=480, correct=305, accuracy=0.635417
2025-10-09 11:30:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:30:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:30:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:30:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=44 reserved=2078MB allocated=1946MB
2025-10-09 11:30:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 44, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.11734110116959, 'train_avg_loss': 0.5926445091764132, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 11:30:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 44, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 297.52911376953125, 'train_avg_loss': 0.6198523203531902, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 11:30:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 44, 'Results_raw': {'train_total': 480, 'train_loss': 297.52911376953125, 'train_avg_loss': 0.6198523203531902, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 11:30:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:30:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:30:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #44, planning to set LR to 1.00e-05
2025-10-09 11:30:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 11:30:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:30:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:30:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:30:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:30:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:31:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:31:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.289062, avg_loss=0.654769, seen=480, correct=296, accuracy=0.616667
2025-10-09 11:31:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:31:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:31:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:31:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=44 reserved=2078MB allocated=1946MB
2025-10-09 11:31:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 44, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.73328977823257, 'train_avg_loss': 0.6394440814852714, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:31:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 44, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.2890625, 'train_avg_loss': 0.6547688802083333, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:31:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 44, 'Results_raw': {'train_total': 480, 'train_loss': 314.2890625, 'train_avg_loss': 0.6547688802083333, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:31:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:31:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:31:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #44, planning to set LR to 1.00e-05
2025-10-09 11:31:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 11:31:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:31:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:31:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:31:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:31:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:32:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:32:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.076996, avg_loss=0.662660, seen=480, correct=286, accuracy=0.595833
2025-10-09 11:32:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:32:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:32:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:32:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=44 reserved=2078MB allocated=1946MB
2025-10-09 11:32:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 44, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.5835429430008, 'train_avg_loss': 0.6965295245250066, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 11:32:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 44, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.0769958496094, 'train_avg_loss': 0.6626604080200196, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 11:32:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 44, 'Results_raw': {'train_total': 480, 'train_loss': 318.0769958496094, 'train_avg_loss': 0.6626604080200196, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 11:32:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:32:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:32:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #44, planning to set LR to 1.00e-05
2025-10-09 11:32:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 11:32:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:32:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:32:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:32:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:32:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:32:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:32:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.545837, avg_loss=0.680304, seen=480, correct=294, accuracy=0.612500
2025-10-09 11:32:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:32:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:32:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:32:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=44 reserved=2078MB allocated=1946MB
2025-10-09 11:32:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 44, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.95702451467514, 'train_avg_loss': 0.6996418709556261, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 11:32:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 44, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.54583740234375, 'train_avg_loss': 0.6803038279215495, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 11:32:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 44, 'Results_raw': {'train_total': 480, 'train_loss': 326.54583740234375, 'train_avg_loss': 0.6803038279215495, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 11:32:56 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #45) -------------
2025-10-09 11:32:56 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=45 aidx=0 | s=5 (candidates=21)
2025-10-09 11:32:56 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[34, 10, 40, 15, 22] (from 21)
2025-10-09 11:32:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:32:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:32:58 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #45, planning to set LR to 1.00e-05
2025-10-09 11:32:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 11:32:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:32:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:32:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:32:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:32:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:33:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:33:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.863556, avg_loss=0.666382, seen=480, correct=295, accuracy=0.614583
2025-10-09 11:33:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:33:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:33:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:33:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=45 reserved=2078MB allocated=1946MB
2025-10-09 11:33:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 45, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.86156713962555, 'train_avg_loss': 0.6821797261635463, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:33:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 45, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.8635559082031, 'train_avg_loss': 0.6663824081420898, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 11:33:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 45, 'Results_raw': {'train_total': 480, 'train_loss': 319.8635559082031, 'train_avg_loss': 0.6663824081420898, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 11:33:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:33:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:33:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #45, planning to set LR to 1.00e-05
2025-10-09 11:33:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 11:33:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:33:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:33:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:33:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:33:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:34:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:34:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=297.293396, avg_loss=0.619361, seen=480, correct=304, accuracy=0.633333
2025-10-09 11:34:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:34:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:34:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:34:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=45 reserved=2078MB allocated=1946MB
2025-10-09 11:34:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 45, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.23247182369232, 'train_avg_loss': 0.5936039318641027, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 11:34:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 45, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 297.29339599609375, 'train_avg_loss': 0.6193612416585287, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 11:34:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 45, 'Results_raw': {'train_total': 480, 'train_loss': 297.29339599609375, 'train_avg_loss': 0.6193612416585287, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 11:34:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:34:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:34:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #45, planning to set LR to 1.00e-05
2025-10-09 11:34:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 11:34:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:34:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:34:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:34:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:34:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:34:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:34:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.414825, avg_loss=0.655031, seen=480, correct=301, accuracy=0.627083
2025-10-09 11:34:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:34:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:35:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:35:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=45 reserved=2078MB allocated=1946MB
2025-10-09 11:35:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 45, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.81170755624771, 'train_avg_loss': 0.6817642296353976, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 11:35:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 45, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.4148254394531, 'train_avg_loss': 0.655030886332194, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 11:35:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 45, 'Results_raw': {'train_total': 480, 'train_loss': 314.4148254394531, 'train_avg_loss': 0.655030886332194, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 11:35:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:35:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:35:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #45, planning to set LR to 1.00e-05
2025-10-09 11:35:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 11:35:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:35:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:35:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:35:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:35:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:35:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:35:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.980072, avg_loss=0.664542, seen=480, correct=296, accuracy=0.616667
2025-10-09 11:35:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:35:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:35:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:35:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=45 reserved=2078MB allocated=1946MB
2025-10-09 11:35:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 45, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.37697613239288, 'train_avg_loss': 0.6948081344366074, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 11:35:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 45, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.9800720214844, 'train_avg_loss': 0.6645418167114258, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:35:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 45, 'Results_raw': {'train_total': 480, 'train_loss': 318.9800720214844, 'train_avg_loss': 0.6645418167114258, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:35:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:35:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:35:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #45, planning to set LR to 1.00e-05
2025-10-09 11:35:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 11:35:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:35:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:35:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:35:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:35:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:36:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:36:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=251.771057, avg_loss=0.524523, seen=480, correct=370, accuracy=0.770833
2025-10-09 11:36:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:36:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:36:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:36:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=45 reserved=2086MB allocated=1946MB
2025-10-09 11:36:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 45, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 66.7463743686676, 'train_avg_loss': 0.5562197864055634, 'train_seen': 120, 'train_correct': 91, 'train_acc': 0.7583333333333333}}
2025-10-09 11:36:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 45, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 251.77105712890625, 'train_avg_loss': 0.5245230356852214, 'train_seen': 480, 'train_correct': 370, 'train_acc': 0.7708333333333334}}
2025-10-09 11:36:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 45, 'Results_raw': {'train_total': 480, 'train_loss': 251.77105712890625, 'train_avg_loss': 0.5245230356852214, 'train_seen': 480, 'train_correct': 370, 'train_acc': 0.7708333333333334}}
2025-10-09 11:36:28 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #46) -------------
2025-10-09 11:36:29 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=46 aidx=0 | s=5 (candidates=21)
2025-10-09 11:36:29 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[40, 34, 26, 46, 13] (from 21)
2025-10-09 11:36:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:36:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:36:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #46, planning to set LR to 1.00e-05
2025-10-09 11:36:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 11:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:36:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:36:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:36:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:36:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:37:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:37:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.035339, avg_loss=0.679240, seen=480, correct=291, accuracy=0.606250
2025-10-09 11:37:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:37:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:37:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:37:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=46 reserved=2078MB allocated=1946MB
2025-10-09 11:37:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 46, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.21898257732391, 'train_avg_loss': 0.7184915214776992, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 11:37:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 46, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.03533935546875, 'train_avg_loss': 0.6792402903238932, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 11:37:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 46, 'Results_raw': {'train_total': 480, 'train_loss': 326.03533935546875, 'train_avg_loss': 0.6792402903238932, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 11:37:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:37:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:37:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #46, planning to set LR to 1.00e-05
2025-10-09 11:37:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 11:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:37:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:37:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:37:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:37:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:37:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:37:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.424988, avg_loss=0.665469, seen=480, correct=302, accuracy=0.629167
2025-10-09 11:37:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:37:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:37:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:37:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=46 reserved=2078MB allocated=1946MB
2025-10-09 11:37:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 46, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.35182631015778, 'train_avg_loss': 0.6945985525846481, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 11:37:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 46, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.42498779296875, 'train_avg_loss': 0.6654687245686849, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 11:37:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 46, 'Results_raw': {'train_total': 480, 'train_loss': 319.42498779296875, 'train_avg_loss': 0.6654687245686849, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 11:37:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:37:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:37:58 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #46, planning to set LR to 1.00e-05
2025-10-09 11:37:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 11:37:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:37:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:37:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:37:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:37:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:38:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:38:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.043579, avg_loss=0.679257, seen=480, correct=283, accuracy=0.589583
2025-10-09 11:38:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:38:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:38:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:38:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=46 reserved=2078MB allocated=1946MB
2025-10-09 11:38:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 46, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.94828835129738, 'train_avg_loss': 0.6662357362608115, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:38:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 46, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.0435791015625, 'train_avg_loss': 0.6792574564615885, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 11:38:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 46, 'Results_raw': {'train_total': 480, 'train_loss': 326.0435791015625, 'train_avg_loss': 0.6792574564615885, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 11:38:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:38:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:38:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #46, planning to set LR to 1.00e-05
2025-10-09 11:38:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 11:38:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:38:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:38:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:38:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:38:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:39:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:39:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.933472, avg_loss=0.668611, seen=480, correct=287, accuracy=0.597917
2025-10-09 11:39:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:39:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:39:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=46 reserved=2106MB allocated=1946MB
2025-10-09 11:39:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 46, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.16002425551414, 'train_avg_loss': 0.6096668687959512, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 11:39:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 46, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.9334716796875, 'train_avg_loss': 0.6686113993326823, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 11:39:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 46, 'Results_raw': {'train_total': 480, 'train_loss': 320.9334716796875, 'train_avg_loss': 0.6686113993326823, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 11:39:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:39:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:39:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #46, planning to set LR to 1.00e-05
2025-10-09 11:39:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 11:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:39:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:39:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:39:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:39:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:40:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:40:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.634033, avg_loss=0.628404, seen=480, correct=312, accuracy=0.650000
2025-10-09 11:40:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:40:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:40:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=46 reserved=2078MB allocated=1946MB
2025-10-09 11:40:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 46, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.33510947227478, 'train_avg_loss': 0.6361259122689565, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:40:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 46, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.634033203125, 'train_avg_loss': 0.6284042358398437, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 11:40:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 46, 'Results_raw': {'train_total': 480, 'train_loss': 301.634033203125, 'train_avg_loss': 0.6284042358398437, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 11:40:04 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #47) -------------
2025-10-09 11:40:05 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=47 aidx=0 | s=5 (candidates=21)
2025-10-09 11:40:05 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[1, 47, 45, 16, 34] (from 21)
2025-10-09 11:40:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:40:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:40:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #47, planning to set LR to 1.00e-05
2025-10-09 11:40:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 11:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:40:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:40:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:40:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:40:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:40:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:40:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.178467, avg_loss=0.696205, seen=480, correct=276, accuracy=0.575000
2025-10-09 11:40:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:40:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:40:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=47 reserved=2078MB allocated=1946MB
2025-10-09 11:40:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 47, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.0578775703907, 'train_avg_loss': 0.6921489797532558, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 11:40:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 47, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.178466796875, 'train_avg_loss': 0.6962051391601562, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 11:40:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 47, 'Results_raw': {'train_total': 480, 'train_loss': 334.178466796875, 'train_avg_loss': 0.6962051391601562, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 11:40:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:40:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:40:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #47, planning to set LR to 1.00e-05
2025-10-09 11:40:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 11:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:40:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:40:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:40:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:40:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:41:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:41:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.225525, avg_loss=0.656720, seen=480, correct=296, accuracy=0.616667
2025-10-09 11:41:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:41:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:41:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=47 reserved=2078MB allocated=1946MB
2025-10-09 11:41:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 47, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.32865116000175, 'train_avg_loss': 0.6277387596666812, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 11:41:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 47, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.22552490234375, 'train_avg_loss': 0.6567198435465494, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:41:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 47, 'Results_raw': {'train_total': 480, 'train_loss': 315.22552490234375, 'train_avg_loss': 0.6567198435465494, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 11:41:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:41:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:41:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #47, planning to set LR to 1.00e-05
2025-10-09 11:41:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 11:41:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:41:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:41:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:41:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:41:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:42:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:42:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.189911, avg_loss=0.667062, seen=480, correct=297, accuracy=0.618750
2025-10-09 11:42:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:42:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:42:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=47 reserved=2078MB allocated=1946MB
2025-10-09 11:42:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 47, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.23243045806885, 'train_avg_loss': 0.6436035871505738, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:42:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 47, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.1899108886719, 'train_avg_loss': 0.6670623143513997, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 11:42:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 47, 'Results_raw': {'train_total': 480, 'train_loss': 320.1899108886719, 'train_avg_loss': 0.6670623143513997, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 11:42:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:42:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:42:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #47, planning to set LR to 1.00e-05
2025-10-09 11:42:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 11:42:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:42:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:42:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:42:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:42:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:42:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:42:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.854736, avg_loss=0.641364, seen=480, correct=311, accuracy=0.647917
2025-10-09 11:42:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:42:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:42:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=47 reserved=2106MB allocated=1946MB
2025-10-09 11:42:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 47, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.49110296368599, 'train_avg_loss': 0.5957591913640499, 'train_seen': 120, 'train_correct': 86, 'train_acc': 0.7166666666666667}}
2025-10-09 11:42:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 47, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.854736328125, 'train_avg_loss': 0.6413640340169271, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 11:42:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 47, 'Results_raw': {'train_total': 480, 'train_loss': 307.854736328125, 'train_avg_loss': 0.6413640340169271, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 11:42:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:42:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:42:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #47, planning to set LR to 1.00e-05
2025-10-09 11:42:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 11:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:42:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:42:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:42:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:42:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:43:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:43:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.263733, avg_loss=0.665133, seen=480, correct=308, accuracy=0.641667
2025-10-09 11:43:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:43:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:43:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:43:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=47 reserved=2078MB allocated=1946MB
2025-10-09 11:43:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 47, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.21357750892639, 'train_avg_loss': 0.6851131459077199, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 11:43:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 47, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.26373291015625, 'train_avg_loss': 0.6651327768961589, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:43:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 47, 'Results_raw': {'train_total': 480, 'train_loss': 319.26373291015625, 'train_avg_loss': 0.6651327768961589, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 11:43:39 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #48) -------------
2025-10-09 11:43:39 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=48 aidx=0 | s=5 (candidates=21)
2025-10-09 11:43:39 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[51, 40, 39, 19, 28] (from 21)
2025-10-09 11:43:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:43:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:43:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #48, planning to set LR to 1.00e-05
2025-10-09 11:43:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 11:43:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:43:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:43:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:43:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:43:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:44:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:44:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.731964, avg_loss=0.630692, seen=480, correct=302, accuracy=0.629167
2025-10-09 11:44:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:44:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:44:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=48 reserved=2088MB allocated=1946MB
2025-10-09 11:44:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 48, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.68958151340485, 'train_avg_loss': 0.680746512611707, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 11:44:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 48, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.7319641113281, 'train_avg_loss': 0.6306915918986002, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 11:44:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 48, 'Results_raw': {'train_total': 480, 'train_loss': 302.7319641113281, 'train_avg_loss': 0.6306915918986002, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 11:44:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:44:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:44:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #48, planning to set LR to 1.00e-05
2025-10-09 11:44:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 11:44:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:44:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:44:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:44:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:44:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:45:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:45:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.055725, avg_loss=0.658449, seen=480, correct=298, accuracy=0.620833
2025-10-09 11:45:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:45:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:45:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:45:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=48 reserved=2078MB allocated=1946MB
2025-10-09 11:45:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 48, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.42844623327255, 'train_avg_loss': 0.6869037186106046, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 11:45:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 48, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.05572509765625, 'train_avg_loss': 0.6584494272867839, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 11:45:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 48, 'Results_raw': {'train_total': 480, 'train_loss': 316.05572509765625, 'train_avg_loss': 0.6584494272867839, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 11:45:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:45:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:45:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #48, planning to set LR to 1.00e-05
2025-10-09 11:45:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 11:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:45:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:45:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:45:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:45:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:45:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:45:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.462158, avg_loss=0.659296, seen=480, correct=304, accuracy=0.633333
2025-10-09 11:45:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:45:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:45:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:45:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=48 reserved=2078MB allocated=1946MB
2025-10-09 11:45:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 48, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.84681424498558, 'train_avg_loss': 0.6320567853748799, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 11:45:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 48, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.462158203125, 'train_avg_loss': 0.659296162923177, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 11:45:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 48, 'Results_raw': {'train_total': 480, 'train_loss': 316.462158203125, 'train_avg_loss': 0.659296162923177, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 11:45:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:45:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:45:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #48, planning to set LR to 1.00e-05
2025-10-09 11:45:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 11:45:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:45:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:45:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:45:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:45:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:46:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:46:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=299.368652, avg_loss=0.623685, seen=480, correct=305, accuracy=0.635417
2025-10-09 11:46:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:46:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:46:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:46:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=48 reserved=2188MB allocated=1946MB
2025-10-09 11:46:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 48, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.2185508608818, 'train_avg_loss': 0.6184879238406817, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 11:46:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 48, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 299.36865234375, 'train_avg_loss': 0.6236846923828125, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 11:46:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 48, 'Results_raw': {'train_total': 480, 'train_loss': 299.36865234375, 'train_avg_loss': 0.6236846923828125, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 11:46:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:46:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:46:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #48, planning to set LR to 1.00e-05
2025-10-09 11:46:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 11:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:46:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:46:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:46:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:46:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:47:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:47:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.469391, avg_loss=0.638478, seen=480, correct=300, accuracy=0.625000
2025-10-09 11:47:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:47:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:47:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=48 reserved=2078MB allocated=1946MB
2025-10-09 11:47:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 48, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.30351856350899, 'train_avg_loss': 0.6525293213625749, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 11:47:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 48, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.4693908691406, 'train_avg_loss': 0.638477897644043, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 11:47:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 48, 'Results_raw': {'train_total': 480, 'train_loss': 306.4693908691406, 'train_avg_loss': 0.638477897644043, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 11:47:15 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #49) -------------
2025-10-09 11:47:16 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=49 aidx=0 | s=5 (candidates=21)
2025-10-09 11:47:16 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[46, 1, 34, 51, 43] (from 21)
2025-10-09 11:47:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:47:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:47:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #49, planning to set LR to 1.00e-05
2025-10-09 11:47:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 11:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:47:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:47:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:47:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:47:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:47:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:47:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.538086, avg_loss=0.655288, seen=480, correct=294, accuracy=0.612500
2025-10-09 11:47:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:47:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:47:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:48:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=49 reserved=2106MB allocated=1946MB
2025-10-09 11:48:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 49, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.72300624847412, 'train_avg_loss': 0.6226917187372843, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 11:48:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 49, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.5380859375, 'train_avg_loss': 0.6552876790364583, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 11:48:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 49, 'Results_raw': {'train_total': 480, 'train_loss': 314.5380859375, 'train_avg_loss': 0.6552876790364583, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 11:48:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:48:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:48:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #49, planning to set LR to 1.00e-05
2025-10-09 11:48:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 11:48:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:48:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:48:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:48:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:48:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:48:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:48:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.879700, avg_loss=0.689333, seen=480, correct=272, accuracy=0.566667
2025-10-09 11:48:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:48:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:48:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=49 reserved=2078MB allocated=1946MB
2025-10-09 11:48:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 49, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.68942856788635, 'train_avg_loss': 0.6890785713990529, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 11:48:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 49, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.87969970703125, 'train_avg_loss': 0.6893327077229817, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 11:48:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 49, 'Results_raw': {'train_total': 480, 'train_loss': 330.87969970703125, 'train_avg_loss': 0.6893327077229817, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 11:48:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:48:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:48:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #49, planning to set LR to 1.00e-05
2025-10-09 11:48:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 11:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:48:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:48:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:48:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:48:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:49:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:49:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.170990, avg_loss=0.646190, seen=480, correct=313, accuracy=0.652083
2025-10-09 11:49:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:49:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:49:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:49:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=49 reserved=2078MB allocated=1946MB
2025-10-09 11:49:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 49, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.25641059875488, 'train_avg_loss': 0.6688034216562907, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 11:49:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 49, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.1709899902344, 'train_avg_loss': 0.646189562479655, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 11:49:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 49, 'Results_raw': {'train_total': 480, 'train_loss': 310.1709899902344, 'train_avg_loss': 0.646189562479655, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 11:49:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:49:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:49:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #49, planning to set LR to 1.00e-05
2025-10-09 11:49:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 11:49:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:49:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:49:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:49:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:49:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:50:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:50:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.600647, avg_loss=0.632501, seen=480, correct=301, accuracy=0.627083
2025-10-09 11:50:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:50:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:50:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=49 reserved=2088MB allocated=1946MB
2025-10-09 11:50:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 49, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.19200837612152, 'train_avg_loss': 0.684933403134346, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 11:50:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 49, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.60064697265625, 'train_avg_loss': 0.6325013478597005, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 11:50:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 49, 'Results_raw': {'train_total': 480, 'train_loss': 303.60064697265625, 'train_avg_loss': 0.6325013478597005, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 11:50:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:50:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:50:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #49, planning to set LR to 1.00e-05
2025-10-09 11:50:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 11:50:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:50:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:50:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:50:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:50:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:50:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:50:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.075073, avg_loss=0.635573, seen=480, correct=311, accuracy=0.647917
2025-10-09 11:50:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:50:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:50:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:50:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=49 reserved=2078MB allocated=1946MB
2025-10-09 11:50:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 49, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.2638849914074, 'train_avg_loss': 0.660532374928395, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:50:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 49, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.0750732421875, 'train_avg_loss': 0.6355730692545573, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 11:50:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 49, 'Results_raw': {'train_total': 480, 'train_loss': 305.0750732421875, 'train_avg_loss': 0.6355730692545573, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 11:50:52 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #50) -------------
2025-10-09 11:50:53 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=50 aidx=0 | s=5 (candidates=21)
2025-10-09 11:50:53 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[46, 22, 28, 43, 6] (from 21)
2025-10-09 11:50:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:50:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:50:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #50, planning to set LR to 1.00e-05
2025-10-09 11:50:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 11:50:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:50:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:50:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:50:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:50:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:51:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:51:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.141937, avg_loss=0.654462, seen=480, correct=295, accuracy=0.614583
2025-10-09 11:51:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:51:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:51:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:51:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=50 reserved=2106MB allocated=1946MB
2025-10-09 11:51:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 50, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.47229760885239, 'train_avg_loss': 0.6122691467404365, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 11:51:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 50, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.1419372558594, 'train_avg_loss': 0.6544623692830404, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 11:51:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 50, 'Results_raw': {'train_total': 480, 'train_loss': 314.1419372558594, 'train_avg_loss': 0.6544623692830404, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 11:51:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:51:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:51:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #50, planning to set LR to 1.00e-05
2025-10-09 11:51:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 11:51:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:51:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:51:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:51:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:51:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:52:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:52:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=238.129868, avg_loss=0.496104, seen=480, correct=376, accuracy=0.783333
2025-10-09 11:52:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:52:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:52:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=50 reserved=2086MB allocated=1946MB
2025-10-09 11:52:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 50, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 61.45881327986717, 'train_avg_loss': 0.5121567773322264, 'train_seen': 120, 'train_correct': 94, 'train_acc': 0.7833333333333333}}
2025-10-09 11:52:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 50, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 238.12986755371094, 'train_avg_loss': 0.4961038907368978, 'train_seen': 480, 'train_correct': 376, 'train_acc': 0.7833333333333333}}
2025-10-09 11:52:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 50, 'Results_raw': {'train_total': 480, 'train_loss': 238.12986755371094, 'train_avg_loss': 0.4961038907368978, 'train_seen': 480, 'train_correct': 376, 'train_acc': 0.7833333333333333}}
2025-10-09 11:52:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:52:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:52:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #50, planning to set LR to 1.00e-05
2025-10-09 11:52:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 11:52:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:52:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:52:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:52:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:52:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:52:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:52:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.413788, avg_loss=0.625862, seen=480, correct=311, accuracy=0.647917
2025-10-09 11:52:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:52:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:52:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:53:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=50 reserved=2078MB allocated=1946MB
2025-10-09 11:53:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 50, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.58136853575706, 'train_avg_loss': 0.6298447377979756, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:53:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 50, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.4137878417969, 'train_avg_loss': 0.6258620580037435, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 11:53:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 50, 'Results_raw': {'train_total': 480, 'train_loss': 300.4137878417969, 'train_avg_loss': 0.6258620580037435, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 11:53:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:53:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:53:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #50, planning to set LR to 1.00e-05
2025-10-09 11:53:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 11:53:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:53:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:53:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:53:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:53:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:53:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:53:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=294.264496, avg_loss=0.613051, seen=480, correct=324, accuracy=0.675000
2025-10-09 11:53:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:53:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:53:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=50 reserved=2078MB allocated=1946MB
2025-10-09 11:53:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 50, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.16701167821884, 'train_avg_loss': 0.6430584306518237, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 11:53:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 50, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 294.2644958496094, 'train_avg_loss': 0.6130510330200195, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 11:53:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 50, 'Results_raw': {'train_total': 480, 'train_loss': 294.2644958496094, 'train_avg_loss': 0.6130510330200195, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 11:53:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:53:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:53:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #50, planning to set LR to 1.00e-05
2025-10-09 11:53:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 11:53:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:53:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:53:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:53:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:53:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:54:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:54:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.826965, avg_loss=0.662140, seen=480, correct=288, accuracy=0.600000
2025-10-09 11:54:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:54:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:54:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=50 reserved=2086MB allocated=1946MB
2025-10-09 11:54:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 50, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.07804879546165, 'train_avg_loss': 0.6423170732955138, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 11:54:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 50, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.82696533203125, 'train_avg_loss': 0.6621395111083984, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 11:54:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 50, 'Results_raw': {'train_total': 480, 'train_loss': 317.82696533203125, 'train_avg_loss': 0.6621395111083984, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 11:54:25 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #51) -------------
2025-10-09 11:54:25 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=51 aidx=0 | s=5 (candidates=21)
2025-10-09 11:54:25 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[39, 51, 15, 48, 19] (from 21)
2025-10-09 11:54:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:54:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:54:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #51, planning to set LR to 1.00e-05
2025-10-09 11:54:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 11:54:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:54:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:54:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:54:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:54:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:55:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:55:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.935760, avg_loss=0.647783, seen=480, correct=313, accuracy=0.652083
2025-10-09 11:55:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:55:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:55:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:55:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=51 reserved=2078MB allocated=1946MB
2025-10-09 11:55:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 51, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.09057211875916, 'train_avg_loss': 0.6340881009896596, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 11:55:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 51, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.9357604980469, 'train_avg_loss': 0.647782834370931, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 11:55:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 51, 'Results_raw': {'train_total': 480, 'train_loss': 310.9357604980469, 'train_avg_loss': 0.647782834370931, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 11:55:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:55:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:55:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #51, planning to set LR to 1.00e-05
2025-10-09 11:55:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 11:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:55:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:55:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:55:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:55:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:55:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:55:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.423431, avg_loss=0.630049, seen=480, correct=309, accuracy=0.643750
2025-10-09 11:55:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:55:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:55:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:55:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=51 reserved=2088MB allocated=1946MB
2025-10-09 11:55:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 51, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.35871148109436, 'train_avg_loss': 0.677989262342453, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 11:55:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 51, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.4234313964844, 'train_avg_loss': 0.6300488154093424, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 11:55:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 51, 'Results_raw': {'train_total': 480, 'train_loss': 302.4234313964844, 'train_avg_loss': 0.6300488154093424, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 11:55:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:55:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:55:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #51, planning to set LR to 1.00e-05
2025-10-09 11:55:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 11:55:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:55:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:55:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:55:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:55:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:56:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:56:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.531708, avg_loss=0.661524, seen=480, correct=293, accuracy=0.610417
2025-10-09 11:56:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:56:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:56:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:56:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=51 reserved=2078MB allocated=1946MB
2025-10-09 11:56:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 51, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.7703041434288, 'train_avg_loss': 0.6897525345285733, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 11:56:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 51, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.5317077636719, 'train_avg_loss': 0.6615243911743164, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 11:56:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 51, 'Results_raw': {'train_total': 480, 'train_loss': 317.5317077636719, 'train_avg_loss': 0.6615243911743164, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 11:56:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:56:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:56:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #51, planning to set LR to 1.00e-05
2025-10-09 11:56:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 11:56:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:56:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:56:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:56:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:56:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:57:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:57:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.600891, avg_loss=0.651252, seen=480, correct=291, accuracy=0.606250
2025-10-09 11:57:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:57:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:57:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:57:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=51 reserved=2078MB allocated=1946MB
2025-10-09 11:57:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 51, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.73642826080322, 'train_avg_loss': 0.7144702355066935, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 11:57:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 51, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.60089111328125, 'train_avg_loss': 0.6512518564860026, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 11:57:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 51, 'Results_raw': {'train_total': 480, 'train_loss': 312.60089111328125, 'train_avg_loss': 0.6512518564860026, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 11:57:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:57:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:57:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #51, planning to set LR to 1.00e-05
2025-10-09 11:57:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 11:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:57:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:57:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:57:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:57:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:57:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:57:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=297.583099, avg_loss=0.619965, seen=480, correct=317, accuracy=0.660417
2025-10-09 11:57:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:57:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:57:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:57:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=51 reserved=2188MB allocated=1946MB
2025-10-09 11:57:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 51, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.73452806472778, 'train_avg_loss': 0.6144544005393981, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 11:57:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 51, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 297.5830993652344, 'train_avg_loss': 0.6199647903442382, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 11:57:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 51, 'Results_raw': {'train_total': 480, 'train_loss': 297.5830993652344, 'train_avg_loss': 0.6199647903442382, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 11:57:51 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #52) -------------
2025-10-09 11:57:52 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=52 aidx=0 | s=5 (candidates=21)
2025-10-09 11:57:52 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[48, 16, 46, 39, 29] (from 21)
2025-10-09 11:57:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:57:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:57:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #52, planning to set LR to 1.00e-05
2025-10-09 11:57:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 11:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:57:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:57:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:57:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:57:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:58:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:58:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.862549, avg_loss=0.651797, seen=480, correct=291, accuracy=0.606250
2025-10-09 11:58:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:58:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:58:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=52 reserved=2078MB allocated=1946MB
2025-10-09 11:58:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 52, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.76988530158997, 'train_avg_loss': 0.7230823775132497, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 11:58:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 52, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.862548828125, 'train_avg_loss': 0.6517969767252604, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 11:58:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 52, 'Results_raw': {'train_total': 480, 'train_loss': 312.862548828125, 'train_avg_loss': 0.6517969767252604, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 11:58:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:58:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:58:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #52, planning to set LR to 1.00e-05
2025-10-09 11:58:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 11:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:58:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:58:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:58:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:58:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:59:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:59:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.448151, avg_loss=0.632184, seen=480, correct=315, accuracy=0.656250
2025-10-09 11:59:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:59:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:59:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 11:59:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=52 reserved=2108MB allocated=1946MB
2025-10-09 11:59:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 52, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.2284404039383, 'train_avg_loss': 0.5852370033661525, 'train_seen': 120, 'train_correct': 89, 'train_acc': 0.7416666666666667}}
2025-10-09 11:59:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 52, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.4481506347656, 'train_avg_loss': 0.6321836471557617, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 11:59:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 52, 'Results_raw': {'train_total': 480, 'train_loss': 303.4481506347656, 'train_avg_loss': 0.6321836471557617, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 11:59:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 11:59:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 11:59:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #52, planning to set LR to 1.00e-05
2025-10-09 11:59:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 11:59:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 11:59:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 11:59:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 11:59:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 11:59:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 11:59:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 11:59:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.954956, avg_loss=0.651989, seen=480, correct=291, accuracy=0.606250
2025-10-09 11:59:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 11:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:00:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:00:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=52 reserved=2106MB allocated=1946MB
2025-10-09 12:00:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 52, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.59330177307129, 'train_avg_loss': 0.6132775147755941, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 12:00:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 52, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.9549560546875, 'train_avg_loss': 0.651989491780599, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 12:00:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 52, 'Results_raw': {'train_total': 480, 'train_loss': 312.9549560546875, 'train_avg_loss': 0.651989491780599, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 12:00:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:00:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:00:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #52, planning to set LR to 1.00e-05
2025-10-09 12:00:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 12:00:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:00:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:00:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:00:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:00:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:00:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:00:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.998230, avg_loss=0.649996, seen=480, correct=314, accuracy=0.654167
2025-10-09 12:00:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:00:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:00:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:00:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=52 reserved=2078MB allocated=1946MB
2025-10-09 12:00:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 52, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.42738166451454, 'train_avg_loss': 0.6368948472042878, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 12:00:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 52, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.99822998046875, 'train_avg_loss': 0.6499963124593099, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 12:00:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 52, 'Results_raw': {'train_total': 480, 'train_loss': 311.99822998046875, 'train_avg_loss': 0.6499963124593099, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 12:00:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:00:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:00:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #52, planning to set LR to 1.00e-05
2025-10-09 12:00:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 12:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:00:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:00:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:00:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:00:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:01:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:01:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.439392, avg_loss=0.650915, seen=480, correct=292, accuracy=0.608333
2025-10-09 12:01:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:01:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:01:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:01:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=52 reserved=2084MB allocated=1946MB
2025-10-09 12:01:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 52, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.01170873641968, 'train_avg_loss': 0.6334309061368306, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 12:01:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 52, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.43939208984375, 'train_avg_loss': 0.6509154001871745, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 12:01:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 52, 'Results_raw': {'train_total': 480, 'train_loss': 312.43939208984375, 'train_avg_loss': 0.6509154001871745, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 12:01:27 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #53) -------------
2025-10-09 12:01:27 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=53 aidx=0 | s=5 (candidates=21)
2025-10-09 12:01:27 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[22, 46, 15, 48, 43] (from 21)
2025-10-09 12:01:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:01:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:01:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #53, planning to set LR to 1.00e-05
2025-10-09 12:01:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 12:01:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:01:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:01:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:01:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:01:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:02:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:02:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=246.743378, avg_loss=0.514049, seen=480, correct=372, accuracy=0.775000
2025-10-09 12:02:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:02:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:02:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:02:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=53 reserved=2086MB allocated=1946MB
2025-10-09 12:02:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 53, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 65.15256628394127, 'train_avg_loss': 0.5429380523661772, 'train_seen': 120, 'train_correct': 93, 'train_acc': 0.775}}
2025-10-09 12:02:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 53, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 246.74337768554688, 'train_avg_loss': 0.514048703511556, 'train_seen': 480, 'train_correct': 372, 'train_acc': 0.775}}
2025-10-09 12:02:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 53, 'Results_raw': {'train_total': 480, 'train_loss': 246.74337768554688, 'train_avg_loss': 0.514048703511556, 'train_seen': 480, 'train_correct': 372, 'train_acc': 0.775}}
2025-10-09 12:02:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:02:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:02:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #53, planning to set LR to 1.00e-05
2025-10-09 12:02:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 12:02:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:02:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:02:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:02:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:02:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:02:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:02:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.773499, avg_loss=0.655778, seen=480, correct=289, accuracy=0.602083
2025-10-09 12:02:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:02:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:02:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=53 reserved=2106MB allocated=1946MB
2025-10-09 12:02:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 53, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.10382568836212, 'train_avg_loss': 0.617531880736351, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 12:02:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 53, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.77349853515625, 'train_avg_loss': 0.6557781219482421, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 12:02:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 53, 'Results_raw': {'train_total': 480, 'train_loss': 314.77349853515625, 'train_avg_loss': 0.6557781219482421, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 12:02:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:02:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:02:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #53, planning to set LR to 1.00e-05
2025-10-09 12:02:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 12:02:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:02:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:02:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:02:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:02:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:03:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:03:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.887543, avg_loss=0.668516, seen=480, correct=291, accuracy=0.606250
2025-10-09 12:03:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:03:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:03:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=53 reserved=2078MB allocated=1946MB
2025-10-09 12:03:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 53, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.18503546714783, 'train_avg_loss': 0.6932086288928986, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 12:03:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 53, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.8875427246094, 'train_avg_loss': 0.6685157140096029, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 12:03:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 53, 'Results_raw': {'train_total': 480, 'train_loss': 320.8875427246094, 'train_avg_loss': 0.6685157140096029, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 12:03:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:03:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:03:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #53, planning to set LR to 1.00e-05
2025-10-09 12:03:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 12:03:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:03:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:03:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:03:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:03:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:04:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:04:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.995392, avg_loss=0.652074, seen=480, correct=284, accuracy=0.591667
2025-10-09 12:04:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:04:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:04:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=53 reserved=2078MB allocated=1946MB
2025-10-09 12:04:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 53, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.92575097084045, 'train_avg_loss': 0.7243812580903372, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 12:04:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 53, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.9953918457031, 'train_avg_loss': 0.6520737330118815, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 12:04:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 53, 'Results_raw': {'train_total': 480, 'train_loss': 312.9953918457031, 'train_avg_loss': 0.6520737330118815, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 12:04:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:04:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:04:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #53, planning to set LR to 1.00e-05
2025-10-09 12:04:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 12:04:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:04:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:04:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:04:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:04:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:05:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:05:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=296.812073, avg_loss=0.618358, seen=480, correct=321, accuracy=0.668750
2025-10-09 12:05:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:05:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:05:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=53 reserved=2078MB allocated=1946MB
2025-10-09 12:05:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 53, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.941541492939, 'train_avg_loss': 0.6411795124411583, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 12:05:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 53, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 296.81207275390625, 'train_avg_loss': 0.6183584849039714, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 12:05:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 53, 'Results_raw': {'train_total': 480, 'train_loss': 296.81207275390625, 'train_avg_loss': 0.6183584849039714, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 12:05:03 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #54) -------------
2025-10-09 12:05:04 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=54 aidx=0 | s=5 (candidates=21)
2025-10-09 12:05:04 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[22, 1, 13, 19, 28] (from 21)
2025-10-09 12:05:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:05:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:05:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #54, planning to set LR to 1.00e-05
2025-10-09 12:05:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 12:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:05:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:05:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:05:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:05:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:05:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:05:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=242.829147, avg_loss=0.505894, seen=480, correct=374, accuracy=0.779167
2025-10-09 12:05:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:05:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:05:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:05:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=54 reserved=2086MB allocated=1946MB
2025-10-09 12:05:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 54, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 62.87162610888481, 'train_avg_loss': 0.5239302175740401, 'train_seen': 120, 'train_correct': 94, 'train_acc': 0.7833333333333333}}
2025-10-09 12:05:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 54, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 242.8291473388672, 'train_avg_loss': 0.5058940569559733, 'train_seen': 480, 'train_correct': 374, 'train_acc': 0.7791666666666667}}
2025-10-09 12:05:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 54, 'Results_raw': {'train_total': 480, 'train_loss': 242.8291473388672, 'train_avg_loss': 0.5058940569559733, 'train_seen': 480, 'train_correct': 374, 'train_acc': 0.7791666666666667}}
2025-10-09 12:05:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:05:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:05:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #54, planning to set LR to 1.00e-05
2025-10-09 12:05:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 12:05:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:05:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:05:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:05:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:05:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:06:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:06:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.583374, avg_loss=0.678299, seen=480, correct=280, accuracy=0.583333
2025-10-09 12:06:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:06:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:06:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:06:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=54 reserved=2078MB allocated=1946MB
2025-10-09 12:06:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 54, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.53669875860214, 'train_avg_loss': 0.6794724896550178, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 12:06:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 54, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.5833740234375, 'train_avg_loss': 0.6782986958821614, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 12:06:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 54, 'Results_raw': {'train_total': 480, 'train_loss': 325.5833740234375, 'train_avg_loss': 0.6782986958821614, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 12:06:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:06:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:06:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #54, planning to set LR to 1.00e-05
2025-10-09 12:06:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 12:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:06:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:06:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:06:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:06:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:07:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=289.584198, avg_loss=0.603300, seen=480, correct=326, accuracy=0.679167
2025-10-09 12:07:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:07:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:07:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=54 reserved=2078MB allocated=1946MB
2025-10-09 12:07:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 54, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.22861003875732, 'train_avg_loss': 0.6185717503229777, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 12:07:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 54, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 289.5841979980469, 'train_avg_loss': 0.603300412495931, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:07:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 54, 'Results_raw': {'train_total': 480, 'train_loss': 289.5841979980469, 'train_avg_loss': 0.603300412495931, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:07:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:07:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:07:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #54, planning to set LR to 1.00e-05
2025-10-09 12:07:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:07:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:07:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:07:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:07:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:07:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:07:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=285.398712, avg_loss=0.594581, seen=480, correct=330, accuracy=0.687500
2025-10-09 12:07:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:07:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:07:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:07:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=54 reserved=2188MB allocated=1946MB
2025-10-09 12:07:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 54, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.61761182546616, 'train_avg_loss': 0.5718134318788847, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 12:07:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 54, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 285.3987121582031, 'train_avg_loss': 0.5945806503295898, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 12:07:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 54, 'Results_raw': {'train_total': 480, 'train_loss': 285.3987121582031, 'train_avg_loss': 0.5945806503295898, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 12:07:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:07:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:07:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #54, planning to set LR to 1.00e-05
2025-10-09 12:07:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 12:07:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:07:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:07:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:07:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:07:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:08:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:08:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=299.833557, avg_loss=0.624653, seen=480, correct=310, accuracy=0.645833
2025-10-09 12:08:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:08:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:08:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:08:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=54 reserved=2078MB allocated=1946MB
2025-10-09 12:08:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 54, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.54178568720818, 'train_avg_loss': 0.6295148807267348, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 12:08:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 54, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 299.83355712890625, 'train_avg_loss': 0.6246532440185547, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 12:08:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 54, 'Results_raw': {'train_total': 480, 'train_loss': 299.83355712890625, 'train_avg_loss': 0.6246532440185547, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 12:08:36 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #55) -------------
2025-10-09 12:08:37 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=55 aidx=0 | s=5 (candidates=21)
2025-10-09 12:08:37 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[13, 6, 15, 20, 46] (from 21)
2025-10-09 12:08:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:08:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:08:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #55, planning to set LR to 1.00e-05
2025-10-09 12:08:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 12:08:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:08:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:08:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:08:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:08:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:09:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:09:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=291.755585, avg_loss=0.607824, seen=480, correct=318, accuracy=0.662500
2025-10-09 12:09:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:09:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:09:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=55 reserved=2078MB allocated=1946MB
2025-10-09 12:09:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 55, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.81421554088593, 'train_avg_loss': 0.6234517961740493, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 12:09:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 55, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 291.7555847167969, 'train_avg_loss': 0.6078241348266602, 'train_seen': 480, 'train_correct': 318, 'train_acc': 0.6625}}
2025-10-09 12:09:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 55, 'Results_raw': {'train_total': 480, 'train_loss': 291.7555847167969, 'train_avg_loss': 0.6078241348266602, 'train_seen': 480, 'train_correct': 318, 'train_acc': 0.6625}}
2025-10-09 12:09:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:09:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:09:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #55, planning to set LR to 1.00e-05
2025-10-09 12:09:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 12:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:09:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:09:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:09:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:09:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:10:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:10:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.664398, avg_loss=0.653467, seen=480, correct=296, accuracy=0.616667
2025-10-09 12:10:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:10:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:10:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:10:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=55 reserved=2086MB allocated=1946MB
2025-10-09 12:10:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 55, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.49767899513245, 'train_avg_loss': 0.629147324959437, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 12:10:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 55, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.6643981933594, 'train_avg_loss': 0.6534674962361654, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 12:10:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 55, 'Results_raw': {'train_total': 480, 'train_loss': 313.6643981933594, 'train_avg_loss': 0.6534674962361654, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 12:10:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:10:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:10:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #55, planning to set LR to 1.00e-05
2025-10-09 12:10:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 12:10:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:10:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:10:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:10:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:10:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:10:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:10:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.745331, avg_loss=0.666136, seen=480, correct=290, accuracy=0.604167
2025-10-09 12:10:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:10:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:10:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=55 reserved=2078MB allocated=1946MB
2025-10-09 12:10:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 55, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.07756924629211, 'train_avg_loss': 0.7006464103857676, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 12:10:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 55, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.7453308105469, 'train_avg_loss': 0.666136105855306, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 12:10:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 55, 'Results_raw': {'train_total': 480, 'train_loss': 319.7453308105469, 'train_avg_loss': 0.666136105855306, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 12:10:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:10:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:10:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #55, planning to set LR to 1.00e-05
2025-10-09 12:10:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 12:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:10:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:10:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:10:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:10:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:11:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:11:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=280.516174, avg_loss=0.584409, seen=480, correct=339, accuracy=0.706250
2025-10-09 12:11:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:11:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:11:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=55 reserved=2078MB allocated=1946MB
2025-10-09 12:11:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 55, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.56870663166046, 'train_avg_loss': 0.6130725552638372, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 12:11:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 55, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 280.51617431640625, 'train_avg_loss': 0.584408696492513, 'train_seen': 480, 'train_correct': 339, 'train_acc': 0.70625}}
2025-10-09 12:11:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 55, 'Results_raw': {'train_total': 480, 'train_loss': 280.51617431640625, 'train_avg_loss': 0.584408696492513, 'train_seen': 480, 'train_correct': 339, 'train_acc': 0.70625}}
2025-10-09 12:11:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:11:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:11:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #55, planning to set LR to 1.00e-05
2025-10-09 12:11:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 12:11:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:11:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:11:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:11:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:11:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:12:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:12:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.180450, avg_loss=0.656626, seen=480, correct=291, accuracy=0.606250
2025-10-09 12:12:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:12:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:12:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:12:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=55 reserved=2106MB allocated=1946MB
2025-10-09 12:12:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 55, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.38848954439163, 'train_avg_loss': 0.619904079536597, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 12:12:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 55, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.1804504394531, 'train_avg_loss': 0.6566259384155273, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 12:12:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 55, 'Results_raw': {'train_total': 480, 'train_loss': 315.1804504394531, 'train_avg_loss': 0.6566259384155273, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 12:12:12 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #56) -------------
2025-10-09 12:12:13 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=56 aidx=0 | s=5 (candidates=21)
2025-10-09 12:12:13 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[46, 22, 51, 29, 39] (from 21)
2025-10-09 12:12:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:12:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:12:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #56, planning to set LR to 1.00e-05
2025-10-09 12:12:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 12:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:12:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:12:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:12:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:12:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:12:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:12:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.408112, avg_loss=0.642517, seen=480, correct=295, accuracy=0.614583
2025-10-09 12:12:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:12:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:12:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:12:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=56 reserved=2106MB allocated=1946MB
2025-10-09 12:12:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 56, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.20850133895874, 'train_avg_loss': 0.6017375111579895, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 12:12:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 56, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.4081115722656, 'train_avg_loss': 0.6425168991088868, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 12:12:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 56, 'Results_raw': {'train_total': 480, 'train_loss': 308.4081115722656, 'train_avg_loss': 0.6425168991088868, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 12:12:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:12:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:12:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #56, planning to set LR to 1.00e-05
2025-10-09 12:12:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 12:12:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:12:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:12:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:12:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:12:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:13:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:13:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=245.932678, avg_loss=0.512360, seen=480, correct=365, accuracy=0.760417
2025-10-09 12:13:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:13:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:13:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=56 reserved=2086MB allocated=1946MB
2025-10-09 12:13:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 56, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 64.01970708370209, 'train_avg_loss': 0.5334975590308507, 'train_seen': 120, 'train_correct': 94, 'train_acc': 0.7833333333333333}}
2025-10-09 12:13:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 56, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 245.93267822265625, 'train_avg_loss': 0.5123597462972005, 'train_seen': 480, 'train_correct': 365, 'train_acc': 0.7604166666666666}}
2025-10-09 12:13:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 56, 'Results_raw': {'train_total': 480, 'train_loss': 245.93267822265625, 'train_avg_loss': 0.5123597462972005, 'train_seen': 480, 'train_correct': 365, 'train_acc': 0.7604166666666666}}
2025-10-09 12:13:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:13:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:13:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #56, planning to set LR to 1.00e-05
2025-10-09 12:13:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 12:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:13:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:13:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:13:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:13:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:14:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:14:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.009460, avg_loss=0.631270, seen=480, correct=304, accuracy=0.633333
2025-10-09 12:14:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:14:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:14:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:14:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=56 reserved=2088MB allocated=1946MB
2025-10-09 12:14:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 56, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.94554716348648, 'train_avg_loss': 0.6745462263623874, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 12:14:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 56, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.00946044921875, 'train_avg_loss': 0.6312697092692058, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 12:14:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 56, 'Results_raw': {'train_total': 480, 'train_loss': 303.00946044921875, 'train_avg_loss': 0.6312697092692058, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 12:14:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:14:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:14:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #56, planning to set LR to 1.00e-05
2025-10-09 12:14:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 12:14:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:14:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:14:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:14:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:14:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:14:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:14:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.361511, avg_loss=0.642420, seen=480, correct=299, accuracy=0.622917
2025-10-09 12:14:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:14:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:14:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:15:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=56 reserved=2084MB allocated=1946MB
2025-10-09 12:15:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 56, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.72493332624435, 'train_avg_loss': 0.622707777718703, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 12:15:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 56, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.36151123046875, 'train_avg_loss': 0.6424198150634766, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 12:15:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 56, 'Results_raw': {'train_total': 480, 'train_loss': 308.36151123046875, 'train_avg_loss': 0.6424198150634766, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 12:15:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:15:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:15:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #56, planning to set LR to 1.00e-05
2025-10-09 12:15:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 12:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:15:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:15:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:15:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:15:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:15:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.736938, avg_loss=0.647369, seen=480, correct=320, accuracy=0.666667
2025-10-09 12:15:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:15:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:15:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=56 reserved=2078MB allocated=1946MB
2025-10-09 12:15:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 56, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.92456614971161, 'train_avg_loss': 0.64937138458093, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 12:15:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 56, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.7369384765625, 'train_avg_loss': 0.6473686218261718, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 12:15:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 56, 'Results_raw': {'train_total': 480, 'train_loss': 310.7369384765625, 'train_avg_loss': 0.6473686218261718, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 12:15:43 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #57) -------------
2025-10-09 12:15:44 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=57 aidx=0 | s=5 (candidates=21)
2025-10-09 12:15:44 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[29, 34, 19, 6, 47] (from 21)
2025-10-09 12:15:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:15:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:15:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #57, planning to set LR to 1.00e-05
2025-10-09 12:15:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 12:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:15:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:15:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:15:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:15:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:16:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:16:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.930664, avg_loss=0.643606, seen=480, correct=299, accuracy=0.622917
2025-10-09 12:16:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:16:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:16:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=57 reserved=2084MB allocated=1946MB
2025-10-09 12:16:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 57, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.9652628004551, 'train_avg_loss': 0.6247105233371257, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 12:16:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 57, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.9306640625, 'train_avg_loss': 0.6436055501302084, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 12:16:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 57, 'Results_raw': {'train_total': 480, 'train_loss': 308.9306640625, 'train_avg_loss': 0.6436055501302084, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 12:16:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:16:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:16:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #57, planning to set LR to 1.00e-05
2025-10-09 12:16:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 12:16:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:16:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:16:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:16:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:16:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:17:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:17:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.745636, avg_loss=0.645303, seen=480, correct=312, accuracy=0.650000
2025-10-09 12:17:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:17:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:17:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:17:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=57 reserved=2078MB allocated=1946MB
2025-10-09 12:17:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 57, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.62083852291107, 'train_avg_loss': 0.6801736543575923, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 12:17:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 57, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.7456359863281, 'train_avg_loss': 0.6453034083048502, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 12:17:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 57, 'Results_raw': {'train_total': 480, 'train_loss': 309.7456359863281, 'train_avg_loss': 0.6453034083048502, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 12:17:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:17:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:17:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #57, planning to set LR to 1.00e-05
2025-10-09 12:17:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:17:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:17:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:17:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:17:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:17:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:17:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:17:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=287.786743, avg_loss=0.599556, seen=480, correct=326, accuracy=0.679167
2025-10-09 12:17:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:17:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:17:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:17:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=57 reserved=2188MB allocated=1946MB
2025-10-09 12:17:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 57, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.37017342448235, 'train_avg_loss': 0.5780847785373529, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 12:17:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 57, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 287.7867431640625, 'train_avg_loss': 0.5995557149251302, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:17:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 57, 'Results_raw': {'train_total': 480, 'train_loss': 287.7867431640625, 'train_avg_loss': 0.5995557149251302, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:17:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:17:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:17:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #57, planning to set LR to 1.00e-05
2025-10-09 12:17:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 12:17:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:17:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:17:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:17:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:17:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:18:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:18:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.935730, avg_loss=0.645699, seen=480, correct=296, accuracy=0.616667
2025-10-09 12:18:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:18:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:18:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=57 reserved=2086MB allocated=1946MB
2025-10-09 12:18:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 57, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.23271131515503, 'train_avg_loss': 0.618605927626292, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 12:18:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 57, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.93572998046875, 'train_avg_loss': 0.6456994374593099, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 12:18:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 57, 'Results_raw': {'train_total': 480, 'train_loss': 309.93572998046875, 'train_avg_loss': 0.6456994374593099, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 12:18:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:18:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:18:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #57, planning to set LR to 1.00e-05
2025-10-09 12:18:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 12:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:18:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:18:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:18:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:18:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:19:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:19:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.672943, avg_loss=0.651402, seen=480, correct=294, accuracy=0.612500
2025-10-09 12:19:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:19:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:19:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:19:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=57 reserved=2078MB allocated=1946MB
2025-10-09 12:19:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 57, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.8221707046032, 'train_avg_loss': 0.6235180892050266, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 12:19:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 57, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.6729431152344, 'train_avg_loss': 0.651401964823405, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 12:19:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 57, 'Results_raw': {'train_total': 480, 'train_loss': 312.6729431152344, 'train_avg_loss': 0.651401964823405, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 12:19:16 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #58) -------------
2025-10-09 12:19:17 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=58 aidx=0 | s=5 (candidates=21)
2025-10-09 12:19:17 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[45, 19, 39, 51, 13] (from 21)
2025-10-09 12:19:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:19:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:19:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #58, planning to set LR to 1.00e-05
2025-10-09 12:19:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 12:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:19:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:19:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:19:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:19:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:19:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:19:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.498627, avg_loss=0.644789, seen=480, correct=307, accuracy=0.639583
2025-10-09 12:19:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:19:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:20:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:20:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=58 reserved=2078MB allocated=1946MB
2025-10-09 12:20:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 58, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.27876645326614, 'train_avg_loss': 0.6356563871105512, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 12:20:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 58, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.4986267089844, 'train_avg_loss': 0.6447888056437174, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 12:20:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 58, 'Results_raw': {'train_total': 480, 'train_loss': 309.4986267089844, 'train_avg_loss': 0.6447888056437174, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 12:20:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:20:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:20:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #58, planning to set LR to 1.00e-05
2025-10-09 12:20:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:20:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:20:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:20:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:20:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:20:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:20:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:20:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=287.135162, avg_loss=0.598198, seen=480, correct=327, accuracy=0.681250
2025-10-09 12:20:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:20:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:20:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=58 reserved=2188MB allocated=1946MB
2025-10-09 12:20:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 58, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.61387294530869, 'train_avg_loss': 0.5801156078775723, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 12:20:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 58, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 287.1351623535156, 'train_avg_loss': 0.5981982549031576, 'train_seen': 480, 'train_correct': 327, 'train_acc': 0.68125}}
2025-10-09 12:20:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 58, 'Results_raw': {'train_total': 480, 'train_loss': 287.1351623535156, 'train_avg_loss': 0.5981982549031576, 'train_seen': 480, 'train_correct': 327, 'train_acc': 0.68125}}
2025-10-09 12:20:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:20:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:20:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #58, planning to set LR to 1.00e-05
2025-10-09 12:20:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 12:20:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:20:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:20:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:20:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:20:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:21:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.228638, avg_loss=0.635893, seen=480, correct=320, accuracy=0.666667
2025-10-09 12:21:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:21:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:21:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=58 reserved=2078MB allocated=1946MB
2025-10-09 12:21:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 58, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.18858176469803, 'train_avg_loss': 0.6349048480391503, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 12:21:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 58, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.2286376953125, 'train_avg_loss': 0.6358929951985677, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 12:21:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 58, 'Results_raw': {'train_total': 480, 'train_loss': 305.2286376953125, 'train_avg_loss': 0.6358929951985677, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 12:21:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:21:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:21:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #58, planning to set LR to 1.00e-05
2025-10-09 12:21:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 12:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:21:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:21:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:21:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:21:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:22:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:22:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.264465, avg_loss=0.635968, seen=480, correct=299, accuracy=0.622917
2025-10-09 12:22:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:22:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:22:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=58 reserved=2088MB allocated=1946MB
2025-10-09 12:22:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 58, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.0915732383728, 'train_avg_loss': 0.6840964436531067, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 12:22:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 58, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.26446533203125, 'train_avg_loss': 0.6359676361083985, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 12:22:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 58, 'Results_raw': {'train_total': 480, 'train_loss': 305.26446533203125, 'train_avg_loss': 0.6359676361083985, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 12:22:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:22:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:22:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #58, planning to set LR to 1.00e-05
2025-10-09 12:22:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 12:22:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:22:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:22:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:22:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:22:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:22:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=289.332153, avg_loss=0.602775, seen=480, correct=328, accuracy=0.683333
2025-10-09 12:22:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:22:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:22:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=58 reserved=2078MB allocated=1946MB
2025-10-09 12:22:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 58, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.45401018857956, 'train_avg_loss': 0.6204500849048297, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 12:22:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 58, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 289.3321533203125, 'train_avg_loss': 0.6027753194173177, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 12:22:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 58, 'Results_raw': {'train_total': 480, 'train_loss': 289.3321533203125, 'train_avg_loss': 0.6027753194173177, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 12:22:48 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #59) -------------
2025-10-09 12:22:48 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=59 aidx=0 | s=5 (candidates=21)
2025-10-09 12:22:48 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[13, 29, 19, 45, 22] (from 21)
2025-10-09 12:22:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:22:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:22:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #59, planning to set LR to 1.00e-05
2025-10-09 12:22:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 12:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:22:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:22:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:22:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:22:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:23:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:23:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=279.940674, avg_loss=0.583210, seen=480, correct=333, accuracy=0.693750
2025-10-09 12:23:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:23:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:23:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=59 reserved=2078MB allocated=1946MB
2025-10-09 12:23:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 59, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.27391695976257, 'train_avg_loss': 0.6022826413313548, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 12:23:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 59, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 279.940673828125, 'train_avg_loss': 0.583209737141927, 'train_seen': 480, 'train_correct': 333, 'train_acc': 0.69375}}
2025-10-09 12:23:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 59, 'Results_raw': {'train_total': 480, 'train_loss': 279.940673828125, 'train_avg_loss': 0.583209737141927, 'train_seen': 480, 'train_correct': 333, 'train_acc': 0.69375}}
2025-10-09 12:23:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:23:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:23:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #59, planning to set LR to 1.00e-05
2025-10-09 12:23:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 12:23:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:23:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:23:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:23:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:23:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:24:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:24:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.504120, avg_loss=0.640634, seen=480, correct=297, accuracy=0.618750
2025-10-09 12:24:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:24:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:24:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=59 reserved=2084MB allocated=1946MB
2025-10-09 12:24:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 59, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.61563539505005, 'train_avg_loss': 0.6301302949587504, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 12:24:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 59, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.5041198730469, 'train_avg_loss': 0.6406335830688477, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 12:24:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 59, 'Results_raw': {'train_total': 480, 'train_loss': 307.5041198730469, 'train_avg_loss': 0.6406335830688477, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 12:24:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:24:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:24:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #59, planning to set LR to 1.00e-05
2025-10-09 12:24:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:24:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:24:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:24:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:24:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:24:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:24:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:24:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=288.024200, avg_loss=0.600050, seen=480, correct=324, accuracy=0.675000
2025-10-09 12:24:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:24:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:24:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=59 reserved=2188MB allocated=1946MB
2025-10-09 12:24:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 59, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.53562140464783, 'train_avg_loss': 0.5877968450387319, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 12:24:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 59, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 288.0242004394531, 'train_avg_loss': 0.600050417582194, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 12:24:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 59, 'Results_raw': {'train_total': 480, 'train_loss': 288.0242004394531, 'train_avg_loss': 0.600050417582194, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 12:24:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:24:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:24:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #59, planning to set LR to 1.00e-05
2025-10-09 12:24:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 12:24:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:24:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:24:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:24:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:24:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:25:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:25:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.394348, avg_loss=0.650822, seen=480, correct=302, accuracy=0.629167
2025-10-09 12:25:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:25:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:25:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:25:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=59 reserved=2078MB allocated=1946MB
2025-10-09 12:25:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 59, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.96879076957703, 'train_avg_loss': 0.6414065897464752, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 12:25:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 59, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.39434814453125, 'train_avg_loss': 0.6508215586344401, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 12:25:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 59, 'Results_raw': {'train_total': 480, 'train_loss': 312.39434814453125, 'train_avg_loss': 0.6508215586344401, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 12:25:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:25:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:25:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #59, planning to set LR to 1.00e-05
2025-10-09 12:25:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 12:25:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:25:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:25:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:25:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:25:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:26:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:26:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=249.518280, avg_loss=0.519830, seen=480, correct=362, accuracy=0.754167
2025-10-09 12:26:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:26:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:26:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:26:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=59 reserved=2086MB allocated=1946MB
2025-10-09 12:26:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 59, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 65.985316157341, 'train_avg_loss': 0.5498776346445083, 'train_seen': 120, 'train_correct': 89, 'train_acc': 0.7416666666666667}}
2025-10-09 12:26:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 59, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 249.51828002929688, 'train_avg_loss': 0.5198297500610352, 'train_seen': 480, 'train_correct': 362, 'train_acc': 0.7541666666666667}}
2025-10-09 12:26:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 59, 'Results_raw': {'train_total': 480, 'train_loss': 249.51828002929688, 'train_avg_loss': 0.5198297500610352, 'train_seen': 480, 'train_correct': 362, 'train_acc': 0.7541666666666667}}
2025-10-09 12:26:23 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #60) -------------
2025-10-09 12:26:23 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=60 aidx=0 | s=5 (candidates=21)
2025-10-09 12:26:23 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[20, 34, 39, 16, 22] (from 21)
2025-10-09 12:26:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:26:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:26:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #60, planning to set LR to 1.00e-05
2025-10-09 12:26:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 12:26:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:26:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:26:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:26:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:26:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:27:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:27:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=279.494263, avg_loss=0.582280, seen=480, correct=333, accuracy=0.693750
2025-10-09 12:27:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:27:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:27:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:27:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=60 reserved=2078MB allocated=1946MB
2025-10-09 12:27:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 60, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.50891584157944, 'train_avg_loss': 0.6125742986798286, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 12:27:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 60, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 279.4942626953125, 'train_avg_loss': 0.5822797139485677, 'train_seen': 480, 'train_correct': 333, 'train_acc': 0.69375}}
2025-10-09 12:27:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 60, 'Results_raw': {'train_total': 480, 'train_loss': 279.4942626953125, 'train_avg_loss': 0.5822797139485677, 'train_seen': 480, 'train_correct': 333, 'train_acc': 0.69375}}
2025-10-09 12:27:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:27:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:27:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #60, planning to set LR to 1.00e-05
2025-10-09 12:27:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 12:27:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:27:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:27:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:27:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:27:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:27:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:27:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.440033, avg_loss=0.642583, seen=480, correct=320, accuracy=0.666667
2025-10-09 12:27:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:27:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:27:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:27:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=60 reserved=2078MB allocated=1946MB
2025-10-09 12:27:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 60, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.21277612447739, 'train_avg_loss': 0.6767731343706449, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 12:27:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 60, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.4400329589844, 'train_avg_loss': 0.6425834019978841, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 12:27:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 60, 'Results_raw': {'train_total': 480, 'train_loss': 308.4400329589844, 'train_avg_loss': 0.6425834019978841, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 12:27:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:27:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:27:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #60, planning to set LR to 1.00e-05
2025-10-09 12:27:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 12:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:27:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:27:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:27:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:27:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:28:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:28:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.311920, avg_loss=0.648567, seen=480, correct=317, accuracy=0.660417
2025-10-09 12:28:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:28:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:28:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:28:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=60 reserved=2078MB allocated=1946MB
2025-10-09 12:28:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 60, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.65473154187202, 'train_avg_loss': 0.6304560961822668, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 12:28:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 60, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.3119201660156, 'train_avg_loss': 0.6485665003458659, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 12:28:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 60, 'Results_raw': {'train_total': 480, 'train_loss': 311.3119201660156, 'train_avg_loss': 0.6485665003458659, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 12:28:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:28:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:28:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #60, planning to set LR to 1.00e-05
2025-10-09 12:28:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 12:28:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:28:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:28:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:28:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:28:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:29:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:29:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.260803, avg_loss=0.633877, seen=480, correct=316, accuracy=0.658333
2025-10-09 12:29:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:29:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:29:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:29:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=60 reserved=2106MB allocated=1946MB
2025-10-09 12:29:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 60, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.29383164644241, 'train_avg_loss': 0.5774485970536868, 'train_seen': 120, 'train_correct': 89, 'train_acc': 0.7416666666666667}}
2025-10-09 12:29:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 60, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.26080322265625, 'train_avg_loss': 0.6338766733805339, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 12:29:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 60, 'Results_raw': {'train_total': 480, 'train_loss': 304.26080322265625, 'train_avg_loss': 0.6338766733805339, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 12:29:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:29:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:29:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #60, planning to set LR to 1.00e-05
2025-10-09 12:29:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 12:29:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:29:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:29:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:29:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:29:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:29:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:29:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=218.282623, avg_loss=0.454755, seen=480, correct=382, accuracy=0.795833
2025-10-09 12:29:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:29:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:29:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:29:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=60 reserved=2086MB allocated=1946MB
2025-10-09 12:29:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 60, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 58.083063781261444, 'train_avg_loss': 0.48402553151051203, 'train_seen': 120, 'train_correct': 96, 'train_acc': 0.8}}
2025-10-09 12:29:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 60, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 218.28262329101562, 'train_avg_loss': 0.4547554651896159, 'train_seen': 480, 'train_correct': 382, 'train_acc': 0.7958333333333333}}
2025-10-09 12:29:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 60, 'Results_raw': {'train_total': 480, 'train_loss': 218.28262329101562, 'train_avg_loss': 0.4547554651896159, 'train_seen': 480, 'train_correct': 382, 'train_acc': 0.7958333333333333}}
2025-10-09 12:29:56 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #61) -------------
2025-10-09 12:29:56 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=61 aidx=0 | s=5 (candidates=21)
2025-10-09 12:29:56 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[26, 1, 39, 28, 43] (from 21)
2025-10-09 12:29:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:29:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:29:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #61, planning to set LR to 1.00e-05
2025-10-09 12:29:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 12:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:29:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:29:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:29:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:29:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:30:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:30:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.191589, avg_loss=0.694149, seen=480, correct=286, accuracy=0.595833
2025-10-09 12:30:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:30:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:30:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:30:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=61 reserved=2078MB allocated=1946MB
2025-10-09 12:30:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 61, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.87144286930561, 'train_avg_loss': 0.6739286905775468, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 12:30:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 61, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.19158935546875, 'train_avg_loss': 0.6941491444905599, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 12:30:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 61, 'Results_raw': {'train_total': 480, 'train_loss': 333.19158935546875, 'train_avg_loss': 0.6941491444905599, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 12:30:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:30:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:30:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #61, planning to set LR to 1.00e-05
2025-10-09 12:30:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 12:30:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:30:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:30:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:30:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:30:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:31:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:31:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=347.445312, avg_loss=0.723844, seen=480, correct=288, accuracy=0.600000
2025-10-09 12:31:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:31:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:31:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:31:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=61 reserved=2078MB allocated=1946MB
2025-10-09 12:31:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 61, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.83146953582764, 'train_avg_loss': 0.7235955794652303, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 12:31:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 61, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 347.4453125, 'train_avg_loss': 0.7238444010416667, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 12:31:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 61, 'Results_raw': {'train_total': 480, 'train_loss': 347.4453125, 'train_avg_loss': 0.7238444010416667, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 12:31:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:31:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:31:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #61, planning to set LR to 1.00e-05
2025-10-09 12:31:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 12:31:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:31:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:31:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:31:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:31:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:32:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:32:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.586365, avg_loss=0.663722, seen=480, correct=317, accuracy=0.660417
2025-10-09 12:32:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:32:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:32:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:32:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=61 reserved=2078MB allocated=1946MB
2025-10-09 12:32:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 61, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.35253828763962, 'train_avg_loss': 0.6362711523969968, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 12:32:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 61, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.58636474609375, 'train_avg_loss': 0.6637215932210286, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 12:32:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 61, 'Results_raw': {'train_total': 480, 'train_loss': 318.58636474609375, 'train_avg_loss': 0.6637215932210286, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 12:32:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:32:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:32:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #61, planning to set LR to 1.00e-05
2025-10-09 12:32:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 12:32:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:32:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:32:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:32:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:32:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:32:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:32:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.969421, avg_loss=0.627020, seen=480, correct=314, accuracy=0.654167
2025-10-09 12:32:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:32:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:32:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=61 reserved=2078MB allocated=1946MB
2025-10-09 12:32:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 61, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.48567582666874, 'train_avg_loss': 0.6373806318889061, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 12:32:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 61, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.96942138671875, 'train_avg_loss': 0.6270196278889973, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 12:32:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 61, 'Results_raw': {'train_total': 480, 'train_loss': 300.96942138671875, 'train_avg_loss': 0.6270196278889973, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 12:32:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:32:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:32:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #61, planning to set LR to 1.00e-05
2025-10-09 12:32:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 12:32:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:32:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:32:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:32:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:32:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:33:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:33:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=294.032166, avg_loss=0.612567, seen=480, correct=323, accuracy=0.672917
2025-10-09 12:33:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:33:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:33:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:33:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=61 reserved=2078MB allocated=1946MB
2025-10-09 12:33:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 61, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.78434732556343, 'train_avg_loss': 0.6732028943796953, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 12:33:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 61, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 294.03216552734375, 'train_avg_loss': 0.6125670115152995, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 12:33:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 61, 'Results_raw': {'train_total': 480, 'train_loss': 294.03216552734375, 'train_avg_loss': 0.6125670115152995, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 12:33:35 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #62) -------------
2025-10-09 12:33:35 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=62 aidx=0 | s=5 (candidates=21)
2025-10-09 12:33:35 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[26, 19, 51, 20, 40] (from 21)
2025-10-09 12:33:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:33:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:33:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #62, planning to set LR to 1.00e-05
2025-10-09 12:33:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 12:33:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:33:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:33:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:33:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:33:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:34:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:34:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.381897, avg_loss=0.679962, seen=480, correct=285, accuracy=0.593750
2025-10-09 12:34:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:34:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:34:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=62 reserved=2078MB allocated=1946MB
2025-10-09 12:34:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 62, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.13589704036713, 'train_avg_loss': 0.6427991420030594, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 12:34:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 62, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.38189697265625, 'train_avg_loss': 0.6799622853597005, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 12:34:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 62, 'Results_raw': {'train_total': 480, 'train_loss': 326.38189697265625, 'train_avg_loss': 0.6799622853597005, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 12:34:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:34:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:34:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #62, planning to set LR to 1.00e-05
2025-10-09 12:34:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:34:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:34:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:34:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:34:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:34:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:34:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:34:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=285.256500, avg_loss=0.594284, seen=480, correct=326, accuracy=0.679167
2025-10-09 12:34:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:34:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:34:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:34:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=62 reserved=2188MB allocated=1946MB
2025-10-09 12:34:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 62, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.14843082427979, 'train_avg_loss': 0.5929035902023315, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 12:34:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 62, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 285.2565002441406, 'train_avg_loss': 0.5942843755086263, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:34:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 62, 'Results_raw': {'train_total': 480, 'train_loss': 285.2565002441406, 'train_avg_loss': 0.5942843755086263, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:34:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:35:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:35:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #62, planning to set LR to 1.00e-05
2025-10-09 12:35:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 12:35:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:35:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:35:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:35:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:35:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:35:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:35:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.060120, avg_loss=0.627209, seen=480, correct=312, accuracy=0.650000
2025-10-09 12:35:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:35:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:35:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:35:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=62 reserved=2088MB allocated=1946MB
2025-10-09 12:35:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 62, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.17283743619919, 'train_avg_loss': 0.6931069786349933, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 12:35:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 62, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.06011962890625, 'train_avg_loss': 0.6272085825602214, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 12:35:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 62, 'Results_raw': {'train_total': 480, 'train_loss': 301.06011962890625, 'train_avg_loss': 0.6272085825602214, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 12:35:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:35:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:35:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #62, planning to set LR to 1.00e-05
2025-10-09 12:35:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 12:35:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:35:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:35:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:35:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:35:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:36:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:36:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=281.565033, avg_loss=0.586594, seen=480, correct=331, accuracy=0.689583
2025-10-09 12:36:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:36:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:36:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:36:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=62 reserved=2078MB allocated=1946MB
2025-10-09 12:36:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 62, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.61921225488186, 'train_avg_loss': 0.6134934354573488, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 12:36:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 62, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 281.5650329589844, 'train_avg_loss': 0.5865938186645507, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 12:36:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 62, 'Results_raw': {'train_total': 480, 'train_loss': 281.5650329589844, 'train_avg_loss': 0.5865938186645507, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 12:36:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:36:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:36:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #62, planning to set LR to 1.00e-05
2025-10-09 12:36:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 12:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:36:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:36:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:36:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:36:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:37:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:37:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.787598, avg_loss=0.668307, seen=480, correct=299, accuracy=0.622917
2025-10-09 12:37:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:37:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:37:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:37:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=62 reserved=2078MB allocated=1946MB
2025-10-09 12:37:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 62, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.63800838589668, 'train_avg_loss': 0.7053167365491391, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 12:37:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 62, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.78759765625, 'train_avg_loss': 0.6683074951171875, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 12:37:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 62, 'Results_raw': {'train_total': 480, 'train_loss': 320.78759765625, 'train_avg_loss': 0.6683074951171875, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 12:37:10 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #63) -------------
2025-10-09 12:37:11 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=63 aidx=0 | s=5 (candidates=21)
2025-10-09 12:37:11 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[20, 43, 26, 19, 29] (from 21)
2025-10-09 12:37:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:37:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:37:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #63, planning to set LR to 1.00e-05
2025-10-09 12:37:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 12:37:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:37:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:37:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:37:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:37:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:37:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:37:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=281.244141, avg_loss=0.585925, seen=480, correct=334, accuracy=0.695833
2025-10-09 12:37:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:37:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:37:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:37:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=63 reserved=2078MB allocated=1946MB
2025-10-09 12:37:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 63, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.00517281889915, 'train_avg_loss': 0.6167097734908263, 'train_seen': 120, 'train_correct': 86, 'train_acc': 0.7166666666666667}}
2025-10-09 12:37:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 63, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 281.244140625, 'train_avg_loss': 0.58592529296875, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 12:37:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 63, 'Results_raw': {'train_total': 480, 'train_loss': 281.244140625, 'train_avg_loss': 0.58592529296875, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 12:37:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:37:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:37:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #63, planning to set LR to 1.00e-05
2025-10-09 12:37:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 12:37:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:37:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:37:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:37:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:37:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:38:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:38:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=283.016907, avg_loss=0.589619, seen=480, correct=330, accuracy=0.687500
2025-10-09 12:38:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:38:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:38:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:38:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=63 reserved=2078MB allocated=1946MB
2025-10-09 12:38:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 63, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.00387120246887, 'train_avg_loss': 0.6250322600205739, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 12:38:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 63, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 283.01690673828125, 'train_avg_loss': 0.5896185557047526, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 12:38:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 63, 'Results_raw': {'train_total': 480, 'train_loss': 283.01690673828125, 'train_avg_loss': 0.5896185557047526, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 12:38:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:38:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:38:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #63, planning to set LR to 1.00e-05
2025-10-09 12:38:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 12:38:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:38:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:38:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:38:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:38:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:39:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:39:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.776337, avg_loss=0.666201, seen=480, correct=289, accuracy=0.602083
2025-10-09 12:39:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:39:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:39:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=63 reserved=2078MB allocated=1946MB
2025-10-09 12:39:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 63, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.22301679849625, 'train_avg_loss': 0.626858473320802, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 12:39:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 63, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.7763366699219, 'train_avg_loss': 0.6662007013956706, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 12:39:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 63, 'Results_raw': {'train_total': 480, 'train_loss': 319.7763366699219, 'train_avg_loss': 0.6662007013956706, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 12:39:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:39:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:39:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #63, planning to set LR to 1.00e-05
2025-10-09 12:39:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:39:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:39:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:39:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:39:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:40:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:40:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=281.324768, avg_loss=0.586093, seen=480, correct=331, accuracy=0.689583
2025-10-09 12:40:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:40:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:40:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:40:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=63 reserved=2188MB allocated=1946MB
2025-10-09 12:40:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 63, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.60407051444054, 'train_avg_loss': 0.5717005876203378, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 12:40:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 63, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 281.32476806640625, 'train_avg_loss': 0.5860932668050131, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 12:40:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 63, 'Results_raw': {'train_total': 480, 'train_loss': 281.32476806640625, 'train_avg_loss': 0.5860932668050131, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 12:40:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:40:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:40:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #63, planning to set LR to 1.00e-05
2025-10-09 12:40:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 12:40:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:40:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:40:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:40:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:40:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:40:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:40:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.283722, avg_loss=0.646424, seen=480, correct=294, accuracy=0.612500
2025-10-09 12:40:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:40:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:40:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:40:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=63 reserved=2084MB allocated=1946MB
2025-10-09 12:40:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 63, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.28844237327576, 'train_avg_loss': 0.6274036864439646, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 12:40:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 63, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.2837219238281, 'train_avg_loss': 0.6464244206746419, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 12:40:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 63, 'Results_raw': {'train_total': 480, 'train_loss': 310.2837219238281, 'train_avg_loss': 0.6464244206746419, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 12:40:47 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #64) -------------
2025-10-09 12:40:47 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=64 aidx=0 | s=5 (candidates=21)
2025-10-09 12:40:47 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[34, 39, 19, 16, 26] (from 21)
2025-10-09 12:40:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:40:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:40:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #64, planning to set LR to 1.00e-05
2025-10-09 12:40:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 12:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:40:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:40:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:40:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:40:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:41:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:41:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.258789, avg_loss=0.646372, seen=480, correct=310, accuracy=0.645833
2025-10-09 12:41:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:41:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:41:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=64 reserved=2078MB allocated=1946MB
2025-10-09 12:41:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 64, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.62001138925552, 'train_avg_loss': 0.671833428243796, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 12:41:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 64, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.2587890625, 'train_avg_loss': 0.6463724772135416, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 12:41:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 64, 'Results_raw': {'train_total': 480, 'train_loss': 310.2587890625, 'train_avg_loss': 0.6463724772135416, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 12:41:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:41:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:41:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #64, planning to set LR to 1.00e-05
2025-10-09 12:41:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 12:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:41:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:41:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:41:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:41:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:42:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:42:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.754883, avg_loss=0.641156, seen=480, correct=323, accuracy=0.672917
2025-10-09 12:42:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:42:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:42:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=64 reserved=2078MB allocated=1946MB
2025-10-09 12:42:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 64, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.72538822889328, 'train_avg_loss': 0.6393782352407773, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 12:42:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 64, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.7548828125, 'train_avg_loss': 0.641156005859375, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 12:42:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 64, 'Results_raw': {'train_total': 480, 'train_loss': 307.7548828125, 'train_avg_loss': 0.641156005859375, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 12:42:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:42:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:42:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #64, planning to set LR to 1.00e-05
2025-10-09 12:42:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:42:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:42:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:42:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:42:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:42:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:42:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:42:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=283.871735, avg_loss=0.591399, seen=480, correct=329, accuracy=0.685417
2025-10-09 12:42:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:42:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:42:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=64 reserved=2188MB allocated=1946MB
2025-10-09 12:42:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 64, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 67.70659872889519, 'train_avg_loss': 0.5642216560741266, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 12:42:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 64, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 283.8717346191406, 'train_avg_loss': 0.5913994471232097, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 12:42:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 64, 'Results_raw': {'train_total': 480, 'train_loss': 283.8717346191406, 'train_avg_loss': 0.5913994471232097, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 12:42:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:42:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:42:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #64, planning to set LR to 1.00e-05
2025-10-09 12:42:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 12:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:42:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:42:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:42:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:42:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:43:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:43:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.485291, avg_loss=0.634344, seen=480, correct=313, accuracy=0.652083
2025-10-09 12:43:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:43:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:43:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:43:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=64 reserved=2106MB allocated=1946MB
2025-10-09 12:43:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 64, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.96340963244438, 'train_avg_loss': 0.5913617469370365, 'train_seen': 120, 'train_correct': 87, 'train_acc': 0.725}}
2025-10-09 12:43:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 64, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.48529052734375, 'train_avg_loss': 0.6343443552652995, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 12:43:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 64, 'Results_raw': {'train_total': 480, 'train_loss': 304.48529052734375, 'train_avg_loss': 0.6343443552652995, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 12:43:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:43:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:43:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #64, planning to set LR to 1.00e-05
2025-10-09 12:43:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 12:43:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:43:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:43:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:43:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:43:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:44:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:44:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.614990, avg_loss=0.661698, seen=480, correct=288, accuracy=0.600000
2025-10-09 12:44:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:44:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:44:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:44:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=64 reserved=2078MB allocated=1946MB
2025-10-09 12:44:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 64, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.32545134425163, 'train_avg_loss': 0.6277120945354303, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 12:44:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 64, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.614990234375, 'train_avg_loss': 0.6616978963216146, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 12:44:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 64, 'Results_raw': {'train_total': 480, 'train_loss': 317.614990234375, 'train_avg_loss': 0.6616978963216146, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 12:44:20 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #65) -------------
2025-10-09 12:44:20 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=65 aidx=0 | s=5 (candidates=21)
2025-10-09 12:44:20 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[19, 6, 29, 26, 1] (from 21)
2025-10-09 12:44:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:44:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:44:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #65, planning to set LR to 1.00e-05
2025-10-09 12:44:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:44:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:44:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:44:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:44:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:44:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:44:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:44:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=283.597321, avg_loss=0.590828, seen=480, correct=328, accuracy=0.683333
2025-10-09 12:44:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:45:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:45:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=65 reserved=2188MB allocated=1946MB
2025-10-09 12:45:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 65, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.90528282523155, 'train_avg_loss': 0.5742106902102629, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 12:45:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 65, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 283.5973205566406, 'train_avg_loss': 0.590827751159668, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 12:45:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 65, 'Results_raw': {'train_total': 480, 'train_loss': 283.5973205566406, 'train_avg_loss': 0.590827751159668, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 12:45:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:45:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:45:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #65, planning to set LR to 1.00e-05
2025-10-09 12:45:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 12:45:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:45:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:45:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:45:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:45:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:45:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:45:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.193024, avg_loss=0.652485, seen=480, correct=298, accuracy=0.620833
2025-10-09 12:45:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:45:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:45:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:45:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=65 reserved=2086MB allocated=1946MB
2025-10-09 12:45:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 65, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.69270753860474, 'train_avg_loss': 0.6224392294883728, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 12:45:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 65, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.1930236816406, 'train_avg_loss': 0.652485466003418, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 12:45:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 65, 'Results_raw': {'train_total': 480, 'train_loss': 313.1930236816406, 'train_avg_loss': 0.652485466003418, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 12:45:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:45:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:45:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #65, planning to set LR to 1.00e-05
2025-10-09 12:45:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 12:45:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:45:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:45:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:45:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:45:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:46:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:46:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=298.154968, avg_loss=0.621156, seen=480, correct=313, accuracy=0.652083
2025-10-09 12:46:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:46:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:46:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:46:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=65 reserved=2084MB allocated=1946MB
2025-10-09 12:46:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 65, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.7813572883606, 'train_avg_loss': 0.6065113107363383, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 12:46:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 65, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 298.15496826171875, 'train_avg_loss': 0.6211561838785807, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 12:46:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 65, 'Results_raw': {'train_total': 480, 'train_loss': 298.15496826171875, 'train_avg_loss': 0.6211561838785807, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 12:46:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:46:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:46:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #65, planning to set LR to 1.00e-05
2025-10-09 12:46:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 12:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:46:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:46:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:46:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:46:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:47:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:47:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.440643, avg_loss=0.642585, seen=480, correct=300, accuracy=0.625000
2025-10-09 12:47:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:47:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:47:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:47:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=65 reserved=2078MB allocated=1946MB
2025-10-09 12:47:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 65, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.7482119500637, 'train_avg_loss': 0.6062350995838642, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 12:47:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 65, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.4406433105469, 'train_avg_loss': 0.6425846735636394, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 12:47:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 65, 'Results_raw': {'train_total': 480, 'train_loss': 308.4406433105469, 'train_avg_loss': 0.6425846735636394, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 12:47:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:47:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:47:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #65, planning to set LR to 1.00e-05
2025-10-09 12:47:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 12:47:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:47:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:47:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:47:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:47:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:47:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:47:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.240631, avg_loss=0.675501, seen=480, correct=277, accuracy=0.577083
2025-10-09 12:47:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:47:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:47:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:47:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=65 reserved=2078MB allocated=1946MB
2025-10-09 12:47:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 65, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.46723091602325, 'train_avg_loss': 0.6705602576335271, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 12:47:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 65, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.2406311035156, 'train_avg_loss': 0.6755013147989909, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 12:47:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 65, 'Results_raw': {'train_total': 480, 'train_loss': 324.2406311035156, 'train_avg_loss': 0.6755013147989909, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 12:47:55 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #66) -------------
2025-10-09 12:47:56 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=66 aidx=0 | s=5 (candidates=21)
2025-10-09 12:47:56 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[19, 39, 6, 16, 10] (from 21)
2025-10-09 12:47:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:47:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:47:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #66, planning to set LR to 1.00e-05
2025-10-09 12:47:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:47:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:47:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:47:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:47:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:47:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:48:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:48:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=285.591095, avg_loss=0.594981, seen=480, correct=334, accuracy=0.695833
2025-10-09 12:48:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:48:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:48:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:48:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=66 reserved=2188MB allocated=1946MB
2025-10-09 12:48:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 66, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.22994434833527, 'train_avg_loss': 0.5769162029027939, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 12:48:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 66, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 285.5910949707031, 'train_avg_loss': 0.5949814478556316, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 12:48:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 66, 'Results_raw': {'train_total': 480, 'train_loss': 285.5910949707031, 'train_avg_loss': 0.5949814478556316, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 12:48:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:48:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:48:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #66, planning to set LR to 1.00e-05
2025-10-09 12:48:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 12:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:48:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:48:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:48:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:48:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:49:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:49:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.727783, avg_loss=0.641100, seen=480, correct=320, accuracy=0.666667
2025-10-09 12:49:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:49:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:49:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:49:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=66 reserved=2078MB allocated=1946MB
2025-10-09 12:49:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 66, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.60735785961151, 'train_avg_loss': 0.6383946488300959, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 12:49:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 66, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.727783203125, 'train_avg_loss': 0.6410995483398437, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 12:49:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 66, 'Results_raw': {'train_total': 480, 'train_loss': 307.727783203125, 'train_avg_loss': 0.6410995483398437, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 12:49:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:49:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:49:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #66, planning to set LR to 1.00e-05
2025-10-09 12:49:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 12:49:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:49:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:49:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:49:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:49:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:50:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:50:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.725067, avg_loss=0.649427, seen=480, correct=294, accuracy=0.612500
2025-10-09 12:50:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:50:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:50:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:50:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=66 reserved=2084MB allocated=1946MB
2025-10-09 12:50:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 66, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.42453783750534, 'train_avg_loss': 0.6285378153125445, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 12:50:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 66, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.7250671386719, 'train_avg_loss': 0.6494272232055665, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 12:50:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 66, 'Results_raw': {'train_total': 480, 'train_loss': 311.7250671386719, 'train_avg_loss': 0.6494272232055665, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 12:50:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:50:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:50:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #66, planning to set LR to 1.00e-05
2025-10-09 12:50:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 12:50:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:50:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:50:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:50:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:50:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:50:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:50:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.333984, avg_loss=0.638196, seen=480, correct=317, accuracy=0.660417
2025-10-09 12:50:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:50:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:50:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=66 reserved=2108MB allocated=1946MB
2025-10-09 12:50:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 66, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.3750571012497, 'train_avg_loss': 0.6031254758437474, 'train_seen': 120, 'train_correct': 90, 'train_acc': 0.75}}
2025-10-09 12:50:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 66, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.333984375, 'train_avg_loss': 0.63819580078125, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 12:50:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 66, 'Results_raw': {'train_total': 480, 'train_loss': 306.333984375, 'train_avg_loss': 0.63819580078125, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 12:50:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:50:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:50:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #66, planning to set LR to 1.00e-05
2025-10-09 12:50:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 12:50:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:50:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:50:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:50:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:50:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:51:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:51:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=290.983002, avg_loss=0.606215, seen=480, correct=326, accuracy=0.679167
2025-10-09 12:51:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:51:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:51:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=66 reserved=2078MB allocated=1946MB
2025-10-09 12:51:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 66, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.49209734797478, 'train_avg_loss': 0.5874341445664565, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 12:51:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 66, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 290.9830017089844, 'train_avg_loss': 0.6062145868937174, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:51:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 66, 'Results_raw': {'train_total': 480, 'train_loss': 290.9830017089844, 'train_avg_loss': 0.6062145868937174, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:51:29 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #67) -------------
2025-10-09 12:51:29 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=67 aidx=0 | s=5 (candidates=21)
2025-10-09 12:51:29 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[13, 19, 47, 22, 28] (from 21)
2025-10-09 12:51:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:51:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:51:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #67, planning to set LR to 1.00e-05
2025-10-09 12:51:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 12:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:51:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:51:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:51:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:51:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:52:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:52:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=284.946564, avg_loss=0.593639, seen=480, correct=331, accuracy=0.689583
2025-10-09 12:52:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:52:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:52:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:52:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=67 reserved=2078MB allocated=1946MB
2025-10-09 12:52:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 67, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.31311494112015, 'train_avg_loss': 0.6109426245093346, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 12:52:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 67, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 284.9465637207031, 'train_avg_loss': 0.5936386744181316, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 12:52:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 67, 'Results_raw': {'train_total': 480, 'train_loss': 284.9465637207031, 'train_avg_loss': 0.5936386744181316, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 12:52:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:52:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:52:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #67, planning to set LR to 1.00e-05
2025-10-09 12:52:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:52:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:52:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:52:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:52:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:52:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:52:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=282.836365, avg_loss=0.589242, seen=480, correct=330, accuracy=0.687500
2025-10-09 12:52:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:52:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:52:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:52:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=67 reserved=2188MB allocated=1946MB
2025-10-09 12:52:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 67, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 66.88829779624939, 'train_avg_loss': 0.5574024816354116, 'train_seen': 120, 'train_correct': 86, 'train_acc': 0.7166666666666667}}
2025-10-09 12:52:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 67, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 282.83636474609375, 'train_avg_loss': 0.589242426554362, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 12:52:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 67, 'Results_raw': {'train_total': 480, 'train_loss': 282.83636474609375, 'train_avg_loss': 0.589242426554362, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 12:52:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:52:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:52:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #67, planning to set LR to 1.00e-05
2025-10-09 12:52:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 12:52:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:52:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:52:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:52:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:52:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:53:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:53:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.823273, avg_loss=0.632965, seen=480, correct=304, accuracy=0.633333
2025-10-09 12:53:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:53:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:53:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=67 reserved=2078MB allocated=1946MB
2025-10-09 12:53:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 67, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.18427032232285, 'train_avg_loss': 0.6098689193526904, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 12:53:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 67, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.8232727050781, 'train_avg_loss': 0.6329651514689127, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 12:53:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 67, 'Results_raw': {'train_total': 480, 'train_loss': 303.8232727050781, 'train_avg_loss': 0.6329651514689127, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 12:53:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:53:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:53:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #67, planning to set LR to 1.00e-05
2025-10-09 12:53:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 12:53:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:53:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:53:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:53:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:53:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:54:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:54:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=224.472794, avg_loss=0.467652, seen=480, correct=394, accuracy=0.820833
2025-10-09 12:54:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:54:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:54:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:54:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=67 reserved=2086MB allocated=1946MB
2025-10-09 12:54:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 67, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 59.31437774002552, 'train_avg_loss': 0.49428648116687934, 'train_seen': 120, 'train_correct': 99, 'train_acc': 0.825}}
2025-10-09 12:54:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 67, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 224.47279357910156, 'train_avg_loss': 0.4676516532897949, 'train_seen': 480, 'train_correct': 394, 'train_acc': 0.8208333333333333}}
2025-10-09 12:54:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 67, 'Results_raw': {'train_total': 480, 'train_loss': 224.47279357910156, 'train_avg_loss': 0.4676516532897949, 'train_seen': 480, 'train_correct': 394, 'train_acc': 0.8208333333333333}}
2025-10-09 12:54:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:54:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:54:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #67, planning to set LR to 1.00e-05
2025-10-09 12:54:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 12:54:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:54:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:54:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:54:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:54:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:55:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:55:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=295.851532, avg_loss=0.616357, seen=480, correct=318, accuracy=0.662500
2025-10-09 12:55:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:55:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:55:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=67 reserved=2078MB allocated=1946MB
2025-10-09 12:55:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 67, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.07658898830414, 'train_avg_loss': 0.6256382415692011, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 12:55:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 67, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 295.8515319824219, 'train_avg_loss': 0.6163573582967122, 'train_seen': 480, 'train_correct': 318, 'train_acc': 0.6625}}
2025-10-09 12:55:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 67, 'Results_raw': {'train_total': 480, 'train_loss': 295.8515319824219, 'train_avg_loss': 0.6163573582967122, 'train_seen': 480, 'train_correct': 318, 'train_acc': 0.6625}}
2025-10-09 12:55:10 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #68) -------------
2025-10-09 12:55:10 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=68 aidx=0 | s=5 (candidates=21)
2025-10-09 12:55:10 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[19, 39, 43, 22, 40] (from 21)
2025-10-09 12:55:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:55:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:55:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #68, planning to set LR to 1.00e-05
2025-10-09 12:55:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 12:55:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:55:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:55:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:55:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:55:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:55:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:55:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=283.555115, avg_loss=0.590740, seen=480, correct=326, accuracy=0.679167
2025-10-09 12:55:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:55:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:55:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=68 reserved=2188MB allocated=1946MB
2025-10-09 12:55:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 68, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.09054966270924, 'train_avg_loss': 0.575754580522577, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 12:55:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 68, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 283.55511474609375, 'train_avg_loss': 0.5907398223876953, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:55:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 68, 'Results_raw': {'train_total': 480, 'train_loss': 283.55511474609375, 'train_avg_loss': 0.5907398223876953, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 12:55:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:55:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:55:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #68, planning to set LR to 1.00e-05
2025-10-09 12:55:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 12:55:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:55:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:55:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:55:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:55:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:56:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:56:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.034302, avg_loss=0.637571, seen=480, correct=321, accuracy=0.668750
2025-10-09 12:56:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:56:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:56:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:56:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=68 reserved=2078MB allocated=1946MB
2025-10-09 12:56:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 68, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.58530122041702, 'train_avg_loss': 0.6382108435034752, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 12:56:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 68, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.0343017578125, 'train_avg_loss': 0.6375714619954427, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 12:56:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 68, 'Results_raw': {'train_total': 480, 'train_loss': 306.0343017578125, 'train_avg_loss': 0.6375714619954427, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 12:56:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:56:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:56:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #68, planning to set LR to 1.00e-05
2025-10-09 12:56:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-09 12:56:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:56:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:56:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:56:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:56:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:57:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:57:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=278.844849, avg_loss=0.580927, seen=480, correct=345, accuracy=0.718750
2025-10-09 12:57:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:57:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:57:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:57:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=68 reserved=2078MB allocated=1946MB
2025-10-09 12:57:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 68, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.41875460743904, 'train_avg_loss': 0.611822955061992, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 12:57:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 68, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 278.8448486328125, 'train_avg_loss': 0.5809267679850261, 'train_seen': 480, 'train_correct': 345, 'train_acc': 0.71875}}
2025-10-09 12:57:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 68, 'Results_raw': {'train_total': 480, 'train_loss': 278.8448486328125, 'train_avg_loss': 0.5809267679850261, 'train_seen': 480, 'train_correct': 345, 'train_acc': 0.71875}}
2025-10-09 12:57:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:57:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:57:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #68, planning to set LR to 1.00e-05
2025-10-09 12:57:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 12:57:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:57:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:57:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:57:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:57:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:58:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:58:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=219.574570, avg_loss=0.457447, seen=480, correct=393, accuracy=0.818750
2025-10-09 12:58:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:58:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:58:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:58:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=68 reserved=2086MB allocated=1946MB
2025-10-09 12:58:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 68, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 58.93352188169956, 'train_avg_loss': 0.49111268234749633, 'train_seen': 120, 'train_correct': 100, 'train_acc': 0.8333333333333334}}
2025-10-09 12:58:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 68, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 219.57456970214844, 'train_avg_loss': 0.45744702021280925, 'train_seen': 480, 'train_correct': 393, 'train_acc': 0.81875}}
2025-10-09 12:58:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 68, 'Results_raw': {'train_total': 480, 'train_loss': 219.57456970214844, 'train_avg_loss': 0.45744702021280925, 'train_seen': 480, 'train_correct': 393, 'train_acc': 0.81875}}
2025-10-09 12:58:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:58:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:58:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #68, planning to set LR to 1.00e-05
2025-10-09 12:58:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-09 12:58:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:58:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:58:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:58:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:58:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:58:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:58:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.031219, avg_loss=0.637565, seen=480, correct=312, accuracy=0.650000
2025-10-09 12:58:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:58:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:58:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:58:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=68 reserved=2078MB allocated=1946MB
2025-10-09 12:58:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 68, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.98530146479607, 'train_avg_loss': 0.6498775122066339, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 12:58:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 68, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.0312194824219, 'train_avg_loss': 0.6375650405883789, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 12:58:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 68, 'Results_raw': {'train_total': 480, 'train_loss': 306.0312194824219, 'train_avg_loss': 0.6375650405883789, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 12:58:51 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #69) -------------
2025-10-09 12:58:52 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=69 aidx=0 | s=5 (candidates=21)
2025-10-09 12:58:52 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[34, 26, 47, 28, 45] (from 21)
2025-10-09 12:58:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:58:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:58:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #69, planning to set LR to 1.00e-05
2025-10-09 12:58:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 12:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:58:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:58:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:58:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:58:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 12:59:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 12:59:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.282959, avg_loss=0.644339, seen=480, correct=310, accuracy=0.645833
2025-10-09 12:59:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 12:59:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:59:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 12:59:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=69 reserved=2078MB allocated=1946MB
2025-10-09 12:59:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 69, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.6362275481224, 'train_avg_loss': 0.67196856290102, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 12:59:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 69, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.282958984375, 'train_avg_loss': 0.6443394978841146, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 12:59:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 69, 'Results_raw': {'train_total': 480, 'train_loss': 309.282958984375, 'train_avg_loss': 0.6443394978841146, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 12:59:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 12:59:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 12:59:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #69, planning to set LR to 1.00e-05
2025-10-09 12:59:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 12:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 12:59:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 12:59:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 12:59:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 12:59:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:00:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:00:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.856110, avg_loss=0.645534, seen=480, correct=303, accuracy=0.631250
2025-10-09 13:00:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:00:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:00:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:00:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=69 reserved=2078MB allocated=1946MB
2025-10-09 13:00:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 69, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.01984402537346, 'train_avg_loss': 0.6001653668781122, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 13:00:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 69, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.8561096191406, 'train_avg_loss': 0.645533561706543, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 13:00:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 69, 'Results_raw': {'train_total': 480, 'train_loss': 309.8561096191406, 'train_avg_loss': 0.645533561706543, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 13:00:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:00:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:00:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #69, planning to set LR to 1.00e-05
2025-10-09 13:00:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 13:00:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:00:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:00:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:00:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:00:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:01:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:01:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.324097, avg_loss=0.631925, seen=480, correct=303, accuracy=0.631250
2025-10-09 13:01:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:01:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:01:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:01:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=69 reserved=2078MB allocated=1946MB
2025-10-09 13:01:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 69, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.31542760133743, 'train_avg_loss': 0.6026285633444786, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 13:01:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 69, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.3240966796875, 'train_avg_loss': 0.6319252014160156, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 13:01:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 69, 'Results_raw': {'train_total': 480, 'train_loss': 303.3240966796875, 'train_avg_loss': 0.6319252014160156, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 13:01:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:01:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:01:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #69, planning to set LR to 1.00e-05
2025-10-09 13:01:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 13:01:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:01:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:01:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:01:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:01:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:01:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:01:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=286.933472, avg_loss=0.597778, seen=480, correct=329, accuracy=0.685417
2025-10-09 13:01:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:01:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:01:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:01:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=69 reserved=2078MB allocated=1946MB
2025-10-09 13:01:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 69, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.33074334263802, 'train_avg_loss': 0.6027561945219835, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 13:01:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 69, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 286.9334716796875, 'train_avg_loss': 0.5977780659993489, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 13:01:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 69, 'Results_raw': {'train_total': 480, 'train_loss': 286.9334716796875, 'train_avg_loss': 0.5977780659993489, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 13:01:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:01:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:01:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #69, planning to set LR to 1.00e-05
2025-10-09 13:01:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 13:01:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:01:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:01:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:01:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:01:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:02:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:02:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.671143, avg_loss=0.649315, seen=480, correct=296, accuracy=0.616667
2025-10-09 13:02:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:02:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:02:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:02:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=69 reserved=2078MB allocated=1946MB
2025-10-09 13:02:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 69, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.80914229154587, 'train_avg_loss': 0.6400761857628823, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 13:02:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 69, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.671142578125, 'train_avg_loss': 0.6493148803710938, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 13:02:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 69, 'Results_raw': {'train_total': 480, 'train_loss': 311.671142578125, 'train_avg_loss': 0.6493148803710938, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 13:02:29 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #70) -------------
2025-10-09 13:02:30 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=70 aidx=0 | s=5 (candidates=21)
2025-10-09 13:02:30 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[19, 48, 15, 26, 47] (from 21)
2025-10-09 13:02:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:02:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:02:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #70, planning to set LR to 1.00e-05
2025-10-09 13:02:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 13:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:02:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:02:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:02:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:02:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:03:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:03:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=282.358643, avg_loss=0.588247, seen=480, correct=329, accuracy=0.685417
2025-10-09 13:03:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:03:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:03:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:03:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=70 reserved=2188MB allocated=1946MB
2025-10-09 13:03:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 70, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.44243854284286, 'train_avg_loss': 0.5703536545236906, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 13:03:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 70, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 282.358642578125, 'train_avg_loss': 0.5882471720377604, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 13:03:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 70, 'Results_raw': {'train_total': 480, 'train_loss': 282.358642578125, 'train_avg_loss': 0.5882471720377604, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 13:03:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:03:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:03:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #70, planning to set LR to 1.00e-05
2025-10-09 13:03:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 13:03:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:03:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:03:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:03:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:03:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:03:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:03:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.201324, avg_loss=0.644169, seen=480, correct=296, accuracy=0.616667
2025-10-09 13:03:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:03:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:03:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:03:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=70 reserved=2078MB allocated=1946MB
2025-10-09 13:03:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 70, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.60909855365753, 'train_avg_loss': 0.7300758212804794, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 13:03:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 70, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.2013244628906, 'train_avg_loss': 0.6441694259643554, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 13:03:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 70, 'Results_raw': {'train_total': 480, 'train_loss': 309.2013244628906, 'train_avg_loss': 0.6441694259643554, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 13:03:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:03:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:03:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #70, planning to set LR to 1.00e-05
2025-10-09 13:03:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 13:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:03:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:03:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:03:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:03:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:04:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:04:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.503204, avg_loss=0.663548, seen=480, correct=294, accuracy=0.612500
2025-10-09 13:04:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:04:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:04:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:04:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=70 reserved=2078MB allocated=1946MB
2025-10-09 13:04:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 70, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.69580835103989, 'train_avg_loss': 0.6891317362586658, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 13:04:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 70, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.5032043457031, 'train_avg_loss': 0.6635483423868815, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 13:04:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 70, 'Results_raw': {'train_total': 480, 'train_loss': 318.5032043457031, 'train_avg_loss': 0.6635483423868815, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 13:04:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:04:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:04:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #70, planning to set LR to 1.00e-05
2025-10-09 13:04:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 13:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:04:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:04:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:04:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:04:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:05:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:05:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.474731, avg_loss=0.644739, seen=480, correct=298, accuracy=0.620833
2025-10-09 13:05:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:05:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:05:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:05:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=70 reserved=2078MB allocated=1946MB
2025-10-09 13:05:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 70, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.56843447685242, 'train_avg_loss': 0.5964036206404368, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 13:05:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 70, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.4747314453125, 'train_avg_loss': 0.6447390238444011, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 13:05:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 70, 'Results_raw': {'train_total': 480, 'train_loss': 309.4747314453125, 'train_avg_loss': 0.6447390238444011, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 13:05:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:05:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:05:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #70, planning to set LR to 1.00e-05
2025-10-09 13:05:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 13:05:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:05:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:05:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:05:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:05:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:05:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:05:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.741302, avg_loss=0.626544, seen=480, correct=308, accuracy=0.641667
2025-10-09 13:05:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:06:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:06:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=70 reserved=2078MB allocated=1946MB
2025-10-09 13:06:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 70, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.78696554899216, 'train_avg_loss': 0.598224712908268, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 13:06:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 70, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.7413024902344, 'train_avg_loss': 0.6265443801879883, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:06:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 70, 'Results_raw': {'train_total': 480, 'train_loss': 300.7413024902344, 'train_avg_loss': 0.6265443801879883, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:06:01 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #71) -------------
2025-10-09 13:06:02 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=71 aidx=0 | s=5 (candidates=21)
2025-10-09 13:06:02 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[10, 29, 47, 22, 15] (from 21)
2025-10-09 13:06:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:06:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:06:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #71, planning to set LR to 1.00e-05
2025-10-09 13:06:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 13:06:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:06:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:06:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:06:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:06:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:06:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:06:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=282.909058, avg_loss=0.589394, seen=480, correct=322, accuracy=0.670833
2025-10-09 13:06:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:06:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:06:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=71 reserved=2078MB allocated=1946MB
2025-10-09 13:06:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 71, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.05469542741776, 'train_avg_loss': 0.5671224618951479, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 13:06:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 71, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 282.9090576171875, 'train_avg_loss': 0.5893938700358073, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 13:06:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 71, 'Results_raw': {'train_total': 480, 'train_loss': 282.9090576171875, 'train_avg_loss': 0.5893938700358073, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 13:06:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:06:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:06:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #71, planning to set LR to 1.00e-05
2025-10-09 13:06:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 13:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:06:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:06:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:06:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:06:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:07:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:07:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.784119, avg_loss=0.626634, seen=480, correct=308, accuracy=0.641667
2025-10-09 13:07:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:07:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:07:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=71 reserved=2084MB allocated=1946MB
2025-10-09 13:07:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 71, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.7151543200016, 'train_avg_loss': 0.6059596193333466, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 13:07:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 71, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.78411865234375, 'train_avg_loss': 0.6266335805257162, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:07:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 71, 'Results_raw': {'train_total': 480, 'train_loss': 300.78411865234375, 'train_avg_loss': 0.6266335805257162, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:07:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:07:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:07:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #71, planning to set LR to 1.00e-05
2025-10-09 13:07:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 13:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:07:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:07:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:07:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:07:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:08:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:08:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.651764, avg_loss=0.611775, seen=480, correct=319, accuracy=0.664583
2025-10-09 13:08:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:08:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:08:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:08:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=71 reserved=2078MB allocated=1946MB
2025-10-09 13:08:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 71, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.49689322710037, 'train_avg_loss': 0.5708074435591698, 'train_seen': 120, 'train_correct': 86, 'train_acc': 0.7166666666666667}}
2025-10-09 13:08:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 71, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.6517639160156, 'train_avg_loss': 0.6117745081583659, 'train_seen': 480, 'train_correct': 319, 'train_acc': 0.6645833333333333}}
2025-10-09 13:08:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 71, 'Results_raw': {'train_total': 480, 'train_loss': 293.6517639160156, 'train_avg_loss': 0.6117745081583659, 'train_seen': 480, 'train_correct': 319, 'train_acc': 0.6645833333333333}}
2025-10-09 13:08:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:08:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:08:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #71, planning to set LR to 1.00e-05
2025-10-09 13:08:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 13:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:08:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:08:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:08:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:08:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:08:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:08:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=223.875061, avg_loss=0.466406, seen=480, correct=386, accuracy=0.804167
2025-10-09 13:08:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:08:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:08:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=71 reserved=2086MB allocated=1946MB
2025-10-09 13:08:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 71, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 59.098597407341, 'train_avg_loss': 0.4924883117278417, 'train_seen': 120, 'train_correct': 98, 'train_acc': 0.8166666666666667}}
2025-10-09 13:08:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 71, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 223.87506103515625, 'train_avg_loss': 0.4664063771565755, 'train_seen': 480, 'train_correct': 386, 'train_acc': 0.8041666666666667}}
2025-10-09 13:08:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 71, 'Results_raw': {'train_total': 480, 'train_loss': 223.87506103515625, 'train_avg_loss': 0.4664063771565755, 'train_seen': 480, 'train_correct': 386, 'train_acc': 0.8041666666666667}}
2025-10-09 13:08:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:08:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:08:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #71, planning to set LR to 1.00e-05
2025-10-09 13:08:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 13:08:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:08:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:08:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:08:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:08:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:09:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:09:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.793518, avg_loss=0.662070, seen=480, correct=296, accuracy=0.616667
2025-10-09 13:09:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:09:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:09:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=71 reserved=2078MB allocated=1946MB
2025-10-09 13:09:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 71, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.85444754362106, 'train_avg_loss': 0.6904537295301755, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 13:09:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 71, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.79351806640625, 'train_avg_loss': 0.662069829305013, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 13:09:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 71, 'Results_raw': {'train_total': 480, 'train_loss': 317.79351806640625, 'train_avg_loss': 0.662069829305013, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 13:09:37 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #72) -------------
2025-10-09 13:09:38 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=72 aidx=0 | s=5 (candidates=21)
2025-10-09 13:09:38 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[6, 51, 1, 34, 22] (from 21)
2025-10-09 13:09:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:09:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:09:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #72, planning to set LR to 1.00e-05
2025-10-09 13:09:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 13:09:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:09:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:09:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:09:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:09:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:10:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:10:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.124634, avg_loss=0.641926, seen=480, correct=305, accuracy=0.635417
2025-10-09 13:10:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:10:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:10:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:10:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=72 reserved=2086MB allocated=1946MB
2025-10-09 13:10:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 72, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.87962585687637, 'train_avg_loss': 0.6239968821406364, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 13:10:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 72, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.1246337890625, 'train_avg_loss': 0.6419263203938802, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 13:10:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 72, 'Results_raw': {'train_total': 480, 'train_loss': 308.1246337890625, 'train_avg_loss': 0.6419263203938802, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 13:10:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:10:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:10:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #72, planning to set LR to 1.00e-05
2025-10-09 13:10:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 13:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:10:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:10:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:10:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:10:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:11:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:11:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.925720, avg_loss=0.631095, seen=480, correct=297, accuracy=0.618750
2025-10-09 13:11:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:11:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:11:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:11:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=72 reserved=2088MB allocated=1946MB
2025-10-09 13:11:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 72, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.28269010782242, 'train_avg_loss': 0.6690224175651868, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 13:11:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 72, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.92572021484375, 'train_avg_loss': 0.6310952504475912, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 13:11:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 72, 'Results_raw': {'train_total': 480, 'train_loss': 302.92572021484375, 'train_avg_loss': 0.6310952504475912, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 13:11:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:11:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:11:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #72, planning to set LR to 1.00e-05
2025-10-09 13:11:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-09 13:11:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:11:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:11:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:11:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:11:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:11:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:11:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.593109, avg_loss=0.659569, seen=480, correct=288, accuracy=0.600000
2025-10-09 13:11:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:11:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:11:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:11:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=72 reserved=2078MB allocated=1946MB
2025-10-09 13:11:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 72, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.0604916214943, 'train_avg_loss': 0.6505040968457858, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 13:11:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 72, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.5931091308594, 'train_avg_loss': 0.659568977355957, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 13:11:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 72, 'Results_raw': {'train_total': 480, 'train_loss': 316.5931091308594, 'train_avg_loss': 0.659568977355957, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 13:11:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:11:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:11:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #72, planning to set LR to 1.00e-05
2025-10-09 13:11:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 13:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:11:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:11:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:11:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:11:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:12:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:12:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.646698, avg_loss=0.643014, seen=480, correct=306, accuracy=0.637500
2025-10-09 13:12:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:12:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:12:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=72 reserved=2078MB allocated=1946MB
2025-10-09 13:12:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 72, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.6243656873703, 'train_avg_loss': 0.6718697140614192, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 13:12:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 72, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.6466979980469, 'train_avg_loss': 0.6430139541625977, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 13:12:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 72, 'Results_raw': {'train_total': 480, 'train_loss': 308.6466979980469, 'train_avg_loss': 0.6430139541625977, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 13:12:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:12:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:12:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #72, planning to set LR to 1.00e-05
2025-10-09 13:12:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 13:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:12:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:12:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:12:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:12:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:13:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:13:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=231.662994, avg_loss=0.482631, seen=480, correct=380, accuracy=0.791667
2025-10-09 13:13:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:13:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:13:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=72 reserved=2086MB allocated=1946MB
2025-10-09 13:13:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 72, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 59.89233133196831, 'train_avg_loss': 0.4991027610997359, 'train_seen': 120, 'train_correct': 96, 'train_acc': 0.8}}
2025-10-09 13:13:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 72, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 231.66299438476562, 'train_avg_loss': 0.48263123830159504, 'train_seen': 480, 'train_correct': 380, 'train_acc': 0.7916666666666666}}
2025-10-09 13:13:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 72, 'Results_raw': {'train_total': 480, 'train_loss': 231.66299438476562, 'train_avg_loss': 0.48263123830159504, 'train_seen': 480, 'train_correct': 380, 'train_acc': 0.7916666666666666}}
2025-10-09 13:13:12 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #73) -------------
2025-10-09 13:13:12 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=73 aidx=0 | s=5 (candidates=21)
2025-10-09 13:13:12 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[29, 48, 51, 19, 28] (from 21)
2025-10-09 13:13:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:13:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:13:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #73, planning to set LR to 1.00e-05
2025-10-09 13:13:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 13:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:13:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:13:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:13:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:13:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:13:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:13:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.339661, avg_loss=0.638208, seen=480, correct=300, accuracy=0.625000
2025-10-09 13:13:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:13:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:13:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:13:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=73 reserved=2084MB allocated=1946MB
2025-10-09 13:13:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 73, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.60502867400646, 'train_avg_loss': 0.6133752389500539, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 13:13:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 73, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.33966064453125, 'train_avg_loss': 0.6382076263427734, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 13:13:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 73, 'Results_raw': {'train_total': 480, 'train_loss': 306.33966064453125, 'train_avg_loss': 0.6382076263427734, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 13:13:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:13:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:13:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #73, planning to set LR to 1.00e-05
2025-10-09 13:13:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-09 13:13:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:13:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:13:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:13:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:13:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:14:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:14:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.967834, avg_loss=0.666600, seen=480, correct=291, accuracy=0.606250
2025-10-09 13:14:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:14:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:14:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=73 reserved=2078MB allocated=1946MB
2025-10-09 13:14:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 73, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 94.49702650308609, 'train_avg_loss': 0.7874752208590508, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 13:14:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 73, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.96783447265625, 'train_avg_loss': 0.6665996551513672, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 13:14:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 73, 'Results_raw': {'train_total': 480, 'train_loss': 319.96783447265625, 'train_avg_loss': 0.6665996551513672, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 13:14:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:14:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:14:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #73, planning to set LR to 1.00e-05
2025-10-09 13:14:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 13:14:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:14:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:14:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:14:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:14:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:15:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:15:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.850403, avg_loss=0.635105, seen=480, correct=309, accuracy=0.643750
2025-10-09 13:15:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:15:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:15:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=73 reserved=2088MB allocated=1946MB
2025-10-09 13:15:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 73, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.42833006381989, 'train_avg_loss': 0.6785694171984991, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 13:15:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 73, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.85040283203125, 'train_avg_loss': 0.6351050059000651, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 13:15:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 73, 'Results_raw': {'train_total': 480, 'train_loss': 304.85040283203125, 'train_avg_loss': 0.6351050059000651, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 13:15:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:15:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:15:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #73, planning to set LR to 1.00e-05
2025-10-09 13:15:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 13:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:15:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:15:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:15:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:15:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:15:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:15:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=283.067261, avg_loss=0.589723, seen=480, correct=326, accuracy=0.679167
2025-10-09 13:15:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:15:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:15:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:16:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=73 reserved=2188MB allocated=1946MB
2025-10-09 13:16:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 73, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.92060919106007, 'train_avg_loss': 0.5826717432588339, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 13:16:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 73, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 283.0672607421875, 'train_avg_loss': 0.5897234598795573, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 13:16:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 73, 'Results_raw': {'train_total': 480, 'train_loss': 283.0672607421875, 'train_avg_loss': 0.5897234598795573, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 13:16:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:16:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:16:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #73, planning to set LR to 1.00e-05
2025-10-09 13:16:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 13:16:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:16:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:16:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:16:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:16:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:16:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:16:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=290.433716, avg_loss=0.605070, seen=480, correct=324, accuracy=0.675000
2025-10-09 13:16:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:16:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:16:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=73 reserved=2078MB allocated=1946MB
2025-10-09 13:16:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 73, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.21662020683289, 'train_avg_loss': 0.6018051683902741, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 13:16:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 73, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 290.4337158203125, 'train_avg_loss': 0.6050702412923177, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 13:16:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 73, 'Results_raw': {'train_total': 480, 'train_loss': 290.4337158203125, 'train_avg_loss': 0.6050702412923177, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 13:16:45 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #74) -------------
2025-10-09 13:16:45 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=74 aidx=0 | s=5 (candidates=21)
2025-10-09 13:16:45 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[16, 19, 10, 6, 28] (from 21)
2025-10-09 13:16:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:16:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:16:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #74, planning to set LR to 1.00e-05
2025-10-09 13:16:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 13:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:16:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:16:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:16:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:16:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:17:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:17:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.592834, avg_loss=0.636652, seen=480, correct=322, accuracy=0.670833
2025-10-09 13:17:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:17:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:17:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:17:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=74 reserved=2106MB allocated=1946MB
2025-10-09 13:17:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 74, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.68514244258404, 'train_avg_loss': 0.5890428536882003, 'train_seen': 120, 'train_correct': 89, 'train_acc': 0.7416666666666667}}
2025-10-09 13:17:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 74, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.59283447265625, 'train_avg_loss': 0.6366517384847005, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 13:17:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 74, 'Results_raw': {'train_total': 480, 'train_loss': 305.59283447265625, 'train_avg_loss': 0.6366517384847005, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 13:17:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:17:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:17:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #74, planning to set LR to 1.00e-05
2025-10-09 13:17:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-09 13:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:17:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:17:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:17:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:17:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:18:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:18:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=282.854584, avg_loss=0.589280, seen=480, correct=328, accuracy=0.683333
2025-10-09 13:18:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:18:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:18:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:18:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=74 reserved=2188MB allocated=1946MB
2025-10-09 13:18:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 74, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.53105556964874, 'train_avg_loss': 0.5794254630804062, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 13:18:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 74, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 282.8545837402344, 'train_avg_loss': 0.589280382792155, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 13:18:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 74, 'Results_raw': {'train_total': 480, 'train_loss': 282.8545837402344, 'train_avg_loss': 0.589280382792155, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 13:18:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:18:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:18:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #74, planning to set LR to 1.00e-05
2025-10-09 13:18:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-09 13:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:18:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:18:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:18:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:18:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:18:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:18:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=282.391022, avg_loss=0.588315, seen=480, correct=329, accuracy=0.685417
2025-10-09 13:18:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:18:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:18:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=74 reserved=2078MB allocated=1946MB
2025-10-09 13:18:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 74, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 66.61838720738888, 'train_avg_loss': 0.5551532267282406, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 13:18:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 74, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 282.3910217285156, 'train_avg_loss': 0.5883146286010742, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 13:18:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 74, 'Results_raw': {'train_total': 480, 'train_loss': 282.3910217285156, 'train_avg_loss': 0.5883146286010742, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 13:18:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:19:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:19:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #74, planning to set LR to 1.00e-05
2025-10-09 13:19:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 13:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:19:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:19:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:19:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:19:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:19:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:19:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.004852, avg_loss=0.658343, seen=480, correct=302, accuracy=0.629167
2025-10-09 13:19:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:19:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:19:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=74 reserved=2086MB allocated=1946MB
2025-10-09 13:19:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 74, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.71448262035847, 'train_avg_loss': 0.6226206885029872, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 13:19:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 74, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.0048522949219, 'train_avg_loss': 0.6583434422810872, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 13:19:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 74, 'Results_raw': {'train_total': 480, 'train_loss': 316.0048522949219, 'train_avg_loss': 0.6583434422810872, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 13:19:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:19:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:19:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #74, planning to set LR to 1.00e-05
2025-10-09 13:19:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 13:19:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:19:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:19:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:19:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:19:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:20:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:20:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=280.227112, avg_loss=0.583806, seen=480, correct=330, accuracy=0.687500
2025-10-09 13:20:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:20:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:20:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:20:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=74 reserved=2078MB allocated=1946MB
2025-10-09 13:20:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 74, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.7943754196167, 'train_avg_loss': 0.5899531284968058, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 13:20:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 74, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 280.22711181640625, 'train_avg_loss': 0.5838064829508464, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 13:20:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 74, 'Results_raw': {'train_total': 480, 'train_loss': 280.22711181640625, 'train_avg_loss': 0.5838064829508464, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 13:20:23 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #75) -------------
2025-10-09 13:20:23 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=75 aidx=0 | s=5 (candidates=21)
2025-10-09 13:20:23 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[45, 29, 26, 15, 39] (from 21)
2025-10-09 13:20:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:20:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:20:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #75, planning to set LR to 1.00e-05
2025-10-09 13:20:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-09 13:20:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:20:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:20:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:20:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:20:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:21:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:21:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.000000, avg_loss=0.643750, seen=480, correct=315, accuracy=0.656250
2025-10-09 13:21:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:21:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:21:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:21:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=75 reserved=2078MB allocated=1946MB
2025-10-09 13:21:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 75, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.47409471869469, 'train_avg_loss': 0.637284122655789, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 13:21:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 75, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.0, 'train_avg_loss': 0.64375, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 13:21:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 75, 'Results_raw': {'train_total': 480, 'train_loss': 309.0, 'train_avg_loss': 0.64375, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 13:21:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:21:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:21:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #75, planning to set LR to 1.00e-05
2025-10-09 13:21:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 13:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:21:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:21:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:21:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:21:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:21:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:21:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.863159, avg_loss=0.628882, seen=480, correct=308, accuracy=0.641667
2025-10-09 13:21:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:21:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:21:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:21:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=75 reserved=2084MB allocated=1946MB
2025-10-09 13:21:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 75, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.39826664328575, 'train_avg_loss': 0.6033188886940479, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 13:21:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 75, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.8631591796875, 'train_avg_loss': 0.6288815816243489, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:21:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 75, 'Results_raw': {'train_total': 480, 'train_loss': 301.8631591796875, 'train_avg_loss': 0.6288815816243489, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:21:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:21:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:21:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #75, planning to set LR to 1.00e-05
2025-10-09 13:21:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 13:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:21:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:21:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:21:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:21:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:22:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:22:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.353058, avg_loss=0.652819, seen=480, correct=302, accuracy=0.629167
2025-10-09 13:22:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:22:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:22:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=75 reserved=2078MB allocated=1946MB
2025-10-09 13:22:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 75, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.75016650557518, 'train_avg_loss': 0.6062513875464598, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 13:22:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 75, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.3530578613281, 'train_avg_loss': 0.6528188705444335, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 13:22:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 75, 'Results_raw': {'train_total': 480, 'train_loss': 313.3530578613281, 'train_avg_loss': 0.6528188705444335, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 13:22:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:22:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:22:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #75, planning to set LR to 1.00e-05
2025-10-09 13:22:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-09 13:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:22:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:22:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:22:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:22:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:23:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:23:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.319336, avg_loss=0.661082, seen=480, correct=297, accuracy=0.618750
2025-10-09 13:23:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:23:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:23:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:23:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=75 reserved=2078MB allocated=1946MB
2025-10-09 13:23:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 75, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.07921686768532, 'train_avg_loss': 0.7089934738973777, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 13:23:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 75, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.3193359375, 'train_avg_loss': 0.6610819498697916, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 13:23:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 75, 'Results_raw': {'train_total': 480, 'train_loss': 317.3193359375, 'train_avg_loss': 0.6610819498697916, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 13:23:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:23:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:23:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #75, planning to set LR to 1.00e-05
2025-10-09 13:23:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 13:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:23:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:23:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:23:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:23:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:24:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:24:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.053558, avg_loss=0.639695, seen=480, correct=323, accuracy=0.672917
2025-10-09 13:24:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:24:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:24:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:24:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=75 reserved=2078MB allocated=1946MB
2025-10-09 13:24:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 75, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.56469643115997, 'train_avg_loss': 0.6380391369263331, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 13:24:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 75, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.0535583496094, 'train_avg_loss': 0.6396949132283528, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 13:24:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 75, 'Results_raw': {'train_total': 480, 'train_loss': 307.0535583496094, 'train_avg_loss': 0.6396949132283528, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 13:24:04 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #76) -------------
2025-10-09 13:24:04 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=76 aidx=0 | s=5 (candidates=21)
2025-10-09 13:24:04 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[46, 22, 51, 16, 20] (from 21)
2025-10-09 13:24:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:24:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:24:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #76, planning to set LR to 1.00e-05
2025-10-09 13:24:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 13:24:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:24:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:24:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:24:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:24:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:24:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:24:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.679260, avg_loss=0.653498, seen=480, correct=309, accuracy=0.643750
2025-10-09 13:24:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:24:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:24:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:24:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=76 reserved=2104MB allocated=1946MB
2025-10-09 13:24:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 76, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.044942766428, 'train_avg_loss': 0.5837078563868999, 'train_seen': 120, 'train_correct': 86, 'train_acc': 0.7166666666666667}}
2025-10-09 13:24:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 76, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.67926025390625, 'train_avg_loss': 0.6534984588623047, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 13:24:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 76, 'Results_raw': {'train_total': 480, 'train_loss': 313.67926025390625, 'train_avg_loss': 0.6534984588623047, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 13:24:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:24:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:24:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #76, planning to set LR to 1.00e-05
2025-10-09 13:24:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-09 13:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:24:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:24:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:24:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:24:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:25:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:25:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=200.113174, avg_loss=0.416902, seen=480, correct=399, accuracy=0.831250
2025-10-09 13:25:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:25:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:25:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:25:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=76 reserved=2086MB allocated=1946MB
2025-10-09 13:25:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 76, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 55.897969380021095, 'train_avg_loss': 0.4658164115001758, 'train_seen': 120, 'train_correct': 99, 'train_acc': 0.825}}
2025-10-09 13:25:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 76, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 200.11317443847656, 'train_avg_loss': 0.4169024467468262, 'train_seen': 480, 'train_correct': 399, 'train_acc': 0.83125}}
2025-10-09 13:25:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 76, 'Results_raw': {'train_total': 480, 'train_loss': 200.11317443847656, 'train_avg_loss': 0.4169024467468262, 'train_seen': 480, 'train_correct': 399, 'train_acc': 0.83125}}
2025-10-09 13:25:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:25:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:25:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #76, planning to set LR to 1.00e-05
2025-10-09 13:25:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 13:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:25:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:25:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:25:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:25:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:26:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:26:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.384644, avg_loss=0.640385, seen=480, correct=303, accuracy=0.631250
2025-10-09 13:26:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:26:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:26:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=76 reserved=2088MB allocated=1946MB
2025-10-09 13:26:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 76, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.99494856595993, 'train_avg_loss': 0.6832912380496661, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 13:26:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 76, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.3846435546875, 'train_avg_loss': 0.6403846740722656, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 13:26:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 76, 'Results_raw': {'train_total': 480, 'train_loss': 307.3846435546875, 'train_avg_loss': 0.6403846740722656, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 13:26:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:26:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:26:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #76, planning to set LR to 1.00e-05
2025-10-09 13:26:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 13:26:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:26:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:26:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:26:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:26:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:26:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:26:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.756256, avg_loss=0.626576, seen=480, correct=324, accuracy=0.675000
2025-10-09 13:26:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:26:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:26:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:26:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=76 reserved=2108MB allocated=1946MB
2025-10-09 13:26:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 76, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.13881605863571, 'train_avg_loss': 0.5761568004886309, 'train_seen': 120, 'train_correct': 87, 'train_acc': 0.725}}
2025-10-09 13:26:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 76, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.7562561035156, 'train_avg_loss': 0.6265755335489909, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 13:26:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 76, 'Results_raw': {'train_total': 480, 'train_loss': 300.7562561035156, 'train_avg_loss': 0.6265755335489909, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 13:26:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:26:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:26:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #76, planning to set LR to 1.00e-05
2025-10-09 13:26:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 13:26:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:26:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:26:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:26:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:26:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:27:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:27:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=279.596558, avg_loss=0.582493, seen=480, correct=331, accuracy=0.689583
2025-10-09 13:27:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:27:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:27:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:27:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=76 reserved=2078MB allocated=1946MB
2025-10-09 13:27:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 76, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.8240815103054, 'train_avg_loss': 0.6318673459192117, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 13:27:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 76, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 279.5965576171875, 'train_avg_loss': 0.5824928283691406, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 13:27:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 76, 'Results_raw': {'train_total': 480, 'train_loss': 279.5965576171875, 'train_avg_loss': 0.5824928283691406, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 13:27:42 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #77) -------------
2025-10-09 13:27:42 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=77 aidx=0 | s=5 (candidates=21)
2025-10-09 13:27:42 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[28, 20, 29, 47, 26] (from 21)
2025-10-09 13:27:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:27:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:27:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #77, planning to set LR to 1.00e-05
2025-10-09 13:27:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-09 13:27:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:27:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:27:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:27:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:27:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:28:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:28:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=274.387787, avg_loss=0.571641, seen=480, correct=345, accuracy=0.718750
2025-10-09 13:28:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:28:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:28:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:28:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=77 reserved=2078MB allocated=1946MB
2025-10-09 13:28:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 77, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.85100971162319, 'train_avg_loss': 0.5737584142635266, 'train_seen': 120, 'train_correct': 91, 'train_acc': 0.7583333333333333}}
2025-10-09 13:28:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 77, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 274.3877868652344, 'train_avg_loss': 0.571641222635905, 'train_seen': 480, 'train_correct': 345, 'train_acc': 0.71875}}
2025-10-09 13:28:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 77, 'Results_raw': {'train_total': 480, 'train_loss': 274.3877868652344, 'train_avg_loss': 0.571641222635905, 'train_seen': 480, 'train_correct': 345, 'train_acc': 0.71875}}
2025-10-09 13:28:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:28:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:28:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #77, planning to set LR to 1.00e-05
2025-10-09 13:28:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-09 13:28:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:28:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:28:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:28:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:28:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:29:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:29:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=272.126892, avg_loss=0.566931, seen=480, correct=340, accuracy=0.708333
2025-10-09 13:29:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:29:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:29:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:29:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=77 reserved=2078MB allocated=1946MB
2025-10-09 13:29:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 77, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.77187871932983, 'train_avg_loss': 0.6230989893277487, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 13:29:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 77, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 272.12689208984375, 'train_avg_loss': 0.5669310251871745, 'train_seen': 480, 'train_correct': 340, 'train_acc': 0.7083333333333334}}
2025-10-09 13:29:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 77, 'Results_raw': {'train_total': 480, 'train_loss': 272.12689208984375, 'train_avg_loss': 0.5669310251871745, 'train_seen': 480, 'train_correct': 340, 'train_acc': 0.7083333333333334}}
2025-10-09 13:29:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:29:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:29:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #77, planning to set LR to 1.00e-05
2025-10-09 13:29:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 13:29:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:29:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:29:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:29:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:29:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:29:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:29:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.738342, avg_loss=0.630705, seen=480, correct=308, accuracy=0.641667
2025-10-09 13:29:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:29:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:29:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:29:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=77 reserved=2084MB allocated=1946MB
2025-10-09 13:29:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 77, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.85390648245811, 'train_avg_loss': 0.5987825540204843, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 13:29:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 77, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.73834228515625, 'train_avg_loss': 0.6307048797607422, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:29:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 77, 'Results_raw': {'train_total': 480, 'train_loss': 302.73834228515625, 'train_avg_loss': 0.6307048797607422, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:29:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:29:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:29:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #77, planning to set LR to 1.00e-05
2025-10-09 13:29:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-09 13:29:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:29:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:29:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:29:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:29:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:30:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:30:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=298.097168, avg_loss=0.621036, seen=480, correct=313, accuracy=0.652083
2025-10-09 13:30:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:30:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:30:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:30:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=77 reserved=2078MB allocated=1946MB
2025-10-09 13:30:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 77, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.41034299135208, 'train_avg_loss': 0.5867528582612673, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 13:30:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 77, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 298.09716796875, 'train_avg_loss': 0.6210357666015625, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 13:30:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 77, 'Results_raw': {'train_total': 480, 'train_loss': 298.09716796875, 'train_avg_loss': 0.6210357666015625, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 13:30:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:30:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:30:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #77, planning to set LR to 1.00e-05
2025-10-09 13:30:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 13:30:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:30:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:30:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:30:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:30:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:31:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:31:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.306488, avg_loss=0.652722, seen=480, correct=298, accuracy=0.620833
2025-10-09 13:31:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:31:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:31:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:31:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=77 reserved=2078MB allocated=1946MB
2025-10-09 13:31:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 77, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.55616238713264, 'train_avg_loss': 0.6046346865594387, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 13:31:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 77, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.3064880371094, 'train_avg_loss': 0.6527218500773112, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 13:31:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 77, 'Results_raw': {'train_total': 480, 'train_loss': 313.3064880371094, 'train_avg_loss': 0.6527218500773112, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 13:31:17 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #78) -------------
2025-10-09 13:31:17 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=78 aidx=0 | s=5 (candidates=21)
2025-10-09 13:31:17 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[16, 39, 34, 6, 13] (from 21)
2025-10-09 13:31:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:31:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:31:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #78, planning to set LR to 1.00e-05
2025-10-09 13:31:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-09 13:31:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:31:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:31:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:31:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:31:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:31:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:31:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.453888, avg_loss=0.625946, seen=480, correct=330, accuracy=0.687500
2025-10-09 13:31:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:31:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:32:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=78 reserved=2108MB allocated=1946MB
2025-10-09 13:32:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 78, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.23783528804779, 'train_avg_loss': 0.585315294067065, 'train_seen': 120, 'train_correct': 91, 'train_acc': 0.7583333333333333}}
2025-10-09 13:32:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 78, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.4538879394531, 'train_avg_loss': 0.6259455998738607, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 13:32:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 78, 'Results_raw': {'train_total': 480, 'train_loss': 300.4538879394531, 'train_avg_loss': 0.6259455998738607, 'train_seen': 480, 'train_correct': 330, 'train_acc': 0.6875}}
2025-10-09 13:32:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:32:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:32:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #78, planning to set LR to 1.00e-05
2025-10-09 13:32:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-09 13:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:32:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:32:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:32:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:32:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:32:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:32:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.165527, avg_loss=0.625345, seen=480, correct=324, accuracy=0.675000
2025-10-09 13:32:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:32:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:32:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:32:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=78 reserved=2078MB allocated=1946MB
2025-10-09 13:32:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 78, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.03787797689438, 'train_avg_loss': 0.6336489831407864, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 13:32:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 78, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.16552734375, 'train_avg_loss': 0.6253448486328125, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 13:32:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 78, 'Results_raw': {'train_total': 480, 'train_loss': 300.16552734375, 'train_avg_loss': 0.6253448486328125, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 13:32:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:32:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:32:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #78, planning to set LR to 1.00e-05
2025-10-09 13:32:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-09 13:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:32:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:32:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:32:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:32:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:33:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:33:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.047363, avg_loss=0.650099, seen=480, correct=304, accuracy=0.633333
2025-10-09 13:33:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:33:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:33:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=78 reserved=2078MB allocated=1946MB
2025-10-09 13:33:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 78, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.57259649038315, 'train_avg_loss': 0.6797716374198596, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 13:33:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 78, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.04736328125, 'train_avg_loss': 0.6500986735026042, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 13:33:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 78, 'Results_raw': {'train_total': 480, 'train_loss': 312.04736328125, 'train_avg_loss': 0.6500986735026042, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 13:33:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:33:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:33:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #78, planning to set LR to 1.00e-05
2025-10-09 13:33:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-09 13:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:33:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:33:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:33:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:33:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:34:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:34:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.662445, avg_loss=0.651380, seen=480, correct=301, accuracy=0.627083
2025-10-09 13:34:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:34:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:34:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:34:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=78 reserved=2086MB allocated=1946MB
2025-10-09 13:34:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 78, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.0736693739891, 'train_avg_loss': 0.6172805781165759, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 13:34:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 78, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.6624450683594, 'train_avg_loss': 0.6513800938924154, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 13:34:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 78, 'Results_raw': {'train_total': 480, 'train_loss': 312.6624450683594, 'train_avg_loss': 0.6513800938924154, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 13:34:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:34:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:34:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #78, planning to set LR to 1.00e-05
2025-10-09 13:34:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 13:34:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:34:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:34:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:34:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:34:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:34:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:34:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=284.059937, avg_loss=0.591792, seen=480, correct=331, accuracy=0.689583
2025-10-09 13:34:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:34:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:34:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:34:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=78 reserved=2078MB allocated=1946MB
2025-10-09 13:34:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 78, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.55146837234497, 'train_avg_loss': 0.6129289031028747, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 13:34:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 78, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 284.0599365234375, 'train_avg_loss': 0.5917915344238281, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 13:34:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 78, 'Results_raw': {'train_total': 480, 'train_loss': 284.0599365234375, 'train_avg_loss': 0.5917915344238281, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 13:34:58 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #79) -------------
2025-10-09 13:34:58 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=79 aidx=0 | s=5 (candidates=21)
2025-10-09 13:34:58 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[13, 29, 51, 46, 26] (from 21)
2025-10-09 13:35:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:35:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:35:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #79, planning to set LR to 1.00e-05
2025-10-09 13:35:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-09 13:35:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:35:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:35:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:35:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:35:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:35:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:35:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=270.162720, avg_loss=0.562839, seen=480, correct=347, accuracy=0.722917
2025-10-09 13:35:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:35:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:35:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:35:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=79 reserved=2078MB allocated=1946MB
2025-10-09 13:35:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 79, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.18359798192978, 'train_avg_loss': 0.5848633165160815, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 13:35:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 79, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 270.1627197265625, 'train_avg_loss': 0.5628389994303385, 'train_seen': 480, 'train_correct': 347, 'train_acc': 0.7229166666666667}}
2025-10-09 13:35:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 79, 'Results_raw': {'train_total': 480, 'train_loss': 270.1627197265625, 'train_avg_loss': 0.5628389994303385, 'train_seen': 480, 'train_correct': 347, 'train_acc': 0.7229166666666667}}
2025-10-09 13:35:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:35:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:35:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #79, planning to set LR to 1.00e-05
2025-10-09 13:35:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-09 13:35:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:35:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:35:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:35:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:35:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:36:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:36:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=297.842072, avg_loss=0.620504, seen=480, correct=308, accuracy=0.641667
2025-10-09 13:36:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:36:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:36:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:36:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=79 reserved=2084MB allocated=1946MB
2025-10-09 13:36:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 79, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.39188531041145, 'train_avg_loss': 0.5949323775867621, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 13:36:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 79, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 297.8420715332031, 'train_avg_loss': 0.6205043156941732, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:36:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 79, 'Results_raw': {'train_total': 480, 'train_loss': 297.8420715332031, 'train_avg_loss': 0.6205043156941732, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 13:36:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:36:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:36:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #79, planning to set LR to 1.00e-05
2025-10-09 13:36:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-09 13:36:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:36:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:36:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:36:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:36:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:37:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:37:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.321289, avg_loss=0.634003, seen=480, correct=302, accuracy=0.629167
2025-10-09 13:37:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:37:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:37:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:37:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=79 reserved=2088MB allocated=1946MB
2025-10-09 13:37:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 79, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.24021881818771, 'train_avg_loss': 0.6770018234848976, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 13:37:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 79, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.3212890625, 'train_avg_loss': 0.634002685546875, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 13:37:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 79, 'Results_raw': {'train_total': 480, 'train_loss': 304.3212890625, 'train_avg_loss': 0.634002685546875, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 13:37:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:37:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:37:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #79, planning to set LR to 1.00e-05
2025-10-09 13:37:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-09 13:37:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:37:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:37:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:37:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:37:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:37:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:37:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.744781, avg_loss=0.647385, seen=480, correct=299, accuracy=0.622917
2025-10-09 13:37:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:37:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:37:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=79 reserved=2104MB allocated=1946MB
2025-10-09 13:37:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 79, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.18870773911476, 'train_avg_loss': 0.601572564492623, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 13:37:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 79, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.7447814941406, 'train_avg_loss': 0.6473849614461263, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 13:37:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 79, 'Results_raw': {'train_total': 480, 'train_loss': 310.7447814941406, 'train_avg_loss': 0.6473849614461263, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 13:37:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:37:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:37:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #79, planning to set LR to 1.00e-05
2025-10-09 13:37:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-09 13:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:37:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:37:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:37:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:37:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:38:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:38:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.101685, avg_loss=0.627295, seen=480, correct=316, accuracy=0.658333
2025-10-09 13:38:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:38:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:38:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:38:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=79 reserved=2078MB allocated=1946MB
2025-10-09 13:38:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 79, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.90028549730778, 'train_avg_loss': 0.5825023791442315, 'train_seen': 120, 'train_correct': 88, 'train_acc': 0.7333333333333333}}
2025-10-09 13:38:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 79, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.1016845703125, 'train_avg_loss': 0.627295176188151, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 13:38:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 79, 'Results_raw': {'train_total': 480, 'train_loss': 301.1016845703125, 'train_avg_loss': 0.627295176188151, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 13:38:34 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #80) -------------
2025-10-09 13:38:35 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=80 aidx=1 | s=5 (candidates=23)
2025-10-09 13:38:35 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[42, 18, 27, 30, 24] (from 23)
2025-10-09 13:38:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:38:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:38:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #80, planning to set LR to 1.00e-05
2025-10-09 13:38:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 13:38:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:38:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:38:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:38:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:38:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:39:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:39:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=339.738861, avg_loss=0.707789, seen=480, correct=242, accuracy=0.504167
2025-10-09 13:39:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:39:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:39:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=80 reserved=2120MB allocated=1980MB
2025-10-09 13:39:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 80, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.83001232147217, 'train_avg_loss': 0.7069167693456014, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 13:39:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 80, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 339.7388610839844, 'train_avg_loss': 0.7077892939249675, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 13:39:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 80, 'Results_raw': {'train_total': 480, 'train_loss': 339.7388610839844, 'train_avg_loss': 0.7077892939249675, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 13:39:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:39:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:39:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #80, planning to set LR to 1.00e-05
2025-10-09 13:39:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 13:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:39:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:39:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:39:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:39:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:40:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:40:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=341.131805, avg_loss=0.710691, seen=480, correct=256, accuracy=0.533333
2025-10-09 13:40:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:40:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:40:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:40:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=80 reserved=2154MB allocated=1988MB
2025-10-09 13:40:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 80, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.60684168338776, 'train_avg_loss': 0.721723680694898, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 13:40:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 80, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 341.1318054199219, 'train_avg_loss': 0.710691261291504, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 13:40:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 80, 'Results_raw': {'train_total': 480, 'train_loss': 341.1318054199219, 'train_avg_loss': 0.710691261291504, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 13:40:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:40:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:40:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #80, planning to set LR to 1.00e-05
2025-10-09 13:40:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 13:40:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:40:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:40:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:40:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:40:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:40:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:40:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=342.787720, avg_loss=0.714141, seen=480, correct=235, accuracy=0.489583
2025-10-09 13:40:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:40:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:40:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=80 reserved=2104MB allocated=1997MB
2025-10-09 13:40:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 80, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.85657614469528, 'train_avg_loss': 0.7154714678724606, 'train_seen': 120, 'train_correct': 55, 'train_acc': 0.4583333333333333}}
2025-10-09 13:40:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 80, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 342.7877197265625, 'train_avg_loss': 0.7141410827636718, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 13:40:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 80, 'Results_raw': {'train_total': 480, 'train_loss': 342.7877197265625, 'train_avg_loss': 0.7141410827636718, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 13:40:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:40:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:40:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #80, planning to set LR to 1.00e-05
2025-10-09 13:40:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 13:40:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:40:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:40:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:40:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:40:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:41:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:41:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=343.674866, avg_loss=0.715989, seen=480, correct=235, accuracy=0.489583
2025-10-09 13:41:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:41:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:41:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=80 reserved=2104MB allocated=2005MB
2025-10-09 13:41:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 80, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.63483667373657, 'train_avg_loss': 0.7302903056144714, 'train_seen': 120, 'train_correct': 55, 'train_acc': 0.4583333333333333}}
2025-10-09 13:41:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 80, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.67486572265625, 'train_avg_loss': 0.7159893035888671, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 13:41:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 80, 'Results_raw': {'train_total': 480, 'train_loss': 343.67486572265625, 'train_avg_loss': 0.7159893035888671, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 13:41:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:41:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:41:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #80, planning to set LR to 1.00e-05
2025-10-09 13:41:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 13:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:41:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:41:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:41:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:41:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:42:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:42:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=353.471497, avg_loss=0.736399, seen=480, correct=229, accuracy=0.477083
2025-10-09 13:42:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:42:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:42:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=80 reserved=2104MB allocated=2014MB
2025-10-09 13:42:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 80, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.68750929832458, 'train_avg_loss': 0.7307292441527049, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 13:42:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 80, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 353.47149658203125, 'train_avg_loss': 0.7363989512125652, 'train_seen': 480, 'train_correct': 229, 'train_acc': 0.47708333333333336}}
2025-10-09 13:42:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 80, 'Results_raw': {'train_total': 480, 'train_loss': 353.47149658203125, 'train_avg_loss': 0.7363989512125652, 'train_seen': 480, 'train_correct': 229, 'train_acc': 0.47708333333333336}}
2025-10-09 13:42:13 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #81) -------------
2025-10-09 13:42:13 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=81 aidx=1 | s=5 (candidates=23)
2025-10-09 13:42:13 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[17, 4, 25, 7, 14] (from 23)
2025-10-09 13:42:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:42:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:42:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #81, planning to set LR to 1.00e-05
2025-10-09 13:42:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 13:42:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:42:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:42:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:42:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:42:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:42:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:42:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=339.430023, avg_loss=0.707146, seen=480, correct=246, accuracy=0.512500
2025-10-09 13:42:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:42:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:42:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=81 reserved=2184MB allocated=2047MB
2025-10-09 13:42:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 81, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.97028505802155, 'train_avg_loss': 0.7080857088168462, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 13:42:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 81, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 339.4300231933594, 'train_avg_loss': 0.707145881652832, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 13:42:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 81, 'Results_raw': {'train_total': 480, 'train_loss': 339.4300231933594, 'train_avg_loss': 0.707145881652832, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 13:42:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:42:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:42:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #81, planning to set LR to 1.00e-05
2025-10-09 13:42:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 13:42:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:43:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:43:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:43:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:43:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:43:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:43:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=340.927460, avg_loss=0.710266, seen=480, correct=229, accuracy=0.477083
2025-10-09 13:43:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:43:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:43:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:43:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=81 reserved=2176MB allocated=2056MB
2025-10-09 13:43:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 81, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.59455037117004, 'train_avg_loss': 0.6632879197597503, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 13:43:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 81, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.9274597167969, 'train_avg_loss': 0.7102655410766602, 'train_seen': 480, 'train_correct': 229, 'train_acc': 0.47708333333333336}}
2025-10-09 13:43:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 81, 'Results_raw': {'train_total': 480, 'train_loss': 340.9274597167969, 'train_avg_loss': 0.7102655410766602, 'train_seen': 480, 'train_correct': 229, 'train_acc': 0.47708333333333336}}
2025-10-09 13:43:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:43:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:43:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #81, planning to set LR to 1.00e-05
2025-10-09 13:43:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 13:43:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:43:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:43:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:43:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:43:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:44:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:44:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=343.590088, avg_loss=0.715813, seen=480, correct=233, accuracy=0.485417
2025-10-09 13:44:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:44:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:44:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:44:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=81 reserved=2126MB allocated=2064MB
2025-10-09 13:44:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 81, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.86256217956543, 'train_avg_loss': 0.7071880181630452, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 13:44:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 81, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.590087890625, 'train_avg_loss': 0.7158126831054688, 'train_seen': 480, 'train_correct': 233, 'train_acc': 0.48541666666666666}}
2025-10-09 13:44:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 81, 'Results_raw': {'train_total': 480, 'train_loss': 343.590087890625, 'train_avg_loss': 0.7158126831054688, 'train_seen': 480, 'train_correct': 233, 'train_acc': 0.48541666666666666}}
2025-10-09 13:44:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:44:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:44:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #81, planning to set LR to 1.00e-05
2025-10-09 13:44:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 13:44:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:44:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:44:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:44:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:44:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:45:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:45:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=338.205963, avg_loss=0.704596, seen=480, correct=240, accuracy=0.500000
2025-10-09 13:45:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:45:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:45:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:45:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=81 reserved=2158MB allocated=2073MB
2025-10-09 13:45:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 81, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.42979967594147, 'train_avg_loss': 0.7285816639661788, 'train_seen': 120, 'train_correct': 53, 'train_acc': 0.44166666666666665}}
2025-10-09 13:45:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 81, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.2059631347656, 'train_avg_loss': 0.7045957565307617, 'train_seen': 480, 'train_correct': 240, 'train_acc': 0.5}}
2025-10-09 13:45:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 81, 'Results_raw': {'train_total': 480, 'train_loss': 338.2059631347656, 'train_avg_loss': 0.7045957565307617, 'train_seen': 480, 'train_correct': 240, 'train_acc': 0.5}}
2025-10-09 13:45:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:45:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:45:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #81, planning to set LR to 1.00e-05
2025-10-09 13:45:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 13:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:45:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:45:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:45:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:45:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:45:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:45:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=344.769104, avg_loss=0.718269, seen=480, correct=241, accuracy=0.502083
2025-10-09 13:45:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:45:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:45:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=81 reserved=2128MB allocated=2081MB
2025-10-09 13:45:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 81, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.90746659040451, 'train_avg_loss': 0.7158955549200375, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 13:45:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 81, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 344.76910400390625, 'train_avg_loss': 0.7182689666748047, 'train_seen': 480, 'train_correct': 241, 'train_acc': 0.5020833333333333}}
2025-10-09 13:45:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 81, 'Results_raw': {'train_total': 480, 'train_loss': 344.76910400390625, 'train_avg_loss': 0.7182689666748047, 'train_seen': 480, 'train_correct': 241, 'train_acc': 0.5020833333333333}}
2025-10-09 13:45:52 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #82) -------------
2025-10-09 13:45:53 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=82 aidx=1 | s=5 (candidates=23)
2025-10-09 13:45:53 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[27, 30, 2, 42, 9] (from 23)
2025-10-09 13:45:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:45:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:45:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #82, planning to set LR to 1.00e-05
2025-10-09 13:45:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 13:45:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:45:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:45:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:45:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:45:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:46:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:46:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=343.428619, avg_loss=0.715476, seen=480, correct=233, accuracy=0.485417
2025-10-09 13:46:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:46:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:46:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=82 reserved=2106MB allocated=2056MB
2025-10-09 13:46:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 82, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 89.01329499483109, 'train_avg_loss': 0.7417774582902591, 'train_seen': 120, 'train_correct': 51, 'train_acc': 0.425}}
2025-10-09 13:46:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 82, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.4286193847656, 'train_avg_loss': 0.7154762903849284, 'train_seen': 480, 'train_correct': 233, 'train_acc': 0.48541666666666666}}
2025-10-09 13:46:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 82, 'Results_raw': {'train_total': 480, 'train_loss': 343.4286193847656, 'train_avg_loss': 0.7154762903849284, 'train_seen': 480, 'train_correct': 233, 'train_acc': 0.48541666666666666}}
2025-10-09 13:46:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:46:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:46:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #82, planning to set LR to 1.00e-05
2025-10-09 13:46:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 13:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:46:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:46:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:46:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:46:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:47:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:47:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.282776, avg_loss=0.702672, seen=480, correct=252, accuracy=0.525000
2025-10-09 13:47:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:47:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:47:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:47:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=82 reserved=2104MB allocated=2056MB
2025-10-09 13:47:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 82, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.91972023248672, 'train_avg_loss': 0.6909976686040561, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 13:47:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 82, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.28277587890625, 'train_avg_loss': 0.7026724497477214, 'train_seen': 480, 'train_correct': 252, 'train_acc': 0.525}}
2025-10-09 13:47:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 82, 'Results_raw': {'train_total': 480, 'train_loss': 337.28277587890625, 'train_avg_loss': 0.7026724497477214, 'train_seen': 480, 'train_correct': 252, 'train_acc': 0.525}}
2025-10-09 13:47:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:47:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:47:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #82, planning to set LR to 1.00e-05
2025-10-09 13:47:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 13:47:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:47:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:47:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:47:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:47:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:47:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:47:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.356750, avg_loss=0.702827, seen=480, correct=260, accuracy=0.541667
2025-10-09 13:47:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:47:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:47:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:48:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=82 reserved=2198MB allocated=2090MB
2025-10-09 13:48:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 82, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.77194458246231, 'train_avg_loss': 0.698099538187186, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 13:48:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 82, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.35675048828125, 'train_avg_loss': 0.7028265635172526, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 13:48:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 82, 'Results_raw': {'train_total': 480, 'train_loss': 337.35675048828125, 'train_avg_loss': 0.7028265635172526, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 13:48:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:48:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:48:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #82, planning to set LR to 1.00e-05
2025-10-09 13:48:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 13:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:48:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:48:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:48:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:48:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:48:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:48:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=341.472748, avg_loss=0.711402, seen=480, correct=238, accuracy=0.495833
2025-10-09 13:48:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:48:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:48:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:48:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=82 reserved=2104MB allocated=2065MB
2025-10-09 13:48:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 82, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.57733184099197, 'train_avg_loss': 0.7048110986749331, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 13:48:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 82, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 341.4727478027344, 'train_avg_loss': 0.7114015579223633, 'train_seen': 480, 'train_correct': 238, 'train_acc': 0.49583333333333335}}
2025-10-09 13:48:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 82, 'Results_raw': {'train_total': 480, 'train_loss': 341.4727478027344, 'train_avg_loss': 0.7114015579223633, 'train_seen': 480, 'train_correct': 238, 'train_acc': 0.49583333333333335}}
2025-10-09 13:48:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:48:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:48:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #82, planning to set LR to 1.00e-05
2025-10-09 13:48:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 13:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:48:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:48:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:48:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:48:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:49:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:49:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=338.472809, avg_loss=0.705152, seen=480, correct=248, accuracy=0.516667
2025-10-09 13:49:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:49:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:49:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:49:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=82 reserved=2142MB allocated=2098MB
2025-10-09 13:49:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 82, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.0822993516922, 'train_avg_loss': 0.7256858279307683, 'train_seen': 120, 'train_correct': 55, 'train_acc': 0.4583333333333333}}
2025-10-09 13:49:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 82, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.4728088378906, 'train_avg_loss': 0.7051516850789388, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 13:49:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 82, 'Results_raw': {'train_total': 480, 'train_loss': 338.4728088378906, 'train_avg_loss': 0.7051516850789388, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 13:49:28 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #83) -------------
2025-10-09 13:49:28 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=83 aidx=1 | s=5 (candidates=23)
2025-10-09 13:49:28 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[44, 9, 52, 49, 4] (from 23)
2025-10-09 13:49:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:49:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:49:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #83, planning to set LR to 1.00e-05
2025-10-09 13:49:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 13:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:49:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:49:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:49:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:49:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:50:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:50:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.394592, avg_loss=0.700822, seen=480, correct=251, accuracy=0.522917
2025-10-09 13:50:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:50:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:50:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:50:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=83 reserved=2128MB allocated=2107MB
2025-10-09 13:50:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 83, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.7773015499115, 'train_avg_loss': 0.6898108462492625, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 13:50:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 83, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.39459228515625, 'train_avg_loss': 0.7008220672607421, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 13:50:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 83, 'Results_raw': {'train_total': 480, 'train_loss': 336.39459228515625, 'train_avg_loss': 0.7008220672607421, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 13:50:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:50:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:50:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #83, planning to set LR to 1.00e-05
2025-10-09 13:50:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 13:50:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:50:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:50:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:50:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:50:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:50:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:50:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.203369, avg_loss=0.690007, seen=480, correct=236, accuracy=0.491667
2025-10-09 13:50:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:50:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:50:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:50:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=83 reserved=2150MB allocated=2082MB
2025-10-09 13:50:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 83, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.74851655960083, 'train_avg_loss': 0.7062376379966736, 'train_seen': 120, 'train_correct': 53, 'train_acc': 0.44166666666666665}}
2025-10-09 13:50:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 83, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.203369140625, 'train_avg_loss': 0.6900070190429688, 'train_seen': 480, 'train_correct': 236, 'train_acc': 0.49166666666666664}}
2025-10-09 13:50:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 83, 'Results_raw': {'train_total': 480, 'train_loss': 331.203369140625, 'train_avg_loss': 0.6900070190429688, 'train_seen': 480, 'train_correct': 236, 'train_acc': 0.49166666666666664}}
2025-10-09 13:50:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:50:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:50:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #83, planning to set LR to 1.00e-05
2025-10-09 13:50:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 13:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:50:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:50:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:50:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:50:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:51:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:51:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.004700, avg_loss=0.693760, seen=480, correct=253, accuracy=0.527083
2025-10-09 13:51:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:51:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:51:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=83 reserved=2384MB allocated=2115MB
2025-10-09 13:51:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 83, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.6426249742508, 'train_avg_loss': 0.6886885414520899, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 13:51:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 83, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.00469970703125, 'train_avg_loss': 0.6937597910563151, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 13:51:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 83, 'Results_raw': {'train_total': 480, 'train_loss': 333.00469970703125, 'train_avg_loss': 0.6937597910563151, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 13:51:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:51:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:51:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #83, planning to set LR to 1.00e-05
2025-10-09 13:51:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 13:51:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:51:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:51:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:51:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:51:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:52:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:52:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=340.319458, avg_loss=0.708999, seen=480, correct=225, accuracy=0.468750
2025-10-09 13:52:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:52:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:52:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:52:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=83 reserved=2434MB allocated=2124MB
2025-10-09 13:52:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 83, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.52299463748932, 'train_avg_loss': 0.721024955312411, 'train_seen': 120, 'train_correct': 53, 'train_acc': 0.44166666666666665}}
2025-10-09 13:52:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 83, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.3194580078125, 'train_avg_loss': 0.7089988708496093, 'train_seen': 480, 'train_correct': 225, 'train_acc': 0.46875}}
2025-10-09 13:52:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 83, 'Results_raw': {'train_total': 480, 'train_loss': 340.3194580078125, 'train_avg_loss': 0.7089988708496093, 'train_seen': 480, 'train_correct': 225, 'train_acc': 0.46875}}
2025-10-09 13:52:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:52:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:52:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #83, planning to set LR to 1.00e-05
2025-10-09 13:52:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 13:52:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:52:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:52:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:52:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:52:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:53:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:53:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.216644, avg_loss=0.698368, seen=480, correct=243, accuracy=0.506250
2025-10-09 13:53:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:53:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:53:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:53:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=83 reserved=2372MB allocated=2099MB
2025-10-09 13:53:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 83, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.01863968372345, 'train_avg_loss': 0.6584886640310288, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 13:53:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 83, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.2166442871094, 'train_avg_loss': 0.6983680089314779, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 13:53:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 83, 'Results_raw': {'train_total': 480, 'train_loss': 335.2166442871094, 'train_avg_loss': 0.6983680089314779, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 13:53:07 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #84) -------------
2025-10-09 13:53:08 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=84 aidx=1 | s=5 (candidates=23)
2025-10-09 13:53:08 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[38, 9, 18, 42, 7] (from 23)
2025-10-09 13:53:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:53:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:53:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #84, planning to set LR to 1.00e-05
2025-10-09 13:53:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 13:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:53:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:53:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:53:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:53:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:53:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:53:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.344116, avg_loss=0.690300, seen=480, correct=260, accuracy=0.541667
2025-10-09 13:53:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:53:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:53:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:53:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=84 reserved=2364MB allocated=2107MB
2025-10-09 13:53:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 84, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.4877462387085, 'train_avg_loss': 0.6873978853225708, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 13:53:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 84, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.3441162109375, 'train_avg_loss': 0.6903002421061198, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 13:53:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 84, 'Results_raw': {'train_total': 480, 'train_loss': 331.3441162109375, 'train_avg_loss': 0.6903002421061198, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 13:53:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:53:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:53:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #84, planning to set LR to 1.00e-05
2025-10-09 13:53:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 13:53:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:53:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:53:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:53:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:53:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:54:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:54:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.073517, avg_loss=0.689736, seen=480, correct=251, accuracy=0.522917
2025-10-09 13:54:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:54:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:54:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:54:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=84 reserved=2350MB allocated=2082MB
2025-10-09 13:54:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 84, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.01159298419952, 'train_avg_loss': 0.7000966082016628, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 13:54:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 84, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.0735168457031, 'train_avg_loss': 0.6897364934285481, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 13:54:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 84, 'Results_raw': {'train_total': 480, 'train_loss': 331.0735168457031, 'train_avg_loss': 0.6897364934285481, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 13:54:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:54:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:54:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #84, planning to set LR to 1.00e-05
2025-10-09 13:54:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 13:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:54:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:54:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:54:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:54:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:55:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:55:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.302429, avg_loss=0.698547, seen=480, correct=239, accuracy=0.497917
2025-10-09 13:55:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:55:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:55:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:55:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=84 reserved=2372MB allocated=2082MB
2025-10-09 13:55:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 84, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.34923738241196, 'train_avg_loss': 0.7195769781867664, 'train_seen': 120, 'train_correct': 52, 'train_acc': 0.43333333333333335}}
2025-10-09 13:55:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 84, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.30242919921875, 'train_avg_loss': 0.6985467274983724, 'train_seen': 480, 'train_correct': 239, 'train_acc': 0.4979166666666667}}
2025-10-09 13:55:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 84, 'Results_raw': {'train_total': 480, 'train_loss': 335.30242919921875, 'train_avg_loss': 0.6985467274983724, 'train_seen': 480, 'train_correct': 239, 'train_acc': 0.4979166666666667}}
2025-10-09 13:55:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:55:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:55:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #84, planning to set LR to 1.00e-05
2025-10-09 13:55:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 13:55:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:55:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:55:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:55:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:55:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:56:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:56:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.200592, avg_loss=0.702501, seen=480, correct=235, accuracy=0.489583
2025-10-09 13:56:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:56:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:56:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:56:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=84 reserved=2334MB allocated=2082MB
2025-10-09 13:56:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 84, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.20196199417114, 'train_avg_loss': 0.7016830166180928, 'train_seen': 120, 'train_correct': 56, 'train_acc': 0.4666666666666667}}
2025-10-09 13:56:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 84, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.2005920410156, 'train_avg_loss': 0.7025012334187826, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 13:56:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 84, 'Results_raw': {'train_total': 480, 'train_loss': 337.2005920410156, 'train_avg_loss': 0.7025012334187826, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 13:56:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:56:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:56:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #84, planning to set LR to 1.00e-05
2025-10-09 13:56:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 13:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:56:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:56:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:56:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:56:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:56:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:56:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.404358, avg_loss=0.692509, seen=480, correct=254, accuracy=0.529167
2025-10-09 13:56:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:56:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:56:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=84 reserved=2344MB allocated=2082MB
2025-10-09 13:56:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 84, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.22013437747955, 'train_avg_loss': 0.7101677864789963, 'train_seen': 120, 'train_correct': 55, 'train_acc': 0.4583333333333333}}
2025-10-09 13:56:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 84, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.40435791015625, 'train_avg_loss': 0.6925090789794922, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 13:56:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 84, 'Results_raw': {'train_total': 480, 'train_loss': 332.40435791015625, 'train_avg_loss': 0.6925090789794922, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 13:56:54 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #85) -------------
2025-10-09 13:56:54 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=85 aidx=1 | s=5 (candidates=23)
2025-10-09 13:56:54 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[12, 8, 42, 44, 27] (from 23)
2025-10-09 13:56:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:56:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:56:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #85, planning to set LR to 1.00e-05
2025-10-09 13:56:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 13:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:56:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:56:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:56:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:56:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:57:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:57:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.828491, avg_loss=0.697559, seen=480, correct=245, accuracy=0.510417
2025-10-09 13:57:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:57:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:57:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:57:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=85 reserved=2352MB allocated=2116MB
2025-10-09 13:57:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 85, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.24056780338287, 'train_avg_loss': 0.7020047316948573, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 13:57:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 85, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.8284912109375, 'train_avg_loss': 0.6975593566894531, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-10-09 13:57:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 85, 'Results_raw': {'train_total': 480, 'train_loss': 334.8284912109375, 'train_avg_loss': 0.6975593566894531, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-10-09 13:57:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:57:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:57:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #85, planning to set LR to 1.00e-05
2025-10-09 13:57:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 13:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:57:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:57:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:57:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:57:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:58:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:58:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.669525, avg_loss=0.695145, seen=480, correct=259, accuracy=0.539583
2025-10-09 13:58:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:58:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:58:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:58:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=85 reserved=2368MB allocated=2124MB
2025-10-09 13:58:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 85, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.37682366371155, 'train_avg_loss': 0.7114735305309295, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 13:58:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 85, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.6695251464844, 'train_avg_loss': 0.6951448440551757, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 13:58:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 85, 'Results_raw': {'train_total': 480, 'train_loss': 333.6695251464844, 'train_avg_loss': 0.6951448440551757, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 13:58:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:58:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:58:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #85, planning to set LR to 1.00e-05
2025-10-09 13:58:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 13:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:58:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:58:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:58:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:58:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:59:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:59:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.104431, avg_loss=0.702301, seen=480, correct=238, accuracy=0.495833
2025-10-09 13:59:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:59:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:59:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:59:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=85 reserved=2338MB allocated=2099MB
2025-10-09 13:59:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 85, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.55546927452087, 'train_avg_loss': 0.7046289106210073, 'train_seen': 120, 'train_correct': 54, 'train_acc': 0.45}}
2025-10-09 13:59:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 85, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.10443115234375, 'train_avg_loss': 0.7023008982340495, 'train_seen': 480, 'train_correct': 238, 'train_acc': 0.49583333333333335}}
2025-10-09 13:59:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 85, 'Results_raw': {'train_total': 480, 'train_loss': 337.10443115234375, 'train_avg_loss': 0.7023008982340495, 'train_seen': 480, 'train_correct': 238, 'train_acc': 0.49583333333333335}}
2025-10-09 13:59:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:59:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:59:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #85, planning to set LR to 1.00e-05
2025-10-09 13:59:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 13:59:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:59:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:59:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:59:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:59:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 13:59:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 13:59:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.903503, avg_loss=0.695632, seen=480, correct=243, accuracy=0.506250
2025-10-09 13:59:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 13:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:59:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 13:59:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=85 reserved=2330MB allocated=2099MB
2025-10-09 13:59:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 85, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.73975384235382, 'train_avg_loss': 0.6894979486862819, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 13:59:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 85, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.90350341796875, 'train_avg_loss': 0.6956322987874349, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 13:59:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 85, 'Results_raw': {'train_total': 480, 'train_loss': 333.90350341796875, 'train_avg_loss': 0.6956322987874349, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 13:59:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 13:59:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 13:59:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #85, planning to set LR to 1.00e-05
2025-10-09 13:59:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 13:59:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 13:59:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 13:59:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 13:59:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 13:59:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:00:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:00:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.358337, avg_loss=0.700747, seen=480, correct=249, accuracy=0.518750
2025-10-09 14:00:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:00:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:00:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:00:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=85 reserved=2332MB allocated=2099MB
2025-10-09 14:00:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 85, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.5659749507904, 'train_avg_loss': 0.7047164579232533, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 14:00:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 85, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.35833740234375, 'train_avg_loss': 0.7007465362548828, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 14:00:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 85, 'Results_raw': {'train_total': 480, 'train_loss': 336.35833740234375, 'train_avg_loss': 0.7007465362548828, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 14:00:31 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #86) -------------
2025-10-09 14:00:31 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=86 aidx=1 | s=5 (candidates=23)
2025-10-09 14:00:31 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[38, 2, 9, 27, 49] (from 23)
2025-10-09 14:00:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:00:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:00:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #86, planning to set LR to 1.00e-05
2025-10-09 14:00:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 14:00:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:00:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:00:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:00:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:00:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:01:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:01:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.637817, avg_loss=0.688829, seen=480, correct=253, accuracy=0.527083
2025-10-09 14:01:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:01:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:01:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=86 reserved=2336MB allocated=2099MB
2025-10-09 14:01:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 86, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.5213772058487, 'train_avg_loss': 0.6876781433820724, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 14:01:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 86, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.6378173828125, 'train_avg_loss': 0.6888287862141927, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 14:01:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 86, 'Results_raw': {'train_total': 480, 'train_loss': 330.6378173828125, 'train_avg_loss': 0.6888287862141927, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 14:01:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:01:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:01:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #86, planning to set LR to 1.00e-05
2025-10-09 14:01:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 14:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:01:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:01:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:01:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:01:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:01:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:01:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.574371, avg_loss=0.686613, seen=480, correct=267, accuracy=0.556250
2025-10-09 14:01:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:01:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:01:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:01:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=86 reserved=2402MB allocated=2099MB
2025-10-09 14:01:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 86, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.79110014438629, 'train_avg_loss': 0.6815925012032191, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 14:01:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 86, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.5743713378906, 'train_avg_loss': 0.6866132736206054, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 14:01:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 86, 'Results_raw': {'train_total': 480, 'train_loss': 329.5743713378906, 'train_avg_loss': 0.6866132736206054, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 14:01:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:01:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:01:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #86, planning to set LR to 1.00e-05
2025-10-09 14:01:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 14:01:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:01:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:01:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:01:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:01:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:02:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:02:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.319580, avg_loss=0.690249, seen=480, correct=242, accuracy=0.504167
2025-10-09 14:02:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:02:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:02:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:02:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=86 reserved=2344MB allocated=2099MB
2025-10-09 14:02:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 86, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.57941222190857, 'train_avg_loss': 0.6964951018492381, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 14:02:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 86, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.319580078125, 'train_avg_loss': 0.6902491251627604, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 14:02:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 86, 'Results_raw': {'train_total': 480, 'train_loss': 331.319580078125, 'train_avg_loss': 0.6902491251627604, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 14:02:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:02:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:02:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #86, planning to set LR to 1.00e-05
2025-10-09 14:02:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 14:02:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:02:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:02:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:02:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:02:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:03:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:03:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.262115, avg_loss=0.694296, seen=480, correct=262, accuracy=0.545833
2025-10-09 14:03:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:03:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:03:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=86 reserved=2332MB allocated=2099MB
2025-10-09 14:03:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 86, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.617014169693, 'train_avg_loss': 0.6968084514141083, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 14:03:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 86, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.2621154785156, 'train_avg_loss': 0.6942960739135742, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:03:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 86, 'Results_raw': {'train_total': 480, 'train_loss': 333.2621154785156, 'train_avg_loss': 0.6942960739135742, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:03:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:03:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #86, planning to set LR to 1.00e-05
2025-10-09 14:03:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 14:03:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:03:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:03:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:03:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:03:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:04:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:04:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.366943, avg_loss=0.698681, seen=480, correct=235, accuracy=0.489583
2025-10-09 14:04:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:04:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:04:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:04:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=86 reserved=2362MB allocated=2099MB
2025-10-09 14:04:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 86, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.21042835712433, 'train_avg_loss': 0.710086902976036, 'train_seen': 120, 'train_correct': 54, 'train_acc': 0.45}}
2025-10-09 14:04:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 86, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.366943359375, 'train_avg_loss': 0.698681131998698, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 14:04:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 86, 'Results_raw': {'train_total': 480, 'train_loss': 335.366943359375, 'train_avg_loss': 0.698681131998698, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 14:04:09 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #87) -------------
2025-10-09 14:04:09 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=87 aidx=1 | s=5 (candidates=23)
2025-10-09 14:04:09 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[12, 53, 8, 14, 18] (from 23)
2025-10-09 14:04:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:04:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:04:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #87, planning to set LR to 1.00e-05
2025-10-09 14:04:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 14:04:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:04:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:04:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:04:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:04:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:04:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:04:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.984131, avg_loss=0.695800, seen=480, correct=238, accuracy=0.495833
2025-10-09 14:04:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:04:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:04:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:04:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=87 reserved=2336MB allocated=2099MB
2025-10-09 14:04:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 87, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.89275884628296, 'train_avg_loss': 0.6991063237190247, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 14:04:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 87, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.984130859375, 'train_avg_loss': 0.6958002726236979, 'train_seen': 480, 'train_correct': 238, 'train_acc': 0.49583333333333335}}
2025-10-09 14:04:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 87, 'Results_raw': {'train_total': 480, 'train_loss': 333.984130859375, 'train_avg_loss': 0.6958002726236979, 'train_seen': 480, 'train_correct': 238, 'train_acc': 0.49583333333333335}}
2025-10-09 14:04:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:04:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:04:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #87, planning to set LR to 1.00e-05
2025-10-09 14:04:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 14:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:04:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:04:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:04:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:04:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:05:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:05:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.143829, avg_loss=0.698216, seen=480, correct=242, accuracy=0.504167
2025-10-09 14:05:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:05:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:05:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=87 reserved=2358MB allocated=2132MB
2025-10-09 14:05:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 87, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.20770490169525, 'train_avg_loss': 0.6850642075141271, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 14:05:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 87, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.1438293457031, 'train_avg_loss': 0.6982163111368815, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 14:05:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 87, 'Results_raw': {'train_total': 480, 'train_loss': 335.1438293457031, 'train_avg_loss': 0.6982163111368815, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 14:05:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:05:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:05:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #87, planning to set LR to 1.00e-05
2025-10-09 14:05:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 14:05:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:05:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:05:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:05:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:05:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:06:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:06:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.195282, avg_loss=0.694157, seen=480, correct=247, accuracy=0.514583
2025-10-09 14:06:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:06:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:06:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=87 reserved=2354MB allocated=2107MB
2025-10-09 14:06:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 87, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.57478952407837, 'train_avg_loss': 0.7131232460339864, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 14:06:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 87, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.1952819824219, 'train_avg_loss': 0.6941568374633789, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 14:06:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 87, 'Results_raw': {'train_total': 480, 'train_loss': 333.1952819824219, 'train_avg_loss': 0.6941568374633789, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 14:06:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:06:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:06:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #87, planning to set LR to 1.00e-05
2025-10-09 14:06:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 14:06:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:06:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:06:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:06:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:06:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:07:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:07:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.042786, avg_loss=0.698006, seen=480, correct=253, accuracy=0.527083
2025-10-09 14:07:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:07:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:07:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:07:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=87 reserved=2330MB allocated=2107MB
2025-10-09 14:07:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 87, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.0737139582634, 'train_avg_loss': 0.6839476163188617, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 14:07:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 87, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.04278564453125, 'train_avg_loss': 0.6980058034261067, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 14:07:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 87, 'Results_raw': {'train_total': 480, 'train_loss': 335.04278564453125, 'train_avg_loss': 0.6980058034261067, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 14:07:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:07:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:07:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #87, planning to set LR to 1.00e-05
2025-10-09 14:07:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 14:07:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:07:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:07:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:07:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:07:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:07:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:07:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.718842, avg_loss=0.695248, seen=480, correct=237, accuracy=0.493750
2025-10-09 14:07:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:07:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:07:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=87 reserved=2360MB allocated=2107MB
2025-10-09 14:07:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 87, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.00897228717804, 'train_avg_loss': 0.7084081023931503, 'train_seen': 120, 'train_correct': 51, 'train_acc': 0.425}}
2025-10-09 14:07:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 87, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.7188415527344, 'train_avg_loss': 0.6952475865681966, 'train_seen': 480, 'train_correct': 237, 'train_acc': 0.49375}}
2025-10-09 14:07:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 87, 'Results_raw': {'train_total': 480, 'train_loss': 333.7188415527344, 'train_avg_loss': 0.6952475865681966, 'train_seen': 480, 'train_correct': 237, 'train_acc': 0.49375}}
2025-10-09 14:07:49 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #88) -------------
2025-10-09 14:07:49 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=88 aidx=1 | s=5 (candidates=23)
2025-10-09 14:07:49 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[52, 17, 11, 7, 42] (from 23)
2025-10-09 14:07:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:07:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:07:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #88, planning to set LR to 1.00e-05
2025-10-09 14:07:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 14:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:07:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:07:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:07:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:07:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:08:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:08:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.423859, avg_loss=0.688383, seen=480, correct=267, accuracy=0.556250
2025-10-09 14:08:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:08:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:08:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=88 reserved=2338MB allocated=2107MB
2025-10-09 14:08:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 88, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.96642899513245, 'train_avg_loss': 0.6830535749594371, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 14:08:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 88, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.4238586425781, 'train_avg_loss': 0.6883830388387044, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 14:08:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 88, 'Results_raw': {'train_total': 480, 'train_loss': 330.4238586425781, 'train_avg_loss': 0.6883830388387044, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 14:08:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:08:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:08:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #88, planning to set LR to 1.00e-05
2025-10-09 14:08:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 14:08:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:08:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:08:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:08:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:08:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:09:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:09:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.643677, avg_loss=0.690924, seen=480, correct=262, accuracy=0.545833
2025-10-09 14:09:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:09:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:09:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:09:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=88 reserved=2360MB allocated=2107MB
2025-10-09 14:09:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 88, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.18313920497894, 'train_avg_loss': 0.7015261600414912, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 14:09:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 88, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.6436767578125, 'train_avg_loss': 0.690924326578776, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:09:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 88, 'Results_raw': {'train_total': 480, 'train_loss': 331.6436767578125, 'train_avg_loss': 0.690924326578776, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:09:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:09:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:09:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #88, planning to set LR to 1.00e-05
2025-10-09 14:09:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 14:09:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:09:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:09:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:09:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:09:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:10:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:10:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.403503, avg_loss=0.690424, seen=480, correct=255, accuracy=0.531250
2025-10-09 14:10:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:10:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:10:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:10:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=88 reserved=2412MB allocated=2141MB
2025-10-09 14:10:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 88, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.67338383197784, 'train_avg_loss': 0.6806115319331487, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 14:10:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 88, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.40350341796875, 'train_avg_loss': 0.6904239654541016, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 14:10:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 88, 'Results_raw': {'train_total': 480, 'train_loss': 331.40350341796875, 'train_avg_loss': 0.6904239654541016, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 14:10:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:10:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:10:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #88, planning to set LR to 1.00e-05
2025-10-09 14:10:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 14:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:10:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:10:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:10:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:10:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:10:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:10:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.975098, avg_loss=0.687448, seen=480, correct=255, accuracy=0.531250
2025-10-09 14:10:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:10:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:10:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=88 reserved=2344MB allocated=2116MB
2025-10-09 14:10:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 88, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.18417489528656, 'train_avg_loss': 0.6932014574607214, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 14:10:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 88, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.97509765625, 'train_avg_loss': 0.6874481201171875, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 14:10:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 88, 'Results_raw': {'train_total': 480, 'train_loss': 329.97509765625, 'train_avg_loss': 0.6874481201171875, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 14:10:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:10:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:10:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #88, planning to set LR to 1.00e-05
2025-10-09 14:10:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 14:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:10:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:10:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:10:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:10:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:11:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:11:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.333862, avg_loss=0.694446, seen=480, correct=248, accuracy=0.516667
2025-10-09 14:11:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:11:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:11:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:11:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=88 reserved=2338MB allocated=2116MB
2025-10-09 14:11:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 88, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.18089616298676, 'train_avg_loss': 0.6931741346915563, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 14:11:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 88, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.3338623046875, 'train_avg_loss': 0.694445546468099, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 14:11:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 88, 'Results_raw': {'train_total': 480, 'train_loss': 333.3338623046875, 'train_avg_loss': 0.694445546468099, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 14:11:37 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #89) -------------
2025-10-09 14:11:37 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=89 aidx=1 | s=5 (candidates=23)
2025-10-09 14:11:37 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[2, 35, 33, 38, 42] (from 23)
2025-10-09 14:11:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:11:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:11:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #89, planning to set LR to 1.00e-05
2025-10-09 14:11:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 14:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:11:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:11:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:11:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:11:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:12:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:12:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.329590, avg_loss=0.686103, seen=480, correct=261, accuracy=0.543750
2025-10-09 14:12:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:12:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:12:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=89 reserved=2404MB allocated=2116MB
2025-10-09 14:12:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 89, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.08031976222992, 'train_avg_loss': 0.675669331351916, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 14:12:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 89, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.32958984375, 'train_avg_loss': 0.6861033121744792, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 14:12:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 89, 'Results_raw': {'train_total': 480, 'train_loss': 329.32958984375, 'train_avg_loss': 0.6861033121744792, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 14:12:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:12:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:12:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #89, planning to set LR to 1.00e-05
2025-10-09 14:12:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 14:12:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:12:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:12:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:12:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:12:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:13:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:13:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.765533, avg_loss=0.695345, seen=480, correct=261, accuracy=0.543750
2025-10-09 14:13:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:13:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:13:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=89 reserved=2416MB allocated=2150MB
2025-10-09 14:13:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 89, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.46515011787415, 'train_avg_loss': 0.6955429176489513, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 14:13:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 89, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.7655334472656, 'train_avg_loss': 0.6953448613484701, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 14:13:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 89, 'Results_raw': {'train_total': 480, 'train_loss': 333.7655334472656, 'train_avg_loss': 0.6953448613484701, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 14:13:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:13:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:13:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #89, planning to set LR to 1.00e-05
2025-10-09 14:13:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 14:13:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:13:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:13:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:13:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:13:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:13:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:13:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.381744, avg_loss=0.692462, seen=480, correct=249, accuracy=0.518750
2025-10-09 14:13:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:13:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:13:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:13:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=89 reserved=2358MB allocated=2158MB
2025-10-09 14:13:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 89, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.1602988243103, 'train_avg_loss': 0.6930024902025859, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 14:13:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 89, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.3817443847656, 'train_avg_loss': 0.6924619674682617, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 14:13:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 89, 'Results_raw': {'train_total': 480, 'train_loss': 332.3817443847656, 'train_avg_loss': 0.6924619674682617, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 14:13:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:13:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:13:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #89, planning to set LR to 1.00e-05
2025-10-09 14:13:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 14:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:13:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:13:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:13:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:13:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:14:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:14:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.998169, avg_loss=0.687496, seen=480, correct=261, accuracy=0.543750
2025-10-09 14:14:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:14:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:14:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:14:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=89 reserved=2336MB allocated=2133MB
2025-10-09 14:14:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 89, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.66232544183731, 'train_avg_loss': 0.6805193786819775, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 14:14:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 89, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.9981689453125, 'train_avg_loss': 0.6874961853027344, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 14:14:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 89, 'Results_raw': {'train_total': 480, 'train_loss': 329.9981689453125, 'train_avg_loss': 0.6874961853027344, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 14:14:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:14:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:14:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #89, planning to set LR to 1.00e-05
2025-10-09 14:14:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 14:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:14:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:14:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:14:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:14:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:15:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:15:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.237610, avg_loss=0.687995, seen=480, correct=268, accuracy=0.558333
2025-10-09 14:15:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:15:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:15:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:15:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=89 reserved=2336MB allocated=2133MB
2025-10-09 14:15:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 89, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.76547634601593, 'train_avg_loss': 0.6813789695501328, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 14:15:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 89, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.23760986328125, 'train_avg_loss': 0.6879950205485026, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 14:15:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 89, 'Results_raw': {'train_total': 480, 'train_loss': 330.23760986328125, 'train_avg_loss': 0.6879950205485026, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 14:15:18 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #90) -------------
2025-10-09 14:15:19 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=90 aidx=1 | s=5 (candidates=23)
2025-10-09 14:15:19 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[17, 18, 23, 49, 35] (from 23)
2025-10-09 14:15:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:15:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:15:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #90, planning to set LR to 1.00e-05
2025-10-09 14:15:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 14:15:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:15:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:15:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:15:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:15:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:15:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:15:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.190613, avg_loss=0.687897, seen=480, correct=269, accuracy=0.560417
2025-10-09 14:15:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:15:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:16:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:16:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=90 reserved=2360MB allocated=2133MB
2025-10-09 14:16:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 90, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.13797295093536, 'train_avg_loss': 0.6928164412577947, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 14:16:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 90, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.19061279296875, 'train_avg_loss': 0.6878971099853516, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 14:16:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 90, 'Results_raw': {'train_total': 480, 'train_loss': 330.19061279296875, 'train_avg_loss': 0.6878971099853516, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 14:16:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:16:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:16:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #90, planning to set LR to 1.00e-05
2025-10-09 14:16:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 14:16:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:16:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:16:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:16:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:16:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:16:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.749878, avg_loss=0.686979, seen=480, correct=250, accuracy=0.520833
2025-10-09 14:16:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:16:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:16:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=90 reserved=2356MB allocated=2133MB
2025-10-09 14:16:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 90, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.53784018754959, 'train_avg_loss': 0.6961486682295799, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 14:16:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 90, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.7498779296875, 'train_avg_loss': 0.6869789123535156, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 14:16:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 90, 'Results_raw': {'train_total': 480, 'train_loss': 329.7498779296875, 'train_avg_loss': 0.6869789123535156, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 14:16:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:16:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:16:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #90, planning to set LR to 1.00e-05
2025-10-09 14:16:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-09 14:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:16:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:16:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:16:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:16:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:17:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:17:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.326904, avg_loss=0.698598, seen=480, correct=254, accuracy=0.529167
2025-10-09 14:17:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:17:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:17:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:17:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=90 reserved=2356MB allocated=2167MB
2025-10-09 14:17:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 90, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.63013434410095, 'train_avg_loss': 0.6969177862008412, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 14:17:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 90, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.326904296875, 'train_avg_loss': 0.6985977172851563, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 14:17:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 90, 'Results_raw': {'train_total': 480, 'train_loss': 335.326904296875, 'train_avg_loss': 0.6985977172851563, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 14:17:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:17:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:17:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #90, planning to set LR to 1.00e-05
2025-10-09 14:17:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 14:17:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:17:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:17:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:17:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:17:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:18:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:18:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.952942, avg_loss=0.691569, seen=480, correct=253, accuracy=0.527083
2025-10-09 14:18:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:18:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:18:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:18:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=90 reserved=2362MB allocated=2141MB
2025-10-09 14:18:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 90, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.22431993484497, 'train_avg_loss': 0.7102026661237081, 'train_seen': 120, 'train_correct': 56, 'train_acc': 0.4666666666666667}}
2025-10-09 14:18:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 90, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.95294189453125, 'train_avg_loss': 0.6915686289469402, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 14:18:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 90, 'Results_raw': {'train_total': 480, 'train_loss': 331.95294189453125, 'train_avg_loss': 0.6915686289469402, 'train_seen': 480, 'train_correct': 253, 'train_acc': 0.5270833333333333}}
2025-10-09 14:18:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:18:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:18:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #90, planning to set LR to 1.00e-05
2025-10-09 14:18:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 14:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:18:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:18:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:18:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:18:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:18:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:18:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.436615, avg_loss=0.694660, seen=480, correct=265, accuracy=0.552083
2025-10-09 14:18:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:18:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:18:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=90 reserved=2402MB allocated=2141MB
2025-10-09 14:18:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 90, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.99420917034149, 'train_avg_loss': 0.6999517430861791, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 14:18:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 90, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.4366149902344, 'train_avg_loss': 0.6946596145629883, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 14:18:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 90, 'Results_raw': {'train_total': 480, 'train_loss': 333.4366149902344, 'train_avg_loss': 0.6946596145629883, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 14:18:57 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #91) -------------
2025-10-09 14:18:58 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=91 aidx=1 | s=5 (candidates=23)
2025-10-09 14:18:58 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[42, 11, 25, 17, 14] (from 23)
2025-10-09 14:18:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:18:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:18:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #91, planning to set LR to 1.00e-05
2025-10-09 14:18:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 14:18:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:18:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:18:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:18:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:18:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:19:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:19:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.422180, avg_loss=0.677963, seen=480, correct=273, accuracy=0.568750
2025-10-09 14:19:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:19:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:19:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=91 reserved=2340MB allocated=2141MB
2025-10-09 14:19:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 91, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.67038750648499, 'train_avg_loss': 0.6722532292207082, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 14:19:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 91, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.42218017578125, 'train_avg_loss': 0.6779628753662109, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 14:19:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 91, 'Results_raw': {'train_total': 480, 'train_loss': 325.42218017578125, 'train_avg_loss': 0.6779628753662109, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 14:19:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:19:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:19:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #91, planning to set LR to 1.00e-05
2025-10-09 14:19:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 14:19:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:19:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:19:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:19:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:19:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:20:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:20:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.113861, avg_loss=0.683571, seen=480, correct=270, accuracy=0.562500
2025-10-09 14:20:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:20:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:20:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=91 reserved=2416MB allocated=2141MB
2025-10-09 14:20:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 91, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.98055899143219, 'train_avg_loss': 0.6998379915952683, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 14:20:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 91, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.1138610839844, 'train_avg_loss': 0.6835705439249674, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 14:20:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 91, 'Results_raw': {'train_total': 480, 'train_loss': 328.1138610839844, 'train_avg_loss': 0.6835705439249674, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 14:20:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:20:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:20:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #91, planning to set LR to 1.00e-05
2025-10-09 14:20:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 14:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:20:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:20:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:20:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:20:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:21:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:21:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.341492, avg_loss=0.702795, seen=480, correct=244, accuracy=0.508333
2025-10-09 14:21:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:21:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:21:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:21:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=91 reserved=2330MB allocated=2141MB
2025-10-09 14:21:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 91, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.2179342508316, 'train_avg_loss': 0.6934827854235966, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 14:21:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 91, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.34149169921875, 'train_avg_loss': 0.7027947743733723, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 14:21:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 91, 'Results_raw': {'train_total': 480, 'train_loss': 337.34149169921875, 'train_avg_loss': 0.7027947743733723, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 14:21:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:21:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:21:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #91, planning to set LR to 1.00e-05
2025-10-09 14:21:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 14:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:21:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:21:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:21:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:21:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:21:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:21:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.821838, avg_loss=0.687129, seen=480, correct=263, accuracy=0.547917
2025-10-09 14:21:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:21:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:21:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:21:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=91 reserved=2364MB allocated=2141MB
2025-10-09 14:21:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 91, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.78585314750671, 'train_avg_loss': 0.6898821095625559, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 14:21:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 91, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.82183837890625, 'train_avg_loss': 0.6871288299560547, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 14:21:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 91, 'Results_raw': {'train_total': 480, 'train_loss': 329.82183837890625, 'train_avg_loss': 0.6871288299560547, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 14:21:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:21:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:21:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #91, planning to set LR to 1.00e-05
2025-10-09 14:21:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 14:21:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:21:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:21:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:21:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:21:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:22:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:22:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.604431, avg_loss=0.692926, seen=480, correct=259, accuracy=0.539583
2025-10-09 14:22:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:22:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:22:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:22:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=91 reserved=2330MB allocated=2141MB
2025-10-09 14:22:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 91, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.80894684791565, 'train_avg_loss': 0.673407890399297, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 14:22:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 91, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.60443115234375, 'train_avg_loss': 0.6929258982340495, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 14:22:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 91, 'Results_raw': {'train_total': 480, 'train_loss': 332.60443115234375, 'train_avg_loss': 0.6929258982340495, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 14:22:30 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #92) -------------
2025-10-09 14:22:30 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=92 aidx=1 | s=5 (candidates=23)
2025-10-09 14:22:30 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[25, 35, 17, 49, 52] (from 23)
2025-10-09 14:22:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:22:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:22:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #92, planning to set LR to 1.00e-05
2025-10-09 14:22:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 14:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:22:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:22:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:22:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:22:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:23:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:23:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=338.059052, avg_loss=0.704290, seen=480, correct=237, accuracy=0.493750
2025-10-09 14:23:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:23:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:23:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:23:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=92 reserved=2330MB allocated=2141MB
2025-10-09 14:23:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 92, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.19235688447952, 'train_avg_loss': 0.7016029740373294, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 14:23:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 92, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.0590515136719, 'train_avg_loss': 0.7042896906534831, 'train_seen': 480, 'train_correct': 237, 'train_acc': 0.49375}}
2025-10-09 14:23:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 92, 'Results_raw': {'train_total': 480, 'train_loss': 338.0590515136719, 'train_avg_loss': 0.7042896906534831, 'train_seen': 480, 'train_correct': 237, 'train_acc': 0.49375}}
2025-10-09 14:23:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:23:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:23:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #92, planning to set LR to 1.00e-05
2025-10-09 14:23:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 14:23:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:23:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:23:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:23:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:23:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:23:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:23:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.658081, avg_loss=0.695121, seen=480, correct=259, accuracy=0.539583
2025-10-09 14:23:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:23:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:23:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=92 reserved=2402MB allocated=2141MB
2025-10-09 14:23:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 92, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.55473178625107, 'train_avg_loss': 0.6879560982187589, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 14:23:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 92, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.6580810546875, 'train_avg_loss': 0.6951210021972656, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 14:23:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 92, 'Results_raw': {'train_total': 480, 'train_loss': 333.6580810546875, 'train_avg_loss': 0.6951210021972656, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 14:23:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:24:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:24:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #92, planning to set LR to 1.00e-05
2025-10-09 14:24:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 14:24:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:24:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:24:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:24:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:24:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:24:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:24:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.953003, avg_loss=0.699902, seen=480, correct=252, accuracy=0.525000
2025-10-09 14:24:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:24:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:24:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=92 reserved=2366MB allocated=2141MB
2025-10-09 14:24:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 92, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.14526063203812, 'train_avg_loss': 0.6845438386003176, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 14:24:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 92, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.9530029296875, 'train_avg_loss': 0.699902089436849, 'train_seen': 480, 'train_correct': 252, 'train_acc': 0.525}}
2025-10-09 14:24:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 92, 'Results_raw': {'train_total': 480, 'train_loss': 335.9530029296875, 'train_avg_loss': 0.699902089436849, 'train_seen': 480, 'train_correct': 252, 'train_acc': 0.525}}
2025-10-09 14:24:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:24:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:24:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #92, planning to set LR to 1.00e-05
2025-10-09 14:24:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 14:24:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:24:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:24:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:24:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:24:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:25:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:25:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.467041, avg_loss=0.694723, seen=480, correct=259, accuracy=0.539583
2025-10-09 14:25:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:25:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:25:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:25:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=92 reserved=2362MB allocated=2141MB
2025-10-09 14:25:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 92, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.21710085868835, 'train_avg_loss': 0.6851425071557363, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 14:25:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 92, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.467041015625, 'train_avg_loss': 0.6947230021158854, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 14:25:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 92, 'Results_raw': {'train_total': 480, 'train_loss': 333.467041015625, 'train_avg_loss': 0.6947230021158854, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 14:25:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:25:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:25:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #92, planning to set LR to 1.00e-05
2025-10-09 14:25:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 14:25:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:25:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:25:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:25:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:25:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:26:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:26:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.420807, avg_loss=0.696710, seen=480, correct=257, accuracy=0.535417
2025-10-09 14:26:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:26:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:26:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=92 reserved=2338MB allocated=2141MB
2025-10-09 14:26:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 92, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.72829127311707, 'train_avg_loss': 0.6977357606093089, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 14:26:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 92, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.4208068847656, 'train_avg_loss': 0.6967100143432617, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 14:26:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 92, 'Results_raw': {'train_total': 480, 'train_loss': 334.4208068847656, 'train_avg_loss': 0.6967100143432617, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 14:26:11 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #93) -------------
2025-10-09 14:26:12 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=93 aidx=1 | s=5 (candidates=23)
2025-10-09 14:26:12 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[35, 7, 53, 9, 4] (from 23)
2025-10-09 14:26:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:26:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:26:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #93, planning to set LR to 1.00e-05
2025-10-09 14:26:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 14:26:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:26:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:26:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:26:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:26:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:26:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:26:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.606934, avg_loss=0.688764, seen=480, correct=263, accuracy=0.547917
2025-10-09 14:26:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:26:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:26:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:26:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=93 reserved=2402MB allocated=2141MB
2025-10-09 14:26:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 93, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.03370118141174, 'train_avg_loss': 0.6919475098450979, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 14:26:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 93, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.60693359375, 'train_avg_loss': 0.6887644449869792, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 14:26:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 93, 'Results_raw': {'train_total': 480, 'train_loss': 330.60693359375, 'train_avg_loss': 0.6887644449869792, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 14:26:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:26:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:26:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #93, planning to set LR to 1.00e-05
2025-10-09 14:26:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 14:26:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:26:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:26:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:26:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:26:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:27:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:27:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.351196, avg_loss=0.675732, seen=480, correct=281, accuracy=0.585417
2025-10-09 14:27:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:27:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:27:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=93 reserved=2342MB allocated=2141MB
2025-10-09 14:27:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 93, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.41294586658478, 'train_avg_loss': 0.6951078822215399, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 14:27:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 93, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.3511962890625, 'train_avg_loss': 0.6757316589355469, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 14:27:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 93, 'Results_raw': {'train_total': 480, 'train_loss': 324.3511962890625, 'train_avg_loss': 0.6757316589355469, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 14:27:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:27:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:27:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #93, planning to set LR to 1.00e-05
2025-10-09 14:27:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 14:27:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:27:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:27:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:27:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:27:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:28:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:28:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.338318, avg_loss=0.692371, seen=480, correct=254, accuracy=0.529167
2025-10-09 14:28:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:28:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:28:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:28:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=93 reserved=2332MB allocated=2141MB
2025-10-09 14:28:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 93, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.70638865232468, 'train_avg_loss': 0.6725532387693723, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 14:28:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 93, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.33831787109375, 'train_avg_loss': 0.6923714955647786, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 14:28:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 93, 'Results_raw': {'train_total': 480, 'train_loss': 332.33831787109375, 'train_avg_loss': 0.6923714955647786, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 14:28:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:28:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:28:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #93, planning to set LR to 1.00e-05
2025-10-09 14:28:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 14:28:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:28:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:28:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:28:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:28:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:29:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:29:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.938171, avg_loss=0.683205, seen=480, correct=261, accuracy=0.543750
2025-10-09 14:29:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:29:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:29:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:29:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=93 reserved=2350MB allocated=2141MB
2025-10-09 14:29:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 93, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.5804272890091, 'train_avg_loss': 0.6881702274084092, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 14:29:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 93, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.93817138671875, 'train_avg_loss': 0.6832045237223308, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 14:29:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 93, 'Results_raw': {'train_total': 480, 'train_loss': 327.93817138671875, 'train_avg_loss': 0.6832045237223308, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 14:29:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:29:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:29:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #93, planning to set LR to 1.00e-05
2025-10-09 14:29:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 14:29:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:29:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:29:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:29:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:29:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:29:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:29:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.490784, avg_loss=0.667689, seen=480, correct=293, accuracy=0.610417
2025-10-09 14:29:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:29:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:29:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:29:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=93 reserved=2374MB allocated=2141MB
2025-10-09 14:29:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 93, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.25173449516296, 'train_avg_loss': 0.6604311207930247, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 14:29:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 93, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.49078369140625, 'train_avg_loss': 0.6676891326904297, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 14:29:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 93, 'Results_raw': {'train_total': 480, 'train_loss': 320.49078369140625, 'train_avg_loss': 0.6676891326904297, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 14:29:46 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #94) -------------
2025-10-09 14:29:46 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=94 aidx=1 | s=5 (candidates=23)
2025-10-09 14:29:46 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[53, 27, 49, 30, 42] (from 23)
2025-10-09 14:29:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:29:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:29:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #94, planning to set LR to 1.00e-05
2025-10-09 14:29:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 14:29:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:29:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:29:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:29:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:29:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:30:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:30:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.906738, avg_loss=0.695639, seen=480, correct=256, accuracy=0.533333
2025-10-09 14:30:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:30:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:30:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:30:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=94 reserved=2332MB allocated=2141MB
2025-10-09 14:30:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 94, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.01544976234436, 'train_avg_loss': 0.666795414686203, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 14:30:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 94, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.90673828125, 'train_avg_loss': 0.6956390380859375, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 14:30:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 94, 'Results_raw': {'train_total': 480, 'train_loss': 333.90673828125, 'train_avg_loss': 0.6956390380859375, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 14:30:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:30:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:30:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #94, planning to set LR to 1.00e-05
2025-10-09 14:30:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 14:30:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:30:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:30:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:30:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:30:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:31:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:31:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.141388, avg_loss=0.689878, seen=480, correct=259, accuracy=0.539583
2025-10-09 14:31:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:31:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:31:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:31:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=94 reserved=2332MB allocated=2141MB
2025-10-09 14:31:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 94, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.05466437339783, 'train_avg_loss': 0.6921222031116485, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 14:31:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 94, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.1413879394531, 'train_avg_loss': 0.6898778915405274, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 14:31:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 94, 'Results_raw': {'train_total': 480, 'train_loss': 331.1413879394531, 'train_avg_loss': 0.6898778915405274, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 14:31:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:31:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:31:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #94, planning to set LR to 1.00e-05
2025-10-09 14:31:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 14:31:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:31:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:31:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:31:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:31:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:31:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:31:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.478790, avg_loss=0.690581, seen=480, correct=262, accuracy=0.545833
2025-10-09 14:31:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:31:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:31:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:31:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=94 reserved=2362MB allocated=2141MB
2025-10-09 14:31:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 94, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.69278287887573, 'train_avg_loss': 0.7057731906572978, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 14:31:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 94, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.4787902832031, 'train_avg_loss': 0.6905808130900065, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:31:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 94, 'Results_raw': {'train_total': 480, 'train_loss': 331.4787902832031, 'train_avg_loss': 0.6905808130900065, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:31:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:31:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:31:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #94, planning to set LR to 1.00e-05
2025-10-09 14:31:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 14:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:31:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:31:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:31:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:31:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:32:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:32:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.116608, avg_loss=0.687743, seen=480, correct=254, accuracy=0.529167
2025-10-09 14:32:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:32:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:32:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:32:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=94 reserved=2330MB allocated=2141MB
2025-10-09 14:32:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 94, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.30703675746918, 'train_avg_loss': 0.7025586396455765, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 14:32:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 94, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.1166076660156, 'train_avg_loss': 0.6877429326375325, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 14:32:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 94, 'Results_raw': {'train_total': 480, 'train_loss': 330.1166076660156, 'train_avg_loss': 0.6877429326375325, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 14:32:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:32:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:32:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #94, planning to set LR to 1.00e-05
2025-10-09 14:32:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 14:32:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:32:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:32:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:32:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:32:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:33:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:33:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.275085, avg_loss=0.681823, seen=480, correct=274, accuracy=0.570833
2025-10-09 14:33:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:33:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:33:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:33:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=94 reserved=2340MB allocated=2141MB
2025-10-09 14:33:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 94, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.92353630065918, 'train_avg_loss': 0.6743628025054932, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 14:33:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 94, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.27508544921875, 'train_avg_loss': 0.6818230946858724, 'train_seen': 480, 'train_correct': 274, 'train_acc': 0.5708333333333333}}
2025-10-09 14:33:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 94, 'Results_raw': {'train_total': 480, 'train_loss': 327.27508544921875, 'train_avg_loss': 0.6818230946858724, 'train_seen': 480, 'train_correct': 274, 'train_acc': 0.5708333333333333}}
2025-10-09 14:33:24 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #95) -------------
2025-10-09 14:33:24 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=95 aidx=1 | s=5 (candidates=23)
2025-10-09 14:33:24 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[4, 27, 7, 44, 17] (from 23)
2025-10-09 14:33:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:33:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:33:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #95, planning to set LR to 1.00e-05
2025-10-09 14:33:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 14:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:33:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:33:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:33:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:33:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:34:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:34:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.795563, avg_loss=0.647491, seen=480, correct=310, accuracy=0.645833
2025-10-09 14:34:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:34:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:34:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:34:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=95 reserved=2374MB allocated=2141MB
2025-10-09 14:34:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 95, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.31610840559006, 'train_avg_loss': 0.6526342367132505, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 14:34:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 95, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.7955627441406, 'train_avg_loss': 0.6474907557169597, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 14:34:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 95, 'Results_raw': {'train_total': 480, 'train_loss': 310.7955627441406, 'train_avg_loss': 0.6474907557169597, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 14:34:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:34:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:34:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #95, planning to set LR to 1.00e-05
2025-10-09 14:34:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 14:34:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:34:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:34:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:34:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:34:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:34:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:34:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.809326, avg_loss=0.689186, seen=480, correct=265, accuracy=0.552083
2025-10-09 14:34:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:34:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:34:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:34:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=95 reserved=2332MB allocated=2141MB
2025-10-09 14:34:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 95, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.22138422727585, 'train_avg_loss': 0.7018448685606321, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 14:34:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 95, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.809326171875, 'train_avg_loss': 0.6891860961914062, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 14:34:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 95, 'Results_raw': {'train_total': 480, 'train_loss': 330.809326171875, 'train_avg_loss': 0.6891860961914062, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 14:34:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:34:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:34:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #95, planning to set LR to 1.00e-05
2025-10-09 14:34:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 14:34:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:34:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:34:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:34:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:34:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:35:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:35:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.616058, avg_loss=0.672117, seen=480, correct=299, accuracy=0.622917
2025-10-09 14:35:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:35:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:35:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:35:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=95 reserved=2342MB allocated=2141MB
2025-10-09 14:35:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 95, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.8042676448822, 'train_avg_loss': 0.6900355637073516, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 14:35:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 95, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.6160583496094, 'train_avg_loss': 0.6721167882283529, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 14:35:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 95, 'Results_raw': {'train_total': 480, 'train_loss': 322.6160583496094, 'train_avg_loss': 0.6721167882283529, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 14:35:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:35:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:35:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #95, planning to set LR to 1.00e-05
2025-10-09 14:35:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 14:35:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:35:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:35:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:35:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:35:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:36:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:36:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.772766, avg_loss=0.682860, seen=480, correct=262, accuracy=0.545833
2025-10-09 14:36:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:36:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:36:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:36:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=95 reserved=2330MB allocated=2141MB
2025-10-09 14:36:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 95, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.87464582920074, 'train_avg_loss': 0.6739553819100063, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 14:36:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 95, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.77276611328125, 'train_avg_loss': 0.6828599294026693, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:36:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 95, 'Results_raw': {'train_total': 480, 'train_loss': 327.77276611328125, 'train_avg_loss': 0.6828599294026693, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:36:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:36:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:36:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #95, planning to set LR to 1.00e-05
2025-10-09 14:36:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 14:36:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:36:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:36:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:36:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:36:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:36:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:36:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.812714, avg_loss=0.678776, seen=480, correct=287, accuracy=0.597917
2025-10-09 14:36:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:36:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:36:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=95 reserved=2366MB allocated=2141MB
2025-10-09 14:36:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 95, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.28015851974487, 'train_avg_loss': 0.6773346543312073, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 14:36:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 95, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.8127136230469, 'train_avg_loss': 0.6787764867146809, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 14:36:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 95, 'Results_raw': {'train_total': 480, 'train_loss': 325.8127136230469, 'train_avg_loss': 0.6787764867146809, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 14:36:54 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #96) -------------
2025-10-09 14:36:55 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=96 aidx=1 | s=5 (candidates=23)
2025-10-09 14:36:55 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[2, 24, 35, 44, 8] (from 23)
2025-10-09 14:36:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:36:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:36:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #96, planning to set LR to 1.00e-05
2025-10-09 14:36:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 14:36:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:36:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:36:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:36:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:36:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:37:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:37:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.442200, avg_loss=0.675921, seen=480, correct=277, accuracy=0.577083
2025-10-09 14:37:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:37:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:37:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:37:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=96 reserved=2404MB allocated=2141MB
2025-10-09 14:37:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 96, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.29280269145966, 'train_avg_loss': 0.6524400224288305, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 14:37:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 96, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.44219970703125, 'train_avg_loss': 0.6759212493896485, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 14:37:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 96, 'Results_raw': {'train_total': 480, 'train_loss': 324.44219970703125, 'train_avg_loss': 0.6759212493896485, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 14:37:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:37:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:37:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #96, planning to set LR to 1.00e-05
2025-10-09 14:37:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 14:37:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:37:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:37:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:37:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:37:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:38:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:38:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.414886, avg_loss=0.694614, seen=480, correct=242, accuracy=0.504167
2025-10-09 14:38:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:38:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:38:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=96 reserved=2330MB allocated=2141MB
2025-10-09 14:38:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 96, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.93850338459015, 'train_avg_loss': 0.7078208615382512, 'train_seen': 120, 'train_correct': 54, 'train_acc': 0.45}}
2025-10-09 14:38:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 96, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.4148864746094, 'train_avg_loss': 0.6946143468221029, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 14:38:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 96, 'Results_raw': {'train_total': 480, 'train_loss': 333.4148864746094, 'train_avg_loss': 0.6946143468221029, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 14:38:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:38:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:38:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #96, planning to set LR to 1.00e-05
2025-10-09 14:38:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 14:38:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:38:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:38:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:38:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:38:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:38:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:38:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.236298, avg_loss=0.687992, seen=480, correct=270, accuracy=0.562500
2025-10-09 14:38:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:38:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:39:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:39:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=96 reserved=2404MB allocated=2141MB
2025-10-09 14:39:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 96, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.25583600997925, 'train_avg_loss': 0.6937986334164937, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 14:39:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 96, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.2362976074219, 'train_avg_loss': 0.6879922866821289, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 14:39:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 96, 'Results_raw': {'train_total': 480, 'train_loss': 330.2362976074219, 'train_avg_loss': 0.6879922866821289, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 14:39:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:39:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:39:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #96, planning to set LR to 1.00e-05
2025-10-09 14:39:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 14:39:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:39:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:39:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:39:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:39:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:39:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:39:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.820679, avg_loss=0.680876, seen=480, correct=272, accuracy=0.566667
2025-10-09 14:39:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:39:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:39:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=96 reserved=2330MB allocated=2141MB
2025-10-09 14:39:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 96, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.03852987289429, 'train_avg_loss': 0.6669877489407857, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 14:39:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 96, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.8206787109375, 'train_avg_loss': 0.6808764139811198, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 14:39:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 96, 'Results_raw': {'train_total': 480, 'train_loss': 326.8206787109375, 'train_avg_loss': 0.6808764139811198, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 14:39:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:39:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:39:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #96, planning to set LR to 1.00e-05
2025-10-09 14:39:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 14:39:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:39:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:39:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:39:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:39:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:40:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:40:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.562866, avg_loss=0.680339, seen=480, correct=269, accuracy=0.560417
2025-10-09 14:40:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:40:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:40:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:40:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=96 reserved=2352MB allocated=2141MB
2025-10-09 14:40:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 96, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.25330209732056, 'train_avg_loss': 0.6937775174776714, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 14:40:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 96, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.5628662109375, 'train_avg_loss': 0.6803393046061198, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 14:40:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 96, 'Results_raw': {'train_total': 480, 'train_loss': 326.5628662109375, 'train_avg_loss': 0.6803393046061198, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 14:40:30 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #97) -------------
2025-10-09 14:40:30 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=97 aidx=1 | s=5 (candidates=23)
2025-10-09 14:40:30 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[12, 33, 52, 24, 30] (from 23)
2025-10-09 14:40:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:40:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:40:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #97, planning to set LR to 1.00e-05
2025-10-09 14:40:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 14:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:40:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:40:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:40:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:40:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:41:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:41:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.981995, avg_loss=0.681212, seen=480, correct=273, accuracy=0.568750
2025-10-09 14:41:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:41:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:41:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=97 reserved=2330MB allocated=2141MB
2025-10-09 14:41:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 97, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.16955465078354, 'train_avg_loss': 0.6930796220898628, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 14:41:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 97, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.98199462890625, 'train_avg_loss': 0.6812124888102213, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 14:41:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 97, 'Results_raw': {'train_total': 480, 'train_loss': 326.98199462890625, 'train_avg_loss': 0.6812124888102213, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 14:41:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:41:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:41:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #97, planning to set LR to 1.00e-05
2025-10-09 14:41:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 14:41:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:41:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:41:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:41:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:41:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:41:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:41:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.134399, avg_loss=0.681530, seen=480, correct=265, accuracy=0.552083
2025-10-09 14:41:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:41:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:41:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=97 reserved=2342MB allocated=2141MB
2025-10-09 14:41:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 97, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.11724525690079, 'train_avg_loss': 0.6759770438075066, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 14:41:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 97, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.1343994140625, 'train_avg_loss': 0.6815299987792969, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 14:41:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 97, 'Results_raw': {'train_total': 480, 'train_loss': 327.1343994140625, 'train_avg_loss': 0.6815299987792969, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 14:41:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:41:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:41:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #97, planning to set LR to 1.00e-05
2025-10-09 14:41:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 14:41:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:41:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:41:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:41:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:41:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:42:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:42:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.568726, avg_loss=0.672018, seen=480, correct=285, accuracy=0.593750
2025-10-09 14:42:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:42:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:42:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:42:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=97 reserved=2338MB allocated=2141MB
2025-10-09 14:42:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 97, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.56204605102539, 'train_avg_loss': 0.6630170504252116, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 14:42:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 97, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.5687255859375, 'train_avg_loss': 0.6720181783040364, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 14:42:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 97, 'Results_raw': {'train_total': 480, 'train_loss': 322.5687255859375, 'train_avg_loss': 0.6720181783040364, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 14:42:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:42:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:42:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #97, planning to set LR to 1.00e-05
2025-10-09 14:42:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 14:42:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:42:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:42:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:42:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:42:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:43:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:43:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.996918, avg_loss=0.693744, seen=480, correct=260, accuracy=0.541667
2025-10-09 14:43:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:43:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:43:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:43:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=97 reserved=2330MB allocated=2141MB
2025-10-09 14:43:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 97, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.58509242534637, 'train_avg_loss': 0.7048757702112198, 'train_seen': 120, 'train_correct': 56, 'train_acc': 0.4666666666666667}}
2025-10-09 14:43:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 97, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.9969177246094, 'train_avg_loss': 0.6937435785929362, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 14:43:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 97, 'Results_raw': {'train_total': 480, 'train_loss': 332.9969177246094, 'train_avg_loss': 0.6937435785929362, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 14:43:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:43:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:43:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #97, planning to set LR to 1.00e-05
2025-10-09 14:43:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 14:43:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:43:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:43:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:43:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:43:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:44:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:44:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.029053, avg_loss=0.685477, seen=480, correct=252, accuracy=0.525000
2025-10-09 14:44:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:44:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:44:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=97 reserved=2330MB allocated=2141MB
2025-10-09 14:44:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 97, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.91103708744049, 'train_avg_loss': 0.6992586423953374, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 14:44:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 97, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.029052734375, 'train_avg_loss': 0.6854771931966146, 'train_seen': 480, 'train_correct': 252, 'train_acc': 0.525}}
2025-10-09 14:44:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 97, 'Results_raw': {'train_total': 480, 'train_loss': 329.029052734375, 'train_avg_loss': 0.6854771931966146, 'train_seen': 480, 'train_correct': 252, 'train_acc': 0.525}}
2025-10-09 14:44:05 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #98) -------------
2025-10-09 14:44:06 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=98 aidx=1 | s=5 (candidates=23)
2025-10-09 14:44:06 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[49, 38, 44, 25, 17] (from 23)
2025-10-09 14:44:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:44:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:44:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #98, planning to set LR to 1.00e-05
2025-10-09 14:44:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 14:44:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:44:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:44:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:44:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:44:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:44:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:44:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.731262, avg_loss=0.682773, seen=480, correct=262, accuracy=0.545833
2025-10-09 14:44:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:44:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:44:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:44:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=98 reserved=2362MB allocated=2141MB
2025-10-09 14:44:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 98, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.78080934286118, 'train_avg_loss': 0.7065067445238431, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 14:44:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 98, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.73126220703125, 'train_avg_loss': 0.6827734629313151, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:44:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 98, 'Results_raw': {'train_total': 480, 'train_loss': 327.73126220703125, 'train_avg_loss': 0.6827734629313151, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 14:44:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:44:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:44:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #98, planning to set LR to 1.00e-05
2025-10-09 14:44:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 14:44:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:44:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:44:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:44:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:44:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:45:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:45:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.843384, avg_loss=0.678840, seen=480, correct=271, accuracy=0.564583
2025-10-09 14:45:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:45:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:45:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:45:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=98 reserved=2336MB allocated=2141MB
2025-10-09 14:45:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 98, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.94492727518082, 'train_avg_loss': 0.6662077272931735, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 14:45:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 98, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.8433837890625, 'train_avg_loss': 0.6788403828938802, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 14:45:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 98, 'Results_raw': {'train_total': 480, 'train_loss': 325.8433837890625, 'train_avg_loss': 0.6788403828938802, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 14:45:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:45:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:45:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #98, planning to set LR to 1.00e-05
2025-10-09 14:45:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 14:45:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:45:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:45:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:45:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:45:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:46:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:46:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.489288, avg_loss=0.678103, seen=480, correct=279, accuracy=0.581250
2025-10-09 14:46:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:46:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:46:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:46:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=98 reserved=2330MB allocated=2141MB
2025-10-09 14:46:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 98, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.31734472513199, 'train_avg_loss': 0.6609778727094332, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 14:46:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 98, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.4892883300781, 'train_avg_loss': 0.678102684020996, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 14:46:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 98, 'Results_raw': {'train_total': 480, 'train_loss': 325.4892883300781, 'train_avg_loss': 0.678102684020996, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 14:46:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:46:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:46:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #98, planning to set LR to 1.00e-05
2025-10-09 14:46:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 14:46:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:46:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:46:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:46:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:46:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:46:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:46:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.390503, avg_loss=0.694564, seen=480, correct=267, accuracy=0.556250
2025-10-09 14:46:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:46:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:46:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=98 reserved=2330MB allocated=2141MB
2025-10-09 14:46:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 98, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.14971578121185, 'train_avg_loss': 0.6929142981767654, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 14:46:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 98, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.3905029296875, 'train_avg_loss': 0.6945635477701823, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 14:46:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 98, 'Results_raw': {'train_total': 480, 'train_loss': 333.3905029296875, 'train_avg_loss': 0.6945635477701823, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 14:46:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:47:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:47:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #98, planning to set LR to 1.00e-05
2025-10-09 14:47:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 14:47:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:47:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:47:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:47:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:47:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:47:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:47:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.844727, avg_loss=0.668427, seen=480, correct=289, accuracy=0.602083
2025-10-09 14:47:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:47:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:47:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=98 reserved=2366MB allocated=2141MB
2025-10-09 14:47:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 98, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.4089207649231, 'train_avg_loss': 0.6700743397076925, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 14:47:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 98, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.8447265625, 'train_avg_loss': 0.668426513671875, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 14:47:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 98, 'Results_raw': {'train_total': 480, 'train_loss': 320.8447265625, 'train_avg_loss': 0.668426513671875, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 14:47:43 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #99) -------------
2025-10-09 14:47:43 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=99 aidx=1 | s=5 (candidates=23)
2025-10-09 14:47:43 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[9, 17, 35, 12, 14] (from 23)
2025-10-09 14:47:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:47:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:47:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #99, planning to set LR to 1.00e-05
2025-10-09 14:47:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 14:47:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:47:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:47:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:47:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:47:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:48:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:48:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.126282, avg_loss=0.677346, seen=480, correct=278, accuracy=0.579167
2025-10-09 14:48:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:48:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:48:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:48:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=99 reserved=2350MB allocated=2141MB
2025-10-09 14:48:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 99, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.56282770633698, 'train_avg_loss': 0.6963568975528082, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 14:48:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 99, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.12628173828125, 'train_avg_loss': 0.6773464202880859, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 14:48:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 99, 'Results_raw': {'train_total': 480, 'train_loss': 325.12628173828125, 'train_avg_loss': 0.6773464202880859, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 14:48:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:48:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:48:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #99, planning to set LR to 1.00e-05
2025-10-09 14:48:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 14:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:48:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:48:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:48:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:48:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:49:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:49:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.275269, avg_loss=0.660990, seen=480, correct=294, accuracy=0.612500
2025-10-09 14:49:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:49:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:49:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=99 reserved=2366MB allocated=2141MB
2025-10-09 14:49:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 99, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.90914535522461, 'train_avg_loss': 0.6575762112935384, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 14:49:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 99, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.2752685546875, 'train_avg_loss': 0.6609901428222656, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 14:49:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 99, 'Results_raw': {'train_total': 480, 'train_loss': 317.2752685546875, 'train_avg_loss': 0.6609901428222656, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 14:49:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:49:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:49:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #99, planning to set LR to 1.00e-05
2025-10-09 14:49:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 14:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:49:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:49:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:49:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:49:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:49:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:49:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.801117, avg_loss=0.687086, seen=480, correct=269, accuracy=0.560417
2025-10-09 14:49:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:49:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:49:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:50:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=99 reserved=2404MB allocated=2141MB
2025-10-09 14:50:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 99, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.59786427021027, 'train_avg_loss': 0.6966488689184189, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 14:50:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 99, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.8011169433594, 'train_avg_loss': 0.6870856602986654, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 14:50:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 99, 'Results_raw': {'train_total': 480, 'train_loss': 329.8011169433594, 'train_avg_loss': 0.6870856602986654, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 14:50:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:50:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:50:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #99, planning to set LR to 1.00e-05
2025-10-09 14:50:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 14:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:50:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:50:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:50:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:50:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:50:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:50:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.198425, avg_loss=0.681663, seen=480, correct=283, accuracy=0.589583
2025-10-09 14:50:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:50:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:50:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:50:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=99 reserved=2330MB allocated=2141MB
2025-10-09 14:50:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 99, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.004181265831, 'train_avg_loss': 0.6917015105485916, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 14:50:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 99, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.19842529296875, 'train_avg_loss': 0.6816633860270183, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 14:50:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 99, 'Results_raw': {'train_total': 480, 'train_loss': 327.19842529296875, 'train_avg_loss': 0.6816633860270183, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 14:50:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:50:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:50:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #99, planning to set LR to 1.00e-05
2025-10-09 14:50:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 14:50:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:50:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:50:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:50:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:50:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:51:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:51:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.945526, avg_loss=0.670720, seen=480, correct=283, accuracy=0.589583
2025-10-09 14:51:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:51:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:51:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=99 reserved=2330MB allocated=2141MB
2025-10-09 14:51:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 99, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.02084004878998, 'train_avg_loss': 0.6501736670732499, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 14:51:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 99, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.9455261230469, 'train_avg_loss': 0.6707198460896809, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 14:51:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 99, 'Results_raw': {'train_total': 480, 'train_loss': 321.9455261230469, 'train_avg_loss': 0.6707198460896809, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 14:51:29 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #100) -------------
2025-10-09 14:51:29 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=100 aidx=1 | s=5 (candidates=23)
2025-10-09 14:51:29 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[23, 14, 9, 24, 2] (from 23)
2025-10-09 14:51:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:51:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:51:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #100, planning to set LR to 1.00e-05
2025-10-09 14:51:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-09 14:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:51:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:51:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:51:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:51:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:52:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:52:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=340.058044, avg_loss=0.708454, seen=480, correct=245, accuracy=0.510417
2025-10-09 14:52:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:52:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:52:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:52:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=100 reserved=2330MB allocated=2141MB
2025-10-09 14:52:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 100, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.81044614315033, 'train_avg_loss': 0.7067537178595861, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 14:52:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 100, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.05804443359375, 'train_avg_loss': 0.7084542592366536, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-10-09 14:52:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 100, 'Results_raw': {'train_total': 480, 'train_loss': 340.05804443359375, 'train_avg_loss': 0.7084542592366536, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-10-09 14:52:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:52:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:52:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #100, planning to set LR to 1.00e-05
2025-10-09 14:52:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 14:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:52:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:52:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:52:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:52:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:52:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:52:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.243744, avg_loss=0.679674, seen=480, correct=269, accuracy=0.560417
2025-10-09 14:52:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:52:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:52:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:52:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=100 reserved=2330MB allocated=2141MB
2025-10-09 14:52:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 100, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.81738364696503, 'train_avg_loss': 0.6818115303913752, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 14:52:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 100, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.2437438964844, 'train_avg_loss': 0.6796744664510092, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 14:52:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 100, 'Results_raw': {'train_total': 480, 'train_loss': 326.2437438964844, 'train_avg_loss': 0.6796744664510092, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 14:52:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:52:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:52:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #100, planning to set LR to 1.00e-05
2025-10-09 14:52:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 14:52:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:52:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:52:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:52:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:52:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:53:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:53:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.056641, avg_loss=0.689701, seen=480, correct=258, accuracy=0.537500
2025-10-09 14:53:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:53:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:53:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:53:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=100 reserved=2350MB allocated=2141MB
2025-10-09 14:53:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 100, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.85203409194946, 'train_avg_loss': 0.7071002840995788, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 14:53:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 100, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.056640625, 'train_avg_loss': 0.6897013346354167, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 14:53:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 100, 'Results_raw': {'train_total': 480, 'train_loss': 331.056640625, 'train_avg_loss': 0.6897013346354167, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 14:53:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:53:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:53:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #100, planning to set LR to 1.00e-05
2025-10-09 14:53:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 14:53:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:53:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:53:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:53:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:53:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:54:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:54:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.801117, avg_loss=0.697502, seen=480, correct=256, accuracy=0.533333
2025-10-09 14:54:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:54:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:54:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=100 reserved=2330MB allocated=2141MB
2025-10-09 14:54:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 100, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.84160792827606, 'train_avg_loss': 0.7153467327356339, 'train_seen': 120, 'train_correct': 56, 'train_acc': 0.4666666666666667}}
2025-10-09 14:54:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 100, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.8011169433594, 'train_avg_loss': 0.697502326965332, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 14:54:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 100, 'Results_raw': {'train_total': 480, 'train_loss': 334.8011169433594, 'train_avg_loss': 0.697502326965332, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 14:54:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:54:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:54:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #100, planning to set LR to 1.00e-05
2025-10-09 14:54:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 14:54:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:54:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:54:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:54:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:54:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:55:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:55:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.856781, avg_loss=0.680952, seen=480, correct=278, accuracy=0.579167
2025-10-09 14:55:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:55:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:55:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=100 reserved=2402MB allocated=2141MB
2025-10-09 14:55:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 100, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.34380239248276, 'train_avg_loss': 0.6528650199373564, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 14:55:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 100, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.8567810058594, 'train_avg_loss': 0.6809516270955404, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 14:55:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 100, 'Results_raw': {'train_total': 480, 'train_loss': 326.8567810058594, 'train_avg_loss': 0.6809516270955404, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 14:55:09 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #101) -------------
2025-10-09 14:55:09 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=101 aidx=1 | s=5 (candidates=23)
2025-10-09 14:55:09 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[14, 23, 52, 24, 2] (from 23)
2025-10-09 14:55:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:55:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:55:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #101, planning to set LR to 1.00e-05
2025-10-09 14:55:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 14:55:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:55:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:55:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:55:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:55:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:55:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:55:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.586487, avg_loss=0.653305, seen=480, correct=290, accuracy=0.604167
2025-10-09 14:55:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:55:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:55:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:55:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=101 reserved=2330MB allocated=2141MB
2025-10-09 14:55:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 101, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.78058290481567, 'train_avg_loss': 0.6231715242067973, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 14:55:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 101, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.58648681640625, 'train_avg_loss': 0.653305180867513, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 14:55:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 101, 'Results_raw': {'train_total': 480, 'train_loss': 313.58648681640625, 'train_avg_loss': 0.653305180867513, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 14:55:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:55:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:55:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #101, planning to set LR to 1.00e-05
2025-10-09 14:55:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-09 14:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:55:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:55:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:55:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:55:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:56:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:56:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.263062, avg_loss=0.688048, seen=480, correct=264, accuracy=0.550000
2025-10-09 14:56:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:56:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:56:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=101 reserved=2330MB allocated=2141MB
2025-10-09 14:56:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 101, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.88445442914963, 'train_avg_loss': 0.6907037869095802, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 14:56:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 101, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.2630615234375, 'train_avg_loss': 0.6880480448404948, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 14:56:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 101, 'Results_raw': {'train_total': 480, 'train_loss': 330.2630615234375, 'train_avg_loss': 0.6880480448404948, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 14:56:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:56:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:56:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #101, planning to set LR to 1.00e-05
2025-10-09 14:56:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 14:56:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:56:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:56:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:56:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:56:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:57:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:57:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.706879, avg_loss=0.666056, seen=480, correct=278, accuracy=0.579167
2025-10-09 14:57:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:57:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:57:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:57:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=101 reserved=2338MB allocated=2141MB
2025-10-09 14:57:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 101, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.77203583717346, 'train_avg_loss': 0.6564336319764456, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 14:57:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 101, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.7068786621094, 'train_avg_loss': 0.6660559972127279, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 14:57:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 101, 'Results_raw': {'train_total': 480, 'train_loss': 319.7068786621094, 'train_avg_loss': 0.6660559972127279, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 14:57:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:57:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:57:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #101, planning to set LR to 1.00e-05
2025-10-09 14:57:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 14:57:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:57:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:57:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:57:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:57:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:58:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:58:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.338562, avg_loss=0.692372, seen=480, correct=255, accuracy=0.531250
2025-10-09 14:58:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:58:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:58:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=101 reserved=2330MB allocated=2141MB
2025-10-09 14:58:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 101, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.46555626392365, 'train_avg_loss': 0.7038796355326971, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 14:58:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 101, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.33856201171875, 'train_avg_loss': 0.6923720041910807, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 14:58:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 101, 'Results_raw': {'train_total': 480, 'train_loss': 332.33856201171875, 'train_avg_loss': 0.6923720041910807, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 14:58:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:58:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:58:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #101, planning to set LR to 1.00e-05
2025-10-09 14:58:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 14:58:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:58:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:58:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:58:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:58:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:58:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:58:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.311676, avg_loss=0.648566, seen=480, correct=305, accuracy=0.635417
2025-10-09 14:58:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:58:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:58:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:58:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=101 reserved=2402MB allocated=2141MB
2025-10-09 14:58:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 101, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.72317677736282, 'train_avg_loss': 0.6060264731446902, 'train_seen': 120, 'train_correct': 92, 'train_acc': 0.7666666666666667}}
2025-10-09 14:58:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 101, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.3116760253906, 'train_avg_loss': 0.6485659917195638, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 14:58:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 101, 'Results_raw': {'train_total': 480, 'train_loss': 311.3116760253906, 'train_avg_loss': 0.6485659917195638, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 14:58:49 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #102) -------------
2025-10-09 14:58:49 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=102 aidx=1 | s=5 (candidates=23)
2025-10-09 14:58:49 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[7, 17, 12, 25, 30] (from 23)
2025-10-09 14:58:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:58:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:58:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #102, planning to set LR to 1.00e-05
2025-10-09 14:58:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 14:58:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:58:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:58:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:58:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:58:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 14:59:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 14:59:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.089661, avg_loss=0.654353, seen=480, correct=295, accuracy=0.614583
2025-10-09 14:59:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 14:59:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:59:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 14:59:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=102 reserved=2340MB allocated=2141MB
2025-10-09 14:59:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 102, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.0137340426445, 'train_avg_loss': 0.6834477836887042, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 14:59:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 102, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.08966064453125, 'train_avg_loss': 0.6543534596761068, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 14:59:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 102, 'Results_raw': {'train_total': 480, 'train_loss': 314.08966064453125, 'train_avg_loss': 0.6543534596761068, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 14:59:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 14:59:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 14:59:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #102, planning to set LR to 1.00e-05
2025-10-09 14:59:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 14:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 14:59:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 14:59:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 14:59:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 14:59:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:00:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:00:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.050293, avg_loss=0.656355, seen=480, correct=292, accuracy=0.608333
2025-10-09 15:00:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:00:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:00:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:00:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=102 reserved=2366MB allocated=2141MB
2025-10-09 15:00:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 102, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.85723638534546, 'train_avg_loss': 0.6488103032112121, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 15:00:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 102, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.05029296875, 'train_avg_loss': 0.6563547770182292, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 15:00:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 102, 'Results_raw': {'train_total': 480, 'train_loss': 315.05029296875, 'train_avg_loss': 0.6563547770182292, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 15:00:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:00:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:00:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #102, planning to set LR to 1.00e-05
2025-10-09 15:00:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 15:00:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:00:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:00:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:00:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:00:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:00:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:00:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.531769, avg_loss=0.676108, seen=480, correct=276, accuracy=0.575000
2025-10-09 15:00:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:00:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:00:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:01:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=102 reserved=2330MB allocated=2141MB
2025-10-09 15:01:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 102, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.15167689323425, 'train_avg_loss': 0.6929306407769521, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 15:01:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 102, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.5317687988281, 'train_avg_loss': 0.6761078516642253, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 15:01:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 102, 'Results_raw': {'train_total': 480, 'train_loss': 324.5317687988281, 'train_avg_loss': 0.6761078516642253, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 15:01:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:01:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:01:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #102, planning to set LR to 1.00e-05
2025-10-09 15:01:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 15:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:01:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:01:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:01:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:01:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:01:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:01:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=338.803772, avg_loss=0.705841, seen=480, correct=254, accuracy=0.529167
2025-10-09 15:01:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:01:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:01:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:01:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=102 reserved=2330MB allocated=2141MB
2025-10-09 15:01:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 102, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.71054881811142, 'train_avg_loss': 0.6975879068175952, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 15:01:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 102, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.80377197265625, 'train_avg_loss': 0.7058411916097005, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 15:01:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 102, 'Results_raw': {'train_total': 480, 'train_loss': 338.80377197265625, 'train_avg_loss': 0.7058411916097005, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 15:01:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:01:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:01:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #102, planning to set LR to 1.00e-05
2025-10-09 15:01:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 15:01:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:01:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:01:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:01:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:01:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:02:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:02:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.244995, avg_loss=0.677594, seen=480, correct=277, accuracy=0.577083
2025-10-09 15:02:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:02:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:02:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=102 reserved=2330MB allocated=2141MB
2025-10-09 15:02:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 102, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.05512619018555, 'train_avg_loss': 0.6921260515848796, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 15:02:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 102, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.2449951171875, 'train_avg_loss': 0.677593739827474, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 15:02:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 102, 'Results_raw': {'train_total': 480, 'train_loss': 325.2449951171875, 'train_avg_loss': 0.677593739827474, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 15:02:24 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #103) -------------
2025-10-09 15:02:25 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=103 aidx=1 | s=5 (candidates=23)
2025-10-09 15:02:25 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[35, 44, 11, 25, 9] (from 23)
2025-10-09 15:02:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:02:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:02:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #103, planning to set LR to 1.00e-05
2025-10-09 15:02:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 15:02:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:02:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:02:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:02:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:02:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:03:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:03:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.607574, avg_loss=0.688766, seen=480, correct=275, accuracy=0.572917
2025-10-09 15:03:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:03:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:03:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:03:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=103 reserved=2402MB allocated=2141MB
2025-10-09 15:03:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 103, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.9320130944252, 'train_avg_loss': 0.6994334424535433, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 15:03:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 103, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.6075744628906, 'train_avg_loss': 0.6887657801310222, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 15:03:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 103, 'Results_raw': {'train_total': 480, 'train_loss': 330.6075744628906, 'train_avg_loss': 0.6887657801310222, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 15:03:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:03:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:03:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #103, planning to set LR to 1.00e-05
2025-10-09 15:03:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 15:03:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:03:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:03:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:03:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:03:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:03:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:03:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.908447, avg_loss=0.668559, seen=480, correct=290, accuracy=0.604167
2025-10-09 15:03:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:03:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:03:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:03:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=103 reserved=2330MB allocated=2141MB
2025-10-09 15:03:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 103, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.24572622776031, 'train_avg_loss': 0.643714385231336, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 15:03:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 103, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.908447265625, 'train_avg_loss': 0.6685592651367187, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 15:03:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 103, 'Results_raw': {'train_total': 480, 'train_loss': 320.908447265625, 'train_avg_loss': 0.6685592651367187, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 15:03:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:03:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:03:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #103, planning to set LR to 1.00e-05
2025-10-09 15:03:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 15:03:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:03:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:03:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:03:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:03:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:04:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:04:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.460968, avg_loss=0.665544, seen=480, correct=292, accuracy=0.608333
2025-10-09 15:04:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:04:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:04:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:04:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=103 reserved=2418MB allocated=2141MB
2025-10-09 15:04:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 103, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.44259053468704, 'train_avg_loss': 0.6870215877890586, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 15:04:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 103, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.4609680175781, 'train_avg_loss': 0.6655436833699544, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 15:04:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 103, 'Results_raw': {'train_total': 480, 'train_loss': 319.4609680175781, 'train_avg_loss': 0.6655436833699544, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 15:04:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:04:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:04:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #103, planning to set LR to 1.00e-05
2025-10-09 15:04:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 15:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:04:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:04:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:04:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:04:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:05:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:05:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.692810, avg_loss=0.701443, seen=480, correct=267, accuracy=0.556250
2025-10-09 15:05:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:05:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:05:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:05:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=103 reserved=2330MB allocated=2141MB
2025-10-09 15:05:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 103, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.38279354572296, 'train_avg_loss': 0.6948566128810246, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 15:05:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 103, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.69281005859375, 'train_avg_loss': 0.7014433542887369, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 15:05:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 103, 'Results_raw': {'train_total': 480, 'train_loss': 336.69281005859375, 'train_avg_loss': 0.7014433542887369, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 15:05:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:05:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:05:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #103, planning to set LR to 1.00e-05
2025-10-09 15:05:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 15:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:05:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:05:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:05:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:05:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:05:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:05:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.407349, avg_loss=0.673765, seen=480, correct=279, accuracy=0.581250
2025-10-09 15:05:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:05:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:06:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=103 reserved=2352MB allocated=2141MB
2025-10-09 15:06:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 103, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.14716291427612, 'train_avg_loss': 0.6845596909523011, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 15:06:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 103, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.4073486328125, 'train_avg_loss': 0.6737653096516927, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 15:06:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 103, 'Results_raw': {'train_total': 480, 'train_loss': 323.4073486328125, 'train_avg_loss': 0.6737653096516927, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 15:06:01 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #104) -------------
2025-10-09 15:06:01 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=104 aidx=1 | s=5 (candidates=23)
2025-10-09 15:06:01 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[9, 7, 52, 12, 4] (from 23)
2025-10-09 15:06:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:06:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:06:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #104, planning to set LR to 1.00e-05
2025-10-09 15:06:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 15:06:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:06:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:06:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:06:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:06:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:06:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:06:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.006531, avg_loss=0.666680, seen=480, correct=291, accuracy=0.606250
2025-10-09 15:06:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:06:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:06:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:06:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=104 reserved=2352MB allocated=2141MB
2025-10-09 15:06:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 104, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.70696145296097, 'train_avg_loss': 0.6808913454413414, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 15:06:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 104, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.00653076171875, 'train_avg_loss': 0.6666802724202474, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 15:06:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 104, 'Results_raw': {'train_total': 480, 'train_loss': 320.00653076171875, 'train_avg_loss': 0.6666802724202474, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 15:06:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:06:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:06:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #104, planning to set LR to 1.00e-05
2025-10-09 15:06:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 15:06:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:06:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:06:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:06:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:06:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:07:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:07:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.895081, avg_loss=0.653948, seen=480, correct=303, accuracy=0.631250
2025-10-09 15:07:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:07:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:07:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=104 reserved=2340MB allocated=2141MB
2025-10-09 15:07:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 104, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.25440090894699, 'train_avg_loss': 0.6854533409078916, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 15:07:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 104, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.89508056640625, 'train_avg_loss': 0.6539480845133464, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 15:07:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 104, 'Results_raw': {'train_total': 480, 'train_loss': 313.89508056640625, 'train_avg_loss': 0.6539480845133464, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 15:07:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:07:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:07:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #104, planning to set LR to 1.00e-05
2025-10-09 15:07:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 15:07:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:07:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:07:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:07:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:07:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:08:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:08:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.270538, avg_loss=0.665147, seen=480, correct=278, accuracy=0.579167
2025-10-09 15:08:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:08:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:08:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:08:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=104 reserved=2338MB allocated=2141MB
2025-10-09 15:08:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 104, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.68929070234299, 'train_avg_loss': 0.6557440891861915, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 15:08:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 104, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.2705383300781, 'train_avg_loss': 0.6651469548543294, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 15:08:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 104, 'Results_raw': {'train_total': 480, 'train_loss': 319.2705383300781, 'train_avg_loss': 0.6651469548543294, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 15:08:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:08:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:08:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #104, planning to set LR to 1.00e-05
2025-10-09 15:08:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 15:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:08:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:08:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:08:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:08:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:08:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:08:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.351868, avg_loss=0.673650, seen=480, correct=284, accuracy=0.591667
2025-10-09 15:08:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:08:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:08:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:08:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=104 reserved=2330MB allocated=2141MB
2025-10-09 15:08:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 104, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.58357459306717, 'train_avg_loss': 0.6881964549422264, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 15:08:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 104, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.35186767578125, 'train_avg_loss': 0.6736497243245443, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 15:08:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 104, 'Results_raw': {'train_total': 480, 'train_loss': 323.35186767578125, 'train_avg_loss': 0.6736497243245443, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 15:08:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:08:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:08:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #104, planning to set LR to 1.00e-05
2025-10-09 15:08:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 15:08:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:08:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:08:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:08:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:08:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:09:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:09:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.342041, avg_loss=0.634046, seen=480, correct=306, accuracy=0.637500
2025-10-09 15:09:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:09:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:09:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:09:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=104 reserved=2372MB allocated=2141MB
2025-10-09 15:09:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 104, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.42552053928375, 'train_avg_loss': 0.6618793378273646, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 15:09:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 104, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.342041015625, 'train_avg_loss': 0.6340459187825521, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 15:09:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 104, 'Results_raw': {'train_total': 480, 'train_loss': 304.342041015625, 'train_avg_loss': 0.6340459187825521, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 15:09:38 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #105) -------------
2025-10-09 15:09:38 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=105 aidx=1 | s=5 (candidates=23)
2025-10-09 15:09:38 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[30, 18, 44, 33, 2] (from 23)
2025-10-09 15:09:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:09:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:09:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #105, planning to set LR to 1.00e-05
2025-10-09 15:09:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 15:09:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:09:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:09:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:09:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:09:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:10:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:10:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.715912, avg_loss=0.666075, seen=480, correct=282, accuracy=0.587500
2025-10-09 15:10:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:10:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:10:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=105 reserved=2330MB allocated=2141MB
2025-10-09 15:10:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 105, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.34120082855225, 'train_avg_loss': 0.6861766735712688, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 15:10:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 105, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.7159118652344, 'train_avg_loss': 0.6660748163859049, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 15:10:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 105, 'Results_raw': {'train_total': 480, 'train_loss': 319.7159118652344, 'train_avg_loss': 0.6660748163859049, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 15:10:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:10:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:10:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #105, planning to set LR to 1.00e-05
2025-10-09 15:10:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 15:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:10:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:10:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:10:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:10:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:11:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:11:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.278992, avg_loss=0.671415, seen=480, correct=269, accuracy=0.560417
2025-10-09 15:11:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:11:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:11:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=105 reserved=2360MB allocated=2141MB
2025-10-09 15:11:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 105, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.99883437156677, 'train_avg_loss': 0.6916569530963897, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 15:11:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 105, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.27899169921875, 'train_avg_loss': 0.671414566040039, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 15:11:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 105, 'Results_raw': {'train_total': 480, 'train_loss': 322.27899169921875, 'train_avg_loss': 0.671414566040039, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 15:11:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:11:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:11:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #105, planning to set LR to 1.00e-05
2025-10-09 15:11:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 15:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:11:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:11:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:11:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:11:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:11:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:11:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.621429, avg_loss=0.670045, seen=480, correct=290, accuracy=0.604167
2025-10-09 15:11:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:11:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:11:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=105 reserved=2330MB allocated=2141MB
2025-10-09 15:11:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 105, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.79588854312897, 'train_avg_loss': 0.6482990711927414, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 15:11:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 105, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.6214294433594, 'train_avg_loss': 0.6700446446736653, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 15:11:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 105, 'Results_raw': {'train_total': 480, 'train_loss': 321.6214294433594, 'train_avg_loss': 0.6700446446736653, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 15:11:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:11:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:11:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #105, planning to set LR to 1.00e-05
2025-10-09 15:11:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 15:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:11:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:11:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:11:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:11:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:12:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:12:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.745117, avg_loss=0.678636, seen=480, correct=282, accuracy=0.587500
2025-10-09 15:12:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:12:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:12:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=105 reserved=2342MB allocated=2141MB
2025-10-09 15:12:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 105, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.24903857707977, 'train_avg_loss': 0.6854086548089982, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 15:12:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 105, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.7451171875, 'train_avg_loss': 0.6786356608072917, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 15:12:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 105, 'Results_raw': {'train_total': 480, 'train_loss': 325.7451171875, 'train_avg_loss': 0.6786356608072917, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 15:12:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:12:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:12:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #105, planning to set LR to 1.00e-05
2025-10-09 15:12:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 15:12:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:12:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:12:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:12:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:12:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:13:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:13:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.966064, avg_loss=0.629096, seen=480, correct=309, accuracy=0.643750
2025-10-09 15:13:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:13:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:13:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=105 reserved=2402MB allocated=2141MB
2025-10-09 15:13:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 105, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.1074652671814, 'train_avg_loss': 0.567562210559845, 'train_seen': 120, 'train_correct': 92, 'train_acc': 0.7666666666666667}}
2025-10-09 15:13:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 105, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.966064453125, 'train_avg_loss': 0.6290959676106771, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 15:13:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 105, 'Results_raw': {'train_total': 480, 'train_loss': 301.966064453125, 'train_avg_loss': 0.6290959676106771, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 15:13:17 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #106) -------------
2025-10-09 15:13:18 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=106 aidx=1 | s=5 (candidates=23)
2025-10-09 15:13:18 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[14, 38, 42, 52, 30] (from 23)
2025-10-09 15:13:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:13:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:13:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #106, planning to set LR to 1.00e-05
2025-10-09 15:13:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 15:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:13:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:13:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:13:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:13:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:14:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:14:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.301544, avg_loss=0.638128, seen=480, correct=309, accuracy=0.643750
2025-10-09 15:14:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:14:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:14:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=106 reserved=2330MB allocated=2141MB
2025-10-09 15:14:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 106, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.66173785924911, 'train_avg_loss': 0.597181148827076, 'train_seen': 120, 'train_correct': 87, 'train_acc': 0.725}}
2025-10-09 15:14:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 106, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.3015441894531, 'train_avg_loss': 0.6381282170613607, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 15:14:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 106, 'Results_raw': {'train_total': 480, 'train_loss': 306.3015441894531, 'train_avg_loss': 0.6381282170613607, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 15:14:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:14:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:14:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #106, planning to set LR to 1.00e-05
2025-10-09 15:14:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 15:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:14:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:14:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:14:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:14:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:14:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:14:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.617706, avg_loss=0.670037, seen=480, correct=283, accuracy=0.589583
2025-10-09 15:14:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:14:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:14:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=106 reserved=2336MB allocated=2141MB
2025-10-09 15:14:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 106, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.13209676742554, 'train_avg_loss': 0.6511008063952128, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 15:14:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 106, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.6177062988281, 'train_avg_loss': 0.6700368881225586, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 15:14:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 106, 'Results_raw': {'train_total': 480, 'train_loss': 321.6177062988281, 'train_avg_loss': 0.6700368881225586, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 15:14:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:14:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:14:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #106, planning to set LR to 1.00e-05
2025-10-09 15:14:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 15:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:14:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:14:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:14:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:14:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:15:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:15:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.027954, avg_loss=0.652142, seen=480, correct=302, accuracy=0.629167
2025-10-09 15:15:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:15:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:15:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:15:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=106 reserved=2340MB allocated=2141MB
2025-10-09 15:15:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 106, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.76893448829651, 'train_avg_loss': 0.6397411207358042, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 15:15:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 106, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.0279541015625, 'train_avg_loss': 0.6521415710449219, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 15:15:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 106, 'Results_raw': {'train_total': 480, 'train_loss': 313.0279541015625, 'train_avg_loss': 0.6521415710449219, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 15:15:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:15:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:15:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #106, planning to set LR to 1.00e-05
2025-10-09 15:15:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 15:15:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:15:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:15:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:15:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:15:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:16:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:16:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.649780, avg_loss=0.661770, seen=480, correct=283, accuracy=0.589583
2025-10-09 15:16:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:16:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:16:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:16:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=106 reserved=2338MB allocated=2141MB
2025-10-09 15:16:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 106, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.07631647586823, 'train_avg_loss': 0.6423026372989019, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 15:16:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 106, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.6497802734375, 'train_avg_loss': 0.6617703755696615, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 15:16:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 106, 'Results_raw': {'train_total': 480, 'train_loss': 317.6497802734375, 'train_avg_loss': 0.6617703755696615, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 15:16:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:16:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:16:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #106, planning to set LR to 1.00e-05
2025-10-09 15:16:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 15:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:16:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:16:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:16:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:16:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:16:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:16:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.726929, avg_loss=0.672348, seen=480, correct=279, accuracy=0.581250
2025-10-09 15:16:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:16:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:17:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:17:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=106 reserved=2330MB allocated=2141MB
2025-10-09 15:17:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 106, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.20008426904678, 'train_avg_loss': 0.6850007022420566, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 15:17:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 106, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.7269287109375, 'train_avg_loss': 0.6723477681477864, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 15:17:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 106, 'Results_raw': {'train_total': 480, 'train_loss': 322.7269287109375, 'train_avg_loss': 0.6723477681477864, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 15:17:01 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #107) -------------
2025-10-09 15:17:02 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=107 aidx=1 | s=5 (candidates=23)
2025-10-09 15:17:02 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[42, 52, 25, 44, 7] (from 23)
2025-10-09 15:17:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:17:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:17:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #107, planning to set LR to 1.00e-05
2025-10-09 15:17:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 15:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:17:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:17:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:17:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:17:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:17:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:17:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.041687, avg_loss=0.650087, seen=480, correct=306, accuracy=0.637500
2025-10-09 15:17:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:17:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:17:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:17:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=107 reserved=2340MB allocated=2141MB
2025-10-09 15:17:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 107, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.66581684350967, 'train_avg_loss': 0.6388818070292472, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 15:17:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 107, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.04168701171875, 'train_avg_loss': 0.6500868479410807, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 15:17:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 107, 'Results_raw': {'train_total': 480, 'train_loss': 312.04168701171875, 'train_avg_loss': 0.6500868479410807, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 15:17:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:17:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:17:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #107, planning to set LR to 1.00e-05
2025-10-09 15:17:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 15:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:17:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:17:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:17:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:17:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:18:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:18:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.592285, avg_loss=0.661651, seen=480, correct=290, accuracy=0.604167
2025-10-09 15:18:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:18:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:18:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:18:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=107 reserved=2338MB allocated=2141MB
2025-10-09 15:18:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 107, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.40719497203827, 'train_avg_loss': 0.6450599581003189, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 15:18:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 107, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.59228515625, 'train_avg_loss': 0.6616505940755208, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 15:18:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 107, 'Results_raw': {'train_total': 480, 'train_loss': 317.59228515625, 'train_avg_loss': 0.6616505940755208, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 15:18:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:18:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:18:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #107, planning to set LR to 1.00e-05
2025-10-09 15:18:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 15:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:18:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:18:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:18:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:18:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:19:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:19:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.695496, avg_loss=0.699366, seen=480, correct=265, accuracy=0.552083
2025-10-09 15:19:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:19:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:19:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:19:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=107 reserved=2330MB allocated=2141MB
2025-10-09 15:19:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 107, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.3556078672409, 'train_avg_loss': 0.6946300655603409, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 15:19:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 107, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.69549560546875, 'train_avg_loss': 0.6993656158447266, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 15:19:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 107, 'Results_raw': {'train_total': 480, 'train_loss': 335.69549560546875, 'train_avg_loss': 0.6993656158447266, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 15:19:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:19:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:19:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #107, planning to set LR to 1.00e-05
2025-10-09 15:19:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 15:19:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:19:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:19:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:19:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:19:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:19:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:19:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.977295, avg_loss=0.668703, seen=480, correct=291, accuracy=0.606250
2025-10-09 15:19:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:19:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:20:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:20:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=107 reserved=2330MB allocated=2141MB
2025-10-09 15:20:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 107, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.59346139431, 'train_avg_loss': 0.6382788449525834, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 15:20:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 107, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.977294921875, 'train_avg_loss': 0.6687026977539062, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 15:20:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 107, 'Results_raw': {'train_total': 480, 'train_loss': 320.977294921875, 'train_avg_loss': 0.6687026977539062, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 15:20:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:20:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:20:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #107, planning to set LR to 1.00e-05
2025-10-09 15:20:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 15:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:20:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:20:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:20:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:20:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:20:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:20:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.087738, avg_loss=0.648099, seen=480, correct=304, accuracy=0.633333
2025-10-09 15:20:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:20:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:20:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:20:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=107 reserved=2340MB allocated=2141MB
2025-10-09 15:20:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 107, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.78953570127487, 'train_avg_loss': 0.6815794641772907, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 15:20:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 107, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.0877380371094, 'train_avg_loss': 0.6480994542439779, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 15:20:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 107, 'Results_raw': {'train_total': 480, 'train_loss': 311.0877380371094, 'train_avg_loss': 0.6480994542439779, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 15:20:47 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #108) -------------
2025-10-09 15:20:48 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=108 aidx=1 | s=5 (candidates=23)
2025-10-09 15:20:48 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[9, 52, 42, 25, 35] (from 23)
2025-10-09 15:20:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:20:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:20:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #108, planning to set LR to 1.00e-05
2025-10-09 15:20:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 15:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:20:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:20:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:20:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:20:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:21:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:21:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.584595, avg_loss=0.669968, seen=480, correct=277, accuracy=0.577083
2025-10-09 15:21:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:21:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:21:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=108 reserved=2352MB allocated=2141MB
2025-10-09 15:21:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 108, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.95038378238678, 'train_avg_loss': 0.6745865315198898, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 15:21:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 108, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.5845947265625, 'train_avg_loss': 0.6699679056803386, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 15:21:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 108, 'Results_raw': {'train_total': 480, 'train_loss': 321.5845947265625, 'train_avg_loss': 0.6699679056803386, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 15:21:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:21:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:21:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #108, planning to set LR to 1.00e-05
2025-10-09 15:21:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 15:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:21:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:21:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:21:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:21:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:22:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:22:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.343781, avg_loss=0.661133, seen=480, correct=287, accuracy=0.597917
2025-10-09 15:22:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:22:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:22:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:22:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=108 reserved=2338MB allocated=2141MB
2025-10-09 15:22:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 108, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.49302363395691, 'train_avg_loss': 0.6457751969496409, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 15:22:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 108, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.3437805175781, 'train_avg_loss': 0.6611328760782877, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 15:22:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 108, 'Results_raw': {'train_total': 480, 'train_loss': 317.3437805175781, 'train_avg_loss': 0.6611328760782877, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 15:22:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:22:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:22:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #108, planning to set LR to 1.00e-05
2025-10-09 15:22:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 15:22:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:22:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:22:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:22:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:22:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:23:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:23:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.613037, avg_loss=0.649194, seen=480, correct=300, accuracy=0.625000
2025-10-09 15:23:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:23:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:23:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:23:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=108 reserved=2340MB allocated=2141MB
2025-10-09 15:23:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 108, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.61720144748688, 'train_avg_loss': 0.6384766787290573, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 15:23:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 108, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.613037109375, 'train_avg_loss': 0.6491938273111979, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 15:23:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 108, 'Results_raw': {'train_total': 480, 'train_loss': 311.613037109375, 'train_avg_loss': 0.6491938273111979, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 15:23:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:23:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:23:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #108, planning to set LR to 1.00e-05
2025-10-09 15:23:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 15:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:23:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:23:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:23:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:23:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:23:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:23:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.758606, avg_loss=0.697414, seen=480, correct=274, accuracy=0.570833
2025-10-09 15:23:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:23:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:23:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:23:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=108 reserved=2330MB allocated=2141MB
2025-10-09 15:23:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 108, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.80778706073761, 'train_avg_loss': 0.6983982255061467, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 15:23:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 108, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.75860595703125, 'train_avg_loss': 0.6974137624104818, 'train_seen': 480, 'train_correct': 274, 'train_acc': 0.5708333333333333}}
2025-10-09 15:23:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 108, 'Results_raw': {'train_total': 480, 'train_loss': 334.75860595703125, 'train_avg_loss': 0.6974137624104818, 'train_seen': 480, 'train_correct': 274, 'train_acc': 0.5708333333333333}}
2025-10-09 15:23:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:23:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:23:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #108, planning to set LR to 1.00e-05
2025-10-09 15:23:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 15:23:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:23:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:23:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:23:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:23:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:24:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:24:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.785706, avg_loss=0.691220, seen=480, correct=262, accuracy=0.545833
2025-10-09 15:24:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:24:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:24:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:24:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=108 reserved=2402MB allocated=2141MB
2025-10-09 15:24:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 108, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.55072343349457, 'train_avg_loss': 0.7129226952791214, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 15:24:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 108, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.78570556640625, 'train_avg_loss': 0.691220219930013, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 15:24:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 108, 'Results_raw': {'train_total': 480, 'train_loss': 331.78570556640625, 'train_avg_loss': 0.691220219930013, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 15:24:32 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #109) -------------
2025-10-09 15:24:32 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=109 aidx=1 | s=5 (candidates=23)
2025-10-09 15:24:32 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[4, 18, 49, 17, 11] (from 23)
2025-10-09 15:24:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:24:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:24:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #109, planning to set LR to 1.00e-05
2025-10-09 15:24:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 15:24:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:24:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:24:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:24:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:24:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:25:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:25:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=287.680298, avg_loss=0.599334, seen=480, correct=332, accuracy=0.691667
2025-10-09 15:25:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:25:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:25:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:25:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=109 reserved=2372MB allocated=2141MB
2025-10-09 15:25:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 109, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.99974054098129, 'train_avg_loss': 0.6416645045081775, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 15:25:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 109, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 287.6802978515625, 'train_avg_loss': 0.5993339538574218, 'train_seen': 480, 'train_correct': 332, 'train_acc': 0.6916666666666667}}
2025-10-09 15:25:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 109, 'Results_raw': {'train_total': 480, 'train_loss': 287.6802978515625, 'train_avg_loss': 0.5993339538574218, 'train_seen': 480, 'train_correct': 332, 'train_acc': 0.6916666666666667}}
2025-10-09 15:25:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:25:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:25:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #109, planning to set LR to 1.00e-05
2025-10-09 15:25:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 15:25:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:25:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:25:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:25:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:25:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:25:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:25:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.447784, avg_loss=0.667600, seen=480, correct=279, accuracy=0.581250
2025-10-09 15:25:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:25:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:25:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=109 reserved=2360MB allocated=2141MB
2025-10-09 15:25:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 109, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.83560198545456, 'train_avg_loss': 0.6902966832121213, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 15:25:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 109, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.4477844238281, 'train_avg_loss': 0.6675995508829753, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 15:25:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 109, 'Results_raw': {'train_total': 480, 'train_loss': 320.4477844238281, 'train_avg_loss': 0.6675995508829753, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 15:25:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:25:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:25:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #109, planning to set LR to 1.00e-05
2025-10-09 15:25:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 15:25:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:26:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:26:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:26:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:26:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:26:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:26:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.367157, avg_loss=0.677848, seen=480, correct=273, accuracy=0.568750
2025-10-09 15:26:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:26:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:26:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=109 reserved=2362MB allocated=2141MB
2025-10-09 15:26:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 109, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.99459147453308, 'train_avg_loss': 0.691621595621109, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 15:26:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 109, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.3671569824219, 'train_avg_loss': 0.6778482437133789, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 15:26:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 109, 'Results_raw': {'train_total': 480, 'train_loss': 325.3671569824219, 'train_avg_loss': 0.6778482437133789, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 15:26:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:26:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:26:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #109, planning to set LR to 1.00e-05
2025-10-09 15:26:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 15:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:26:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:26:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:26:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:26:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:27:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:27:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.951355, avg_loss=0.651982, seen=480, correct=295, accuracy=0.614583
2025-10-09 15:27:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:27:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:27:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:27:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=109 reserved=2366MB allocated=2141MB
2025-10-09 15:27:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 109, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.15075999498367, 'train_avg_loss': 0.6345896666248639, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 15:27:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 109, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.95135498046875, 'train_avg_loss': 0.6519819895426432, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 15:27:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 109, 'Results_raw': {'train_total': 480, 'train_loss': 312.95135498046875, 'train_avg_loss': 0.6519819895426432, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 15:27:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:27:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:27:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #109, planning to set LR to 1.00e-05
2025-10-09 15:27:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 15:27:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:27:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:27:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:27:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:27:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:28:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:28:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.341858, avg_loss=0.663212, seen=480, correct=282, accuracy=0.587500
2025-10-09 15:28:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:28:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:28:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:28:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=109 reserved=2418MB allocated=2141MB
2025-10-09 15:28:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 109, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.511006295681, 'train_avg_loss': 0.6792583857973417, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 15:28:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 109, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.34185791015625, 'train_avg_loss': 0.6632122039794922, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 15:28:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 109, 'Results_raw': {'train_total': 480, 'train_loss': 318.34185791015625, 'train_avg_loss': 0.6632122039794922, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 15:28:11 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #110) -------------
2025-10-09 15:28:12 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=110 aidx=1 | s=5 (candidates=23)
2025-10-09 15:28:12 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[14, 27, 12, 44, 9] (from 23)
2025-10-09 15:28:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:28:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:28:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #110, planning to set LR to 1.00e-05
2025-10-09 15:28:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 15:28:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:28:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:28:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:28:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:28:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:28:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:28:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.678955, avg_loss=0.626414, seen=480, correct=314, accuracy=0.654167
2025-10-09 15:28:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:28:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:28:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:28:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=110 reserved=2330MB allocated=2141MB
2025-10-09 15:28:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 110, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.37303406000137, 'train_avg_loss': 0.5781086171666782, 'train_seen': 120, 'train_correct': 88, 'train_acc': 0.7333333333333333}}
2025-10-09 15:28:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 110, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.678955078125, 'train_avg_loss': 0.6264144897460937, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 15:28:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 110, 'Results_raw': {'train_total': 480, 'train_loss': 300.678955078125, 'train_avg_loss': 0.6264144897460937, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 15:28:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:28:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:28:58 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #110, planning to set LR to 1.00e-05
2025-10-09 15:28:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 15:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:28:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:28:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:28:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:28:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:29:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:29:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.173035, avg_loss=0.677444, seen=480, correct=276, accuracy=0.575000
2025-10-09 15:29:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:29:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:29:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:29:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=110 reserved=2332MB allocated=2141MB
2025-10-09 15:29:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 110, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.16113150119781, 'train_avg_loss': 0.6763427625099818, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 15:29:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 110, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.17303466796875, 'train_avg_loss': 0.6774438222249349, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 15:29:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 110, 'Results_raw': {'train_total': 480, 'train_loss': 325.17303466796875, 'train_avg_loss': 0.6774438222249349, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 15:29:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:29:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:29:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #110, planning to set LR to 1.00e-05
2025-10-09 15:29:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 15:29:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:29:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:29:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:29:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:29:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:30:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:30:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.284729, avg_loss=0.667260, seen=480, correct=286, accuracy=0.595833
2025-10-09 15:30:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:30:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:30:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:30:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=110 reserved=2330MB allocated=2141MB
2025-10-09 15:30:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 110, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.27395981550217, 'train_avg_loss': 0.6939496651291848, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 15:30:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 110, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.28472900390625, 'train_avg_loss': 0.6672598520914713, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 15:30:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 110, 'Results_raw': {'train_total': 480, 'train_loss': 320.28472900390625, 'train_avg_loss': 0.6672598520914713, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 15:30:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:30:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:30:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #110, planning to set LR to 1.00e-05
2025-10-09 15:30:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 15:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:30:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:30:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:30:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:30:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:31:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:31:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.969666, avg_loss=0.670770, seen=480, correct=298, accuracy=0.620833
2025-10-09 15:31:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:31:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:31:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:31:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=110 reserved=2330MB allocated=2141MB
2025-10-09 15:31:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 110, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.98511290550232, 'train_avg_loss': 0.6415426075458527, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 15:31:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 110, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.96966552734375, 'train_avg_loss': 0.6707701365152995, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 15:31:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 110, 'Results_raw': {'train_total': 480, 'train_loss': 321.96966552734375, 'train_avg_loss': 0.6707701365152995, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 15:31:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:31:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:31:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #110, planning to set LR to 1.00e-05
2025-10-09 15:31:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 15:31:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:31:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:31:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:31:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:31:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:31:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:31:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.630524, avg_loss=0.667980, seen=480, correct=277, accuracy=0.577083
2025-10-09 15:31:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:31:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:31:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:31:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=110 reserved=2352MB allocated=2141MB
2025-10-09 15:31:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 110, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.4076538681984, 'train_avg_loss': 0.6783971155683199, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 15:31:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 110, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.6305236816406, 'train_avg_loss': 0.6679802576700846, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 15:31:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 110, 'Results_raw': {'train_total': 480, 'train_loss': 320.6305236816406, 'train_avg_loss': 0.6679802576700846, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 15:31:49 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #111) -------------
2025-10-09 15:31:50 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=111 aidx=1 | s=5 (candidates=23)
2025-10-09 15:31:50 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[11, 27, 14, 2, 53] (from 23)
2025-10-09 15:31:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:31:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:31:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #111, planning to set LR to 1.00e-05
2025-10-09 15:31:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 15:31:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:31:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:31:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:31:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:31:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:32:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:32:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.263336, avg_loss=0.652632, seen=480, correct=294, accuracy=0.612500
2025-10-09 15:32:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:32:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:32:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:32:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=111 reserved=2418MB allocated=2141MB
2025-10-09 15:32:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 111, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.64905822277069, 'train_avg_loss': 0.6804088185230891, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 15:32:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 111, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.2633361816406, 'train_avg_loss': 0.6526319503784179, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 15:32:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 111, 'Results_raw': {'train_total': 480, 'train_loss': 313.2633361816406, 'train_avg_loss': 0.6526319503784179, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 15:32:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:32:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:32:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #111, planning to set LR to 1.00e-05
2025-10-09 15:32:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 15:32:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:32:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:32:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:32:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:32:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:33:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:33:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.477234, avg_loss=0.680161, seen=480, correct=267, accuracy=0.556250
2025-10-09 15:33:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:33:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:33:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:33:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=111 reserved=2332MB allocated=2141MB
2025-10-09 15:33:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 111, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.54793614149094, 'train_avg_loss': 0.6878994678457578, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 15:33:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 111, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.47723388671875, 'train_avg_loss': 0.6801609039306641, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 15:33:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 111, 'Results_raw': {'train_total': 480, 'train_loss': 326.47723388671875, 'train_avg_loss': 0.6801609039306641, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 15:33:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:33:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:33:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #111, planning to set LR to 1.00e-05
2025-10-09 15:33:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 15:33:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:33:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:33:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:33:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:33:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:33:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:33:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.554047, avg_loss=0.638654, seen=480, correct=307, accuracy=0.639583
2025-10-09 15:33:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:33:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:33:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:33:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=111 reserved=2330MB allocated=2141MB
2025-10-09 15:33:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 111, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.46913707256317, 'train_avg_loss': 0.6039094756046931, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 15:33:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 111, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.5540466308594, 'train_avg_loss': 0.6386542638142904, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 15:33:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 111, 'Results_raw': {'train_total': 480, 'train_loss': 306.5540466308594, 'train_avg_loss': 0.6386542638142904, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 15:33:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:33:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:33:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #111, planning to set LR to 1.00e-05
2025-10-09 15:33:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 15:33:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:33:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:33:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:33:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:33:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:34:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:34:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=287.596527, avg_loss=0.599159, seen=480, correct=334, accuracy=0.695833
2025-10-09 15:34:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:34:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:34:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:34:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=111 reserved=2402MB allocated=2141MB
2025-10-09 15:34:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 111, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 62.63041442632675, 'train_avg_loss': 0.5219201202193896, 'train_seen': 120, 'train_correct': 93, 'train_acc': 0.775}}
2025-10-09 15:34:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 111, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 287.5965270996094, 'train_avg_loss': 0.5991594314575195, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 15:34:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 111, 'Results_raw': {'train_total': 480, 'train_loss': 287.5965270996094, 'train_avg_loss': 0.5991594314575195, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 15:34:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:34:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:34:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #111, planning to set LR to 1.00e-05
2025-10-09 15:34:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 15:34:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:34:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:34:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:34:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:34:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:35:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:35:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.730164, avg_loss=0.695271, seen=480, correct=275, accuracy=0.572917
2025-10-09 15:35:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:35:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:35:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:35:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=111 reserved=2332MB allocated=2141MB
2025-10-09 15:35:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 111, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.60042470693588, 'train_avg_loss': 0.6300035392244657, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 15:35:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 111, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.73016357421875, 'train_avg_loss': 0.6952711741129557, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 15:35:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 111, 'Results_raw': {'train_total': 480, 'train_loss': 333.73016357421875, 'train_avg_loss': 0.6952711741129557, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 15:35:18 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #112) -------------
2025-10-09 15:35:18 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=112 aidx=1 | s=5 (candidates=23)
2025-10-09 15:35:18 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[18, 23, 8, 30, 38] (from 23)
2025-10-09 15:35:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:35:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:35:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #112, planning to set LR to 1.00e-05
2025-10-09 15:35:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 15:35:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:35:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:35:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:35:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:35:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:35:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:35:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.571259, avg_loss=0.663690, seen=480, correct=281, accuracy=0.585417
2025-10-09 15:35:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:35:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:36:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:36:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=112 reserved=2360MB allocated=2141MB
2025-10-09 15:36:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 112, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.96985471248627, 'train_avg_loss': 0.6830821226040522, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 15:36:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 112, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.5712585449219, 'train_avg_loss': 0.6636901219685872, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 15:36:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 112, 'Results_raw': {'train_total': 480, 'train_loss': 318.5712585449219, 'train_avg_loss': 0.6636901219685872, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 15:36:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:36:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:36:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #112, planning to set LR to 1.00e-05
2025-10-09 15:36:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-09 15:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:36:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:36:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:36:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:36:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:36:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:36:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.851135, avg_loss=0.687190, seen=480, correct=262, accuracy=0.545833
2025-10-09 15:36:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:36:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:36:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:36:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=112 reserved=2330MB allocated=2141MB
2025-10-09 15:36:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 112, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.61387830972672, 'train_avg_loss': 0.6967823192477226, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 15:36:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 112, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.85113525390625, 'train_avg_loss': 0.6871898651123047, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 15:36:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 112, 'Results_raw': {'train_total': 480, 'train_loss': 329.85113525390625, 'train_avg_loss': 0.6871898651123047, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 15:36:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:36:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:36:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #112, planning to set LR to 1.00e-05
2025-10-09 15:36:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 15:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:36:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:36:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:36:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:36:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:37:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:37:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.073364, avg_loss=0.660570, seen=480, correct=297, accuracy=0.618750
2025-10-09 15:37:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:37:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:37:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:37:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=112 reserved=2354MB allocated=2141MB
2025-10-09 15:37:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 112, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.78431159257889, 'train_avg_loss': 0.6732025966048241, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 15:37:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 112, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.0733642578125, 'train_avg_loss': 0.6605695088704427, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 15:37:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 112, 'Results_raw': {'train_total': 480, 'train_loss': 317.0733642578125, 'train_avg_loss': 0.6605695088704427, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 15:37:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:37:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:37:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #112, planning to set LR to 1.00e-05
2025-10-09 15:37:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 15:37:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:37:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:37:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:37:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:37:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:38:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:38:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.670868, avg_loss=0.657648, seen=480, correct=280, accuracy=0.583333
2025-10-09 15:38:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:38:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:38:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=112 reserved=2330MB allocated=2141MB
2025-10-09 15:38:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 112, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.69110608100891, 'train_avg_loss': 0.6724258840084076, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 15:38:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 112, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.6708679199219, 'train_avg_loss': 0.6576476414998372, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 15:38:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 112, 'Results_raw': {'train_total': 480, 'train_loss': 315.6708679199219, 'train_avg_loss': 0.6576476414998372, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 15:38:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:38:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:38:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #112, planning to set LR to 1.00e-05
2025-10-09 15:38:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 15:38:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:38:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:38:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:38:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:38:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:38:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:38:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.898865, avg_loss=0.666456, seen=480, correct=296, accuracy=0.616667
2025-10-09 15:38:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:38:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:38:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:38:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=112 reserved=2336MB allocated=2141MB
2025-10-09 15:38:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 112, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.85109049081802, 'train_avg_loss': 0.6570924207568168, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 15:38:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 112, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.89886474609375, 'train_avg_loss': 0.6664559682210286, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 15:38:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 112, 'Results_raw': {'train_total': 480, 'train_loss': 319.89886474609375, 'train_avg_loss': 0.6664559682210286, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 15:38:58 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #113) -------------
2025-10-09 15:38:58 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=113 aidx=1 | s=5 (candidates=23)
2025-10-09 15:38:58 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[30, 49, 38, 12, 8] (from 23)
2025-10-09 15:38:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:39:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:39:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #113, planning to set LR to 1.00e-05
2025-10-09 15:39:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 15:39:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:39:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:39:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:39:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:39:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:39:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:39:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.241821, avg_loss=0.658837, seen=480, correct=285, accuracy=0.593750
2025-10-09 15:39:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:39:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:39:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:39:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=113 reserved=2330MB allocated=2141MB
2025-10-09 15:39:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 113, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.81537383794785, 'train_avg_loss': 0.6734614486495654, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 15:39:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 113, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.2418212890625, 'train_avg_loss': 0.6588371276855469, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 15:39:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 113, 'Results_raw': {'train_total': 480, 'train_loss': 316.2418212890625, 'train_avg_loss': 0.6588371276855469, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 15:39:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:39:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:39:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #113, planning to set LR to 1.00e-05
2025-10-09 15:39:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 15:39:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:39:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:39:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:39:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:39:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:40:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:40:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.790741, avg_loss=0.672481, seen=480, correct=275, accuracy=0.572917
2025-10-09 15:40:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:40:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:40:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=113 reserved=2362MB allocated=2141MB
2025-10-09 15:40:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 113, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.99859845638275, 'train_avg_loss': 0.691654987136523, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 15:40:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 113, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.7907409667969, 'train_avg_loss': 0.6724807103474935, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 15:40:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 113, 'Results_raw': {'train_total': 480, 'train_loss': 322.7907409667969, 'train_avg_loss': 0.6724807103474935, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 15:40:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:40:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:40:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #113, planning to set LR to 1.00e-05
2025-10-09 15:40:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 15:40:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:40:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:40:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:40:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:40:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:41:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:41:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.028687, avg_loss=0.654226, seen=480, correct=298, accuracy=0.620833
2025-10-09 15:41:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:41:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:41:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:41:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=113 reserved=2336MB allocated=2141MB
2025-10-09 15:41:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 113, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.15518802404404, 'train_avg_loss': 0.642959900200367, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 15:41:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 113, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.0286865234375, 'train_avg_loss': 0.6542264302571614, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 15:41:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 113, 'Results_raw': {'train_total': 480, 'train_loss': 314.0286865234375, 'train_avg_loss': 0.6542264302571614, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 15:41:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:41:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:41:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #113, planning to set LR to 1.00e-05
2025-10-09 15:41:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 15:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:41:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:41:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:41:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:41:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:41:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:41:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.619629, avg_loss=0.663791, seen=480, correct=288, accuracy=0.600000
2025-10-09 15:41:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:41:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:41:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:41:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=113 reserved=2330MB allocated=2141MB
2025-10-09 15:41:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 113, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.06633526086807, 'train_avg_loss': 0.6838861271739006, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 15:41:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 113, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.61962890625, 'train_avg_loss': 0.6637908935546875, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 15:41:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 113, 'Results_raw': {'train_total': 480, 'train_loss': 318.61962890625, 'train_avg_loss': 0.6637908935546875, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 15:41:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:41:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:41:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #113, planning to set LR to 1.00e-05
2025-10-09 15:41:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 15:41:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:41:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:41:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:41:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:41:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:42:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:42:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.856018, avg_loss=0.662200, seen=480, correct=298, accuracy=0.620833
2025-10-09 15:42:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:42:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:42:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=113 reserved=2356MB allocated=2141MB
2025-10-09 15:42:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 113, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.4591772556305, 'train_avg_loss': 0.6788264771302541, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 15:42:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 113, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.85601806640625, 'train_avg_loss': 0.6622000376383463, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 15:42:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 113, 'Results_raw': {'train_total': 480, 'train_loss': 317.85601806640625, 'train_avg_loss': 0.6622000376383463, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 15:42:36 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #114) -------------
2025-10-09 15:42:37 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=114 aidx=1 | s=5 (candidates=23)
2025-10-09 15:42:37 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[25, 18, 38, 24, 17] (from 23)
2025-10-09 15:42:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:42:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:42:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #114, planning to set LR to 1.00e-05
2025-10-09 15:42:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 15:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:42:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:42:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:42:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:42:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:43:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:43:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.802338, avg_loss=0.693338, seen=480, correct=276, accuracy=0.575000
2025-10-09 15:43:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:43:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:43:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:43:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=114 reserved=2330MB allocated=2141MB
2025-10-09 15:43:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 114, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.52914470434189, 'train_avg_loss': 0.6877428725361824, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 15:43:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 114, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.8023376464844, 'train_avg_loss': 0.6933382034301758, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 15:43:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 114, 'Results_raw': {'train_total': 480, 'train_loss': 332.8023376464844, 'train_avg_loss': 0.6933382034301758, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 15:43:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:43:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:43:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #114, planning to set LR to 1.00e-05
2025-10-09 15:43:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 15:43:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:43:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:43:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:43:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:43:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:44:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:44:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.277527, avg_loss=0.660995, seen=480, correct=281, accuracy=0.585417
2025-10-09 15:44:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:44:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:44:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=114 reserved=2360MB allocated=2141MB
2025-10-09 15:44:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 114, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.67995148897171, 'train_avg_loss': 0.6806662624080976, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 15:44:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 114, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.27752685546875, 'train_avg_loss': 0.6609948476155599, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 15:44:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 114, 'Results_raw': {'train_total': 480, 'train_loss': 317.27752685546875, 'train_avg_loss': 0.6609948476155599, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 15:44:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:44:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:44:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #114, planning to set LR to 1.00e-05
2025-10-09 15:44:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 15:44:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:44:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:44:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:44:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:44:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:44:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:44:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.759613, avg_loss=0.655749, seen=480, correct=302, accuracy=0.629167
2025-10-09 15:44:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:44:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:44:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:44:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=114 reserved=2336MB allocated=2141MB
2025-10-09 15:44:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 114, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.9602460861206, 'train_avg_loss': 0.6413353840510051, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 15:44:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 114, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.7596130371094, 'train_avg_loss': 0.6557491938273112, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 15:44:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 114, 'Results_raw': {'train_total': 480, 'train_loss': 314.7596130371094, 'train_avg_loss': 0.6557491938273112, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 15:44:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:44:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:44:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #114, planning to set LR to 1.00e-05
2025-10-09 15:44:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 15:44:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:44:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:44:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:44:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:44:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:45:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:45:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.228973, avg_loss=0.692144, seen=480, correct=265, accuracy=0.552083
2025-10-09 15:45:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:45:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:45:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:45:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=114 reserved=2330MB allocated=2141MB
2025-10-09 15:45:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 114, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.01523214578629, 'train_avg_loss': 0.7001269345482191, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 15:45:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 114, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.2289733886719, 'train_avg_loss': 0.6921436945597331, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 15:45:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 114, 'Results_raw': {'train_total': 480, 'train_loss': 332.2289733886719, 'train_avg_loss': 0.6921436945597331, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 15:45:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:45:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:45:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #114, planning to set LR to 1.00e-05
2025-10-09 15:45:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 15:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:45:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:45:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:45:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:45:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:46:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:46:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.183960, avg_loss=0.639967, seen=480, correct=300, accuracy=0.625000
2025-10-09 15:46:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:46:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:46:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=114 reserved=2366MB allocated=2141MB
2025-10-09 15:46:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 114, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.46383011341095, 'train_avg_loss': 0.6288652509450913, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 15:46:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 114, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.1839599609375, 'train_avg_loss': 0.6399665832519531, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 15:46:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 114, 'Results_raw': {'train_total': 480, 'train_loss': 307.1839599609375, 'train_avg_loss': 0.6399665832519531, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 15:46:15 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #115) -------------
2025-10-09 15:46:16 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=115 aidx=1 | s=5 (candidates=23)
2025-10-09 15:46:16 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[18, 17, 8, 24, 27] (from 23)
2025-10-09 15:46:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:46:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:46:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #115, planning to set LR to 1.00e-05
2025-10-09 15:46:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 15:46:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:46:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:46:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:46:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:46:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:46:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:46:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.657684, avg_loss=0.655537, seen=480, correct=295, accuracy=0.614583
2025-10-09 15:46:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:46:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:47:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=115 reserved=2360MB allocated=2141MB
2025-10-09 15:47:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 115, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.24682760238647, 'train_avg_loss': 0.677056896686554, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 15:47:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 115, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.6576843261719, 'train_avg_loss': 0.6555368423461914, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 15:47:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 115, 'Results_raw': {'train_total': 480, 'train_loss': 314.6576843261719, 'train_avg_loss': 0.6555368423461914, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 15:47:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:47:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:47:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #115, planning to set LR to 1.00e-05
2025-10-09 15:47:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 15:47:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:47:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:47:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:47:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:47:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:47:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:47:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.002838, avg_loss=0.627089, seen=480, correct=307, accuracy=0.639583
2025-10-09 15:47:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:47:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:47:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:47:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=115 reserved=2366MB allocated=2141MB
2025-10-09 15:47:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 115, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.30188292264938, 'train_avg_loss': 0.6108490243554116, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 15:47:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 115, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.0028381347656, 'train_avg_loss': 0.627089246114095, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 15:47:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 115, 'Results_raw': {'train_total': 480, 'train_loss': 301.0028381347656, 'train_avg_loss': 0.627089246114095, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 15:47:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:47:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:47:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #115, planning to set LR to 1.00e-05
2025-10-09 15:47:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 15:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:47:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:47:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:47:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:47:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:48:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:48:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.997437, avg_loss=0.649995, seen=480, correct=314, accuracy=0.654167
2025-10-09 15:48:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:48:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:48:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:48:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=115 reserved=2354MB allocated=2141MB
2025-10-09 15:48:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 115, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.9516459107399, 'train_avg_loss': 0.6662637159228325, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 15:48:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 115, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.9974365234375, 'train_avg_loss': 0.6499946594238282, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 15:48:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 115, 'Results_raw': {'train_total': 480, 'train_loss': 311.9974365234375, 'train_avg_loss': 0.6499946594238282, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 15:48:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:48:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:48:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #115, planning to set LR to 1.00e-05
2025-10-09 15:48:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 15:48:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:48:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:48:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:48:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:48:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:49:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:49:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.672211, avg_loss=0.693067, seen=480, correct=259, accuracy=0.539583
2025-10-09 15:49:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:49:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:49:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:49:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=115 reserved=2330MB allocated=2141MB
2025-10-09 15:49:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 115, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.21715915203094, 'train_avg_loss': 0.7184763262669246, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 15:49:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 115, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.6722106933594, 'train_avg_loss': 0.6930671056111654, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 15:49:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 115, 'Results_raw': {'train_total': 480, 'train_loss': 332.6722106933594, 'train_avg_loss': 0.6930671056111654, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 15:49:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:49:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:49:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #115, planning to set LR to 1.00e-05
2025-10-09 15:49:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 15:49:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:49:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:49:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:49:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:49:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:49:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:49:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.770508, avg_loss=0.674522, seen=480, correct=287, accuracy=0.597917
2025-10-09 15:49:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:49:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:49:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:49:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=115 reserved=2332MB allocated=2141MB
2025-10-09 15:49:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 115, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.29434311389923, 'train_avg_loss': 0.685786192615827, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 15:49:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 115, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.7705078125, 'train_avg_loss': 0.6745218912760417, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 15:49:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 115, 'Results_raw': {'train_total': 480, 'train_loss': 323.7705078125, 'train_avg_loss': 0.6745218912760417, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 15:49:57 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #116) -------------
2025-10-09 15:49:57 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=116 aidx=1 | s=5 (candidates=23)
2025-10-09 15:49:57 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[44, 8, 4, 7, 14] (from 23)
2025-10-09 15:49:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:49:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:49:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #116, planning to set LR to 1.00e-05
2025-10-09 15:49:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 15:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:49:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:49:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:49:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:49:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:50:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:50:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.428955, avg_loss=0.655060, seen=480, correct=301, accuracy=0.627083
2025-10-09 15:50:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:50:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:50:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:50:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=116 reserved=2330MB allocated=2141MB
2025-10-09 15:50:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 116, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.79339933395386, 'train_avg_loss': 0.6399449944496155, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 15:50:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 116, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.428955078125, 'train_avg_loss': 0.6550603230794271, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 15:50:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 116, 'Results_raw': {'train_total': 480, 'train_loss': 314.428955078125, 'train_avg_loss': 0.6550603230794271, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 15:50:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:50:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:50:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #116, planning to set LR to 1.00e-05
2025-10-09 15:50:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 15:50:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:50:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:50:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:50:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:50:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:51:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:51:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.707428, avg_loss=0.649390, seen=480, correct=310, accuracy=0.645833
2025-10-09 15:51:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:51:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:51:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=116 reserved=2354MB allocated=2141MB
2025-10-09 15:51:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 116, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.8431841135025, 'train_avg_loss': 0.6653598676125209, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 15:51:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 116, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.7074279785156, 'train_avg_loss': 0.6493904749552409, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 15:51:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 116, 'Results_raw': {'train_total': 480, 'train_loss': 311.7074279785156, 'train_avg_loss': 0.6493904749552409, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 15:51:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:51:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:51:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #116, planning to set LR to 1.00e-05
2025-10-09 15:51:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 15:51:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:51:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:51:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:51:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:51:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:52:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:52:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=285.751587, avg_loss=0.595316, seen=480, correct=337, accuracy=0.702083
2025-10-09 15:52:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:52:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:52:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:52:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=116 reserved=2372MB allocated=2141MB
2025-10-09 15:52:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 116, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.16735899448395, 'train_avg_loss': 0.6430613249540329, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 15:52:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 116, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 285.7515869140625, 'train_avg_loss': 0.5953158060709636, 'train_seen': 480, 'train_correct': 337, 'train_acc': 0.7020833333333333}}
2025-10-09 15:52:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 116, 'Results_raw': {'train_total': 480, 'train_loss': 285.7515869140625, 'train_avg_loss': 0.5953158060709636, 'train_seen': 480, 'train_correct': 337, 'train_acc': 0.7020833333333333}}
2025-10-09 15:52:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:52:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:52:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #116, planning to set LR to 1.00e-05
2025-10-09 15:52:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 15:52:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:52:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:52:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:52:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:52:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:52:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:52:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.091766, avg_loss=0.639775, seen=480, correct=311, accuracy=0.647917
2025-10-09 15:52:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:52:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:52:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=116 reserved=2340MB allocated=2141MB
2025-10-09 15:52:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 116, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.36544013023376, 'train_avg_loss': 0.6780453344186147, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 15:52:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 116, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.0917663574219, 'train_avg_loss': 0.6397745132446289, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 15:52:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 116, 'Results_raw': {'train_total': 480, 'train_loss': 307.0917663574219, 'train_avg_loss': 0.6397745132446289, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 15:52:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:52:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:52:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #116, planning to set LR to 1.00e-05
2025-10-09 15:52:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 15:52:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:52:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:52:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:52:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:52:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:53:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:53:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.812439, avg_loss=0.630859, seen=480, correct=307, accuracy=0.639583
2025-10-09 15:53:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:53:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:53:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:53:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=116 reserved=2330MB allocated=2141MB
2025-10-09 15:53:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 116, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.39908257126808, 'train_avg_loss': 0.586659021427234, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 15:53:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 116, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.81243896484375, 'train_avg_loss': 0.6308592478434245, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 15:53:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 116, 'Results_raw': {'train_total': 480, 'train_loss': 302.81243896484375, 'train_avg_loss': 0.6308592478434245, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 15:53:38 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #117) -------------
2025-10-09 15:53:38 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=117 aidx=1 | s=5 (candidates=23)
2025-10-09 15:53:38 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[11, 33, 35, 38, 8] (from 23)
2025-10-09 15:53:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:53:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:53:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #117, planning to set LR to 1.00e-05
2025-10-09 15:53:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 15:53:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:53:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:53:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:53:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:53:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:54:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:54:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.389679, avg_loss=0.654978, seen=480, correct=286, accuracy=0.595833
2025-10-09 15:54:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:54:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:54:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:54:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=117 reserved=2418MB allocated=2141MB
2025-10-09 15:54:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 117, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.96985340118408, 'train_avg_loss': 0.683082111676534, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 15:54:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 117, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.3896789550781, 'train_avg_loss': 0.6549784978230794, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 15:54:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 117, 'Results_raw': {'train_total': 480, 'train_loss': 314.3896789550781, 'train_avg_loss': 0.6549784978230794, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 15:54:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:54:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:54:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #117, planning to set LR to 1.00e-05
2025-10-09 15:54:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 15:54:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:54:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:54:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:54:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:54:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:55:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:55:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.464874, avg_loss=0.684302, seen=480, correct=277, accuracy=0.577083
2025-10-09 15:55:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:55:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:55:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=117 reserved=2342MB allocated=2141MB
2025-10-09 15:55:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 117, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.6801535487175, 'train_avg_loss': 0.6806679462393125, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 15:55:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 117, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.4648742675781, 'train_avg_loss': 0.6843018213907878, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 15:55:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 117, 'Results_raw': {'train_total': 480, 'train_loss': 328.4648742675781, 'train_avg_loss': 0.6843018213907878, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 15:55:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:55:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:55:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #117, planning to set LR to 1.00e-05
2025-10-09 15:55:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 15:55:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:55:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:55:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:55:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:55:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:55:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:55:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.700409, avg_loss=0.678543, seen=480, correct=288, accuracy=0.600000
2025-10-09 15:55:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:55:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:55:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:55:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=117 reserved=2402MB allocated=2141MB
2025-10-09 15:55:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 117, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.39881491661072, 'train_avg_loss': 0.6783234576384226, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 15:55:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 117, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.7004089355469, 'train_avg_loss': 0.6785425186157227, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 15:55:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 117, 'Results_raw': {'train_total': 480, 'train_loss': 325.7004089355469, 'train_avg_loss': 0.6785425186157227, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 15:55:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:55:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:55:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #117, planning to set LR to 1.00e-05
2025-10-09 15:55:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 15:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:55:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:55:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:55:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:55:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:56:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:56:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.768097, avg_loss=0.655767, seen=480, correct=302, accuracy=0.629167
2025-10-09 15:56:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:56:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:56:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=117 reserved=2336MB allocated=2141MB
2025-10-09 15:56:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 117, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.54374176263809, 'train_avg_loss': 0.6461978480219841, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 15:56:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 117, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.7680969238281, 'train_avg_loss': 0.6557668685913086, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 15:56:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 117, 'Results_raw': {'train_total': 480, 'train_loss': 314.7680969238281, 'train_avg_loss': 0.6557668685913086, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 15:56:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:56:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:56:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #117, planning to set LR to 1.00e-05
2025-10-09 15:56:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 15:56:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:56:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:56:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:56:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:56:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:57:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:57:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.016357, avg_loss=0.654201, seen=480, correct=300, accuracy=0.625000
2025-10-09 15:57:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:57:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:57:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:57:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=117 reserved=2354MB allocated=2141MB
2025-10-09 15:57:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 117, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.37573570013046, 'train_avg_loss': 0.6697977975010871, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 15:57:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 117, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.016357421875, 'train_avg_loss': 0.6542007446289062, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 15:57:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 117, 'Results_raw': {'train_total': 480, 'train_loss': 314.016357421875, 'train_avg_loss': 0.6542007446289062, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 15:57:24 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #118) -------------
2025-10-09 15:57:24 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=118 aidx=1 | s=5 (candidates=23)
2025-10-09 15:57:24 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[14, 30, 25, 44, 17] (from 23)
2025-10-09 15:57:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:57:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:57:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #118, planning to set LR to 1.00e-05
2025-10-09 15:57:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 15:57:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:57:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:57:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:57:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:57:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:58:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:58:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=296.109070, avg_loss=0.616894, seen=480, correct=314, accuracy=0.654167
2025-10-09 15:58:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:58:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:58:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:58:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=118 reserved=2330MB allocated=2141MB
2025-10-09 15:58:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 118, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.89351096749306, 'train_avg_loss': 0.5741125913957755, 'train_seen': 120, 'train_correct': 87, 'train_acc': 0.725}}
2025-10-09 15:58:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 118, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 296.10906982421875, 'train_avg_loss': 0.6168938954671224, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 15:58:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 118, 'Results_raw': {'train_total': 480, 'train_loss': 296.10906982421875, 'train_avg_loss': 0.6168938954671224, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 15:58:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:58:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:58:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #118, planning to set LR to 1.00e-05
2025-10-09 15:58:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 15:58:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:58:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:58:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:58:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:58:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:58:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:58:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.321472, avg_loss=0.656920, seen=480, correct=282, accuracy=0.587500
2025-10-09 15:58:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:58:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:58:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:58:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=118 reserved=2330MB allocated=2141MB
2025-10-09 15:58:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 118, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.18486893177032, 'train_avg_loss': 0.668207241098086, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 15:58:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 118, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.32147216796875, 'train_avg_loss': 0.6569197336832683, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 15:58:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 118, 'Results_raw': {'train_total': 480, 'train_loss': 315.32147216796875, 'train_avg_loss': 0.6569197336832683, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 15:58:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:58:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:58:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #118, planning to set LR to 1.00e-05
2025-10-09 15:58:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 15:58:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:58:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:58:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:58:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:58:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 15:59:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 15:59:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.764526, avg_loss=0.691176, seen=480, correct=281, accuracy=0.585417
2025-10-09 15:59:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 15:59:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:59:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 15:59:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=118 reserved=2330MB allocated=2141MB
2025-10-09 15:59:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 118, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.3581730723381, 'train_avg_loss': 0.6863181089361509, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 15:59:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 118, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.7645263671875, 'train_avg_loss': 0.6911760965983073, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 15:59:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 118, 'Results_raw': {'train_total': 480, 'train_loss': 331.7645263671875, 'train_avg_loss': 0.6911760965983073, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 15:59:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 15:59:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 15:59:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #118, planning to set LR to 1.00e-05
2025-10-09 15:59:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 15:59:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 15:59:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 15:59:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 15:59:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 15:59:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:00:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:00:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.560242, avg_loss=0.649084, seen=480, correct=301, accuracy=0.627083
2025-10-09 16:00:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:00:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:00:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:00:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=118 reserved=2330MB allocated=2141MB
2025-10-09 16:00:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 118, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.88235867023468, 'train_avg_loss': 0.6406863222519557, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 16:00:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 118, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.56024169921875, 'train_avg_loss': 0.6490838368733723, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 16:00:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 118, 'Results_raw': {'train_total': 480, 'train_loss': 311.56024169921875, 'train_avg_loss': 0.6490838368733723, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 16:00:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:00:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:00:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #118, planning to set LR to 1.00e-05
2025-10-09 16:00:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 16:00:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:00:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:00:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:00:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:00:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:00:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:00:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.018738, avg_loss=0.627122, seen=480, correct=302, accuracy=0.629167
2025-10-09 16:00:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:00:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:00:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=118 reserved=2366MB allocated=2141MB
2025-10-09 16:00:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 118, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.83825099468231, 'train_avg_loss': 0.6153187582890193, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 16:00:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 118, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.01873779296875, 'train_avg_loss': 0.6271223704020182, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 16:00:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 118, 'Results_raw': {'train_total': 480, 'train_loss': 301.01873779296875, 'train_avg_loss': 0.6271223704020182, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 16:00:59 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #119) -------------
2025-10-09 16:00:59 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=119 aidx=1 | s=5 (candidates=23)
2025-10-09 16:00:59 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[52, 17, 53, 27, 7] (from 23)
2025-10-09 16:01:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:01:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:01:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #119, planning to set LR to 1.00e-05
2025-10-09 16:01:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 16:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:01:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:01:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:01:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:01:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:01:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:01:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.005554, avg_loss=0.645845, seen=480, correct=299, accuracy=0.622917
2025-10-09 16:01:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:01:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:01:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:01:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=119 reserved=2338MB allocated=2141MB
2025-10-09 16:01:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 119, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.50111997127533, 'train_avg_loss': 0.6291759997606278, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 16:01:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 119, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.00555419921875, 'train_avg_loss': 0.6458449045817057, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 16:01:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 119, 'Results_raw': {'train_total': 480, 'train_loss': 310.00555419921875, 'train_avg_loss': 0.6458449045817057, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 16:01:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:01:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:01:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #119, planning to set LR to 1.00e-05
2025-10-09 16:01:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 16:01:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:01:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:01:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:01:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:01:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:02:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:02:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=292.014496, avg_loss=0.608364, seen=480, correct=320, accuracy=0.666667
2025-10-09 16:02:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:02:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:02:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=119 reserved=2366MB allocated=2141MB
2025-10-09 16:02:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 119, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.8064820766449, 'train_avg_loss': 0.5900540173053741, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 16:02:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 119, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 292.0144958496094, 'train_avg_loss': 0.6083635330200196, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 16:02:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 119, 'Results_raw': {'train_total': 480, 'train_loss': 292.0144958496094, 'train_avg_loss': 0.6083635330200196, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 16:02:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:02:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:02:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #119, planning to set LR to 1.00e-05
2025-10-09 16:02:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 16:02:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:02:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:02:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:02:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:02:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:03:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:03:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.660400, avg_loss=0.684709, seen=480, correct=291, accuracy=0.606250
2025-10-09 16:03:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:03:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:03:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:03:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=119 reserved=2332MB allocated=2141MB
2025-10-09 16:03:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 119, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.61780333518982, 'train_avg_loss': 0.6218150277932485, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 16:03:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 119, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.660400390625, 'train_avg_loss': 0.6847091674804687, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 16:03:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 119, 'Results_raw': {'train_total': 480, 'train_loss': 328.660400390625, 'train_avg_loss': 0.6847091674804687, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 16:03:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:03:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:03:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #119, planning to set LR to 1.00e-05
2025-10-09 16:03:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 16:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:03:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:03:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:03:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:03:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:03:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:03:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.432953, avg_loss=0.661319, seen=480, correct=298, accuracy=0.620833
2025-10-09 16:03:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:03:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:03:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:03:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=119 reserved=2332MB allocated=2141MB
2025-10-09 16:03:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 119, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.56064438819885, 'train_avg_loss': 0.6796720365683238, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 16:03:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 119, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.4329528808594, 'train_avg_loss': 0.6613186518351237, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 16:03:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 119, 'Results_raw': {'train_total': 480, 'train_loss': 317.4329528808594, 'train_avg_loss': 0.6613186518351237, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 16:03:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:03:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:03:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #119, planning to set LR to 1.00e-05
2025-10-09 16:03:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 16:03:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:03:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:03:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:03:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:03:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:04:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:04:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.266113, avg_loss=0.640138, seen=480, correct=302, accuracy=0.629167
2025-10-09 16:04:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:04:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:04:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:04:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=119 reserved=2340MB allocated=2141MB
2025-10-09 16:04:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 119, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.06510835886002, 'train_avg_loss': 0.6922092363238335, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 16:04:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 119, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.26611328125, 'train_avg_loss': 0.6401377360026042, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 16:04:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 119, 'Results_raw': {'train_total': 480, 'train_loss': 307.26611328125, 'train_avg_loss': 0.6401377360026042, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 16:04:37 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #120) -------------
2025-10-09 16:04:37 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=120 aidx=1 | s=5 (candidates=23)
2025-10-09 16:04:37 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[8, 12, 14, 11, 25] (from 23)
2025-10-09 16:04:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:04:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:04:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #120, planning to set LR to 1.00e-05
2025-10-09 16:04:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 16:04:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:04:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:04:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:04:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:04:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:05:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:05:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.526276, avg_loss=0.632346, seen=480, correct=315, accuracy=0.656250
2025-10-09 16:05:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:05:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:05:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:05:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=120 reserved=2354MB allocated=2141MB
2025-10-09 16:05:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 120, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.20241463184357, 'train_avg_loss': 0.643353455265363, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 16:05:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 120, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.5262756347656, 'train_avg_loss': 0.6323464075724284, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 16:05:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 120, 'Results_raw': {'train_total': 480, 'train_loss': 303.5262756347656, 'train_avg_loss': 0.6323464075724284, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 16:05:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:05:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:05:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #120, planning to set LR to 1.00e-05
2025-10-09 16:05:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 16:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:05:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:05:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:05:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:05:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:06:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:06:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.044678, avg_loss=0.654260, seen=480, correct=295, accuracy=0.614583
2025-10-09 16:06:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:06:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:06:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=120 reserved=2330MB allocated=2141MB
2025-10-09 16:06:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 120, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.6470051407814, 'train_avg_loss': 0.6803917095065117, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 16:06:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 120, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.044677734375, 'train_avg_loss': 0.6542597452799479, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 16:06:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 120, 'Results_raw': {'train_total': 480, 'train_loss': 314.044677734375, 'train_avg_loss': 0.6542597452799479, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 16:06:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:06:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:06:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #120, planning to set LR to 1.00e-05
2025-10-09 16:06:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 16:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:06:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:06:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:06:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:06:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:06:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:06:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.505280, avg_loss=0.611469, seen=480, correct=315, accuracy=0.656250
2025-10-09 16:06:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:06:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:06:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=120 reserved=2330MB allocated=2141MB
2025-10-09 16:06:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 120, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 67.67305663228035, 'train_avg_loss': 0.5639421386023362, 'train_seen': 120, 'train_correct': 86, 'train_acc': 0.7166666666666667}}
2025-10-09 16:06:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 120, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.5052795410156, 'train_avg_loss': 0.6114693323771159, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 16:06:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 120, 'Results_raw': {'train_total': 480, 'train_loss': 293.5052795410156, 'train_avg_loss': 0.6114693323771159, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 16:06:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:06:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:06:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #120, planning to set LR to 1.00e-05
2025-10-09 16:06:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 16:06:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:06:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:06:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:06:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:06:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:07:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:07:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.007263, avg_loss=0.643765, seen=480, correct=298, accuracy=0.620833
2025-10-09 16:07:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:07:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:07:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=120 reserved=2418MB allocated=2141MB
2025-10-09 16:07:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 120, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.19401079416275, 'train_avg_loss': 0.6682834232846896, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 16:07:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 120, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.00726318359375, 'train_avg_loss': 0.6437651316324869, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 16:07:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 120, 'Results_raw': {'train_total': 480, 'train_loss': 309.00726318359375, 'train_avg_loss': 0.6437651316324869, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 16:07:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:07:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:07:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #120, planning to set LR to 1.00e-05
2025-10-09 16:07:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 16:07:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:07:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:07:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:07:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:07:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:08:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:08:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.418091, avg_loss=0.692538, seen=480, correct=281, accuracy=0.585417
2025-10-09 16:08:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:08:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:08:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=120 reserved=2330MB allocated=2141MB
2025-10-09 16:08:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 120, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.53872519731522, 'train_avg_loss': 0.6961560433109601, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 16:08:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 120, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.4180908203125, 'train_avg_loss': 0.6925376892089844, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 16:08:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 120, 'Results_raw': {'train_total': 480, 'train_loss': 332.4180908203125, 'train_avg_loss': 0.6925376892089844, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 16:08:17 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #121) -------------
2025-10-09 16:08:18 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=121 aidx=1 | s=5 (candidates=23)
2025-10-09 16:08:18 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[23, 8, 35, 7, 9] (from 23)
2025-10-09 16:08:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:08:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:08:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #121, planning to set LR to 1.00e-05
2025-10-09 16:08:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-09 16:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:08:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:08:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:08:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:08:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:08:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:08:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.975555, avg_loss=0.687449, seen=480, correct=264, accuracy=0.550000
2025-10-09 16:08:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:09:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:09:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=121 reserved=2330MB allocated=2141MB
2025-10-09 16:09:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 121, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.67190736532211, 'train_avg_loss': 0.7139325613776842, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 16:09:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 121, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.9755554199219, 'train_avg_loss': 0.6874490737915039, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 16:09:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 121, 'Results_raw': {'train_total': 480, 'train_loss': 329.9755554199219, 'train_avg_loss': 0.6874490737915039, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 16:09:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:09:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:09:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #121, planning to set LR to 1.00e-05
2025-10-09 16:09:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 16:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:09:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:09:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:09:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:09:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:09:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:09:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.908630, avg_loss=0.637310, seen=480, correct=309, accuracy=0.643750
2025-10-09 16:09:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:09:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:09:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=121 reserved=2354MB allocated=2141MB
2025-10-09 16:09:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 121, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.25677955150604, 'train_avg_loss': 0.6521398295958837, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 16:09:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 121, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.90863037109375, 'train_avg_loss': 0.6373096466064453, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 16:09:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 121, 'Results_raw': {'train_total': 480, 'train_loss': 305.90863037109375, 'train_avg_loss': 0.6373096466064453, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 16:09:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:09:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:09:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #121, planning to set LR to 1.00e-05
2025-10-09 16:09:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 16:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:09:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:09:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:09:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:09:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:10:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:10:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.555206, avg_loss=0.676157, seen=480, correct=290, accuracy=0.604167
2025-10-09 16:10:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:10:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:10:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=121 reserved=2402MB allocated=2141MB
2025-10-09 16:10:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 121, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.35868173837662, 'train_avg_loss': 0.6946556811531385, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 16:10:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 121, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.5552062988281, 'train_avg_loss': 0.6761566797892252, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 16:10:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 121, 'Results_raw': {'train_total': 480, 'train_loss': 324.5552062988281, 'train_avg_loss': 0.6761566797892252, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 16:10:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:10:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:10:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #121, planning to set LR to 1.00e-05
2025-10-09 16:10:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 16:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:10:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:10:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:10:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:10:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:11:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:11:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.042908, avg_loss=0.633423, seen=480, correct=304, accuracy=0.633333
2025-10-09 16:11:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:11:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:11:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=121 reserved=2340MB allocated=2141MB
2025-10-09 16:11:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 121, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.29579961299896, 'train_avg_loss': 0.6774649967749914, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 16:11:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 121, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.04290771484375, 'train_avg_loss': 0.6334227244059245, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 16:11:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 121, 'Results_raw': {'train_total': 480, 'train_loss': 304.04290771484375, 'train_avg_loss': 0.6334227244059245, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 16:11:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:11:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:11:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #121, planning to set LR to 1.00e-05
2025-10-09 16:11:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 16:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:11:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:11:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:11:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:11:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:11:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:11:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.780182, avg_loss=0.664125, seen=480, correct=290, accuracy=0.604167
2025-10-09 16:11:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:11:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:11:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=121 reserved=2352MB allocated=2141MB
2025-10-09 16:11:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 121, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.83975201845169, 'train_avg_loss': 0.6736646001537641, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 16:11:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 121, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.7801818847656, 'train_avg_loss': 0.6641253789265951, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 16:11:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 121, 'Results_raw': {'train_total': 480, 'train_loss': 318.7801818847656, 'train_avg_loss': 0.6641253789265951, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 16:11:59 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #122) -------------
2025-10-09 16:11:59 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=122 aidx=1 | s=5 (candidates=23)
2025-10-09 16:11:59 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[7, 2, 11, 18, 53] (from 23)
2025-10-09 16:12:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:12:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:12:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #122, planning to set LR to 1.00e-05
2025-10-09 16:12:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 16:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:12:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:12:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:12:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:12:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:12:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:12:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.196899, avg_loss=0.629577, seen=480, correct=320, accuracy=0.666667
2025-10-09 16:12:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:12:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:12:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:12:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=122 reserved=2340MB allocated=2141MB
2025-10-09 16:12:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 122, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.77740913629532, 'train_avg_loss': 0.6731450761357943, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 16:12:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 122, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.1968994140625, 'train_avg_loss': 0.6295768737792968, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 16:12:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 122, 'Results_raw': {'train_total': 480, 'train_loss': 302.1968994140625, 'train_avg_loss': 0.6295768737792968, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 16:12:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:12:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:12:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #122, planning to set LR to 1.00e-05
2025-10-09 16:12:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 16:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:12:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:12:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:12:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:12:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:13:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:13:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=290.403198, avg_loss=0.605007, seen=480, correct=325, accuracy=0.677083
2025-10-09 16:13:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:13:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:13:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:13:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=122 reserved=2402MB allocated=2141MB
2025-10-09 16:13:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 122, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 64.98278456926346, 'train_avg_loss': 0.5415232047438622, 'train_seen': 120, 'train_correct': 91, 'train_acc': 0.7583333333333333}}
2025-10-09 16:13:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 122, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 290.4031982421875, 'train_avg_loss': 0.6050066630045573, 'train_seen': 480, 'train_correct': 325, 'train_acc': 0.6770833333333334}}
2025-10-09 16:13:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 122, 'Results_raw': {'train_total': 480, 'train_loss': 290.4031982421875, 'train_avg_loss': 0.6050066630045573, 'train_seen': 480, 'train_correct': 325, 'train_acc': 0.6770833333333334}}
2025-10-09 16:13:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:13:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:13:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #122, planning to set LR to 1.00e-05
2025-10-09 16:13:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 16:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:13:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:13:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:13:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:13:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:14:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:14:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.255676, avg_loss=0.646366, seen=480, correct=287, accuracy=0.597917
2025-10-09 16:14:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:14:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:14:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:14:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=122 reserved=2418MB allocated=2141MB
2025-10-09 16:14:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 122, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.72699749469757, 'train_avg_loss': 0.6643916457891464, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 16:14:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 122, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.25567626953125, 'train_avg_loss': 0.6463659922281901, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 16:14:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 122, 'Results_raw': {'train_total': 480, 'train_loss': 310.25567626953125, 'train_avg_loss': 0.6463659922281901, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 16:14:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:14:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:14:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #122, planning to set LR to 1.00e-05
2025-10-09 16:14:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 16:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:14:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:14:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:14:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:14:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:14:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:14:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.345032, avg_loss=0.654885, seen=480, correct=286, accuracy=0.595833
2025-10-09 16:14:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:14:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:14:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=122 reserved=2360MB allocated=2141MB
2025-10-09 16:14:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 122, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.20142531394958, 'train_avg_loss': 0.6850118776162465, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 16:14:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 122, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.34503173828125, 'train_avg_loss': 0.6548854827880859, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 16:14:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 122, 'Results_raw': {'train_total': 480, 'train_loss': 314.34503173828125, 'train_avg_loss': 0.6548854827880859, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 16:14:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:14:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:14:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #122, planning to set LR to 1.00e-05
2025-10-09 16:14:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 16:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:14:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:14:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:14:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:14:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:15:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:15:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.983704, avg_loss=0.679133, seen=480, correct=297, accuracy=0.618750
2025-10-09 16:15:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:15:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:15:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:15:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=122 reserved=2332MB allocated=2141MB
2025-10-09 16:15:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 122, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.31063574552536, 'train_avg_loss': 0.6109219645460446, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 16:15:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 122, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.98370361328125, 'train_avg_loss': 0.6791327158610027, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 16:15:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 122, 'Results_raw': {'train_total': 480, 'train_loss': 325.98370361328125, 'train_avg_loss': 0.6791327158610027, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 16:15:30 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #123) -------------
2025-10-09 16:15:31 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=123 aidx=1 | s=5 (candidates=23)
2025-10-09 16:15:31 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[52, 9, 11, 38, 8] (from 23)
2025-10-09 16:15:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:15:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:15:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #123, planning to set LR to 1.00e-05
2025-10-09 16:15:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 16:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:15:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:15:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:15:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:15:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:16:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:16:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.217804, avg_loss=0.648370, seen=480, correct=297, accuracy=0.618750
2025-10-09 16:16:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:16:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:16:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=123 reserved=2338MB allocated=2141MB
2025-10-09 16:16:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 123, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.16888165473938, 'train_avg_loss': 0.6347406804561615, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 16:16:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 123, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.2178039550781, 'train_avg_loss': 0.6483704249064127, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 16:16:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 123, 'Results_raw': {'train_total': 480, 'train_loss': 311.2178039550781, 'train_avg_loss': 0.6483704249064127, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 16:16:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:16:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:16:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #123, planning to set LR to 1.00e-05
2025-10-09 16:16:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 16:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:16:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:16:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:16:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:16:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:16:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:16:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.285034, avg_loss=0.650594, seen=480, correct=300, accuracy=0.625000
2025-10-09 16:16:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:16:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:16:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:16:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=123 reserved=2350MB allocated=2141MB
2025-10-09 16:16:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 123, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.9184465110302, 'train_avg_loss': 0.665987054258585, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 16:16:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 123, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.2850341796875, 'train_avg_loss': 0.6505938212076823, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 16:16:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 123, 'Results_raw': {'train_total': 480, 'train_loss': 312.2850341796875, 'train_avg_loss': 0.6505938212076823, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 16:16:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:17:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:17:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #123, planning to set LR to 1.00e-05
2025-10-09 16:17:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 16:17:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:17:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:17:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:17:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:17:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:17:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:17:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.051819, avg_loss=0.643858, seen=480, correct=296, accuracy=0.616667
2025-10-09 16:17:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:17:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:17:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=123 reserved=2418MB allocated=2141MB
2025-10-09 16:17:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 123, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.6382966041565, 'train_avg_loss': 0.6553191383679707, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 16:17:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 123, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.05181884765625, 'train_avg_loss': 0.6438579559326172, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 16:17:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 123, 'Results_raw': {'train_total': 480, 'train_loss': 309.05181884765625, 'train_avg_loss': 0.6438579559326172, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 16:17:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:17:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:17:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #123, planning to set LR to 1.00e-05
2025-10-09 16:17:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 16:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:17:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:17:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:17:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:17:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:18:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:18:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.772400, avg_loss=0.649526, seen=480, correct=298, accuracy=0.620833
2025-10-09 16:18:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:18:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:18:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:18:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=123 reserved=2336MB allocated=2141MB
2025-10-09 16:18:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 123, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.09221732616425, 'train_avg_loss': 0.6341018110513688, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 16:18:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 123, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.77239990234375, 'train_avg_loss': 0.6495258331298828, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 16:18:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 123, 'Results_raw': {'train_total': 480, 'train_loss': 311.77239990234375, 'train_avg_loss': 0.6495258331298828, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 16:18:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:18:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:18:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #123, planning to set LR to 1.00e-05
2025-10-09 16:18:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 16:18:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:18:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:18:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:18:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:18:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:19:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:19:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.749237, avg_loss=0.636978, seen=480, correct=316, accuracy=0.658333
2025-10-09 16:19:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:19:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:19:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:19:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=123 reserved=2354MB allocated=2141MB
2025-10-09 16:19:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 123, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.92315870523453, 'train_avg_loss': 0.6493596558769544, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 16:19:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 123, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.7492370605469, 'train_avg_loss': 0.6369775772094727, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 16:19:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 123, 'Results_raw': {'train_total': 480, 'train_loss': 305.7492370605469, 'train_avg_loss': 0.6369775772094727, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 16:19:08 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #124) -------------
2025-10-09 16:19:09 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=124 aidx=1 | s=5 (candidates=23)
2025-10-09 16:19:09 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[52, 49, 38, 9, 24] (from 23)
2025-10-09 16:19:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:19:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:19:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #124, planning to set LR to 1.00e-05
2025-10-09 16:19:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 16:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:19:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:19:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:19:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:19:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:19:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:19:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.122620, avg_loss=0.644005, seen=480, correct=296, accuracy=0.616667
2025-10-09 16:19:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:19:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:19:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:19:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=124 reserved=2338MB allocated=2141MB
2025-10-09 16:19:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 124, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.54884272813797, 'train_avg_loss': 0.6295736894011498, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 16:19:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 124, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.12261962890625, 'train_avg_loss': 0.6440054575602213, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 16:19:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 124, 'Results_raw': {'train_total': 480, 'train_loss': 309.12261962890625, 'train_avg_loss': 0.6440054575602213, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 16:19:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:19:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:19:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #124, planning to set LR to 1.00e-05
2025-10-09 16:19:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 16:19:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:19:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:19:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:19:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:19:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:20:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:20:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.291901, avg_loss=0.667275, seen=480, correct=285, accuracy=0.593750
2025-10-09 16:20:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:20:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:20:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:20:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=124 reserved=2362MB allocated=2141MB
2025-10-09 16:20:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 124, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.0458659529686, 'train_avg_loss': 0.692048882941405, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 16:20:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 124, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.2919006347656, 'train_avg_loss': 0.6672747929890951, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 16:20:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 124, 'Results_raw': {'train_total': 480, 'train_loss': 320.2919006347656, 'train_avg_loss': 0.6672747929890951, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 16:20:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:20:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:20:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #124, planning to set LR to 1.00e-05
2025-10-09 16:20:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 16:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:20:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:20:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:20:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:20:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:21:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:21:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.864929, avg_loss=0.649719, seen=480, correct=302, accuracy=0.629167
2025-10-09 16:21:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:21:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:21:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:21:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=124 reserved=2336MB allocated=2141MB
2025-10-09 16:21:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 124, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.00169062614441, 'train_avg_loss': 0.6333474218845367, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 16:21:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 124, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.86492919921875, 'train_avg_loss': 0.6497186024983724, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 16:21:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 124, 'Results_raw': {'train_total': 480, 'train_loss': 311.86492919921875, 'train_avg_loss': 0.6497186024983724, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 16:21:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:21:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:21:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #124, planning to set LR to 1.00e-05
2025-10-09 16:21:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 16:21:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:21:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:21:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:21:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:21:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:22:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:22:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.599609, avg_loss=0.649166, seen=480, correct=296, accuracy=0.616667
2025-10-09 16:22:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:22:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:22:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=124 reserved=2352MB allocated=2141MB
2025-10-09 16:22:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 124, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.05899268388748, 'train_avg_loss': 0.667158272365729, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 16:22:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 124, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.599609375, 'train_avg_loss': 0.6491658528645833, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 16:22:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 124, 'Results_raw': {'train_total': 480, 'train_loss': 311.599609375, 'train_avg_loss': 0.6491658528645833, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 16:22:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:22:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:22:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #124, planning to set LR to 1.00e-05
2025-10-09 16:22:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 16:22:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:22:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:22:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:22:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:22:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:22:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.419617, avg_loss=0.688374, seen=480, correct=261, accuracy=0.543750
2025-10-09 16:22:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:22:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:22:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=124 reserved=2330MB allocated=2141MB
2025-10-09 16:22:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 124, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.09817498922348, 'train_avg_loss': 0.709151458243529, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 16:22:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 124, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.41961669921875, 'train_avg_loss': 0.6883742014567057, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 16:22:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 124, 'Results_raw': {'train_total': 480, 'train_loss': 330.41961669921875, 'train_avg_loss': 0.6883742014567057, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 16:22:45 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #125) -------------
2025-10-09 16:22:45 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=125 aidx=1 | s=5 (candidates=23)
2025-10-09 16:22:45 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[42, 17, 53, 7, 2] (from 23)
2025-10-09 16:22:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:22:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:22:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #125, planning to set LR to 1.00e-05
2025-10-09 16:22:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 16:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:22:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:22:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:22:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:22:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:23:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:23:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.286652, avg_loss=0.642264, seen=480, correct=304, accuracy=0.633333
2025-10-09 16:23:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:23:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:23:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:23:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=125 reserved=2340MB allocated=2141MB
2025-10-09 16:23:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 125, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.83754724264145, 'train_avg_loss': 0.6403128936886787, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 16:23:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 125, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.2866516113281, 'train_avg_loss': 0.6422638575236003, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 16:23:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 125, 'Results_raw': {'train_total': 480, 'train_loss': 308.2866516113281, 'train_avg_loss': 0.6422638575236003, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 16:23:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:23:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:23:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #125, planning to set LR to 1.00e-05
2025-10-09 16:23:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 16:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:23:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:23:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:23:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:23:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:24:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:24:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=296.595581, avg_loss=0.617907, seen=480, correct=320, accuracy=0.666667
2025-10-09 16:24:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:24:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:24:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=125 reserved=2366MB allocated=2141MB
2025-10-09 16:24:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 125, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.13305646181107, 'train_avg_loss': 0.6011088038484256, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 16:24:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 125, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 296.5955810546875, 'train_avg_loss': 0.617907460530599, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 16:24:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 125, 'Results_raw': {'train_total': 480, 'train_loss': 296.5955810546875, 'train_avg_loss': 0.617907460530599, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 16:24:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:24:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:24:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #125, planning to set LR to 1.00e-05
2025-10-09 16:24:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 16:24:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:24:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:24:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:24:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:24:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:24:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:24:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.294586, avg_loss=0.671447, seen=480, correct=287, accuracy=0.597917
2025-10-09 16:24:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:24:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:24:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:24:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=125 reserved=2332MB allocated=2141MB
2025-10-09 16:24:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 125, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.71486270427704, 'train_avg_loss': 0.6059571892023087, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 16:24:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 125, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.2945861816406, 'train_avg_loss': 0.6714470545450847, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 16:24:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 125, 'Results_raw': {'train_total': 480, 'train_loss': 322.2945861816406, 'train_avg_loss': 0.6714470545450847, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 16:24:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:24:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:24:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #125, planning to set LR to 1.00e-05
2025-10-09 16:24:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 16:24:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:24:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:24:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:24:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:24:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:25:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:25:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.422974, avg_loss=0.634215, seen=480, correct=308, accuracy=0.641667
2025-10-09 16:25:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:25:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:25:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:25:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=125 reserved=2340MB allocated=2141MB
2025-10-09 16:25:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 125, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.3556587100029, 'train_avg_loss': 0.6862971559166908, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 16:25:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 125, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.4229736328125, 'train_avg_loss': 0.6342145284016927, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 16:25:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 125, 'Results_raw': {'train_total': 480, 'train_loss': 304.4229736328125, 'train_avg_loss': 0.6342145284016927, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 16:25:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:25:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:25:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #125, planning to set LR to 1.00e-05
2025-10-09 16:25:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 16:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:25:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:25:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:25:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:25:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:26:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:26:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=291.257507, avg_loss=0.606786, seen=480, correct=324, accuracy=0.675000
2025-10-09 16:26:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:26:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:26:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:26:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=125 reserved=2402MB allocated=2141MB
2025-10-09 16:26:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 125, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 65.32013791799545, 'train_avg_loss': 0.5443344826499621, 'train_seen': 120, 'train_correct': 91, 'train_acc': 0.7583333333333333}}
2025-10-09 16:26:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 125, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 291.25750732421875, 'train_avg_loss': 0.6067864735921223, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 16:26:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 125, 'Results_raw': {'train_total': 480, 'train_loss': 291.25750732421875, 'train_avg_loss': 0.6067864735921223, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 16:26:18 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #126) -------------
2025-10-09 16:26:19 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=126 aidx=1 | s=5 (candidates=23)
2025-10-09 16:26:19 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[14, 18, 25, 24, 44] (from 23)
2025-10-09 16:26:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:26:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:26:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #126, planning to set LR to 1.00e-05
2025-10-09 16:26:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 16:26:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:26:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:26:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:26:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:26:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:27:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:27:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=295.269165, avg_loss=0.615144, seen=480, correct=311, accuracy=0.647917
2025-10-09 16:27:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:27:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:27:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:27:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=126 reserved=2330MB allocated=2141MB
2025-10-09 16:27:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 126, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.67779022455215, 'train_avg_loss': 0.5723149185379346, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 16:27:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 126, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 295.2691650390625, 'train_avg_loss': 0.6151440938313802, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 16:27:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 126, 'Results_raw': {'train_total': 480, 'train_loss': 295.2691650390625, 'train_avg_loss': 0.6151440938313802, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 16:27:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:27:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:27:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #126, planning to set LR to 1.00e-05
2025-10-09 16:27:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 16:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:27:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:27:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:27:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:27:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:27:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:27:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.176483, avg_loss=0.652451, seen=480, correct=294, accuracy=0.612500
2025-10-09 16:27:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:27:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:27:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:27:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=126 reserved=2360MB allocated=2141MB
2025-10-09 16:27:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 126, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.58224982023239, 'train_avg_loss': 0.6881854151686032, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 16:27:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 126, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.1764831542969, 'train_avg_loss': 0.6524510065714518, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:27:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 126, 'Results_raw': {'train_total': 480, 'train_loss': 313.1764831542969, 'train_avg_loss': 0.6524510065714518, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:27:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:27:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:27:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #126, planning to set LR to 1.00e-05
2025-10-09 16:27:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 16:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:27:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:27:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:27:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:27:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:28:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:28:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.838013, avg_loss=0.687163, seen=480, correct=281, accuracy=0.585417
2025-10-09 16:28:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:28:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:28:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:28:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=126 reserved=2330MB allocated=2141MB
2025-10-09 16:28:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 126, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.17652869224548, 'train_avg_loss': 0.6848044057687124, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 16:28:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 126, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.8380126953125, 'train_avg_loss': 0.6871625264485677, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 16:28:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 126, 'Results_raw': {'train_total': 480, 'train_loss': 329.8380126953125, 'train_avg_loss': 0.6871625264485677, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 16:28:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:28:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:28:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #126, planning to set LR to 1.00e-05
2025-10-09 16:28:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 16:28:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:28:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:28:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:28:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:28:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:29:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:29:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.447510, avg_loss=0.678016, seen=480, correct=284, accuracy=0.591667
2025-10-09 16:29:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:29:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:29:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:29:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=126 reserved=2330MB allocated=2141MB
2025-10-09 16:29:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 126, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.02905017137527, 'train_avg_loss': 0.6919087514281272, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 16:29:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 126, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.447509765625, 'train_avg_loss': 0.6780156453450521, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 16:29:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 126, 'Results_raw': {'train_total': 480, 'train_loss': 325.447509765625, 'train_avg_loss': 0.6780156453450521, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 16:29:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:29:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:29:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #126, planning to set LR to 1.00e-05
2025-10-09 16:29:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 16:29:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:29:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:29:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:29:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:29:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:29:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:29:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.628143, avg_loss=0.645059, seen=480, correct=313, accuracy=0.652083
2025-10-09 16:29:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:29:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:29:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=126 reserved=2330MB allocated=2141MB
2025-10-09 16:29:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 126, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.60361135005951, 'train_avg_loss': 0.6300300945838292, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 16:29:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 126, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.6281433105469, 'train_avg_loss': 0.6450586318969727, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 16:29:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 126, 'Results_raw': {'train_total': 480, 'train_loss': 309.6281433105469, 'train_avg_loss': 0.6450586318969727, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 16:29:59 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #127) -------------
2025-10-09 16:29:59 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=127 aidx=1 | s=5 (candidates=23)
2025-10-09 16:29:59 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[42, 49, 2, 4, 11] (from 23)
2025-10-09 16:30:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:30:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:30:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #127, planning to set LR to 1.00e-05
2025-10-09 16:30:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 16:30:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:30:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:30:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:30:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:30:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:30:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:30:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.943695, avg_loss=0.643633, seen=480, correct=302, accuracy=0.629167
2025-10-09 16:30:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:30:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:30:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:30:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=127 reserved=2340MB allocated=2141MB
2025-10-09 16:30:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 127, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.00071036815643, 'train_avg_loss': 0.6416725864013036, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 16:30:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 127, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.9436950683594, 'train_avg_loss': 0.6436326980590821, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 16:30:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 127, 'Results_raw': {'train_total': 480, 'train_loss': 308.9436950683594, 'train_avg_loss': 0.6436326980590821, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 16:30:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:30:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:30:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #127, planning to set LR to 1.00e-05
2025-10-09 16:30:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 16:30:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:30:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:30:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:30:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:30:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:31:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:31:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.556732, avg_loss=0.669910, seen=480, correct=277, accuracy=0.577083
2025-10-09 16:31:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:31:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:31:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:31:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=127 reserved=2362MB allocated=2141MB
2025-10-09 16:31:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 127, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.22247803211212, 'train_avg_loss': 0.693520650267601, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 16:31:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 127, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.5567321777344, 'train_avg_loss': 0.6699098587036133, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 16:31:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 127, 'Results_raw': {'train_total': 480, 'train_loss': 321.5567321777344, 'train_avg_loss': 0.6699098587036133, 'train_seen': 480, 'train_correct': 277, 'train_acc': 0.5770833333333333}}
2025-10-09 16:31:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:31:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:31:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #127, planning to set LR to 1.00e-05
2025-10-09 16:31:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 16:31:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:31:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:31:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:31:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:31:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:32:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:32:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=274.742981, avg_loss=0.572381, seen=480, correct=340, accuracy=0.708333
2025-10-09 16:32:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:32:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:32:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:32:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=127 reserved=2402MB allocated=2141MB
2025-10-09 16:32:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 127, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 59.97807061672211, 'train_avg_loss': 0.4998172551393509, 'train_seen': 120, 'train_correct': 93, 'train_acc': 0.775}}
2025-10-09 16:32:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 127, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 274.74298095703125, 'train_avg_loss': 0.5723812103271484, 'train_seen': 480, 'train_correct': 340, 'train_acc': 0.7083333333333334}}
2025-10-09 16:32:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 127, 'Results_raw': {'train_total': 480, 'train_loss': 274.74298095703125, 'train_avg_loss': 0.5723812103271484, 'train_seen': 480, 'train_correct': 340, 'train_acc': 0.7083333333333334}}
2025-10-09 16:32:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:32:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:32:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #127, planning to set LR to 1.00e-05
2025-10-09 16:32:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 16:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:32:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:32:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:32:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:32:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:32:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:32:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=277.589844, avg_loss=0.578312, seen=480, correct=342, accuracy=0.712500
2025-10-09 16:32:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:32:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:32:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=127 reserved=2372MB allocated=2141MB
2025-10-09 16:32:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 127, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.24806261062622, 'train_avg_loss': 0.6520671884218852, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 16:32:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 127, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 277.58984375, 'train_avg_loss': 0.5783121744791667, 'train_seen': 480, 'train_correct': 342, 'train_acc': 0.7125}}
2025-10-09 16:32:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 127, 'Results_raw': {'train_total': 480, 'train_loss': 277.58984375, 'train_avg_loss': 0.5783121744791667, 'train_seen': 480, 'train_correct': 342, 'train_acc': 0.7125}}
2025-10-09 16:32:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:32:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:32:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #127, planning to set LR to 1.00e-05
2025-10-09 16:32:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 16:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:32:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:32:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:32:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:32:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:33:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:33:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.826813, avg_loss=0.639223, seen=480, correct=305, accuracy=0.635417
2025-10-09 16:33:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:33:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:33:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:33:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=127 reserved=2418MB allocated=2141MB
2025-10-09 16:33:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 127, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.67955756187439, 'train_avg_loss': 0.6639963130156199, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 16:33:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 127, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.8268127441406, 'train_avg_loss': 0.639222526550293, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 16:33:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 127, 'Results_raw': {'train_total': 480, 'train_loss': 306.8268127441406, 'train_avg_loss': 0.639222526550293, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 16:33:31 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #128) -------------
2025-10-09 16:33:32 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=128 aidx=1 | s=5 (candidates=23)
2025-10-09 16:33:32 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[52, 53, 8, 25, 42] (from 23)
2025-10-09 16:33:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:33:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:33:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #128, planning to set LR to 1.00e-05
2025-10-09 16:33:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 16:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:33:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:33:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:33:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:33:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:34:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:34:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.981384, avg_loss=0.645795, seen=480, correct=294, accuracy=0.612500
2025-10-09 16:34:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:34:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:34:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:34:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=128 reserved=2338MB allocated=2141MB
2025-10-09 16:34:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 128, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.55452406406403, 'train_avg_loss': 0.6296210338672003, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 16:34:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 128, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.98138427734375, 'train_avg_loss': 0.6457945505777994, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:34:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 128, 'Results_raw': {'train_total': 480, 'train_loss': 309.98138427734375, 'train_avg_loss': 0.6457945505777994, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:34:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:34:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:34:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #128, planning to set LR to 1.00e-05
2025-10-09 16:34:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 16:34:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:34:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:34:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:34:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:34:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:34:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:34:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.720459, avg_loss=0.676501, seen=480, correct=290, accuracy=0.604167
2025-10-09 16:34:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:34:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:34:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:35:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=128 reserved=2332MB allocated=2141MB
2025-10-09 16:35:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 128, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.1405194401741, 'train_avg_loss': 0.6095043286681175, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 16:35:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 128, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.720458984375, 'train_avg_loss': 0.6765009562174479, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 16:35:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 128, 'Results_raw': {'train_total': 480, 'train_loss': 324.720458984375, 'train_avg_loss': 0.6765009562174479, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 16:35:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:35:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:35:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #128, planning to set LR to 1.00e-05
2025-10-09 16:35:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 16:35:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:35:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:35:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:35:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:35:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:35:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:35:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=295.066284, avg_loss=0.614721, seen=480, correct=323, accuracy=0.672917
2025-10-09 16:35:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:35:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:35:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:35:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=128 reserved=2356MB allocated=2141MB
2025-10-09 16:35:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 128, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.77517312765121, 'train_avg_loss': 0.6231264427304268, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 16:35:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 128, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 295.0662841796875, 'train_avg_loss': 0.6147214253743489, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 16:35:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 128, 'Results_raw': {'train_total': 480, 'train_loss': 295.0662841796875, 'train_avg_loss': 0.6147214253743489, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 16:35:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:35:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:35:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #128, planning to set LR to 1.00e-05
2025-10-09 16:35:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 16:35:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:35:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:35:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:35:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:35:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:36:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:36:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.777832, avg_loss=0.684954, seen=480, correct=285, accuracy=0.593750
2025-10-09 16:36:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:36:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:36:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=128 reserved=2330MB allocated=2141MB
2025-10-09 16:36:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 128, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.76440668106079, 'train_avg_loss': 0.6730367223421733, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 16:36:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 128, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.77783203125, 'train_avg_loss': 0.6849538167317708, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 16:36:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 128, 'Results_raw': {'train_total': 480, 'train_loss': 328.77783203125, 'train_avg_loss': 0.6849538167317708, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 16:36:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:36:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:36:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #128, planning to set LR to 1.00e-05
2025-10-09 16:36:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 16:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:36:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:36:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:36:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:36:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:37:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:37:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.414551, avg_loss=0.642530, seen=480, correct=305, accuracy=0.635417
2025-10-09 16:37:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:37:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:37:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:37:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=128 reserved=2342MB allocated=2141MB
2025-10-09 16:37:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 128, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.63123720884323, 'train_avg_loss': 0.6469269767403603, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 16:37:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 128, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.41455078125, 'train_avg_loss': 0.6425303141276042, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 16:37:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 128, 'Results_raw': {'train_total': 480, 'train_loss': 308.41455078125, 'train_avg_loss': 0.6425303141276042, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 16:37:11 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #129) -------------
2025-10-09 16:37:11 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=129 aidx=1 | s=5 (candidates=23)
2025-10-09 16:37:11 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[53, 23, 7, 24, 8] (from 23)
2025-10-09 16:37:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:37:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:37:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #129, planning to set LR to 1.00e-05
2025-10-09 16:37:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 16:37:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:37:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:37:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:37:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:37:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:37:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:37:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.255371, avg_loss=0.673449, seen=480, correct=294, accuracy=0.612500
2025-10-09 16:37:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:37:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:37:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:37:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=129 reserved=2332MB allocated=2141MB
2025-10-09 16:37:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 129, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.7155013680458, 'train_avg_loss': 0.6059625114003817, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 16:37:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 129, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.25537109375, 'train_avg_loss': 0.6734486897786458, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:37:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 129, 'Results_raw': {'train_total': 480, 'train_loss': 323.25537109375, 'train_avg_loss': 0.6734486897786458, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:37:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:37:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:37:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #129, planning to set LR to 1.00e-05
2025-10-09 16:37:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-09 16:37:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:37:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:37:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:37:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:37:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:38:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:38:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.554962, avg_loss=0.682406, seen=480, correct=279, accuracy=0.581250
2025-10-09 16:38:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:38:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:38:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:38:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=129 reserved=2330MB allocated=2141MB
2025-10-09 16:38:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 129, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.63574314117432, 'train_avg_loss': 0.6969645261764527, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 16:38:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 129, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.5549621582031, 'train_avg_loss': 0.6824061711629231, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 16:38:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 129, 'Results_raw': {'train_total': 480, 'train_loss': 327.5549621582031, 'train_avg_loss': 0.6824061711629231, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 16:38:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:38:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:38:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #129, planning to set LR to 1.00e-05
2025-10-09 16:38:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 16:38:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:38:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:38:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:38:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:38:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:39:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:39:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.780365, avg_loss=0.628709, seen=480, correct=305, accuracy=0.635417
2025-10-09 16:39:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:39:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:39:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:39:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=129 reserved=2340MB allocated=2141MB
2025-10-09 16:39:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 129, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.14218181371689, 'train_avg_loss': 0.6845181817809741, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 16:39:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 129, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.7803649902344, 'train_avg_loss': 0.628709093729655, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 16:39:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 129, 'Results_raw': {'train_total': 480, 'train_loss': 301.7803649902344, 'train_avg_loss': 0.628709093729655, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 16:39:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:39:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:39:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #129, planning to set LR to 1.00e-05
2025-10-09 16:39:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 16:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:39:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:39:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:39:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:39:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:40:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:40:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.669983, avg_loss=0.674312, seen=480, correct=276, accuracy=0.575000
2025-10-09 16:40:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:40:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:40:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:40:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=129 reserved=2330MB allocated=2141MB
2025-10-09 16:40:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 129, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.88444864749908, 'train_avg_loss': 0.690703738729159, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 16:40:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 129, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.66998291015625, 'train_avg_loss': 0.6743124643961589, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 16:40:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 129, 'Results_raw': {'train_total': 480, 'train_loss': 323.66998291015625, 'train_avg_loss': 0.6743124643961589, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 16:40:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:40:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:40:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #129, planning to set LR to 1.00e-05
2025-10-09 16:40:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 16:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:40:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:40:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:40:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:40:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:40:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:40:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=296.289520, avg_loss=0.617270, seen=480, correct=317, accuracy=0.660417
2025-10-09 16:40:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:40:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:40:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=129 reserved=2356MB allocated=2141MB
2025-10-09 16:40:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 129, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.53417247533798, 'train_avg_loss': 0.6294514372944832, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 16:40:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 129, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 296.2895202636719, 'train_avg_loss': 0.6172698338826498, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 16:40:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 129, 'Results_raw': {'train_total': 480, 'train_loss': 296.2895202636719, 'train_avg_loss': 0.6172698338826498, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 16:40:50 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #130) -------------
2025-10-09 16:40:50 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=130 aidx=1 | s=5 (candidates=23)
2025-10-09 16:40:50 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[42, 11, 44, 2, 33] (from 23)
2025-10-09 16:40:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:40:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:40:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #130, planning to set LR to 1.00e-05
2025-10-09 16:40:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 16:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:40:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:40:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:40:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:40:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:41:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:41:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.301331, avg_loss=0.629794, seen=480, correct=312, accuracy=0.650000
2025-10-09 16:41:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:41:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:41:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:41:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=130 reserved=2338MB allocated=2141MB
2025-10-09 16:41:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 130, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.26312291622162, 'train_avg_loss': 0.6271926909685135, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 16:41:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 130, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.30133056640625, 'train_avg_loss': 0.6297944386800131, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 16:41:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 130, 'Results_raw': {'train_total': 480, 'train_loss': 302.30133056640625, 'train_avg_loss': 0.6297944386800131, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 16:41:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:41:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:41:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #130, planning to set LR to 1.00e-05
2025-10-09 16:41:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 16:41:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:41:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:41:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:41:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:41:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:42:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:42:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=299.834717, avg_loss=0.624656, seen=480, correct=308, accuracy=0.641667
2025-10-09 16:42:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:42:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:42:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=130 reserved=2418MB allocated=2141MB
2025-10-09 16:42:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 130, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.09194740653038, 'train_avg_loss': 0.6590995617210865, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 16:42:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 130, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 299.834716796875, 'train_avg_loss': 0.6246556599934896, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 16:42:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 130, 'Results_raw': {'train_total': 480, 'train_loss': 299.834716796875, 'train_avg_loss': 0.6246556599934896, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 16:42:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:42:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:42:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #130, planning to set LR to 1.00e-05
2025-10-09 16:42:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 16:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:42:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:42:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:42:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:42:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:43:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:43:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.920044, avg_loss=0.631083, seen=480, correct=318, accuracy=0.662500
2025-10-09 16:43:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:43:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:43:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:43:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=130 reserved=2330MB allocated=2141MB
2025-10-09 16:43:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 130, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.53809252381325, 'train_avg_loss': 0.6211507710317771, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 16:43:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 130, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.9200439453125, 'train_avg_loss': 0.6310834248860677, 'train_seen': 480, 'train_correct': 318, 'train_acc': 0.6625}}
2025-10-09 16:43:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 130, 'Results_raw': {'train_total': 480, 'train_loss': 302.9200439453125, 'train_avg_loss': 0.6310834248860677, 'train_seen': 480, 'train_correct': 318, 'train_acc': 0.6625}}
2025-10-09 16:43:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:43:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:43:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #130, planning to set LR to 1.00e-05
2025-10-09 16:43:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 16:43:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:43:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:43:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:43:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:43:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:43:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:43:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=275.668671, avg_loss=0.574310, seen=480, correct=336, accuracy=0.700000
2025-10-09 16:43:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:43:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:43:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:43:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=130 reserved=2402MB allocated=2141MB
2025-10-09 16:43:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 130, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 59.26306477189064, 'train_avg_loss': 0.49385887309908866, 'train_seen': 120, 'train_correct': 94, 'train_acc': 0.7833333333333333}}
2025-10-09 16:43:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 130, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 275.6686706542969, 'train_avg_loss': 0.5743097305297852, 'train_seen': 480, 'train_correct': 336, 'train_acc': 0.7}}
2025-10-09 16:43:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 130, 'Results_raw': {'train_total': 480, 'train_loss': 275.6686706542969, 'train_avg_loss': 0.5743097305297852, 'train_seen': 480, 'train_correct': 336, 'train_acc': 0.7}}
2025-10-09 16:43:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:43:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:43:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #130, planning to set LR to 1.00e-05
2025-10-09 16:43:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 16:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:43:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:43:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:43:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:43:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:44:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:44:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.443054, avg_loss=0.678006, seen=480, correct=280, accuracy=0.583333
2025-10-09 16:44:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:44:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:44:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:44:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=130 reserved=2342MB allocated=2141MB
2025-10-09 16:44:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 130, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.09272199869156, 'train_avg_loss': 0.6757726833224297, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 16:44:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 130, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.44305419921875, 'train_avg_loss': 0.678006362915039, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 16:44:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 130, 'Results_raw': {'train_total': 480, 'train_loss': 325.44305419921875, 'train_avg_loss': 0.678006362915039, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 16:44:35 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #131) -------------
2025-10-09 16:44:36 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=131 aidx=1 | s=5 (candidates=23)
2025-10-09 16:44:36 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[30, 33, 12, 49, 44] (from 23)
2025-10-09 16:44:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:44:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:44:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #131, planning to set LR to 1.00e-05
2025-10-09 16:44:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 16:44:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:44:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:44:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:44:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:44:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:45:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:45:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.985413, avg_loss=0.656220, seen=480, correct=283, accuracy=0.589583
2025-10-09 16:45:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:45:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:45:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:45:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=131 reserved=2330MB allocated=2141MB
2025-10-09 16:45:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 131, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.61909872293472, 'train_avg_loss': 0.6718258226911227, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 16:45:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 131, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.98541259765625, 'train_avg_loss': 0.6562196095784505, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 16:45:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 131, 'Results_raw': {'train_total': 480, 'train_loss': 314.98541259765625, 'train_avg_loss': 0.6562196095784505, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 16:45:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:45:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:45:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #131, planning to set LR to 1.00e-05
2025-10-09 16:45:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 16:45:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:45:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:45:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:45:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:45:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:46:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:46:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.551880, avg_loss=0.663650, seen=480, correct=287, accuracy=0.597917
2025-10-09 16:46:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:46:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:46:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:46:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=131 reserved=2342MB allocated=2141MB
2025-10-09 16:46:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 131, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.67468196153641, 'train_avg_loss': 0.6556223496794701, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 16:46:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 131, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.5518798828125, 'train_avg_loss': 0.6636497497558593, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 16:46:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 131, 'Results_raw': {'train_total': 480, 'train_loss': 318.5518798828125, 'train_avg_loss': 0.6636497497558593, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 16:46:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:46:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:46:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #131, planning to set LR to 1.00e-05
2025-10-09 16:46:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 16:46:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:46:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:46:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:46:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:46:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:46:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:46:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.384003, avg_loss=0.648717, seen=480, correct=297, accuracy=0.618750
2025-10-09 16:46:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:46:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:46:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:46:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=131 reserved=2330MB allocated=2141MB
2025-10-09 16:46:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 131, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.75632134079933, 'train_avg_loss': 0.6646360111733277, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 16:46:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 131, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.3840026855469, 'train_avg_loss': 0.6487166722615559, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 16:46:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 131, 'Results_raw': {'train_total': 480, 'train_loss': 311.3840026855469, 'train_avg_loss': 0.6487166722615559, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 16:46:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:46:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:46:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #131, planning to set LR to 1.00e-05
2025-10-09 16:46:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 16:46:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:46:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:46:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:46:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:46:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:47:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:47:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.813873, avg_loss=0.666279, seen=480, correct=289, accuracy=0.602083
2025-10-09 16:47:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:47:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:47:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:47:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=131 reserved=2362MB allocated=2141MB
2025-10-09 16:47:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 131, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.45523899793625, 'train_avg_loss': 0.6954603249828021, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 16:47:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 131, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.8138732910156, 'train_avg_loss': 0.6662789026896159, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 16:47:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 131, 'Results_raw': {'train_total': 480, 'train_loss': 319.8138732910156, 'train_avg_loss': 0.6662789026896159, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 16:47:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:47:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:47:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #131, planning to set LR to 1.00e-05
2025-10-09 16:47:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 16:47:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:47:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:47:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:47:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:47:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:48:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:48:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.077209, avg_loss=0.631411, seen=480, correct=313, accuracy=0.652083
2025-10-09 16:48:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:48:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:48:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:48:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=131 reserved=2330MB allocated=2141MB
2025-10-09 16:48:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 131, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.96981155872345, 'train_avg_loss': 0.6247484296560287, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 16:48:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 131, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.07720947265625, 'train_avg_loss': 0.6314108530680339, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 16:48:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 131, 'Results_raw': {'train_total': 480, 'train_loss': 303.07720947265625, 'train_avg_loss': 0.6314108530680339, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 16:48:13 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #132) -------------
2025-10-09 16:48:13 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=132 aidx=1 | s=5 (candidates=23)
2025-10-09 16:48:13 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[4, 9, 30, 53, 33] (from 23)
2025-10-09 16:48:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:48:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:48:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #132, planning to set LR to 1.00e-05
2025-10-09 16:48:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 16:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:48:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:48:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:48:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:48:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:48:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:48:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=276.656982, avg_loss=0.576369, seen=480, correct=336, accuracy=0.700000
2025-10-09 16:48:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:48:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:48:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:48:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=132 reserved=2372MB allocated=2141MB
2025-10-09 16:48:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 132, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.45466095209122, 'train_avg_loss': 0.6537888412674268, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 16:48:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 132, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 276.656982421875, 'train_avg_loss': 0.5763687133789063, 'train_seen': 480, 'train_correct': 336, 'train_acc': 0.7}}
2025-10-09 16:48:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 132, 'Results_raw': {'train_total': 480, 'train_loss': 276.656982421875, 'train_avg_loss': 0.5763687133789063, 'train_seen': 480, 'train_correct': 336, 'train_acc': 0.7}}
2025-10-09 16:48:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:48:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:48:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #132, planning to set LR to 1.00e-05
2025-10-09 16:48:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 16:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:48:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:48:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:48:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:48:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:49:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:49:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.637146, avg_loss=0.655494, seen=480, correct=294, accuracy=0.612500
2025-10-09 16:49:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:49:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:49:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:49:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=132 reserved=2352MB allocated=2141MB
2025-10-09 16:49:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 132, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.74461778998375, 'train_avg_loss': 0.664538481583198, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 16:49:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 132, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.63714599609375, 'train_avg_loss': 0.6554940541585287, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:49:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 132, 'Results_raw': {'train_total': 480, 'train_loss': 314.63714599609375, 'train_avg_loss': 0.6554940541585287, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:49:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:49:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:49:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #132, planning to set LR to 1.00e-05
2025-10-09 16:49:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 16:49:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:49:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:49:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:49:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:49:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:50:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:50:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.462860, avg_loss=0.655131, seen=480, correct=286, accuracy=0.595833
2025-10-09 16:50:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:50:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:50:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=132 reserved=2330MB allocated=2141MB
2025-10-09 16:50:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 132, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.63852506875992, 'train_avg_loss': 0.6636543755729993, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 16:50:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 132, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.4628601074219, 'train_avg_loss': 0.6551309585571289, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 16:50:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 132, 'Results_raw': {'train_total': 480, 'train_loss': 314.4628601074219, 'train_avg_loss': 0.6551309585571289, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 16:50:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:50:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:50:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #132, planning to set LR to 1.00e-05
2025-10-09 16:50:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 16:50:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:50:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:50:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:50:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:50:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:51:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:51:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.657043, avg_loss=0.672202, seen=480, correct=291, accuracy=0.606250
2025-10-09 16:51:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:51:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:51:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:51:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=132 reserved=2332MB allocated=2141MB
2025-10-09 16:51:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 132, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.353744328022, 'train_avg_loss': 0.6112812027335167, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 16:51:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 132, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.65704345703125, 'train_avg_loss': 0.6722021738688151, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 16:51:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 132, 'Results_raw': {'train_total': 480, 'train_loss': 322.65704345703125, 'train_avg_loss': 0.6722021738688151, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 16:51:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:51:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:51:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #132, planning to set LR to 1.00e-05
2025-10-09 16:51:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 16:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:51:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:51:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:51:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:51:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:51:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:51:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=318.657074, avg_loss=0.663869, seen=480, correct=292, accuracy=0.608333
2025-10-09 16:51:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:51:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:51:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:51:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=132 reserved=2342MB allocated=2141MB
2025-10-09 16:51:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 132, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.05756610631943, 'train_avg_loss': 0.6504797175526619, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 16:51:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 132, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 318.6570739746094, 'train_avg_loss': 0.6638689041137695, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 16:51:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 132, 'Results_raw': {'train_total': 480, 'train_loss': 318.6570739746094, 'train_avg_loss': 0.6638689041137695, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 16:51:49 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #133) -------------
2025-10-09 16:51:49 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=133 aidx=1 | s=5 (candidates=23)
2025-10-09 16:51:49 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[53, 11, 18, 30, 14] (from 23)
2025-10-09 16:51:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:51:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:51:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #133, planning to set LR to 1.00e-05
2025-10-09 16:51:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 16:51:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:51:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:51:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:51:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:51:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:52:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:52:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.057739, avg_loss=0.668870, seen=480, correct=299, accuracy=0.622917
2025-10-09 16:52:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:52:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:52:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:52:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=133 reserved=2332MB allocated=2141MB
2025-10-09 16:52:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 133, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.39104741811752, 'train_avg_loss': 0.6199253951509793, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 16:52:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 133, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.0577392578125, 'train_avg_loss': 0.6688702901204427, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 16:52:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 133, 'Results_raw': {'train_total': 480, 'train_loss': 321.0577392578125, 'train_avg_loss': 0.6688702901204427, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 16:52:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:52:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:52:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #133, planning to set LR to 1.00e-05
2025-10-09 16:52:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 16:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:52:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:52:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:52:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:52:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:53:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:53:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.562347, avg_loss=0.630338, seen=480, correct=310, accuracy=0.645833
2025-10-09 16:53:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:53:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:53:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=133 reserved=2418MB allocated=2141MB
2025-10-09 16:53:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 133, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.17072409391403, 'train_avg_loss': 0.6514227007826169, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 16:53:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 133, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.5623474121094, 'train_avg_loss': 0.6303382237752279, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 16:53:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 133, 'Results_raw': {'train_total': 480, 'train_loss': 302.5623474121094, 'train_avg_loss': 0.6303382237752279, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 16:53:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:53:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:53:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #133, planning to set LR to 1.00e-05
2025-10-09 16:53:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 16:53:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:53:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:53:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:53:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:53:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:54:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:54:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.854889, avg_loss=0.655948, seen=480, correct=294, accuracy=0.612500
2025-10-09 16:54:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:54:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:54:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:54:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=133 reserved=2362MB allocated=2141MB
2025-10-09 16:54:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 133, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.66059792041779, 'train_avg_loss': 0.6888383160034816, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 16:54:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 133, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.8548889160156, 'train_avg_loss': 0.6559476852416992, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:54:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 133, 'Results_raw': {'train_total': 480, 'train_loss': 314.8548889160156, 'train_avg_loss': 0.6559476852416992, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 16:54:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:54:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:54:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #133, planning to set LR to 1.00e-05
2025-10-09 16:54:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 16:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:54:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:54:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:54:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:54:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:54:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:54:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.880219, avg_loss=0.653917, seen=480, correct=284, accuracy=0.591667
2025-10-09 16:54:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:54:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:54:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:54:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=133 reserved=2330MB allocated=2141MB
2025-10-09 16:54:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 133, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.49012225866318, 'train_avg_loss': 0.6707510188221931, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 16:54:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 133, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.8802185058594, 'train_avg_loss': 0.6539171218872071, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 16:54:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 133, 'Results_raw': {'train_total': 480, 'train_loss': 313.8802185058594, 'train_avg_loss': 0.6539171218872071, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 16:54:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:54:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:54:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #133, planning to set LR to 1.00e-05
2025-10-09 16:54:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 16:54:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:54:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:54:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:54:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:54:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:55:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:55:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=292.781921, avg_loss=0.609962, seen=480, correct=314, accuracy=0.654167
2025-10-09 16:55:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:55:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:55:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:55:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=133 reserved=2330MB allocated=2141MB
2025-10-09 16:55:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 133, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.54241573810577, 'train_avg_loss': 0.5711867978175481, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 16:55:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 133, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 292.78192138671875, 'train_avg_loss': 0.6099623362223308, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 16:55:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 133, 'Results_raw': {'train_total': 480, 'train_loss': 292.78192138671875, 'train_avg_loss': 0.6099623362223308, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 16:55:31 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #134) -------------
2025-10-09 16:55:31 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=134 aidx=1 | s=5 (candidates=23)
2025-10-09 16:55:31 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[17, 38, 49, 12, 42] (from 23)
2025-10-09 16:55:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:55:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:55:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #134, planning to set LR to 1.00e-05
2025-10-09 16:55:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 16:55:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:55:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:55:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:55:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:55:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:56:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:56:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=297.893280, avg_loss=0.620611, seen=480, correct=320, accuracy=0.666667
2025-10-09 16:56:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:56:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:56:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:56:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=134 reserved=2366MB allocated=2141MB
2025-10-09 16:56:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 134, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.00259590148926, 'train_avg_loss': 0.5833549658457439, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 16:56:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 134, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 297.8932800292969, 'train_avg_loss': 0.6206110000610352, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 16:56:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 134, 'Results_raw': {'train_total': 480, 'train_loss': 297.8932800292969, 'train_avg_loss': 0.6206110000610352, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 16:56:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:56:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:56:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #134, planning to set LR to 1.00e-05
2025-10-09 16:56:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 16:56:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:56:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:56:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:56:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:56:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:56:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:56:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.120331, avg_loss=0.646084, seen=480, correct=310, accuracy=0.645833
2025-10-09 16:56:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:56:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:57:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:57:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=134 reserved=2336MB allocated=2141MB
2025-10-09 16:57:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 134, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.98345506191254, 'train_avg_loss': 0.6331954588492711, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 16:57:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 134, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.1203308105469, 'train_avg_loss': 0.6460840225219726, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 16:57:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 134, 'Results_raw': {'train_total': 480, 'train_loss': 310.1203308105469, 'train_avg_loss': 0.6460840225219726, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 16:57:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:57:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:57:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #134, planning to set LR to 1.00e-05
2025-10-09 16:57:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 16:57:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:57:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:57:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:57:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:57:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:57:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:57:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.639465, avg_loss=0.672166, seen=480, correct=278, accuracy=0.579167
2025-10-09 16:57:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:57:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:57:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:57:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=134 reserved=2362MB allocated=2141MB
2025-10-09 16:57:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 134, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.52595341205597, 'train_avg_loss': 0.6627162784337998, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 16:57:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 134, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.63946533203125, 'train_avg_loss': 0.6721655527750651, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 16:57:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 134, 'Results_raw': {'train_total': 480, 'train_loss': 322.63946533203125, 'train_avg_loss': 0.6721655527750651, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 16:57:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:57:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:57:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #134, planning to set LR to 1.00e-05
2025-10-09 16:57:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 16:57:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:57:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:57:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:57:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:57:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:58:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:58:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.891602, avg_loss=0.662274, seen=480, correct=296, accuracy=0.616667
2025-10-09 16:58:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:58:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:58:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=134 reserved=2330MB allocated=2141MB
2025-10-09 16:58:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 134, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.30946785211563, 'train_avg_loss': 0.6692455654342969, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 16:58:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 134, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.8916015625, 'train_avg_loss': 0.662274169921875, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 16:58:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 134, 'Results_raw': {'train_total': 480, 'train_loss': 317.8916015625, 'train_avg_loss': 0.662274169921875, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 16:58:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:58:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:58:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #134, planning to set LR to 1.00e-05
2025-10-09 16:58:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 16:58:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:58:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:58:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:58:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:58:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 16:59:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 16:59:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.049866, avg_loss=0.643854, seen=480, correct=304, accuracy=0.633333
2025-10-09 16:59:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 16:59:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:59:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 16:59:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=134 reserved=2342MB allocated=2141MB
2025-10-09 16:59:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 134, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.7593544125557, 'train_avg_loss': 0.6396612867712974, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 16:59:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 134, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.04986572265625, 'train_avg_loss': 0.6438538869222005, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 16:59:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 134, 'Results_raw': {'train_total': 480, 'train_loss': 309.04986572265625, 'train_avg_loss': 0.6438538869222005, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 16:59:16 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #135) -------------
2025-10-09 16:59:16 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=135 aidx=1 | s=5 (candidates=23)
2025-10-09 16:59:16 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[35, 25, 11, 7, 4] (from 23)
2025-10-09 16:59:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 16:59:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 16:59:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #135, planning to set LR to 1.00e-05
2025-10-09 16:59:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 16:59:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 16:59:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 16:59:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 16:59:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 16:59:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:00:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:00:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.481750, avg_loss=0.665587, seen=480, correct=292, accuracy=0.608333
2025-10-09 17:00:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:00:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:00:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:00:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=135 reserved=2402MB allocated=2141MB
2025-10-09 17:00:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 135, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.8733057975769, 'train_avg_loss': 0.6656108816464742, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 17:00:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 135, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.48175048828125, 'train_avg_loss': 0.6655869801839193, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 17:00:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 135, 'Results_raw': {'train_total': 480, 'train_loss': 319.48175048828125, 'train_avg_loss': 0.6655869801839193, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 17:00:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:00:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:00:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #135, planning to set LR to 1.00e-05
2025-10-09 17:00:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 17:00:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:00:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:00:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:00:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:00:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:00:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:00:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.069275, avg_loss=0.681394, seen=480, correct=288, accuracy=0.600000
2025-10-09 17:00:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:00:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:00:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:00:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=135 reserved=2330MB allocated=2141MB
2025-10-09 17:00:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 135, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.17482143640518, 'train_avg_loss': 0.6764568453033765, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 17:00:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 135, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.06927490234375, 'train_avg_loss': 0.6813943227132161, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 17:00:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 135, 'Results_raw': {'train_total': 480, 'train_loss': 327.06927490234375, 'train_avg_loss': 0.6813943227132161, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 17:00:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:00:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:00:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #135, planning to set LR to 1.00e-05
2025-10-09 17:00:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 17:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:00:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:00:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:00:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:00:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:01:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:01:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.896362, avg_loss=0.631034, seen=480, correct=300, accuracy=0.625000
2025-10-09 17:01:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:01:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:01:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=135 reserved=2418MB allocated=2141MB
2025-10-09 17:01:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 135, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.72185266017914, 'train_avg_loss': 0.6560154388348262, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 17:01:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 135, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.8963623046875, 'train_avg_loss': 0.6310340881347656, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 17:01:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 135, 'Results_raw': {'train_total': 480, 'train_loss': 302.8963623046875, 'train_avg_loss': 0.6310340881347656, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 17:01:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:01:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:01:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #135, planning to set LR to 1.00e-05
2025-10-09 17:01:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 17:01:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:01:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:01:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:01:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:01:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:02:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:02:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.936951, avg_loss=0.631119, seen=480, correct=312, accuracy=0.650000
2025-10-09 17:02:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:02:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:02:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=135 reserved=2340MB allocated=2141MB
2025-10-09 17:02:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 135, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.28328895568848, 'train_avg_loss': 0.6856940746307373, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 17:02:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 135, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.93695068359375, 'train_avg_loss': 0.631118647257487, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 17:02:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 135, 'Results_raw': {'train_total': 480, 'train_loss': 302.93695068359375, 'train_avg_loss': 0.631118647257487, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 17:02:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:02:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:02:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #135, planning to set LR to 1.00e-05
2025-10-09 17:02:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 17:02:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:02:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:02:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:02:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:02:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:02:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:02:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=276.757019, avg_loss=0.576577, seen=480, correct=332, accuracy=0.691667
2025-10-09 17:02:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:02:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:02:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:02:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=135 reserved=2372MB allocated=2141MB
2025-10-09 17:02:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 135, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.38918542861938, 'train_avg_loss': 0.6532432119051615, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 17:02:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 135, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 276.75701904296875, 'train_avg_loss': 0.5765771230061849, 'train_seen': 480, 'train_correct': 332, 'train_acc': 0.6916666666666667}}
2025-10-09 17:02:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 135, 'Results_raw': {'train_total': 480, 'train_loss': 276.75701904296875, 'train_avg_loss': 0.5765771230061849, 'train_seen': 480, 'train_correct': 332, 'train_acc': 0.6916666666666667}}
2025-10-09 17:02:57 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #136) -------------
2025-10-09 17:02:58 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=136 aidx=1 | s=5 (candidates=23)
2025-10-09 17:02:58 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[27, 8, 4, 11, 7] (from 23)
2025-10-09 17:02:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:02:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:02:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #136, planning to set LR to 1.00e-05
2025-10-09 17:02:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 17:02:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:02:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:02:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:02:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:02:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:03:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:03:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.490479, avg_loss=0.655188, seen=480, correct=288, accuracy=0.600000
2025-10-09 17:03:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:03:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:03:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:03:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=136 reserved=2332MB allocated=2141MB
2025-10-09 17:03:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 136, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.12040138244629, 'train_avg_loss': 0.6593366781870524, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 17:03:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 136, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.490478515625, 'train_avg_loss': 0.6551884969075521, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 17:03:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 136, 'Results_raw': {'train_total': 480, 'train_loss': 314.490478515625, 'train_avg_loss': 0.6551884969075521, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 17:03:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:03:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:03:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #136, planning to set LR to 1.00e-05
2025-10-09 17:03:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 17:03:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:03:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:03:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:03:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:03:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:04:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:04:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=287.011719, avg_loss=0.597941, seen=480, correct=333, accuracy=0.693750
2025-10-09 17:04:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:04:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:04:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:04:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=136 reserved=2354MB allocated=2141MB
2025-10-09 17:04:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 136, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.14410462975502, 'train_avg_loss': 0.5928675385812918, 'train_seen': 120, 'train_correct': 87, 'train_acc': 0.725}}
2025-10-09 17:04:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 136, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 287.01171875, 'train_avg_loss': 0.5979410807291666, 'train_seen': 480, 'train_correct': 333, 'train_acc': 0.69375}}
2025-10-09 17:04:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 136, 'Results_raw': {'train_total': 480, 'train_loss': 287.01171875, 'train_avg_loss': 0.5979410807291666, 'train_seen': 480, 'train_correct': 333, 'train_acc': 0.69375}}
2025-10-09 17:04:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:04:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:04:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #136, planning to set LR to 1.00e-05
2025-10-09 17:04:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 17:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:04:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:04:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:04:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:04:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:05:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:05:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=251.492004, avg_loss=0.523942, seen=480, correct=360, accuracy=0.750000
2025-10-09 17:05:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:05:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:05:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=136 reserved=2372MB allocated=2141MB
2025-10-09 17:05:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 136, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.93948392570019, 'train_avg_loss': 0.6161623660475015, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 17:05:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 136, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 251.49200439453125, 'train_avg_loss': 0.5239416758219401, 'train_seen': 480, 'train_correct': 360, 'train_acc': 0.75}}
2025-10-09 17:05:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 136, 'Results_raw': {'train_total': 480, 'train_loss': 251.49200439453125, 'train_avg_loss': 0.5239416758219401, 'train_seen': 480, 'train_correct': 360, 'train_acc': 0.75}}
2025-10-09 17:05:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:05:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:05:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #136, planning to set LR to 1.00e-05
2025-10-09 17:05:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 17:05:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:05:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:05:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:05:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:05:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:05:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:05:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.038483, avg_loss=0.631330, seen=480, correct=310, accuracy=0.645833
2025-10-09 17:05:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:05:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:05:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=136 reserved=2418MB allocated=2141MB
2025-10-09 17:05:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 136, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.78710767626762, 'train_avg_loss': 0.6482258973022302, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 17:05:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 136, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.0384826660156, 'train_avg_loss': 0.6313301722208658, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 17:05:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 136, 'Results_raw': {'train_total': 480, 'train_loss': 303.0384826660156, 'train_avg_loss': 0.6313301722208658, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 17:05:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:05:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:05:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #136, planning to set LR to 1.00e-05
2025-10-09 17:05:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 17:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:05:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:05:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:05:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:05:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:06:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:06:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.915497, avg_loss=0.635241, seen=480, correct=305, accuracy=0.635417
2025-10-09 17:06:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:06:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:06:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=136 reserved=2340MB allocated=2141MB
2025-10-09 17:06:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 136, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.18427354097366, 'train_avg_loss': 0.6848689461747806, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 17:06:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 136, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.9154968261719, 'train_avg_loss': 0.6352406183878581, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 17:06:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 136, 'Results_raw': {'train_total': 480, 'train_loss': 304.9154968261719, 'train_avg_loss': 0.6352406183878581, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 17:06:35 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #137) -------------
2025-10-09 17:06:35 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=137 aidx=1 | s=5 (candidates=23)
2025-10-09 17:06:35 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[4, 49, 24, 30, 27] (from 23)
2025-10-09 17:06:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:06:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:06:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #137, planning to set LR to 1.00e-05
2025-10-09 17:06:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 17:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:06:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:06:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:06:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:06:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:07:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=255.489838, avg_loss=0.532270, seen=480, correct=355, accuracy=0.739583
2025-10-09 17:07:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:07:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:07:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=137 reserved=2372MB allocated=2141MB
2025-10-09 17:07:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 137, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.12970757484436, 'train_avg_loss': 0.617747563123703, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 17:07:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 137, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 255.48983764648438, 'train_avg_loss': 0.5322704950968424, 'train_seen': 480, 'train_correct': 355, 'train_acc': 0.7395833333333334}}
2025-10-09 17:07:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 137, 'Results_raw': {'train_total': 480, 'train_loss': 255.48983764648438, 'train_avg_loss': 0.5322704950968424, 'train_seen': 480, 'train_correct': 355, 'train_acc': 0.7395833333333334}}
2025-10-09 17:07:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:07:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:07:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #137, planning to set LR to 1.00e-05
2025-10-09 17:07:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 17:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:07:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:07:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:07:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:07:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:07:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:07:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.218048, avg_loss=0.671288, seen=480, correct=287, accuracy=0.597917
2025-10-09 17:07:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:07:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:07:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=137 reserved=2362MB allocated=2141MB
2025-10-09 17:07:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 137, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.74848288297653, 'train_avg_loss': 0.7062373573581378, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 17:07:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 137, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.2180480957031, 'train_avg_loss': 0.6712876001993815, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 17:07:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 137, 'Results_raw': {'train_total': 480, 'train_loss': 322.2180480957031, 'train_avg_loss': 0.6712876001993815, 'train_seen': 480, 'train_correct': 287, 'train_acc': 0.5979166666666667}}
2025-10-09 17:07:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:07:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:07:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #137, planning to set LR to 1.00e-05
2025-10-09 17:07:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 17:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:07:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:07:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:07:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:07:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:08:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:08:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.318756, avg_loss=0.673581, seen=480, correct=282, accuracy=0.587500
2025-10-09 17:08:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:08:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:08:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:08:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=137 reserved=2330MB allocated=2141MB
2025-10-09 17:08:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 137, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.72161787748337, 'train_avg_loss': 0.6893468156456948, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 17:08:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 137, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.3187561035156, 'train_avg_loss': 0.6735807418823242, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 17:08:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 137, 'Results_raw': {'train_total': 480, 'train_loss': 323.3187561035156, 'train_avg_loss': 0.6735807418823242, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 17:08:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:08:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:08:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #137, planning to set LR to 1.00e-05
2025-10-09 17:08:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 17:08:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:08:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:08:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:08:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:08:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:09:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:09:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.412720, avg_loss=0.659193, seen=480, correct=280, accuracy=0.583333
2025-10-09 17:09:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:09:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:09:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=137 reserved=2330MB allocated=2141MB
2025-10-09 17:09:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 137, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.54071843624115, 'train_avg_loss': 0.6795059869686763, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 17:09:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 137, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.4127197265625, 'train_avg_loss': 0.6591931660970052, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 17:09:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 137, 'Results_raw': {'train_total': 480, 'train_loss': 316.4127197265625, 'train_avg_loss': 0.6591931660970052, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 17:09:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:09:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:09:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #137, planning to set LR to 1.00e-05
2025-10-09 17:09:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 17:09:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:09:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:09:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:09:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:09:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:10:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:10:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.129822, avg_loss=0.656520, seen=480, correct=286, accuracy=0.595833
2025-10-09 17:10:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:10:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:10:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:10:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=137 reserved=2332MB allocated=2141MB
2025-10-09 17:10:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 137, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.24407982826233, 'train_avg_loss': 0.6603673319021861, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 17:10:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 137, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.12982177734375, 'train_avg_loss': 0.6565204620361328, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 17:10:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 137, 'Results_raw': {'train_total': 480, 'train_loss': 315.12982177734375, 'train_avg_loss': 0.6565204620361328, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 17:10:06 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #138) -------------
2025-10-09 17:10:07 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=138 aidx=1 | s=5 (candidates=23)
2025-10-09 17:10:07 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[25, 9, 14, 12, 52] (from 23)
2025-10-09 17:10:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:10:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:10:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #138, planning to set LR to 1.00e-05
2025-10-09 17:10:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 17:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:10:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:10:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:10:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:10:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:10:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:10:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.132812, avg_loss=0.677360, seen=480, correct=285, accuracy=0.593750
2025-10-09 17:10:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:10:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:10:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=138 reserved=2330MB allocated=2141MB
2025-10-09 17:10:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 138, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.70758491754532, 'train_avg_loss': 0.6808965409795443, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 17:10:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 138, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.1328125, 'train_avg_loss': 0.6773600260416667, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 17:10:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 138, 'Results_raw': {'train_total': 480, 'train_loss': 325.1328125, 'train_avg_loss': 0.6773600260416667, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 17:10:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:10:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:10:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #138, planning to set LR to 1.00e-05
2025-10-09 17:10:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 17:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:10:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:10:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:10:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:10:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:11:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:11:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.353027, avg_loss=0.656985, seen=480, correct=300, accuracy=0.625000
2025-10-09 17:11:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:11:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:11:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:11:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=138 reserved=2352MB allocated=2141MB
2025-10-09 17:11:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 138, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.00884705781937, 'train_avg_loss': 0.6667403921484947, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 17:11:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 138, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.35302734375, 'train_avg_loss': 0.6569854736328125, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 17:11:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 138, 'Results_raw': {'train_total': 480, 'train_loss': 315.35302734375, 'train_avg_loss': 0.6569854736328125, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 17:11:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:11:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:11:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #138, planning to set LR to 1.00e-05
2025-10-09 17:11:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 17:11:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:11:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:11:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:11:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:11:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:12:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:12:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=288.554047, avg_loss=0.601154, seen=480, correct=319, accuracy=0.664583
2025-10-09 17:12:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:12:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:12:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=138 reserved=2330MB allocated=2141MB
2025-10-09 17:12:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 138, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 66.17148092389107, 'train_avg_loss': 0.5514290076990922, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 17:12:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 138, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 288.5540466308594, 'train_avg_loss': 0.6011542638142904, 'train_seen': 480, 'train_correct': 319, 'train_acc': 0.6645833333333333}}
2025-10-09 17:12:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 138, 'Results_raw': {'train_total': 480, 'train_loss': 288.5540466308594, 'train_avg_loss': 0.6011542638142904, 'train_seen': 480, 'train_correct': 319, 'train_acc': 0.6645833333333333}}
2025-10-09 17:12:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:12:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:12:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #138, planning to set LR to 1.00e-05
2025-10-09 17:12:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 17:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:12:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:12:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:12:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:12:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:12:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:12:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.842651, avg_loss=0.641339, seen=480, correct=295, accuracy=0.614583
2025-10-09 17:12:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:12:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:12:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:12:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=138 reserved=2330MB allocated=2141MB
2025-10-09 17:12:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 138, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.83079540729523, 'train_avg_loss': 0.6652566283941269, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 17:12:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 138, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.8426513671875, 'train_avg_loss': 0.641338857014974, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 17:12:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 138, 'Results_raw': {'train_total': 480, 'train_loss': 307.8426513671875, 'train_avg_loss': 0.641338857014974, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 17:12:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:13:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:13:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #138, planning to set LR to 1.00e-05
2025-10-09 17:13:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 17:13:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:13:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:13:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:13:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:13:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:13:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:13:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.886719, avg_loss=0.639347, seen=480, correct=301, accuracy=0.627083
2025-10-09 17:13:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:13:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:13:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=138 reserved=2338MB allocated=2141MB
2025-10-09 17:13:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 138, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.08271169662476, 'train_avg_loss': 0.6256892641385396, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 17:13:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 138, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.88671875, 'train_avg_loss': 0.6393473307291667, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 17:13:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 138, 'Results_raw': {'train_total': 480, 'train_loss': 306.88671875, 'train_avg_loss': 0.6393473307291667, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 17:13:43 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #139) -------------
2025-10-09 17:13:43 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=139 aidx=1 | s=5 (candidates=23)
2025-10-09 17:13:43 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[35, 14, 11, 17, 52] (from 23)
2025-10-09 17:13:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:13:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:13:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #139, planning to set LR to 1.00e-05
2025-10-09 17:13:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 17:13:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:13:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:13:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:13:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:13:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:14:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:14:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.476013, avg_loss=0.665575, seen=480, correct=299, accuracy=0.622917
2025-10-09 17:14:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:14:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:14:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=139 reserved=2402MB allocated=2141MB
2025-10-09 17:14:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 139, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.66585123538971, 'train_avg_loss': 0.6638820936282476, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 17:14:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 139, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.47601318359375, 'train_avg_loss': 0.6655750274658203, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 17:14:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 139, 'Results_raw': {'train_total': 480, 'train_loss': 319.47601318359375, 'train_avg_loss': 0.6655750274658203, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 17:14:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:14:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:14:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #139, planning to set LR to 1.00e-05
2025-10-09 17:14:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 17:14:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:14:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:14:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:14:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:14:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:15:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:15:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=291.453400, avg_loss=0.607195, seen=480, correct=312, accuracy=0.650000
2025-10-09 17:15:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:15:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:15:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=139 reserved=2330MB allocated=2141MB
2025-10-09 17:15:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 139, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 67.15668734908104, 'train_avg_loss': 0.559639061242342, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 17:15:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 139, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 291.4533996582031, 'train_avg_loss': 0.6071945826212565, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 17:15:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 139, 'Results_raw': {'train_total': 480, 'train_loss': 291.4533996582031, 'train_avg_loss': 0.6071945826212565, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 17:15:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:15:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:15:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #139, planning to set LR to 1.00e-05
2025-10-09 17:15:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 17:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:15:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:15:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:15:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:15:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:15:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.146851, avg_loss=0.627389, seen=480, correct=308, accuracy=0.641667
2025-10-09 17:15:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:15:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:15:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=139 reserved=2418MB allocated=2141MB
2025-10-09 17:15:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 139, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.43266069889069, 'train_avg_loss': 0.6452721724907557, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 17:15:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 139, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.1468505859375, 'train_avg_loss': 0.6273892720540365, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 17:15:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 139, 'Results_raw': {'train_total': 480, 'train_loss': 301.1468505859375, 'train_avg_loss': 0.6273892720540365, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 17:15:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:15:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:15:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #139, planning to set LR to 1.00e-05
2025-10-09 17:15:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 17:15:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:15:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:15:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:15:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:15:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:16:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:16:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=290.327911, avg_loss=0.604850, seen=480, correct=319, accuracy=0.664583
2025-10-09 17:16:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:16:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:16:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=139 reserved=2366MB allocated=2141MB
2025-10-09 17:16:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 139, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.84916287660599, 'train_avg_loss': 0.5904096906383832, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 17:16:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 139, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 290.3279113769531, 'train_avg_loss': 0.6048498153686523, 'train_seen': 480, 'train_correct': 319, 'train_acc': 0.6645833333333333}}
2025-10-09 17:16:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 139, 'Results_raw': {'train_total': 480, 'train_loss': 290.3279113769531, 'train_avg_loss': 0.6048498153686523, 'train_seen': 480, 'train_correct': 319, 'train_acc': 0.6645833333333333}}
2025-10-09 17:16:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:16:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:16:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #139, planning to set LR to 1.00e-05
2025-10-09 17:16:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 17:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:16:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:16:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:16:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:16:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:17:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.344147, avg_loss=0.625717, seen=480, correct=316, accuracy=0.658333
2025-10-09 17:17:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:17:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:17:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=139 reserved=2338MB allocated=2141MB
2025-10-09 17:17:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 139, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.58913064002991, 'train_avg_loss': 0.6049094220002492, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 17:17:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 139, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.3441467285156, 'train_avg_loss': 0.6257169723510743, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 17:17:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 139, 'Results_raw': {'train_total': 480, 'train_loss': 300.3441467285156, 'train_avg_loss': 0.6257169723510743, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 17:17:27 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #140) -------------
2025-10-09 17:17:28 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=140 aidx=1 | s=5 (candidates=23)
2025-10-09 17:17:28 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[35, 17, 30, 11, 38] (from 23)
2025-10-09 17:17:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:17:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:17:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #140, planning to set LR to 1.00e-05
2025-10-09 17:17:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 17:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:17:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:17:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:17:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:17:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:18:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:18:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.635284, avg_loss=0.670074, seen=480, correct=292, accuracy=0.608333
2025-10-09 17:18:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:18:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:18:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:18:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=140 reserved=2402MB allocated=2141MB
2025-10-09 17:18:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 140, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.44864016771317, 'train_avg_loss': 0.6620720013976097, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 17:18:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 140, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.6352844238281, 'train_avg_loss': 0.6700735092163086, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 17:18:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 140, 'Results_raw': {'train_total': 480, 'train_loss': 321.6352844238281, 'train_avg_loss': 0.6700735092163086, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 17:18:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:18:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:18:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #140, planning to set LR to 1.00e-05
2025-10-09 17:18:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 17:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:18:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:18:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:18:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:18:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:18:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=288.201538, avg_loss=0.600420, seen=480, correct=317, accuracy=0.660417
2025-10-09 17:18:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:18:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:18:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=140 reserved=2366MB allocated=2141MB
2025-10-09 17:18:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 140, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.08758044242859, 'train_avg_loss': 0.5840631703535716, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 17:18:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 140, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 288.2015380859375, 'train_avg_loss': 0.6004198710123698, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 17:18:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 140, 'Results_raw': {'train_total': 480, 'train_loss': 288.2015380859375, 'train_avg_loss': 0.6004198710123698, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 17:18:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:18:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:18:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #140, planning to set LR to 1.00e-05
2025-10-09 17:18:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 17:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:18:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:18:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:18:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:18:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:19:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:19:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.516174, avg_loss=0.659409, seen=480, correct=285, accuracy=0.593750
2025-10-09 17:19:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:19:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:19:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:19:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=140 reserved=2330MB allocated=2141MB
2025-10-09 17:19:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 140, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.70770454406738, 'train_avg_loss': 0.6725642045338949, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 17:19:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 140, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.51617431640625, 'train_avg_loss': 0.659408696492513, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 17:19:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 140, 'Results_raw': {'train_total': 480, 'train_loss': 316.51617431640625, 'train_avg_loss': 0.659408696492513, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 17:19:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:19:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:19:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #140, planning to set LR to 1.00e-05
2025-10-09 17:19:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 17:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:19:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:19:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:19:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:19:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:20:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:20:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.569794, avg_loss=0.628270, seen=480, correct=307, accuracy=0.639583
2025-10-09 17:20:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:20:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:20:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:20:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=140 reserved=2418MB allocated=2141MB
2025-10-09 17:20:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 140, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.21496111154556, 'train_avg_loss': 0.6434580092628797, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 17:20:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 140, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.5697937011719, 'train_avg_loss': 0.6282704035441081, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 17:20:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 140, 'Results_raw': {'train_total': 480, 'train_loss': 301.5697937011719, 'train_avg_loss': 0.6282704035441081, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 17:20:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:20:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:20:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #140, planning to set LR to 1.00e-05
2025-10-09 17:20:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 17:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:20:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:20:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:20:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:20:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:21:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:21:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.676880, avg_loss=0.636827, seen=480, correct=311, accuracy=0.647917
2025-10-09 17:21:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:21:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:21:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:21:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=140 reserved=2336MB allocated=2141MB
2025-10-09 17:21:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 140, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.99284583330154, 'train_avg_loss': 0.6249403819441796, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 17:21:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 140, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.6768798828125, 'train_avg_loss': 0.6368268330891927, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 17:21:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 140, 'Results_raw': {'train_total': 480, 'train_loss': 305.6768798828125, 'train_avg_loss': 0.6368268330891927, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 17:21:11 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #141) -------------
2025-10-09 17:21:11 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=141 aidx=1 | s=5 (candidates=23)
2025-10-09 17:21:11 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[44, 35, 2, 25, 14] (from 23)
2025-10-09 17:21:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:21:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:21:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #141, planning to set LR to 1.00e-05
2025-10-09 17:21:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 17:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:21:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:21:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:21:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:21:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:21:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:21:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.179596, avg_loss=0.610791, seen=480, correct=320, accuracy=0.666667
2025-10-09 17:21:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:21:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:21:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:21:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=141 reserved=2330MB allocated=2141MB
2025-10-09 17:21:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 141, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.86523553729057, 'train_avg_loss': 0.6155436294774215, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 17:21:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 141, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.1795959472656, 'train_avg_loss': 0.6107908248901367, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 17:21:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 141, 'Results_raw': {'train_total': 480, 'train_loss': 293.1795959472656, 'train_avg_loss': 0.6107908248901367, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 17:21:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:21:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:21:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #141, planning to set LR to 1.00e-05
2025-10-09 17:21:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 17:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:21:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:21:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:21:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:21:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:22:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:22:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.625336, avg_loss=0.665886, seen=480, correct=295, accuracy=0.614583
2025-10-09 17:22:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:22:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:22:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=141 reserved=2402MB allocated=2141MB
2025-10-09 17:22:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 141, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.27177339792252, 'train_avg_loss': 0.6522647783160209, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 17:22:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 141, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.6253356933594, 'train_avg_loss': 0.665886116027832, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 17:22:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 141, 'Results_raw': {'train_total': 480, 'train_loss': 319.6253356933594, 'train_avg_loss': 0.665886116027832, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 17:22:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:22:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:22:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #141, planning to set LR to 1.00e-05
2025-10-09 17:22:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 17:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:22:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:22:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:22:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:22:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:23:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:23:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=280.793762, avg_loss=0.584987, seen=480, correct=325, accuracy=0.677083
2025-10-09 17:23:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:23:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:23:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:23:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=141 reserved=2402MB allocated=2141MB
2025-10-09 17:23:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 141, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 61.0963978767395, 'train_avg_loss': 0.5091366489728292, 'train_seen': 120, 'train_correct': 88, 'train_acc': 0.7333333333333333}}
2025-10-09 17:23:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 141, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 280.79376220703125, 'train_avg_loss': 0.5849870045979818, 'train_seen': 480, 'train_correct': 325, 'train_acc': 0.6770833333333334}}
2025-10-09 17:23:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 141, 'Results_raw': {'train_total': 480, 'train_loss': 280.79376220703125, 'train_avg_loss': 0.5849870045979818, 'train_seen': 480, 'train_correct': 325, 'train_acc': 0.6770833333333334}}
2025-10-09 17:23:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:23:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:23:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #141, planning to set LR to 1.00e-05
2025-10-09 17:23:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 17:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:23:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:23:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:23:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:23:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:24:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:24:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.361023, avg_loss=0.675752, seen=480, correct=291, accuracy=0.606250
2025-10-09 17:24:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:24:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:24:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:24:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=141 reserved=2330MB allocated=2141MB
2025-10-09 17:24:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 141, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.37780755758286, 'train_avg_loss': 0.6698150629798572, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 17:24:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 141, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.36102294921875, 'train_avg_loss': 0.6757521311442057, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 17:24:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 141, 'Results_raw': {'train_total': 480, 'train_loss': 324.36102294921875, 'train_avg_loss': 0.6757521311442057, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 17:24:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:24:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:24:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #141, planning to set LR to 1.00e-05
2025-10-09 17:24:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 17:24:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:24:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:24:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:24:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:24:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:24:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:24:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=289.201843, avg_loss=0.602504, seen=480, correct=320, accuracy=0.666667
2025-10-09 17:24:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:24:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:24:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:24:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=141 reserved=2330MB allocated=2141MB
2025-10-09 17:24:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 141, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 65.54444134235382, 'train_avg_loss': 0.5462036778529485, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 17:24:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 141, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 289.20184326171875, 'train_avg_loss': 0.6025038401285807, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 17:24:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 141, 'Results_raw': {'train_total': 480, 'train_loss': 289.20184326171875, 'train_avg_loss': 0.6025038401285807, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 17:24:48 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #142) -------------
2025-10-09 17:24:49 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=142 aidx=1 | s=5 (candidates=23)
2025-10-09 17:24:49 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[30, 4, 44, 18, 17] (from 23)
2025-10-09 17:24:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:24:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:24:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #142, planning to set LR to 1.00e-05
2025-10-09 17:24:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 17:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:24:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:24:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:24:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:24:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:25:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:25:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.049805, avg_loss=0.664687, seen=480, correct=280, accuracy=0.583333
2025-10-09 17:25:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:25:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:25:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:25:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=142 reserved=2330MB allocated=2141MB
2025-10-09 17:25:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 142, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.2648736834526, 'train_avg_loss': 0.6605406140287717, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 17:25:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 142, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.0498046875, 'train_avg_loss': 0.6646870930989583, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 17:25:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 142, 'Results_raw': {'train_total': 480, 'train_loss': 319.0498046875, 'train_avg_loss': 0.6646870930989583, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 17:25:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:25:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:25:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #142, planning to set LR to 1.00e-05
2025-10-09 17:25:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 17:25:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:25:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:25:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:25:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:25:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:26:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:26:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=260.554962, avg_loss=0.542823, seen=480, correct=349, accuracy=0.727083
2025-10-09 17:26:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:26:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:26:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=142 reserved=2372MB allocated=2141MB
2025-10-09 17:26:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 142, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.4575725197792, 'train_avg_loss': 0.6121464376648267, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 17:26:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 142, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 260.5549621582031, 'train_avg_loss': 0.5428228378295898, 'train_seen': 480, 'train_correct': 349, 'train_acc': 0.7270833333333333}}
2025-10-09 17:26:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 142, 'Results_raw': {'train_total': 480, 'train_loss': 260.5549621582031, 'train_avg_loss': 0.5428228378295898, 'train_seen': 480, 'train_correct': 349, 'train_acc': 0.7270833333333333}}
2025-10-09 17:26:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:26:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:26:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #142, planning to set LR to 1.00e-05
2025-10-09 17:26:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 17:26:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:26:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:26:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:26:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:26:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:26:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:26:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=296.544891, avg_loss=0.617802, seen=480, correct=321, accuracy=0.668750
2025-10-09 17:26:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:26:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:26:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:26:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=142 reserved=2330MB allocated=2141MB
2025-10-09 17:26:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 142, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.6010377407074, 'train_avg_loss': 0.6300086478392283, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 17:26:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 142, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 296.5448913574219, 'train_avg_loss': 0.6178018569946289, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 17:26:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 142, 'Results_raw': {'train_total': 480, 'train_loss': 296.5448913574219, 'train_avg_loss': 0.6178018569946289, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 17:26:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:27:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:27:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #142, planning to set LR to 1.00e-05
2025-10-09 17:27:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 17:27:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:27:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:27:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:27:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:27:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:27:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:27:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.281128, avg_loss=0.650586, seen=480, correct=298, accuracy=0.620833
2025-10-09 17:27:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:27:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:27:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:27:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=142 reserved=2360MB allocated=2141MB
2025-10-09 17:27:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 142, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.24215722084045, 'train_avg_loss': 0.6686846435070037, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 17:27:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 142, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.2811279296875, 'train_avg_loss': 0.650585683186849, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 17:27:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 142, 'Results_raw': {'train_total': 480, 'train_loss': 312.2811279296875, 'train_avg_loss': 0.650585683186849, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 17:27:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:27:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:27:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #142, planning to set LR to 1.00e-05
2025-10-09 17:27:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 17:27:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:27:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:27:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:27:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:27:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:28:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:28:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=297.444794, avg_loss=0.619677, seen=480, correct=314, accuracy=0.654167
2025-10-09 17:28:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:28:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:28:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:28:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=142 reserved=2366MB allocated=2141MB
2025-10-09 17:28:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 142, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.67755246162415, 'train_avg_loss': 0.5889796038468679, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 17:28:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 142, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 297.4447937011719, 'train_avg_loss': 0.6196766535441081, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 17:28:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 142, 'Results_raw': {'train_total': 480, 'train_loss': 297.4447937011719, 'train_avg_loss': 0.6196766535441081, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 17:28:27 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #143) -------------
2025-10-09 17:28:27 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=143 aidx=1 | s=5 (candidates=23)
2025-10-09 17:28:27 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[4, 38, 53, 18, 35] (from 23)
2025-10-09 17:28:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:28:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:28:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #143, planning to set LR to 1.00e-05
2025-10-09 17:28:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 17:28:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:28:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:28:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:28:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:28:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:29:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:29:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=254.246796, avg_loss=0.529681, seen=480, correct=359, accuracy=0.747917
2025-10-09 17:29:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:29:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:29:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:29:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=143 reserved=2372MB allocated=2141MB
2025-10-09 17:29:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 143, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.40015715360641, 'train_avg_loss': 0.6116679762800534, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 17:29:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 143, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 254.24679565429688, 'train_avg_loss': 0.5296808242797851, 'train_seen': 480, 'train_correct': 359, 'train_acc': 0.7479166666666667}}
2025-10-09 17:29:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 143, 'Results_raw': {'train_total': 480, 'train_loss': 254.24679565429688, 'train_avg_loss': 0.5296808242797851, 'train_seen': 480, 'train_correct': 359, 'train_acc': 0.7479166666666667}}
2025-10-09 17:29:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:29:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:29:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #143, planning to set LR to 1.00e-05
2025-10-09 17:29:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 17:29:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:29:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:29:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:29:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:29:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:29:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:29:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=298.516235, avg_loss=0.621909, seen=480, correct=322, accuracy=0.670833
2025-10-09 17:29:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:29:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:29:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:29:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=143 reserved=2336MB allocated=2141MB
2025-10-09 17:29:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 143, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.4329046010971, 'train_avg_loss': 0.6036075383424759, 'train_seen': 120, 'train_correct': 88, 'train_acc': 0.7333333333333333}}
2025-10-09 17:29:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 143, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 298.5162353515625, 'train_avg_loss': 0.6219088236490885, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 17:29:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 143, 'Results_raw': {'train_total': 480, 'train_loss': 298.5162353515625, 'train_avg_loss': 0.6219088236490885, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 17:29:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:29:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:29:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #143, planning to set LR to 1.00e-05
2025-10-09 17:29:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 17:29:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:29:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:29:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:29:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:29:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:30:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:30:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.034363, avg_loss=0.670905, seen=480, correct=296, accuracy=0.616667
2025-10-09 17:30:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:30:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:30:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:30:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=143 reserved=2332MB allocated=2141MB
2025-10-09 17:30:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 143, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.06605210900307, 'train_avg_loss': 0.6255504342416922, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 17:30:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 143, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.03436279296875, 'train_avg_loss': 0.6709049224853516, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 17:30:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 143, 'Results_raw': {'train_total': 480, 'train_loss': 322.03436279296875, 'train_avg_loss': 0.6709049224853516, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 17:30:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:30:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:30:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #143, planning to set LR to 1.00e-05
2025-10-09 17:30:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 17:30:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:30:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:30:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:30:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:30:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:31:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:31:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=307.389984, avg_loss=0.640396, seen=480, correct=303, accuracy=0.631250
2025-10-09 17:31:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:31:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:31:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=143 reserved=2362MB allocated=2141MB
2025-10-09 17:31:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 143, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.01990872621536, 'train_avg_loss': 0.6668325727184613, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 17:31:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 143, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 307.3899841308594, 'train_avg_loss': 0.6403958002726237, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 17:31:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 143, 'Results_raw': {'train_total': 480, 'train_loss': 307.3899841308594, 'train_avg_loss': 0.6403958002726237, 'train_seen': 480, 'train_correct': 303, 'train_acc': 0.63125}}
2025-10-09 17:31:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:31:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:31:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #143, planning to set LR to 1.00e-05
2025-10-09 17:31:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 17:31:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:31:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:31:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:31:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:31:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:32:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:32:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.308258, avg_loss=0.667309, seen=480, correct=296, accuracy=0.616667
2025-10-09 17:32:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:32:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:32:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=143 reserved=2402MB allocated=2141MB
2025-10-09 17:32:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 143, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.81023371219635, 'train_avg_loss': 0.6567519476016362, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 17:32:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 143, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.3082580566406, 'train_avg_loss': 0.6673088709513346, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 17:32:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 143, 'Results_raw': {'train_total': 480, 'train_loss': 320.3082580566406, 'train_avg_loss': 0.6673088709513346, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 17:32:06 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #144) -------------
2025-10-09 17:32:07 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=144 aidx=1 | s=5 (candidates=23)
2025-10-09 17:32:07 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[38, 7, 53, 14, 52] (from 23)
2025-10-09 17:32:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:32:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:32:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #144, planning to set LR to 1.00e-05
2025-10-09 17:32:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 17:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:32:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:32:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:32:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:32:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:32:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:32:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=295.264038, avg_loss=0.615133, seen=480, correct=321, accuracy=0.668750
2025-10-09 17:32:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:32:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:32:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:32:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=144 reserved=2336MB allocated=2141MB
2025-10-09 17:32:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 144, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.54864627122879, 'train_avg_loss': 0.5962387189269066, 'train_seen': 120, 'train_correct': 87, 'train_acc': 0.725}}
2025-10-09 17:32:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 144, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 295.2640380859375, 'train_avg_loss': 0.6151334126790364, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 17:32:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 144, 'Results_raw': {'train_total': 480, 'train_loss': 295.2640380859375, 'train_avg_loss': 0.6151334126790364, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 17:32:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:32:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:32:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #144, planning to set LR to 1.00e-05
2025-10-09 17:32:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 17:32:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:32:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:32:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:32:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:32:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:33:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:33:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.967896, avg_loss=0.629100, seen=480, correct=315, accuracy=0.656250
2025-10-09 17:33:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:33:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:33:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=144 reserved=2340MB allocated=2141MB
2025-10-09 17:33:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 144, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.68104392290115, 'train_avg_loss': 0.6723420326908429, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 17:33:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 144, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.9678955078125, 'train_avg_loss': 0.6290997823079427, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 17:33:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 144, 'Results_raw': {'train_total': 480, 'train_loss': 301.9678955078125, 'train_avg_loss': 0.6290997823079427, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 17:33:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:33:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:33:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #144, planning to set LR to 1.00e-05
2025-10-09 17:33:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 17:33:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:33:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:33:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:33:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:33:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:34:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:34:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.320190, avg_loss=0.667334, seen=480, correct=301, accuracy=0.627083
2025-10-09 17:34:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:34:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:34:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=144 reserved=2332MB allocated=2141MB
2025-10-09 17:34:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 144, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.04830729961395, 'train_avg_loss': 0.617069227496783, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 17:34:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 144, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.3201904296875, 'train_avg_loss': 0.6673337300618489, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 17:34:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 144, 'Results_raw': {'train_total': 480, 'train_loss': 320.3201904296875, 'train_avg_loss': 0.6673337300618489, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 17:34:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:34:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:34:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #144, planning to set LR to 1.00e-05
2025-10-09 17:34:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 17:34:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:34:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:34:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:34:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:34:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:34:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:34:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=287.103760, avg_loss=0.598133, seen=480, correct=318, accuracy=0.662500
2025-10-09 17:34:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:34:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:34:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:34:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=144 reserved=2330MB allocated=2141MB
2025-10-09 17:34:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 144, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 66.59994488954544, 'train_avg_loss': 0.554999540746212, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 17:34:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 144, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 287.103759765625, 'train_avg_loss': 0.598132832845052, 'train_seen': 480, 'train_correct': 318, 'train_acc': 0.6625}}
2025-10-09 17:34:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 144, 'Results_raw': {'train_total': 480, 'train_loss': 287.103759765625, 'train_avg_loss': 0.598132832845052, 'train_seen': 480, 'train_correct': 318, 'train_acc': 0.6625}}
2025-10-09 17:34:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:34:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:34:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #144, planning to set LR to 1.00e-05
2025-10-09 17:34:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 17:34:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:34:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:34:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:34:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:34:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:35:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:35:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.144897, avg_loss=0.610719, seen=480, correct=328, accuracy=0.683333
2025-10-09 17:35:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:35:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:35:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:35:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=144 reserved=2338MB allocated=2141MB
2025-10-09 17:35:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 144, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.31437319517136, 'train_avg_loss': 0.5942864432930947, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 17:35:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 144, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.1448974609375, 'train_avg_loss': 0.6107185363769532, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 17:35:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 144, 'Results_raw': {'train_total': 480, 'train_loss': 293.1448974609375, 'train_avg_loss': 0.6107185363769532, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 17:35:40 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #145) -------------
2025-10-09 17:35:41 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=145 aidx=1 | s=5 (candidates=23)
2025-10-09 17:35:41 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[7, 30, 4, 24, 49] (from 23)
2025-10-09 17:35:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:35:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:35:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #145, planning to set LR to 1.00e-05
2025-10-09 17:35:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 17:35:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:35:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:35:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:35:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:35:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:36:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:36:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=303.437164, avg_loss=0.632161, seen=480, correct=307, accuracy=0.639583
2025-10-09 17:36:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:36:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:36:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:36:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=145 reserved=2340MB allocated=2141MB
2025-10-09 17:36:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 145, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.1830849647522, 'train_avg_loss': 0.6765257080396017, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 17:36:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 145, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 303.4371643066406, 'train_avg_loss': 0.632160758972168, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 17:36:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 145, 'Results_raw': {'train_total': 480, 'train_loss': 303.4371643066406, 'train_avg_loss': 0.632160758972168, 'train_seen': 480, 'train_correct': 307, 'train_acc': 0.6395833333333333}}
2025-10-09 17:36:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:36:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:36:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #145, planning to set LR to 1.00e-05
2025-10-09 17:36:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 17:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:36:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:36:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:36:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:36:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:37:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:37:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.077515, avg_loss=0.660578, seen=480, correct=281, accuracy=0.585417
2025-10-09 17:37:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:37:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:37:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:37:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=145 reserved=2330MB allocated=2141MB
2025-10-09 17:37:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 145, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.12898361682892, 'train_avg_loss': 0.667741530140241, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 17:37:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 145, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.0775146484375, 'train_avg_loss': 0.6605781555175781, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 17:37:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 145, 'Results_raw': {'train_total': 480, 'train_loss': 317.0775146484375, 'train_avg_loss': 0.6605781555175781, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 17:37:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:37:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:37:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #145, planning to set LR to 1.00e-05
2025-10-09 17:37:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 17:37:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:37:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:37:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:37:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:37:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:37:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:37:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=252.970825, avg_loss=0.527023, seen=480, correct=353, accuracy=0.735417
2025-10-09 17:37:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:37:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:37:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:37:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=145 reserved=2372MB allocated=2141MB
2025-10-09 17:37:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 145, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.95022159814835, 'train_avg_loss': 0.6162518466512362, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 17:37:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 145, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 252.9708251953125, 'train_avg_loss': 0.5270225524902343, 'train_seen': 480, 'train_correct': 353, 'train_acc': 0.7354166666666667}}
2025-10-09 17:37:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 145, 'Results_raw': {'train_total': 480, 'train_loss': 252.9708251953125, 'train_avg_loss': 0.5270225524902343, 'train_seen': 480, 'train_correct': 353, 'train_acc': 0.7354166666666667}}
2025-10-09 17:37:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:37:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:37:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #145, planning to set LR to 1.00e-05
2025-10-09 17:37:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 17:37:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:37:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:37:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:37:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:37:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:38:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:38:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.902252, avg_loss=0.670630, seen=480, correct=284, accuracy=0.591667
2025-10-09 17:38:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:38:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:38:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:38:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=145 reserved=2330MB allocated=2141MB
2025-10-09 17:38:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 145, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.57705807685852, 'train_avg_loss': 0.696475483973821, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 17:38:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 145, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.9022521972656, 'train_avg_loss': 0.6706296920776367, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 17:38:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 145, 'Results_raw': {'train_total': 480, 'train_loss': 321.9022521972656, 'train_avg_loss': 0.6706296920776367, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 17:38:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:38:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:38:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #145, planning to set LR to 1.00e-05
2025-10-09 17:38:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 17:38:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:38:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:38:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:38:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:38:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:39:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:39:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.048645, avg_loss=0.675101, seen=480, correct=279, accuracy=0.581250
2025-10-09 17:39:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:39:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:39:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=145 reserved=2362MB allocated=2141MB
2025-10-09 17:39:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 145, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.28847622871399, 'train_avg_loss': 0.68573730190595, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 17:39:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 145, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.04864501953125, 'train_avg_loss': 0.6751013437906901, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 17:39:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 145, 'Results_raw': {'train_total': 480, 'train_loss': 324.04864501953125, 'train_avg_loss': 0.6751013437906901, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 17:39:23 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #146) -------------
2025-10-09 17:39:23 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=146 aidx=1 | s=5 (candidates=23)
2025-10-09 17:39:23 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[49, 9, 2, 27, 18] (from 23)
2025-10-09 17:39:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:39:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:39:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #146, planning to set LR to 1.00e-05
2025-10-09 17:39:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 17:39:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:39:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:39:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:39:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:39:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:40:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:40:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.482635, avg_loss=0.661422, seen=480, correct=284, accuracy=0.591667
2025-10-09 17:40:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:40:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:40:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:40:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=146 reserved=2362MB allocated=2141MB
2025-10-09 17:40:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 146, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.36205899715424, 'train_avg_loss': 0.6780171583096186, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 17:40:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 146, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.4826354980469, 'train_avg_loss': 0.6614221572875977, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 17:40:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 146, 'Results_raw': {'train_total': 480, 'train_loss': 317.4826354980469, 'train_avg_loss': 0.6614221572875977, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 17:40:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:40:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:40:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #146, planning to set LR to 1.00e-05
2025-10-09 17:40:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 17:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:40:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:40:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:40:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:40:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:40:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:40:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.292389, avg_loss=0.650609, seen=480, correct=291, accuracy=0.606250
2025-10-09 17:40:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:40:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:40:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:40:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=146 reserved=2350MB allocated=2141MB
2025-10-09 17:40:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 146, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.55521285533905, 'train_avg_loss': 0.6629601071278254, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 17:40:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 146, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.2923889160156, 'train_avg_loss': 0.6506091435750325, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 17:40:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 146, 'Results_raw': {'train_total': 480, 'train_loss': 312.2923889160156, 'train_avg_loss': 0.6506091435750325, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 17:40:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:40:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:40:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #146, planning to set LR to 1.00e-05
2025-10-09 17:40:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 17:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:40:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:40:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:40:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:40:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:41:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:41:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=282.640900, avg_loss=0.588835, seen=480, correct=323, accuracy=0.672917
2025-10-09 17:41:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:41:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:41:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=146 reserved=2402MB allocated=2141MB
2025-10-09 17:41:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 146, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 61.81653103232384, 'train_avg_loss': 0.5151377586026986, 'train_seen': 120, 'train_correct': 92, 'train_acc': 0.7666666666666667}}
2025-10-09 17:41:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 146, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 282.6408996582031, 'train_avg_loss': 0.5888352076212565, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 17:41:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 146, 'Results_raw': {'train_total': 480, 'train_loss': 282.6408996582031, 'train_avg_loss': 0.5888352076212565, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 17:41:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:41:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:41:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #146, planning to set LR to 1.00e-05
2025-10-09 17:41:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 17:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:41:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:41:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:41:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:41:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:42:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:42:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=308.004303, avg_loss=0.641676, seen=480, correct=297, accuracy=0.618750
2025-10-09 17:42:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:42:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:42:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=146 reserved=2332MB allocated=2141MB
2025-10-09 17:42:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 146, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.49564176797867, 'train_avg_loss': 0.6457970147331555, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 17:42:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 146, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 308.0043029785156, 'train_avg_loss': 0.6416756312052408, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 17:42:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 146, 'Results_raw': {'train_total': 480, 'train_loss': 308.0043029785156, 'train_avg_loss': 0.6416756312052408, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 17:42:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:42:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:42:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #146, planning to set LR to 1.00e-05
2025-10-09 17:42:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 17:42:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:42:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:42:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:42:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:42:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:42:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:42:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=309.755402, avg_loss=0.645324, seen=480, correct=294, accuracy=0.612500
2025-10-09 17:42:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:42:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:42:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=146 reserved=2360MB allocated=2141MB
2025-10-09 17:42:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 146, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.91668033599854, 'train_avg_loss': 0.6743056694666545, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 17:42:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 146, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 309.7554016113281, 'train_avg_loss': 0.6453237533569336, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 17:42:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 146, 'Results_raw': {'train_total': 480, 'train_loss': 309.7554016113281, 'train_avg_loss': 0.6453237533569336, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 17:42:55 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #147) -------------
2025-10-09 17:42:56 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=147 aidx=1 | s=5 (candidates=23)
2025-10-09 17:42:56 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[24, 38, 49, 30, 42] (from 23)
2025-10-09 17:42:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:42:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:42:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #147, planning to set LR to 1.00e-05
2025-10-09 17:42:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 17:42:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:42:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:42:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:42:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:42:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:43:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:43:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.568054, avg_loss=0.672017, seen=480, correct=280, accuracy=0.583333
2025-10-09 17:43:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:43:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:43:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:43:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=147 reserved=2330MB allocated=2141MB
2025-10-09 17:43:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 147, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.25585913658142, 'train_avg_loss': 0.6937988261381786, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 17:43:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 147, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.56805419921875, 'train_avg_loss': 0.6720167795817057, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 17:43:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 147, 'Results_raw': {'train_total': 480, 'train_loss': 322.56805419921875, 'train_avg_loss': 0.6720167795817057, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 17:43:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:43:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:43:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #147, planning to set LR to 1.00e-05
2025-10-09 17:43:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 17:43:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:43:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:43:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:43:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:43:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:44:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:44:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=299.291901, avg_loss=0.623525, seen=480, correct=316, accuracy=0.658333
2025-10-09 17:44:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:44:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:44:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:44:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=147 reserved=2336MB allocated=2141MB
2025-10-09 17:44:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 147, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.84157940745354, 'train_avg_loss': 0.6153464950621128, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 17:44:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 147, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 299.2919006347656, 'train_avg_loss': 0.623524792989095, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 17:44:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 147, 'Results_raw': {'train_total': 480, 'train_loss': 299.2919006347656, 'train_avg_loss': 0.623524792989095, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 17:44:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:44:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:44:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #147, planning to set LR to 1.00e-05
2025-10-09 17:44:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 17:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:44:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:44:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:44:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:44:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:44:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:44:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.486115, avg_loss=0.661429, seen=480, correct=286, accuracy=0.595833
2025-10-09 17:44:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:44:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:45:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:45:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=147 reserved=2362MB allocated=2141MB
2025-10-09 17:45:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 147, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.40581238269806, 'train_avg_loss': 0.6867151031891505, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 17:45:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 147, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.4861145019531, 'train_avg_loss': 0.6614294052124023, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 17:45:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 147, 'Results_raw': {'train_total': 480, 'train_loss': 317.4861145019531, 'train_avg_loss': 0.6614294052124023, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 17:45:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:45:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:45:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #147, planning to set LR to 1.00e-05
2025-10-09 17:45:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 17:45:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:45:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:45:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:45:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:45:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:45:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:45:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.374725, avg_loss=0.661197, seen=480, correct=296, accuracy=0.616667
2025-10-09 17:45:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:45:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:45:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:45:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=147 reserved=2330MB allocated=2141MB
2025-10-09 17:45:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 147, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.87154757976532, 'train_avg_loss': 0.6822628964980443, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 17:45:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 147, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.3747253417969, 'train_avg_loss': 0.6611973444620768, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 17:45:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 147, 'Results_raw': {'train_total': 480, 'train_loss': 317.3747253417969, 'train_avg_loss': 0.6611973444620768, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 17:45:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:45:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:45:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #147, planning to set LR to 1.00e-05
2025-10-09 17:45:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 17:45:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:45:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:45:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:45:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:45:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:46:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:46:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=298.289093, avg_loss=0.621436, seen=480, correct=315, accuracy=0.656250
2025-10-09 17:46:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:46:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:46:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:46:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=147 reserved=2340MB allocated=2141MB
2025-10-09 17:46:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 147, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.43241286277771, 'train_avg_loss': 0.6119367738564809, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 17:46:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 147, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 298.2890930175781, 'train_avg_loss': 0.6214356104532878, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 17:46:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 147, 'Results_raw': {'train_total': 480, 'train_loss': 298.2890930175781, 'train_avg_loss': 0.6214356104532878, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 17:46:28 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #148) -------------
2025-10-09 17:46:28 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=148 aidx=1 | s=5 (candidates=23)
2025-10-09 17:46:28 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[35, 12, 7, 53, 44] (from 23)
2025-10-09 17:46:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:46:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:46:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #148, planning to set LR to 1.00e-05
2025-10-09 17:46:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 17:46:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:46:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:46:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:46:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:46:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:47:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:47:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.885864, avg_loss=0.658096, seen=480, correct=299, accuracy=0.622917
2025-10-09 17:47:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:47:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:47:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=148 reserved=2402MB allocated=2141MB
2025-10-09 17:47:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 148, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.59736454486847, 'train_avg_loss': 0.6466447045405705, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 17:47:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 148, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.8858642578125, 'train_avg_loss': 0.6580955505371093, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 17:47:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 148, 'Results_raw': {'train_total': 480, 'train_loss': 315.8858642578125, 'train_avg_loss': 0.6580955505371093, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 17:47:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:47:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:47:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #148, planning to set LR to 1.00e-05
2025-10-09 17:47:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 17:47:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:47:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:47:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:47:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:47:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:47:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:47:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=305.380463, avg_loss=0.636209, seen=480, correct=298, accuracy=0.620833
2025-10-09 17:47:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:47:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:47:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:47:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=148 reserved=2330MB allocated=2141MB
2025-10-09 17:47:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 148, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.56002676486969, 'train_avg_loss': 0.6463335563739141, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 17:47:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 148, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 305.3804626464844, 'train_avg_loss': 0.6362092971801758, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 17:47:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 148, 'Results_raw': {'train_total': 480, 'train_loss': 305.3804626464844, 'train_avg_loss': 0.6362092971801758, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 17:47:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:47:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:47:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #148, planning to set LR to 1.00e-05
2025-10-09 17:47:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 17:47:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:47:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:47:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:47:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:47:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:48:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:48:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.773651, avg_loss=0.626612, seen=480, correct=315, accuracy=0.656250
2025-10-09 17:48:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:48:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:48:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:48:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=148 reserved=2340MB allocated=2141MB
2025-10-09 17:48:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 148, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.82155156135559, 'train_avg_loss': 0.6818462630112966, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 17:48:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 148, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.7736511230469, 'train_avg_loss': 0.6266117731730143, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 17:48:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 148, 'Results_raw': {'train_total': 480, 'train_loss': 300.7736511230469, 'train_avg_loss': 0.6266117731730143, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 17:48:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:48:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:48:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #148, planning to set LR to 1.00e-05
2025-10-09 17:48:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 17:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:48:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:48:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:48:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:48:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:49:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:49:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.508575, avg_loss=0.673976, seen=480, correct=298, accuracy=0.620833
2025-10-09 17:49:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:49:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:49:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:49:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=148 reserved=2332MB allocated=2141MB
2025-10-09 17:49:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 148, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.38221979141235, 'train_avg_loss': 0.6198518315951029, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 17:49:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 148, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.5085754394531, 'train_avg_loss': 0.6739761988321941, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 17:49:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 148, 'Results_raw': {'train_total': 480, 'train_loss': 323.5085754394531, 'train_avg_loss': 0.6739761988321941, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 17:49:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:49:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:49:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #148, planning to set LR to 1.00e-05
2025-10-09 17:49:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 17:49:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:49:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:49:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:49:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:49:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:49:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:49:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.407654, avg_loss=0.611266, seen=480, correct=320, accuracy=0.666667
2025-10-09 17:49:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:50:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:50:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=148 reserved=2330MB allocated=2141MB
2025-10-09 17:50:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 148, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.80382442474365, 'train_avg_loss': 0.6150318702061971, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 17:50:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 148, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.40765380859375, 'train_avg_loss': 0.6112659454345704, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 17:50:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 148, 'Results_raw': {'train_total': 480, 'train_loss': 293.40765380859375, 'train_avg_loss': 0.6112659454345704, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 17:50:02 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #149) -------------
2025-10-09 17:50:02 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=149 aidx=1 | s=5 (candidates=23)
2025-10-09 17:50:02 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[2, 25, 17, 33, 7] (from 23)
2025-10-09 17:50:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:50:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:50:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #149, planning to set LR to 1.00e-05
2025-10-09 17:50:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 17:50:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:50:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:50:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:50:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:50:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:50:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:50:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=282.098114, avg_loss=0.587704, seen=480, correct=324, accuracy=0.675000
2025-10-09 17:50:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:50:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:50:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=149 reserved=2402MB allocated=2141MB
2025-10-09 17:50:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 149, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 61.58699581027031, 'train_avg_loss': 0.5132249650855859, 'train_seen': 120, 'train_correct': 91, 'train_acc': 0.7583333333333333}}
2025-10-09 17:50:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 149, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 282.0981140136719, 'train_avg_loss': 0.5877044041951497, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 17:50:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 149, 'Results_raw': {'train_total': 480, 'train_loss': 282.0981140136719, 'train_avg_loss': 0.5877044041951497, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 17:50:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:50:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:50:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #149, planning to set LR to 1.00e-05
2025-10-09 17:50:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 17:50:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:50:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:50:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:50:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:50:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:51:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:51:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.014008, avg_loss=0.681279, seen=480, correct=286, accuracy=0.595833
2025-10-09 17:51:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:51:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:51:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:51:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=149 reserved=2330MB allocated=2141MB
2025-10-09 17:51:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 149, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.44379764795303, 'train_avg_loss': 0.6703649803996086, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 17:51:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 149, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.0140075683594, 'train_avg_loss': 0.681279182434082, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 17:51:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 149, 'Results_raw': {'train_total': 480, 'train_loss': 327.0140075683594, 'train_avg_loss': 0.681279182434082, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 17:51:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:51:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:51:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #149, planning to set LR to 1.00e-05
2025-10-09 17:51:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 17:51:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:51:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:51:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:51:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:51:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:52:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:52:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=280.283264, avg_loss=0.583923, seen=480, correct=334, accuracy=0.695833
2025-10-09 17:52:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:52:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:52:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=149 reserved=2366MB allocated=2141MB
2025-10-09 17:52:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 149, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.77978119254112, 'train_avg_loss': 0.573164843271176, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 17:52:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 149, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 280.28326416015625, 'train_avg_loss': 0.5839234670003255, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 17:52:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 149, 'Results_raw': {'train_total': 480, 'train_loss': 280.28326416015625, 'train_avg_loss': 0.5839234670003255, 'train_seen': 480, 'train_correct': 334, 'train_acc': 0.6958333333333333}}
2025-10-09 17:52:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:52:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:52:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #149, planning to set LR to 1.00e-05
2025-10-09 17:52:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 17:52:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:52:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:52:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:52:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:52:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:52:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:52:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.771942, avg_loss=0.662025, seen=480, correct=291, accuracy=0.606250
2025-10-09 17:52:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:52:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:52:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=149 reserved=2342MB allocated=2141MB
2025-10-09 17:52:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 149, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.09084957838058, 'train_avg_loss': 0.6507570798198382, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 17:52:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 149, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.7719421386719, 'train_avg_loss': 0.6620248794555664, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 17:52:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 149, 'Results_raw': {'train_total': 480, 'train_loss': 317.7719421386719, 'train_avg_loss': 0.6620248794555664, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 17:52:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:52:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:52:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #149, planning to set LR to 1.00e-05
2025-10-09 17:52:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 17:52:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:52:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:52:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:52:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:52:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:53:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:53:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.349792, avg_loss=0.629895, seen=480, correct=314, accuracy=0.654167
2025-10-09 17:53:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:53:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:53:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=149 reserved=2340MB allocated=2141MB
2025-10-09 17:53:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 149, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.96303737163544, 'train_avg_loss': 0.6830253114302953, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 17:53:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 149, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.34979248046875, 'train_avg_loss': 0.6298954010009765, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 17:53:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 149, 'Results_raw': {'train_total': 480, 'train_loss': 302.34979248046875, 'train_avg_loss': 0.6298954010009765, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 17:53:36 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #150) -------------
2025-10-09 17:53:36 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=150 aidx=1 | s=5 (candidates=23)
2025-10-09 17:53:36 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[9, 7, 27, 25, 18] (from 23)
2025-10-09 17:53:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:53:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:53:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #150, planning to set LR to 1.00e-05
2025-10-09 17:53:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 17:53:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:53:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:53:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:53:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:53:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:54:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:54:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.535706, avg_loss=0.651116, seen=480, correct=296, accuracy=0.616667
2025-10-09 17:54:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:54:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:54:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:54:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=150 reserved=2352MB allocated=2141MB
2025-10-09 17:54:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 150, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.41882315278053, 'train_avg_loss': 0.6618235262731711, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 17:54:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 150, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.53570556640625, 'train_avg_loss': 0.6511160532633463, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 17:54:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 150, 'Results_raw': {'train_total': 480, 'train_loss': 312.53570556640625, 'train_avg_loss': 0.6511160532633463, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 17:54:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:54:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:54:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #150, planning to set LR to 1.00e-05
2025-10-09 17:54:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 17:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:54:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:54:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:54:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:54:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:55:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:55:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=294.292908, avg_loss=0.613110, seen=480, correct=324, accuracy=0.675000
2025-10-09 17:55:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:55:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:55:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:55:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=150 reserved=2342MB allocated=2141MB
2025-10-09 17:55:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 150, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.1780132651329, 'train_avg_loss': 0.6681501105427742, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 17:55:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 150, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 294.29290771484375, 'train_avg_loss': 0.6131102244059244, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 17:55:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 150, 'Results_raw': {'train_total': 480, 'train_loss': 294.29290771484375, 'train_avg_loss': 0.6131102244059244, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 17:55:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:55:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:55:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #150, planning to set LR to 1.00e-05
2025-10-09 17:55:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 17:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:55:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:55:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:55:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:55:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:55:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:55:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.313202, avg_loss=0.638153, seen=480, correct=304, accuracy=0.633333
2025-10-09 17:55:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:55:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:55:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:55:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=150 reserved=2332MB allocated=2141MB
2025-10-09 17:55:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 150, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.62373912334442, 'train_avg_loss': 0.6385311593612035, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 17:55:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 150, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.3132019042969, 'train_avg_loss': 0.6381525039672852, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 17:55:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 150, 'Results_raw': {'train_total': 480, 'train_loss': 306.3132019042969, 'train_avg_loss': 0.6381525039672852, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 17:55:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:55:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:55:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #150, planning to set LR to 1.00e-05
2025-10-09 17:55:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 17:55:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:55:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:55:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:55:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:55:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:56:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:56:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.669128, avg_loss=0.670144, seen=480, correct=290, accuracy=0.604167
2025-10-09 17:56:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:56:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:56:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:56:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=150 reserved=2330MB allocated=2141MB
2025-10-09 17:56:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 150, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.0615710914135, 'train_avg_loss': 0.6671797590951125, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 17:56:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 150, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.66912841796875, 'train_avg_loss': 0.6701440175374349, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 17:56:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 150, 'Results_raw': {'train_total': 480, 'train_loss': 321.66912841796875, 'train_avg_loss': 0.6701440175374349, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 17:56:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:56:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:56:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #150, planning to set LR to 1.00e-05
2025-10-09 17:56:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 17:56:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:56:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:56:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:56:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:56:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:57:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:57:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=304.091034, avg_loss=0.633523, seen=480, correct=306, accuracy=0.637500
2025-10-09 17:57:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:57:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:57:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=150 reserved=2360MB allocated=2141MB
2025-10-09 17:57:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 150, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.71318811178207, 'train_avg_loss': 0.664276567598184, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 17:57:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 150, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 304.0910339355469, 'train_avg_loss': 0.6335229873657227, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 17:57:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 150, 'Results_raw': {'train_total': 480, 'train_loss': 304.0910339355469, 'train_avg_loss': 0.6335229873657227, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 17:57:16 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #151) -------------
2025-10-09 17:57:16 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=151 aidx=1 | s=5 (candidates=23)
2025-10-09 17:57:16 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[14, 38, 30, 44, 12] (from 23)
2025-10-09 17:57:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:57:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:57:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #151, planning to set LR to 1.00e-05
2025-10-09 17:57:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 17:57:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:57:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:57:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:57:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:57:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:57:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:57:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=280.662598, avg_loss=0.584714, seen=480, correct=326, accuracy=0.679167
2025-10-09 17:57:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:57:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:57:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:57:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=151 reserved=2330MB allocated=2141MB
2025-10-09 17:57:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 151, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 64.82014495134354, 'train_avg_loss': 0.5401678745945294, 'train_seen': 120, 'train_correct': 87, 'train_acc': 0.725}}
2025-10-09 17:57:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 151, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 280.66259765625, 'train_avg_loss': 0.5847137451171875, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 17:57:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 151, 'Results_raw': {'train_total': 480, 'train_loss': 280.66259765625, 'train_avg_loss': 0.5847137451171875, 'train_seen': 480, 'train_correct': 326, 'train_acc': 0.6791666666666667}}
2025-10-09 17:57:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:58:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:58:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #151, planning to set LR to 1.00e-05
2025-10-09 17:58:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-09 17:58:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:58:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:58:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:58:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:58:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:58:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:58:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.026093, avg_loss=0.629221, seen=480, correct=315, accuracy=0.656250
2025-10-09 17:58:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:58:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:58:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:58:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=151 reserved=2336MB allocated=2141MB
2025-10-09 17:58:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 151, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.29991781711578, 'train_avg_loss': 0.6191659818092982, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 17:58:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 151, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.0260925292969, 'train_avg_loss': 0.6292210261027018, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 17:58:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 151, 'Results_raw': {'train_total': 480, 'train_loss': 302.0260925292969, 'train_avg_loss': 0.6292210261027018, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 17:58:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:58:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:58:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #151, planning to set LR to 1.00e-05
2025-10-09 17:58:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 17:58:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:58:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:58:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:58:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:58:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 17:59:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 17:59:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.672485, avg_loss=0.661818, seen=480, correct=294, accuracy=0.612500
2025-10-09 17:59:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 17:59:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:59:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 17:59:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=151 reserved=2330MB allocated=2141MB
2025-10-09 17:59:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 151, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.67940139770508, 'train_avg_loss': 0.680661678314209, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 17:59:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 151, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.6724853515625, 'train_avg_loss': 0.6618176778157552, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 17:59:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 151, 'Results_raw': {'train_total': 480, 'train_loss': 317.6724853515625, 'train_avg_loss': 0.6618176778157552, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 17:59:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 17:59:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 17:59:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #151, planning to set LR to 1.00e-05
2025-10-09 17:59:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 17:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 17:59:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 17:59:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 17:59:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 17:59:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:00:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:00:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=289.297516, avg_loss=0.602703, seen=480, correct=327, accuracy=0.681250
2025-10-09 18:00:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:00:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:00:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:00:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=151 reserved=2330MB allocated=2141MB
2025-10-09 18:00:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 151, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.16610652208328, 'train_avg_loss': 0.6013842210173607, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 18:00:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 151, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 289.2975158691406, 'train_avg_loss': 0.6027031580607096, 'train_seen': 480, 'train_correct': 327, 'train_acc': 0.68125}}
2025-10-09 18:00:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 151, 'Results_raw': {'train_total': 480, 'train_loss': 289.2975158691406, 'train_avg_loss': 0.6027031580607096, 'train_seen': 480, 'train_correct': 327, 'train_acc': 0.68125}}
2025-10-09 18:00:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:00:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:00:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #151, planning to set LR to 1.00e-05
2025-10-09 18:00:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 18:00:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:00:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:00:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:00:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:00:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:00:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:00:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.119995, avg_loss=0.637750, seen=480, correct=301, accuracy=0.627083
2025-10-09 18:00:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:00:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:00:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:00:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=151 reserved=2330MB allocated=2141MB
2025-10-09 18:00:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 151, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.83971258997917, 'train_avg_loss': 0.6486642715831598, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 18:00:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 151, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.1199951171875, 'train_avg_loss': 0.637749989827474, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 18:00:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 151, 'Results_raw': {'train_total': 480, 'train_loss': 306.1199951171875, 'train_avg_loss': 0.637749989827474, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 18:00:54 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #152) -------------
2025-10-09 18:00:54 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=152 aidx=1 | s=5 (candidates=23)
2025-10-09 18:00:54 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[42, 18, 7, 9, 27] (from 23)
2025-10-09 18:00:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:00:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:00:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #152, planning to set LR to 1.00e-05
2025-10-09 18:00:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 18:00:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:00:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:00:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:00:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:00:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:01:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:01:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=291.859161, avg_loss=0.608040, seen=480, correct=325, accuracy=0.677083
2025-10-09 18:01:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:01:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:01:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:01:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=152 reserved=2340MB allocated=2141MB
2025-10-09 18:01:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 152, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.62372833490372, 'train_avg_loss': 0.5968644027908643, 'train_seen': 120, 'train_correct': 87, 'train_acc': 0.725}}
2025-10-09 18:01:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 152, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 291.8591613769531, 'train_avg_loss': 0.608039919535319, 'train_seen': 480, 'train_correct': 325, 'train_acc': 0.6770833333333334}}
2025-10-09 18:01:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 152, 'Results_raw': {'train_total': 480, 'train_loss': 291.8591613769531, 'train_avg_loss': 0.608039919535319, 'train_seen': 480, 'train_correct': 325, 'train_acc': 0.6770833333333334}}
2025-10-09 18:01:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:01:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:01:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #152, planning to set LR to 1.00e-05
2025-10-09 18:01:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 18:01:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:01:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:01:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:01:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:01:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:02:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:02:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.908234, avg_loss=0.612309, seen=480, correct=315, accuracy=0.656250
2025-10-09 18:02:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:02:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:02:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:02:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=152 reserved=2360MB allocated=2141MB
2025-10-09 18:02:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 152, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.35097342729568, 'train_avg_loss': 0.6529247785607973, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 18:02:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 152, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.9082336425781, 'train_avg_loss': 0.6123088200887045, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 18:02:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 152, 'Results_raw': {'train_total': 480, 'train_loss': 293.9082336425781, 'train_avg_loss': 0.6123088200887045, 'train_seen': 480, 'train_correct': 315, 'train_acc': 0.65625}}
2025-10-09 18:02:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:02:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:02:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #152, planning to set LR to 1.00e-05
2025-10-09 18:02:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 18:02:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:02:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:02:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:02:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:02:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:03:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:03:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=294.799103, avg_loss=0.614165, seen=480, correct=314, accuracy=0.654167
2025-10-09 18:03:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:03:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:03:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:03:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=152 reserved=2340MB allocated=2141MB
2025-10-09 18:03:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 152, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.18768340349197, 'train_avg_loss': 0.6682306950290998, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 18:03:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 152, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 294.7991027832031, 'train_avg_loss': 0.6141647974650065, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 18:03:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 152, 'Results_raw': {'train_total': 480, 'train_loss': 294.7991027832031, 'train_avg_loss': 0.6141647974650065, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 18:03:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:03:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:03:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #152, planning to set LR to 1.00e-05
2025-10-09 18:03:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 18:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:03:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:03:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:03:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:03:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:03:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:03:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.269501, avg_loss=0.648478, seen=480, correct=298, accuracy=0.620833
2025-10-09 18:03:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:03:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:03:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:03:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=152 reserved=2352MB allocated=2141MB
2025-10-09 18:03:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 152, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.11321425437927, 'train_avg_loss': 0.6676101187864939, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 18:03:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 152, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.2695007324219, 'train_avg_loss': 0.6484781265258789, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 18:03:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 152, 'Results_raw': {'train_total': 480, 'train_loss': 311.2695007324219, 'train_avg_loss': 0.6484781265258789, 'train_seen': 480, 'train_correct': 298, 'train_acc': 0.6208333333333333}}
2025-10-09 18:03:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:03:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:03:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #152, planning to set LR to 1.00e-05
2025-10-09 18:03:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 18:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:03:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:03:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:03:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:03:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:04:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:04:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=306.760315, avg_loss=0.639084, seen=480, correct=301, accuracy=0.627083
2025-10-09 18:04:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:04:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:04:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:04:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=152 reserved=2332MB allocated=2141MB
2025-10-09 18:04:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 152, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.12185049057007, 'train_avg_loss': 0.6426820874214172, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 18:04:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 152, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 306.76031494140625, 'train_avg_loss': 0.639083989461263, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 18:04:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 152, 'Results_raw': {'train_total': 480, 'train_loss': 306.76031494140625, 'train_avg_loss': 0.639083989461263, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 18:04:40 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #153) -------------
2025-10-09 18:04:40 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=153 aidx=1 | s=5 (candidates=23)
2025-10-09 18:04:40 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[7, 14, 9, 52, 18] (from 23)
2025-10-09 18:04:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:04:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:04:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #153, planning to set LR to 1.00e-05
2025-10-09 18:04:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 18:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:04:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:04:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:04:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:04:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:05:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:05:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=295.363342, avg_loss=0.615340, seen=480, correct=309, accuracy=0.643750
2025-10-09 18:05:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:05:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:05:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=153 reserved=2340MB allocated=2141MB
2025-10-09 18:05:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 153, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.47736114263535, 'train_avg_loss': 0.6706446761886279, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 18:05:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 153, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 295.36334228515625, 'train_avg_loss': 0.6153402964274088, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 18:05:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 153, 'Results_raw': {'train_total': 480, 'train_loss': 295.36334228515625, 'train_avg_loss': 0.6153402964274088, 'train_seen': 480, 'train_correct': 309, 'train_acc': 0.64375}}
2025-10-09 18:05:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:05:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:05:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #153, planning to set LR to 1.00e-05
2025-10-09 18:05:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 18:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:05:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:05:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:05:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:05:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:06:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:06:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=287.067780, avg_loss=0.598058, seen=480, correct=319, accuracy=0.664583
2025-10-09 18:06:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:06:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:06:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=153 reserved=2330MB allocated=2141MB
2025-10-09 18:06:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 153, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 66.2626143693924, 'train_avg_loss': 0.5521884530782699, 'train_seen': 120, 'train_correct': 84, 'train_acc': 0.7}}
2025-10-09 18:06:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 153, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 287.0677795410156, 'train_avg_loss': 0.5980578740437825, 'train_seen': 480, 'train_correct': 319, 'train_acc': 0.6645833333333333}}
2025-10-09 18:06:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 153, 'Results_raw': {'train_total': 480, 'train_loss': 287.0677795410156, 'train_avg_loss': 0.5980578740437825, 'train_seen': 480, 'train_correct': 319, 'train_acc': 0.6645833333333333}}
2025-10-09 18:06:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:06:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:06:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #153, planning to set LR to 1.00e-05
2025-10-09 18:06:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 18:06:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:06:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:06:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:06:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:06:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:06:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:06:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.110992, avg_loss=0.652315, seen=480, correct=293, accuracy=0.610417
2025-10-09 18:06:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:06:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:06:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=153 reserved=2352MB allocated=2141MB
2025-10-09 18:06:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 153, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.33436095714569, 'train_avg_loss': 0.6694530079762141, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 18:06:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 153, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.1109924316406, 'train_avg_loss': 0.652314567565918, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 18:06:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 153, 'Results_raw': {'train_total': 480, 'train_loss': 313.1109924316406, 'train_avg_loss': 0.652314567565918, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 18:06:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:06:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:06:58 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #153, planning to set LR to 1.00e-05
2025-10-09 18:06:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 18:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:06:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:06:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:06:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:06:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:07:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:07:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=291.187439, avg_loss=0.606640, seen=480, correct=317, accuracy=0.660417
2025-10-09 18:07:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:07:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:07:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:07:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=153 reserved=2338MB allocated=2141MB
2025-10-09 18:07:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 153, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.41313481330872, 'train_avg_loss': 0.5951094567775727, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 18:07:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 153, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 291.18743896484375, 'train_avg_loss': 0.6066404978434244, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 18:07:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 153, 'Results_raw': {'train_total': 480, 'train_loss': 291.18743896484375, 'train_avg_loss': 0.6066404978434244, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 18:07:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:07:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:07:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #153, planning to set LR to 1.00e-05
2025-10-09 18:07:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 18:07:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:07:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:07:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:07:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:07:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:08:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:08:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=296.945129, avg_loss=0.618636, seen=480, correct=314, accuracy=0.654167
2025-10-09 18:08:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:08:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:08:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=153 reserved=2360MB allocated=2141MB
2025-10-09 18:08:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 153, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.82325184345245, 'train_avg_loss': 0.6568604320287704, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 18:08:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 153, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 296.94512939453125, 'train_avg_loss': 0.6186356862386068, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 18:08:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 153, 'Results_raw': {'train_total': 480, 'train_loss': 296.94512939453125, 'train_avg_loss': 0.6186356862386068, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 18:08:26 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #154) -------------
2025-10-09 18:08:26 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=154 aidx=1 | s=5 (candidates=23)
2025-10-09 18:08:26 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[4, 52, 42, 44, 33] (from 23)
2025-10-09 18:08:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:08:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:08:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #154, planning to set LR to 1.00e-05
2025-10-09 18:08:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 18:08:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:08:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:08:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:08:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:08:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:09:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:09:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=260.113312, avg_loss=0.541903, seen=480, correct=333, accuracy=0.693750
2025-10-09 18:09:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:09:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:09:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:09:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=154 reserved=2372MB allocated=2141MB
2025-10-09 18:09:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 154, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.04488033056259, 'train_avg_loss': 0.6253740027546882, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 18:09:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 154, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 260.1133117675781, 'train_avg_loss': 0.541902732849121, 'train_seen': 480, 'train_correct': 333, 'train_acc': 0.69375}}
2025-10-09 18:09:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 154, 'Results_raw': {'train_total': 480, 'train_loss': 260.1133117675781, 'train_avg_loss': 0.541902732849121, 'train_seen': 480, 'train_correct': 333, 'train_acc': 0.69375}}
2025-10-09 18:09:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:09:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:09:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #154, planning to set LR to 1.00e-05
2025-10-09 18:09:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 18:09:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:09:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:09:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:09:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:09:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:09:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:09:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.921600, avg_loss=0.612337, seen=480, correct=314, accuracy=0.654167
2025-10-09 18:09:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:09:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:09:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:09:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=154 reserved=2338MB allocated=2141MB
2025-10-09 18:09:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 154, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.63400658965111, 'train_avg_loss': 0.5969500549137592, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 18:09:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 154, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.9216003417969, 'train_avg_loss': 0.6123366673787435, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 18:09:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 154, 'Results_raw': {'train_total': 480, 'train_loss': 293.9216003417969, 'train_avg_loss': 0.6123366673787435, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 18:09:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:09:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:09:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #154, planning to set LR to 1.00e-05
2025-10-09 18:09:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 18:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:09:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:09:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:09:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:09:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:10:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:10:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=290.341705, avg_loss=0.604879, seen=480, correct=327, accuracy=0.681250
2025-10-09 18:10:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:10:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:10:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:10:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=154 reserved=2340MB allocated=2141MB
2025-10-09 18:10:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 154, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.49880173802376, 'train_avg_loss': 0.5958233478168646, 'train_seen': 120, 'train_correct': 86, 'train_acc': 0.7166666666666667}}
2025-10-09 18:10:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 154, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 290.3417053222656, 'train_avg_loss': 0.60487855275472, 'train_seen': 480, 'train_correct': 327, 'train_acc': 0.68125}}
2025-10-09 18:10:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 154, 'Results_raw': {'train_total': 480, 'train_loss': 290.3417053222656, 'train_avg_loss': 0.60487855275472, 'train_seen': 480, 'train_correct': 327, 'train_acc': 0.68125}}
2025-10-09 18:10:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:10:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:10:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #154, planning to set LR to 1.00e-05
2025-10-09 18:10:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 18:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:10:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:10:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:10:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:10:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:11:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:11:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=288.675293, avg_loss=0.601407, seen=480, correct=323, accuracy=0.672917
2025-10-09 18:11:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:11:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:11:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:11:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=154 reserved=2330MB allocated=2141MB
2025-10-09 18:11:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 154, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.95920279622078, 'train_avg_loss': 0.5996600233018399, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 18:11:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 154, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 288.67529296875, 'train_avg_loss': 0.6014068603515625, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 18:11:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 154, 'Results_raw': {'train_total': 480, 'train_loss': 288.67529296875, 'train_avg_loss': 0.6014068603515625, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 18:11:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:11:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:11:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #154, planning to set LR to 1.00e-05
2025-10-09 18:11:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 18:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:11:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:11:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:11:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:11:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:11:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:11:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.043365, avg_loss=0.656340, seen=480, correct=291, accuracy=0.606250
2025-10-09 18:11:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:12:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:12:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=154 reserved=2342MB allocated=2141MB
2025-10-09 18:12:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 154, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.19411420822144, 'train_avg_loss': 0.6516176184018453, 'train_seen': 120, 'train_correct': 81, 'train_acc': 0.675}}
2025-10-09 18:12:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 154, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.0433654785156, 'train_avg_loss': 0.6563403447469075, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 18:12:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 154, 'Results_raw': {'train_total': 480, 'train_loss': 315.0433654785156, 'train_avg_loss': 0.6563403447469075, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 18:12:03 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #155) -------------
2025-10-09 18:12:03 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=155 aidx=1 | s=5 (candidates=23)
2025-10-09 18:12:03 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[30, 42, 27, 25, 35] (from 23)
2025-10-09 18:12:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:12:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:12:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #155, planning to set LR to 1.00e-05
2025-10-09 18:12:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 18:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:12:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:12:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:12:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:12:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:12:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:12:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.531738, avg_loss=0.653191, seen=480, correct=294, accuracy=0.612500
2025-10-09 18:12:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:12:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:12:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:12:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=155 reserved=2330MB allocated=2141MB
2025-10-09 18:12:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 155, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.36246591806412, 'train_avg_loss': 0.6696872159838676, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 18:12:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 155, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.53173828125, 'train_avg_loss': 0.6531911214192708, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 18:12:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 155, 'Results_raw': {'train_total': 480, 'train_loss': 313.53173828125, 'train_avg_loss': 0.6531911214192708, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 18:12:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:12:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:12:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #155, planning to set LR to 1.00e-05
2025-10-09 18:12:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 18:12:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:12:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:12:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:12:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:12:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:13:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:13:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=291.907898, avg_loss=0.608141, seen=480, correct=328, accuracy=0.683333
2025-10-09 18:13:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:13:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:13:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:13:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=155 reserved=2340MB allocated=2141MB
2025-10-09 18:13:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 155, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 71.32070201635361, 'train_avg_loss': 0.5943391834696133, 'train_seen': 120, 'train_correct': 88, 'train_acc': 0.7333333333333333}}
2025-10-09 18:13:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 155, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 291.90789794921875, 'train_avg_loss': 0.6081414540608724, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 18:13:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 155, 'Results_raw': {'train_total': 480, 'train_loss': 291.90789794921875, 'train_avg_loss': 0.6081414540608724, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 18:13:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:13:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:13:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #155, planning to set LR to 1.00e-05
2025-10-09 18:13:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 18:13:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:13:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:13:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:13:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:13:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:14:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:14:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=299.173065, avg_loss=0.623277, seen=480, correct=321, accuracy=0.668750
2025-10-09 18:14:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:14:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:14:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=155 reserved=2332MB allocated=2141MB
2025-10-09 18:14:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 155, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.71117633581161, 'train_avg_loss': 0.6309264694650968, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 18:14:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 155, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 299.1730651855469, 'train_avg_loss': 0.623277219136556, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 18:14:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 155, 'Results_raw': {'train_total': 480, 'train_loss': 299.1730651855469, 'train_avg_loss': 0.623277219136556, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 18:14:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:14:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:14:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #155, planning to set LR to 1.00e-05
2025-10-09 18:14:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 18:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:14:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:14:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:14:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:14:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:14:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:14:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.345032, avg_loss=0.675719, seen=480, correct=283, accuracy=0.589583
2025-10-09 18:14:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:14:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:14:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:14:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=155 reserved=2330MB allocated=2141MB
2025-10-09 18:14:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 155, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.04584038257599, 'train_avg_loss': 0.6670486698547999, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 18:14:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 155, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.34503173828125, 'train_avg_loss': 0.6757188161214193, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 18:14:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 155, 'Results_raw': {'train_total': 480, 'train_loss': 324.34503173828125, 'train_avg_loss': 0.6757188161214193, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 18:14:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:14:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:14:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #155, planning to set LR to 1.00e-05
2025-10-09 18:14:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-09 18:14:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:14:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:14:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:14:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:14:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:15:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:15:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.976013, avg_loss=0.654117, seen=480, correct=295, accuracy=0.614583
2025-10-09 18:15:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:15:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:15:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:15:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=155 reserved=2402MB allocated=2141MB
2025-10-09 18:15:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 155, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.02084964513779, 'train_avg_loss': 0.6501737470428149, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 18:15:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 155, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.97601318359375, 'train_avg_loss': 0.654116694132487, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 18:15:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 155, 'Results_raw': {'train_total': 480, 'train_loss': 313.97601318359375, 'train_avg_loss': 0.654116694132487, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 18:15:41 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #156) -------------
2025-10-09 18:15:42 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=156 aidx=1 | s=5 (candidates=23)
2025-10-09 18:15:42 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[4, 18, 11, 44, 25] (from 23)
2025-10-09 18:15:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:15:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:15:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #156, planning to set LR to 1.00e-05
2025-10-09 18:15:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 18:15:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:15:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:15:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:15:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:15:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:16:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:16:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=262.993134, avg_loss=0.547902, seen=480, correct=342, accuracy=0.712500
2025-10-09 18:16:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:16:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:16:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:16:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=156 reserved=2372MB allocated=2141MB
2025-10-09 18:16:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 156, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.29857632517815, 'train_avg_loss': 0.6108214693764845, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 18:16:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 156, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 262.9931335449219, 'train_avg_loss': 0.5479023615519206, 'train_seen': 480, 'train_correct': 342, 'train_acc': 0.7125}}
2025-10-09 18:16:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 156, 'Results_raw': {'train_total': 480, 'train_loss': 262.9931335449219, 'train_avg_loss': 0.5479023615519206, 'train_seen': 480, 'train_correct': 342, 'train_acc': 0.7125}}
2025-10-09 18:16:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:16:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:16:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #156, planning to set LR to 1.00e-05
2025-10-09 18:16:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 18:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:16:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:16:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:16:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:16:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:17:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:17:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=291.434998, avg_loss=0.607156, seen=480, correct=321, accuracy=0.668750
2025-10-09 18:17:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:17:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:17:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:17:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=156 reserved=2360MB allocated=2141MB
2025-10-09 18:17:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 156, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.73596769571304, 'train_avg_loss': 0.6394663974642754, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 18:17:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 156, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 291.43499755859375, 'train_avg_loss': 0.607156244913737, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 18:17:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 156, 'Results_raw': {'train_total': 480, 'train_loss': 291.43499755859375, 'train_avg_loss': 0.607156244913737, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 18:17:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:17:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:17:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #156, planning to set LR to 1.00e-05
2025-10-09 18:17:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 18:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:17:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:17:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:17:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:17:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:17:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:17:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.303558, avg_loss=0.625632, seen=480, correct=306, accuracy=0.637500
2025-10-09 18:17:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:17:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:17:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:17:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=156 reserved=2418MB allocated=2141MB
2025-10-09 18:17:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 156, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.16717681288719, 'train_avg_loss': 0.6513931401073932, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 18:17:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 156, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.3035583496094, 'train_avg_loss': 0.6256324132283528, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 18:17:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 156, 'Results_raw': {'train_total': 480, 'train_loss': 300.3035583496094, 'train_avg_loss': 0.6256324132283528, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 18:17:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:17:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:17:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #156, planning to set LR to 1.00e-05
2025-10-09 18:17:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 18:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:17:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:17:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:17:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:17:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:18:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:18:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=289.761597, avg_loss=0.603670, seen=480, correct=314, accuracy=0.654167
2025-10-09 18:18:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:18:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:18:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:18:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=156 reserved=2330MB allocated=2141MB
2025-10-09 18:18:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 156, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.80512988567352, 'train_avg_loss': 0.6150427490472794, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 18:18:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 156, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 289.7615966796875, 'train_avg_loss': 0.6036699930826823, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 18:18:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 156, 'Results_raw': {'train_total': 480, 'train_loss': 289.7615966796875, 'train_avg_loss': 0.6036699930826823, 'train_seen': 480, 'train_correct': 314, 'train_acc': 0.6541666666666667}}
2025-10-09 18:18:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:18:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:18:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #156, planning to set LR to 1.00e-05
2025-10-09 18:18:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 18:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:18:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:18:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:18:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:18:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:19:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:19:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.163361, avg_loss=0.673257, seen=480, correct=283, accuracy=0.589583
2025-10-09 18:19:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:19:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:19:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:19:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=156 reserved=2330MB allocated=2141MB
2025-10-09 18:19:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 156, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.39321708679199, 'train_avg_loss': 0.6699434757232666, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 18:19:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 156, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.1633605957031, 'train_avg_loss': 0.6732570012410481, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 18:19:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 156, 'Results_raw': {'train_total': 480, 'train_loss': 323.1633605957031, 'train_avg_loss': 0.6732570012410481, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 18:19:18 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #157) -------------
2025-10-09 18:19:19 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=157 aidx=1 | s=5 (candidates=23)
2025-10-09 18:19:19 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[33, 12, 44, 2, 53] (from 23)
2025-10-09 18:19:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:19:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:19:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #157, planning to set LR to 1.00e-05
2025-10-09 18:19:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 18:19:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:19:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:19:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:19:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:19:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:20:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:20:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.425446, avg_loss=0.652970, seen=480, correct=289, accuracy=0.602083
2025-10-09 18:20:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:20:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:20:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:20:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=157 reserved=2342MB allocated=2141MB
2025-10-09 18:20:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 157, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.64559888839722, 'train_avg_loss': 0.6470466574033101, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 18:20:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 157, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.4254455566406, 'train_avg_loss': 0.6529696782430013, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 18:20:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 157, 'Results_raw': {'train_total': 480, 'train_loss': 313.4254455566406, 'train_avg_loss': 0.6529696782430013, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 18:20:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:20:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:20:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #157, planning to set LR to 1.00e-05
2025-10-09 18:20:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 18:20:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:20:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:20:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:20:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:20:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:20:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.753937, avg_loss=0.626571, seen=480, correct=311, accuracy=0.647917
2025-10-09 18:20:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:20:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:20:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=157 reserved=2330MB allocated=2141MB
2025-10-09 18:20:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 157, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.57105621695518, 'train_avg_loss': 0.6380921351412933, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 18:20:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 157, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.7539367675781, 'train_avg_loss': 0.6265707015991211, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 18:20:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 157, 'Results_raw': {'train_total': 480, 'train_loss': 300.7539367675781, 'train_avg_loss': 0.6265707015991211, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 18:20:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:20:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:20:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #157, planning to set LR to 1.00e-05
2025-10-09 18:20:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 18:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:20:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:20:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:20:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:20:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:21:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:21:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=292.129364, avg_loss=0.608603, seen=480, correct=321, accuracy=0.668750
2025-10-09 18:21:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:21:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:21:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:21:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=157 reserved=2330MB allocated=2141MB
2025-10-09 18:21:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 157, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.897266715765, 'train_avg_loss': 0.6241438892980417, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 18:21:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 157, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 292.1293640136719, 'train_avg_loss': 0.6086028416951498, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 18:21:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 157, 'Results_raw': {'train_total': 480, 'train_loss': 292.1293640136719, 'train_avg_loss': 0.6086028416951498, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 18:21:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:21:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:21:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #157, planning to set LR to 1.00e-05
2025-10-09 18:21:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 18:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:21:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:21:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:21:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:21:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:22:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:22:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=283.666534, avg_loss=0.590972, seen=480, correct=328, accuracy=0.683333
2025-10-09 18:22:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:22:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:22:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:22:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=157 reserved=2402MB allocated=2141MB
2025-10-09 18:22:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 157, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 63.94615525007248, 'train_avg_loss': 0.5328846270839374, 'train_seen': 120, 'train_correct': 93, 'train_acc': 0.775}}
2025-10-09 18:22:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 157, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 283.6665344238281, 'train_avg_loss': 0.5909719467163086, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 18:22:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 157, 'Results_raw': {'train_total': 480, 'train_loss': 283.6665344238281, 'train_avg_loss': 0.5909719467163086, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 18:22:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:22:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:22:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #157, planning to set LR to 1.00e-05
2025-10-09 18:22:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 18:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:22:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:22:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:22:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:22:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:22:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:22:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.473755, avg_loss=0.665570, seen=480, correct=297, accuracy=0.618750
2025-10-09 18:22:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:22:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:22:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:22:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=157 reserved=2332MB allocated=2141MB
2025-10-09 18:22:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 157, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.7979648411274, 'train_avg_loss': 0.6233163736760616, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 18:22:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 157, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.4737548828125, 'train_avg_loss': 0.665570322672526, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 18:22:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 157, 'Results_raw': {'train_total': 480, 'train_loss': 319.4737548828125, 'train_avg_loss': 0.665570322672526, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 18:22:56 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #158) -------------
2025-10-09 18:22:57 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=158 aidx=1 | s=5 (candidates=23)
2025-10-09 18:22:57 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[11, 14, 27, 33, 17] (from 23)
2025-10-09 18:22:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:22:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:22:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #158, planning to set LR to 1.00e-05
2025-10-09 18:22:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 18:22:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:22:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:22:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:22:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:22:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:23:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:23:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.398712, avg_loss=0.627914, seen=480, correct=312, accuracy=0.650000
2025-10-09 18:23:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:23:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:23:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=158 reserved=2418MB allocated=2141MB
2025-10-09 18:23:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 158, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.59098836779594, 'train_avg_loss': 0.6382582363982995, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 18:23:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 158, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.3987121582031, 'train_avg_loss': 0.6279139836629232, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 18:23:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 158, 'Results_raw': {'train_total': 480, 'train_loss': 301.3987121582031, 'train_avg_loss': 0.6279139836629232, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 18:23:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:23:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:23:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #158, planning to set LR to 1.00e-05
2025-10-09 18:23:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 18:23:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:23:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:23:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:23:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:23:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:24:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:24:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=292.407043, avg_loss=0.609181, seen=480, correct=312, accuracy=0.650000
2025-10-09 18:24:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:24:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:24:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=158 reserved=2330MB allocated=2141MB
2025-10-09 18:24:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 158, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.51435393095016, 'train_avg_loss': 0.579286282757918, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 18:24:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 158, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 292.40704345703125, 'train_avg_loss': 0.6091813405354818, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 18:24:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 158, 'Results_raw': {'train_total': 480, 'train_loss': 292.40704345703125, 'train_avg_loss': 0.6091813405354818, 'train_seen': 480, 'train_correct': 312, 'train_acc': 0.65}}
2025-10-09 18:24:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:24:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:24:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #158, planning to set LR to 1.00e-05
2025-10-09 18:24:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 18:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:24:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:24:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:24:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:24:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:25:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:25:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=301.579895, avg_loss=0.628291, seen=480, correct=311, accuracy=0.647917
2025-10-09 18:25:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:25:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:25:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:25:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=158 reserved=2332MB allocated=2141MB
2025-10-09 18:25:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 158, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.30191498994827, 'train_avg_loss': 0.6358492915829023, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 18:25:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 158, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 301.57989501953125, 'train_avg_loss': 0.6282914479573568, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 18:25:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 158, 'Results_raw': {'train_total': 480, 'train_loss': 301.57989501953125, 'train_avg_loss': 0.6282914479573568, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 18:25:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:25:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:25:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #158, planning to set LR to 1.00e-05
2025-10-09 18:25:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 18:25:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:25:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:25:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:25:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:25:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:25:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:25:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.324219, avg_loss=0.646509, seen=480, correct=300, accuracy=0.625000
2025-10-09 18:25:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:25:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:25:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:25:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=158 reserved=2342MB allocated=2141MB
2025-10-09 18:25:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 158, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.8348258137703, 'train_avg_loss': 0.6402902151147525, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 18:25:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 158, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.32421875, 'train_avg_loss': 0.6465087890625, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 18:25:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 158, 'Results_raw': {'train_total': 480, 'train_loss': 310.32421875, 'train_avg_loss': 0.6465087890625, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 18:25:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:25:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:25:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #158, planning to set LR to 1.00e-05
2025-10-09 18:25:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 18:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:25:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:25:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:25:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:25:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:26:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:26:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=284.348206, avg_loss=0.592392, seen=480, correct=328, accuracy=0.683333
2025-10-09 18:26:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:26:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:26:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:26:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=158 reserved=2366MB allocated=2141MB
2025-10-09 18:26:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 158, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 70.24652606248856, 'train_avg_loss': 0.5853877171874047, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 18:26:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 158, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 284.34820556640625, 'train_avg_loss': 0.592392094930013, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 18:26:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 158, 'Results_raw': {'train_total': 480, 'train_loss': 284.34820556640625, 'train_avg_loss': 0.592392094930013, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 18:26:37 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #159) -------------
2025-10-09 18:26:37 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=159 aidx=1 | s=5 (candidates=23)
2025-10-09 18:26:37 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[14, 4, 30, 12, 49] (from 23)
2025-10-09 18:26:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:26:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:26:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #159, planning to set LR to 1.00e-05
2025-10-09 18:26:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 18:26:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:26:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:26:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:26:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:26:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:27:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:27:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=289.692017, avg_loss=0.603525, seen=480, correct=322, accuracy=0.670833
2025-10-09 18:27:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:27:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:27:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:27:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=159 reserved=2330MB allocated=2141MB
2025-10-09 18:27:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 159, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.85359448194504, 'train_avg_loss': 0.5737799540162086, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 18:27:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 159, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 289.6920166015625, 'train_avg_loss': 0.6035250345865886, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 18:27:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 159, 'Results_raw': {'train_total': 480, 'train_loss': 289.6920166015625, 'train_avg_loss': 0.6035250345865886, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 18:27:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:27:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:27:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #159, planning to set LR to 1.00e-05
2025-10-09 18:27:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 18:27:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:27:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:27:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:27:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:27:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:28:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:28:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=259.659485, avg_loss=0.540957, seen=480, correct=338, accuracy=0.704167
2025-10-09 18:28:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:28:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:28:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:28:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=159 reserved=2372MB allocated=2141MB
2025-10-09 18:28:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 159, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.47382274270058, 'train_avg_loss': 0.6289485228558381, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 18:28:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 159, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 259.65948486328125, 'train_avg_loss': 0.540957260131836, 'train_seen': 480, 'train_correct': 338, 'train_acc': 0.7041666666666667}}
2025-10-09 18:28:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 159, 'Results_raw': {'train_total': 480, 'train_loss': 259.65948486328125, 'train_avg_loss': 0.540957260131836, 'train_seen': 480, 'train_correct': 338, 'train_acc': 0.7041666666666667}}
2025-10-09 18:28:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:28:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:28:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #159, planning to set LR to 1.00e-05
2025-10-09 18:28:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-09 18:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:28:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:28:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:28:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:28:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:28:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:28:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.073242, avg_loss=0.654319, seen=480, correct=288, accuracy=0.600000
2025-10-09 18:28:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:28:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:28:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:28:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=159 reserved=2330MB allocated=2141MB
2025-10-09 18:28:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 159, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.36003249883652, 'train_avg_loss': 0.661333604156971, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 18:28:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 159, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.0732421875, 'train_avg_loss': 0.6543192545572917, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 18:28:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 159, 'Results_raw': {'train_total': 480, 'train_loss': 314.0732421875, 'train_avg_loss': 0.6543192545572917, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 18:28:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:28:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:28:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #159, planning to set LR to 1.00e-05
2025-10-09 18:28:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 18:28:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:28:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:28:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:28:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:28:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:29:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:29:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=299.320526, avg_loss=0.623584, seen=480, correct=306, accuracy=0.637500
2025-10-09 18:29:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:29:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:29:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=159 reserved=2330MB allocated=2141MB
2025-10-09 18:29:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 159, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.42556571960449, 'train_avg_loss': 0.6285463809967041, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 18:29:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 159, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 299.3205261230469, 'train_avg_loss': 0.6235844294230143, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 18:29:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 159, 'Results_raw': {'train_total': 480, 'train_loss': 299.3205261230469, 'train_avg_loss': 0.6235844294230143, 'train_seen': 480, 'train_correct': 306, 'train_acc': 0.6375}}
2025-10-09 18:29:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:29:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:29:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #159, planning to set LR to 1.00e-05
2025-10-09 18:29:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 18:29:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:29:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:29:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:29:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:29:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:30:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:30:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.583069, avg_loss=0.672048, seen=480, correct=282, accuracy=0.587500
2025-10-09 18:30:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:30:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:30:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:30:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=159 reserved=2362MB allocated=2141MB
2025-10-09 18:30:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 159, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.04369550943375, 'train_avg_loss': 0.6920307959119479, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 18:30:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 159, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.58306884765625, 'train_avg_loss': 0.6720480600992839, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 18:30:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 159, 'Results_raw': {'train_total': 480, 'train_loss': 322.58306884765625, 'train_avg_loss': 0.6720480600992839, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 18:30:15 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #160) -------------
2025-10-09 18:30:16 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=160 aidx=1 | s=5 (candidates=23)
2025-10-09 18:30:16 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[9, 4, 49, 44, 2] (from 23)
2025-10-09 18:30:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:30:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:30:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #160, planning to set LR to 1.00e-05
2025-10-09 18:30:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 18:30:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:30:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:30:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:30:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:30:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:30:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:30:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.492859, avg_loss=0.651027, seen=480, correct=291, accuracy=0.606250
2025-10-09 18:30:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:31:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:31:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=160 reserved=2352MB allocated=2141MB
2025-10-09 18:31:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 160, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.11150550842285, 'train_avg_loss': 0.6759292125701905, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 18:31:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 160, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.49285888671875, 'train_avg_loss': 0.6510267893473307, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 18:31:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 160, 'Results_raw': {'train_total': 480, 'train_loss': 312.49285888671875, 'train_avg_loss': 0.6510267893473307, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 18:31:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:31:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:31:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #160, planning to set LR to 1.00e-05
2025-10-09 18:31:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 18:31:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:31:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:31:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:31:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:31:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:31:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:31:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=259.476868, avg_loss=0.540577, seen=480, correct=349, accuracy=0.727083
2025-10-09 18:31:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:31:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:31:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:31:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=160 reserved=2372MB allocated=2141MB
2025-10-09 18:31:44 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 160, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.58027690649033, 'train_avg_loss': 0.6131689742207527, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 18:31:44 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 160, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 259.47686767578125, 'train_avg_loss': 0.5405768076578776, 'train_seen': 480, 'train_correct': 349, 'train_acc': 0.7270833333333333}}
2025-10-09 18:31:44 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 160, 'Results_raw': {'train_total': 480, 'train_loss': 259.47686767578125, 'train_avg_loss': 0.5405768076578776, 'train_seen': 480, 'train_correct': 349, 'train_acc': 0.7270833333333333}}
2025-10-09 18:31:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:31:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:31:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #160, planning to set LR to 1.00e-05
2025-10-09 18:31:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 18:31:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:31:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:31:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:31:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:31:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:32:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:32:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.636414, avg_loss=0.653409, seen=480, correct=290, accuracy=0.604167
2025-10-09 18:32:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:32:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:32:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:32:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=160 reserved=2362MB allocated=2141MB
2025-10-09 18:32:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 160, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.97059243917465, 'train_avg_loss': 0.6664216036597888, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 18:32:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 160, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.63641357421875, 'train_avg_loss': 0.653409194946289, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 18:32:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 160, 'Results_raw': {'train_total': 480, 'train_loss': 313.63641357421875, 'train_avg_loss': 0.653409194946289, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 18:32:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:32:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:32:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #160, planning to set LR to 1.00e-05
2025-10-09 18:32:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 18:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:32:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:32:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:32:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:32:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:33:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:33:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=288.815887, avg_loss=0.601700, seen=480, correct=320, accuracy=0.666667
2025-10-09 18:33:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:33:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:33:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:33:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=160 reserved=2330MB allocated=2141MB
2025-10-09 18:33:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 160, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.2401475906372, 'train_avg_loss': 0.6103345632553101, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 18:33:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 160, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 288.8158874511719, 'train_avg_loss': 0.6016997655232748, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 18:33:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 160, 'Results_raw': {'train_total': 480, 'train_loss': 288.8158874511719, 'train_avg_loss': 0.6016997655232748, 'train_seen': 480, 'train_correct': 320, 'train_acc': 0.6666666666666666}}
2025-10-09 18:33:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:33:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:33:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #160, planning to set LR to 1.00e-05
2025-10-09 18:33:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 18:33:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:33:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:33:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:33:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:33:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:33:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:33:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=283.405670, avg_loss=0.590428, seen=480, correct=328, accuracy=0.683333
2025-10-09 18:33:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:33:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:33:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:33:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=160 reserved=2402MB allocated=2141MB
2025-10-09 18:33:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 160, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 62.44314122200012, 'train_avg_loss': 0.5203595101833344, 'train_seen': 120, 'train_correct': 91, 'train_acc': 0.7583333333333333}}
2025-10-09 18:33:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 160, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 283.4056701660156, 'train_avg_loss': 0.5904284795125325, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 18:33:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 160, 'Results_raw': {'train_total': 480, 'train_loss': 283.4056701660156, 'train_avg_loss': 0.5904284795125325, 'train_seen': 480, 'train_correct': 328, 'train_acc': 0.6833333333333333}}
2025-10-09 18:33:54 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #161) -------------
2025-10-09 18:33:54 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=161 aidx=1 | s=5 (candidates=23)
2025-10-09 18:33:54 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[27, 33, 8, 53, 42] (from 23)
2025-10-09 18:33:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:33:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:33:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #161, planning to set LR to 1.00e-05
2025-10-09 18:33:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 18:33:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:33:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:33:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:33:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:33:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:34:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:34:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=300.386566, avg_loss=0.625805, seen=480, correct=313, accuracy=0.652083
2025-10-09 18:34:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:34:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:34:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:34:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=161 reserved=2332MB allocated=2141MB
2025-10-09 18:34:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 161, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.03100669384003, 'train_avg_loss': 0.6335917224486669, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 18:34:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 161, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 300.3865661621094, 'train_avg_loss': 0.6258053461710612, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 18:34:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 161, 'Results_raw': {'train_total': 480, 'train_loss': 300.3865661621094, 'train_avg_loss': 0.6258053461710612, 'train_seen': 480, 'train_correct': 313, 'train_acc': 0.6520833333333333}}
2025-10-09 18:34:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:34:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:34:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #161, planning to set LR to 1.00e-05
2025-10-09 18:34:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 18:34:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:34:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:34:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:34:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:34:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:35:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:35:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.253845, avg_loss=0.646362, seen=480, correct=299, accuracy=0.622917
2025-10-09 18:35:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:35:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:35:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:35:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=161 reserved=2342MB allocated=2141MB
2025-10-09 18:35:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 161, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.84985214471817, 'train_avg_loss': 0.6320821012059847, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 18:35:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 161, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.25384521484375, 'train_avg_loss': 0.6463621775309245, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 18:35:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 161, 'Results_raw': {'train_total': 480, 'train_loss': 310.25384521484375, 'train_avg_loss': 0.6463621775309245, 'train_seen': 480, 'train_correct': 299, 'train_acc': 0.6229166666666667}}
2025-10-09 18:35:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:35:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:35:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #161, planning to set LR to 1.00e-05
2025-10-09 18:35:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 18:35:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:35:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:35:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:35:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:35:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:36:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:36:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.825226, avg_loss=0.612136, seen=480, correct=323, accuracy=0.672917
2025-10-09 18:36:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:36:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:36:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:36:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=161 reserved=2356MB allocated=2141MB
2025-10-09 18:36:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 161, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 74.4736265540123, 'train_avg_loss': 0.6206135546167691, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 18:36:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 161, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.8252258300781, 'train_avg_loss': 0.6121358871459961, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 18:36:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 161, 'Results_raw': {'train_total': 480, 'train_loss': 293.8252258300781, 'train_avg_loss': 0.6121358871459961, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 18:36:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:36:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:36:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #161, planning to set LR to 1.00e-05
2025-10-09 18:36:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 18:36:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:36:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:36:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:36:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:36:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:36:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:36:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.885132, avg_loss=0.662261, seen=480, correct=302, accuracy=0.629167
2025-10-09 18:36:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:36:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:36:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:36:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=161 reserved=2332MB allocated=2141MB
2025-10-09 18:36:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 161, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.12172096967697, 'train_avg_loss': 0.6093476747473081, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 18:36:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 161, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.8851318359375, 'train_avg_loss': 0.6622606913248698, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 18:36:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 161, 'Results_raw': {'train_total': 480, 'train_loss': 317.8851318359375, 'train_avg_loss': 0.6622606913248698, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 18:36:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:36:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:36:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #161, planning to set LR to 1.00e-05
2025-10-09 18:36:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-09 18:36:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:36:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:36:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:36:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:36:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:37:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:37:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=292.560089, avg_loss=0.609500, seen=480, correct=323, accuracy=0.672917
2025-10-09 18:37:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:37:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:37:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:37:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=161 reserved=2342MB allocated=2141MB
2025-10-09 18:37:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 161, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.34023827314377, 'train_avg_loss': 0.6028353189428647, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 18:37:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 161, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 292.5600891113281, 'train_avg_loss': 0.6095001856486003, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 18:37:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 161, 'Results_raw': {'train_total': 480, 'train_loss': 292.5600891113281, 'train_avg_loss': 0.6095001856486003, 'train_seen': 480, 'train_correct': 323, 'train_acc': 0.6729166666666667}}
2025-10-09 18:37:37 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #162) -------------
2025-10-09 18:37:38 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=162 aidx=1 | s=5 (candidates=23)
2025-10-09 18:37:38 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[49, 25, 8, 52, 24] (from 23)
2025-10-09 18:37:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:37:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:37:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #162, planning to set LR to 1.00e-05
2025-10-09 18:37:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-09 18:37:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:37:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:37:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:37:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:37:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:38:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:38:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.182648, avg_loss=0.658714, seen=480, correct=289, accuracy=0.602083
2025-10-09 18:38:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:38:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:38:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:38:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=162 reserved=2362MB allocated=2141MB
2025-10-09 18:38:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 162, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.15279787778854, 'train_avg_loss': 0.6762733156482379, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 18:38:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 162, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.1826477050781, 'train_avg_loss': 0.6587138493855794, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 18:38:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 162, 'Results_raw': {'train_total': 480, 'train_loss': 316.1826477050781, 'train_avg_loss': 0.6587138493855794, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 18:38:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:38:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:38:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #162, planning to set LR to 1.00e-05
2025-10-09 18:38:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 18:38:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:38:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:38:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:38:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:38:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:39:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:39:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.314636, avg_loss=0.667322, seen=480, correct=296, accuracy=0.616667
2025-10-09 18:39:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:39:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:39:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=162 reserved=2330MB allocated=2141MB
2025-10-09 18:39:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 162, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.18824005126953, 'train_avg_loss': 0.6682353337605794, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 18:39:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 162, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.31463623046875, 'train_avg_loss': 0.6673221588134766, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 18:39:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 162, 'Results_raw': {'train_total': 480, 'train_loss': 320.31463623046875, 'train_avg_loss': 0.6673221588134766, 'train_seen': 480, 'train_correct': 296, 'train_acc': 0.6166666666666667}}
2025-10-09 18:39:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:39:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:39:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #162, planning to set LR to 1.00e-05
2025-10-09 18:39:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 18:39:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:39:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:39:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:39:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:39:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:39:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:39:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.743286, avg_loss=0.611965, seen=480, correct=324, accuracy=0.675000
2025-10-09 18:39:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:39:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:39:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=162 reserved=2356MB allocated=2141MB
2025-10-09 18:39:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 162, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.27195340394974, 'train_avg_loss': 0.6105996116995811, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 18:39:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 162, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.7432861328125, 'train_avg_loss': 0.6119651794433594, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 18:39:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 162, 'Results_raw': {'train_total': 480, 'train_loss': 293.7432861328125, 'train_avg_loss': 0.6119651794433594, 'train_seen': 480, 'train_correct': 324, 'train_acc': 0.675}}
2025-10-09 18:39:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:39:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:39:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #162, planning to set LR to 1.00e-05
2025-10-09 18:39:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-09 18:39:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:39:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:39:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:39:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:39:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:40:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:40:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=286.811310, avg_loss=0.597524, seen=480, correct=325, accuracy=0.677083
2025-10-09 18:40:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:40:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:40:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:40:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=162 reserved=2338MB allocated=2141MB
2025-10-09 18:40:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 162, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.06591594219208, 'train_avg_loss': 0.5755492995182673, 'train_seen': 120, 'train_correct': 83, 'train_acc': 0.6916666666666667}}
2025-10-09 18:40:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 162, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 286.8113098144531, 'train_avg_loss': 0.597523562113444, 'train_seen': 480, 'train_correct': 325, 'train_acc': 0.6770833333333334}}
2025-10-09 18:40:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 162, 'Results_raw': {'train_total': 480, 'train_loss': 286.8113098144531, 'train_avg_loss': 0.597523562113444, 'train_seen': 480, 'train_correct': 325, 'train_acc': 0.6770833333333334}}
2025-10-09 18:40:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:40:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:40:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #162, planning to set LR to 1.00e-05
2025-10-09 18:40:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 18:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:40:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:40:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:40:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:40:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:41:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:41:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.565277, avg_loss=0.672011, seen=480, correct=286, accuracy=0.595833
2025-10-09 18:41:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:41:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:41:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=162 reserved=2330MB allocated=2141MB
2025-10-09 18:41:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 162, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.93922525644302, 'train_avg_loss': 0.6994935438036919, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 18:41:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 162, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.5652770996094, 'train_avg_loss': 0.6720109939575195, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 18:41:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 162, 'Results_raw': {'train_total': 480, 'train_loss': 322.5652770996094, 'train_avg_loss': 0.6720109939575195, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 18:41:14 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #163) -------------
2025-10-09 18:41:14 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=163 aidx=1 | s=5 (candidates=23)
2025-10-09 18:41:14 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[9, 53, 7, 8, 12] (from 23)
2025-10-09 18:41:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:41:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:41:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #163, planning to set LR to 1.00e-05
2025-10-09 18:41:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-09 18:41:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:41:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:41:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:41:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:41:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:41:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:41:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=310.989441, avg_loss=0.647895, seen=480, correct=294, accuracy=0.612500
2025-10-09 18:41:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:41:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:41:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:41:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=163 reserved=2352MB allocated=2141MB
2025-10-09 18:41:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 163, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.62498462200165, 'train_avg_loss': 0.6552082051833471, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 18:41:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 163, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 310.98944091796875, 'train_avg_loss': 0.6478946685791016, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 18:41:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 163, 'Results_raw': {'train_total': 480, 'train_loss': 310.98944091796875, 'train_avg_loss': 0.6478946685791016, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 18:41:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:41:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:41:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #163, planning to set LR to 1.00e-05
2025-10-09 18:41:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-09 18:41:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:41:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:41:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:41:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:41:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:42:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:42:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.703430, avg_loss=0.659799, seen=480, correct=297, accuracy=0.618750
2025-10-09 18:42:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:42:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:42:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:42:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=163 reserved=2332MB allocated=2141MB
2025-10-09 18:42:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 163, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.89403709769249, 'train_avg_loss': 0.6157836424807708, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 18:42:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 163, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.70343017578125, 'train_avg_loss': 0.6597988128662109, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 18:42:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 163, 'Results_raw': {'train_total': 480, 'train_loss': 316.70343017578125, 'train_avg_loss': 0.6597988128662109, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 18:42:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:42:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:42:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #163, planning to set LR to 1.00e-05
2025-10-09 18:42:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-09 18:42:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:42:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:42:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:42:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:42:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:43:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:43:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=297.377655, avg_loss=0.619537, seen=480, correct=311, accuracy=0.647917
2025-10-09 18:43:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:43:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:43:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:43:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=163 reserved=2340MB allocated=2141MB
2025-10-09 18:43:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 163, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.00472640991211, 'train_avg_loss': 0.6667060534159343, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 18:43:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 163, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 297.3776550292969, 'train_avg_loss': 0.6195367813110352, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 18:43:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 163, 'Results_raw': {'train_total': 480, 'train_loss': 297.3776550292969, 'train_avg_loss': 0.6195367813110352, 'train_seen': 480, 'train_correct': 311, 'train_acc': 0.6479166666666667}}
2025-10-09 18:43:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:43:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:43:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #163, planning to set LR to 1.00e-05
2025-10-09 18:43:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 18:43:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:43:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:43:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:43:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:43:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:44:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:44:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=294.204803, avg_loss=0.612927, seen=480, correct=329, accuracy=0.685417
2025-10-09 18:44:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:44:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:44:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=163 reserved=2356MB allocated=2141MB
2025-10-09 18:44:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 163, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.06985837221146, 'train_avg_loss': 0.6089154864350955, 'train_seen': 120, 'train_correct': 85, 'train_acc': 0.7083333333333334}}
2025-10-09 18:44:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 163, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 294.2048034667969, 'train_avg_loss': 0.6129266738891601, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 18:44:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 163, 'Results_raw': {'train_total': 480, 'train_loss': 294.2048034667969, 'train_avg_loss': 0.6129266738891601, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 18:44:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:44:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:44:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #163, planning to set LR to 1.00e-05
2025-10-09 18:44:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-09 18:44:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:44:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:44:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:44:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:44:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:44:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:44:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=299.155151, avg_loss=0.623240, seen=480, correct=305, accuracy=0.635417
2025-10-09 18:44:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:44:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:44:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:44:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=163 reserved=2330MB allocated=2141MB
2025-10-09 18:44:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 163, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.77046221494675, 'train_avg_loss': 0.6397538517912229, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 18:44:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 163, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 299.1551513671875, 'train_avg_loss': 0.6232398986816406, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 18:44:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 163, 'Results_raw': {'train_total': 480, 'train_loss': 299.1551513671875, 'train_avg_loss': 0.6232398986816406, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 18:44:46 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #164) -------------
2025-10-09 18:44:46 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=164 aidx=1 | s=5 (candidates=23)
2025-10-09 18:44:46 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[44, 8, 18, 11, 17] (from 23)
2025-10-09 18:44:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:44:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:44:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #164, planning to set LR to 1.00e-05
2025-10-09 18:44:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-09 18:44:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:44:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:44:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:44:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:44:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:45:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:45:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=289.341125, avg_loss=0.602794, seen=480, correct=317, accuracy=0.660417
2025-10-09 18:45:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:45:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:45:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:45:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=164 reserved=2330MB allocated=2141MB
2025-10-09 18:45:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 164, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.52652284502983, 'train_avg_loss': 0.6043876903752486, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 18:45:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 164, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 289.34112548828125, 'train_avg_loss': 0.6027940114339193, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 18:45:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 164, 'Results_raw': {'train_total': 480, 'train_loss': 289.34112548828125, 'train_avg_loss': 0.6027940114339193, 'train_seen': 480, 'train_correct': 317, 'train_acc': 0.6604166666666667}}
2025-10-09 18:45:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:45:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:45:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #164, planning to set LR to 1.00e-05
2025-10-09 18:45:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 18:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:45:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:45:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:45:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:45:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:46:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:46:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=293.482391, avg_loss=0.611422, seen=480, correct=329, accuracy=0.685417
2025-10-09 18:46:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:46:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:46:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=164 reserved=2356MB allocated=2141MB
2025-10-09 18:46:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 164, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.20853424072266, 'train_avg_loss': 0.6017377853393555, 'train_seen': 120, 'train_correct': 87, 'train_acc': 0.725}}
2025-10-09 18:46:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 164, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 293.4823913574219, 'train_avg_loss': 0.6114216486612956, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 18:46:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 164, 'Results_raw': {'train_total': 480, 'train_loss': 293.4823913574219, 'train_avg_loss': 0.6114216486612956, 'train_seen': 480, 'train_correct': 329, 'train_acc': 0.6854166666666667}}
2025-10-09 18:46:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:46:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:46:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #164, planning to set LR to 1.00e-05
2025-10-09 18:46:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-09 18:46:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:46:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:46:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:46:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:46:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:46:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:46:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=290.752686, avg_loss=0.605735, seen=480, correct=331, accuracy=0.689583
2025-10-09 18:46:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:46:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:47:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:47:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=164 reserved=2362MB allocated=2141MB
2025-10-09 18:47:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 164, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.02078977227211, 'train_avg_loss': 0.6501732481022676, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 18:47:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 164, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 290.752685546875, 'train_avg_loss': 0.6057347615559896, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 18:47:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 164, 'Results_raw': {'train_total': 480, 'train_loss': 290.752685546875, 'train_avg_loss': 0.6057347615559896, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 18:47:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:47:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:47:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #164, planning to set LR to 1.00e-05
2025-10-09 18:47:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-09 18:47:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:47:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:47:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:47:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:47:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:47:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:47:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=298.427124, avg_loss=0.621723, seen=480, correct=308, accuracy=0.641667
2025-10-09 18:47:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:47:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:47:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:47:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=164 reserved=2418MB allocated=2141MB
2025-10-09 18:47:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 164, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.57331439852715, 'train_avg_loss': 0.6381109533210595, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 18:47:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 164, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 298.4271240234375, 'train_avg_loss': 0.6217231750488281, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 18:47:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 164, 'Results_raw': {'train_total': 480, 'train_loss': 298.4271240234375, 'train_avg_loss': 0.6217231750488281, 'train_seen': 480, 'train_correct': 308, 'train_acc': 0.6416666666666667}}
2025-10-09 18:47:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:47:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:47:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #164, planning to set LR to 1.00e-05
2025-10-09 18:47:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 18:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:47:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:47:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:47:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:47:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:48:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:48:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=276.147339, avg_loss=0.575307, seen=480, correct=331, accuracy=0.689583
2025-10-09 18:48:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:48:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:48:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:48:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=164 reserved=2366MB allocated=2141MB
2025-10-09 18:48:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 164, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 68.76727068424225, 'train_avg_loss': 0.5730605890353521, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 18:48:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 164, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 276.1473388671875, 'train_avg_loss': 0.5753069559733073, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 18:48:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 164, 'Results_raw': {'train_total': 480, 'train_loss': 276.1473388671875, 'train_avg_loss': 0.5753069559733073, 'train_seen': 480, 'train_correct': 331, 'train_acc': 0.6895833333333333}}
2025-10-09 18:48:31 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #165) -------------
2025-10-09 18:48:31 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=165 aidx=1 | s=5 (candidates=23)
2025-10-09 18:48:31 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[4, 17, 8, 27, 25] (from 23)
2025-10-09 18:48:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:48:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:48:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #165, planning to set LR to 1.00e-05
2025-10-09 18:48:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-09 18:48:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:48:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:48:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:48:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:48:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:49:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:49:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=256.947021, avg_loss=0.535306, seen=480, correct=352, accuracy=0.733333
2025-10-09 18:49:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:49:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:49:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=165 reserved=2372MB allocated=2141MB
2025-10-09 18:49:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 165, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 75.07880344986916, 'train_avg_loss': 0.6256566954155763, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 18:49:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 165, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 256.947021484375, 'train_avg_loss': 0.5353062947591146, 'train_seen': 480, 'train_correct': 352, 'train_acc': 0.7333333333333333}}
2025-10-09 18:49:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 165, 'Results_raw': {'train_total': 480, 'train_loss': 256.947021484375, 'train_avg_loss': 0.5353062947591146, 'train_seen': 480, 'train_correct': 352, 'train_acc': 0.7333333333333333}}
2025-10-09 18:49:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:49:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:49:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #165, planning to set LR to 1.00e-05
2025-10-09 18:49:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-09 18:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:49:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:49:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:49:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:49:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:49:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:49:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=264.571655, avg_loss=0.551191, seen=480, correct=339, accuracy=0.706250
2025-10-09 18:49:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:49:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:49:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:49:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=165 reserved=2366MB allocated=2141MB
2025-10-09 18:49:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 165, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 65.16233977675438, 'train_avg_loss': 0.5430194981396198, 'train_seen': 120, 'train_correct': 86, 'train_acc': 0.7166666666666667}}
2025-10-09 18:49:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 165, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 264.5716552734375, 'train_avg_loss': 0.5511909484863281, 'train_seen': 480, 'train_correct': 339, 'train_acc': 0.70625}}
2025-10-09 18:49:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 165, 'Results_raw': {'train_total': 480, 'train_loss': 264.5716552734375, 'train_avg_loss': 0.5511909484863281, 'train_seen': 480, 'train_correct': 339, 'train_acc': 0.70625}}
2025-10-09 18:49:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:49:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:49:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #165, planning to set LR to 1.00e-05
2025-10-09 18:49:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 18:49:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:49:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:49:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:49:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:49:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:50:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:50:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=296.392181, avg_loss=0.617484, seen=480, correct=322, accuracy=0.670833
2025-10-09 18:50:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:50:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:50:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:50:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=165 reserved=2354MB allocated=2141MB
2025-10-09 18:50:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 165, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 72.3357925415039, 'train_avg_loss': 0.6027982711791993, 'train_seen': 120, 'train_correct': 86, 'train_acc': 0.7166666666666667}}
2025-10-09 18:50:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 165, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 296.3921813964844, 'train_avg_loss': 0.6174837112426758, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 18:50:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 165, 'Results_raw': {'train_total': 480, 'train_loss': 296.3921813964844, 'train_avg_loss': 0.6174837112426758, 'train_seen': 480, 'train_correct': 322, 'train_acc': 0.6708333333333333}}
2025-10-09 18:50:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:50:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:50:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #165, planning to set LR to 1.00e-05
2025-10-09 18:50:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-09 18:50:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:50:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:50:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:50:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:50:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:51:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:51:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=302.079895, avg_loss=0.629333, seen=480, correct=310, accuracy=0.645833
2025-10-09 18:51:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:51:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:51:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:51:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=165 reserved=2332MB allocated=2141MB
2025-10-09 18:51:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 165, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.92254620790482, 'train_avg_loss': 0.6410212183992068, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 18:51:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 165, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 302.07989501953125, 'train_avg_loss': 0.6293331146240234, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 18:51:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 165, 'Results_raw': {'train_total': 480, 'train_loss': 302.07989501953125, 'train_avg_loss': 0.6293331146240234, 'train_seen': 480, 'train_correct': 310, 'train_acc': 0.6458333333333334}}
2025-10-09 18:51:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:51:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:51:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #165, planning to set LR to 1.00e-05
2025-10-09 18:51:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-09 18:51:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:51:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:51:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:51:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:51:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:52:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:52:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.788483, avg_loss=0.674559, seen=480, correct=294, accuracy=0.612500
2025-10-09 18:52:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:52:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:52:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=165 reserved=2330MB allocated=2141MB
2025-10-09 18:52:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 165, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.00858581066132, 'train_avg_loss': 0.6667382150888443, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 18:52:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 165, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.7884826660156, 'train_avg_loss': 0.6745593388875325, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 18:52:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 165, 'Results_raw': {'train_total': 480, 'train_loss': 323.7884826660156, 'train_avg_loss': 0.6745593388875325, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 18:52:05 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #166) -------------
2025-10-09 18:52:05 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=166 aidx=1 | s=5 (candidates=23)
2025-10-09 18:52:05 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[33, 8, 2, 24, 14] (from 23)
2025-10-09 18:52:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:52:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:52:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #166, planning to set LR to 1.00e-05
2025-10-09 18:52:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-09 18:52:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:52:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:52:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:52:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:52:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:52:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:52:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=311.152100, avg_loss=0.648234, seen=480, correct=295, accuracy=0.614583
2025-10-09 18:52:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:52:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:52:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=166 reserved=2342MB allocated=2141MB
2025-10-09 18:52:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 166, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.19048070907593, 'train_avg_loss': 0.6432540059089661, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 18:52:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 166, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 311.152099609375, 'train_avg_loss': 0.6482335408528646, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 18:52:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 166, 'Results_raw': {'train_total': 480, 'train_loss': 311.152099609375, 'train_avg_loss': 0.6482335408528646, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 18:52:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:52:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:52:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #166, planning to set LR to 1.00e-05
2025-10-09 18:52:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-09 18:52:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:52:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:52:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:52:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:52:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:53:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:53:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=294.303558, avg_loss=0.613132, seen=480, correct=321, accuracy=0.668750
2025-10-09 18:53:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:53:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:53:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:53:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=166 reserved=2356MB allocated=2141MB
2025-10-09 18:53:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 166, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 73.04349237680435, 'train_avg_loss': 0.6086957698067029, 'train_seen': 120, 'train_correct': 82, 'train_acc': 0.6833333333333333}}
2025-10-09 18:53:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 166, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 294.3035583496094, 'train_avg_loss': 0.6131324132283529, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 18:53:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 166, 'Results_raw': {'train_total': 480, 'train_loss': 294.3035583496094, 'train_avg_loss': 0.6131324132283529, 'train_seen': 480, 'train_correct': 321, 'train_acc': 0.66875}}
2025-10-09 18:53:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:53:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:53:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #166, planning to set LR to 1.00e-05
2025-10-09 18:53:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-09 18:53:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:53:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:53:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:53:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:53:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:54:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:54:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=265.705780, avg_loss=0.553554, seen=480, correct=336, accuracy=0.700000
2025-10-09 18:54:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:54:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:54:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=166 reserved=2402MB allocated=2141MB
2025-10-09 18:54:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 166, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 57.54647532105446, 'train_avg_loss': 0.47955396100878717, 'train_seen': 120, 'train_correct': 94, 'train_acc': 0.7833333333333333}}
2025-10-09 18:54:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 166, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 265.7057800292969, 'train_avg_loss': 0.5535537083943685, 'train_seen': 480, 'train_correct': 336, 'train_acc': 0.7}}
2025-10-09 18:54:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 166, 'Results_raw': {'train_total': 480, 'train_loss': 265.7057800292969, 'train_avg_loss': 0.5535537083943685, 'train_seen': 480, 'train_correct': 336, 'train_acc': 0.7}}
2025-10-09 18:54:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:54:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:54:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #166, planning to set LR to 1.00e-05
2025-10-09 18:54:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-09 18:54:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:54:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:54:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:54:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:54:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:54:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:54:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.122009, avg_loss=0.654421, seen=480, correct=297, accuracy=0.618750
2025-10-09 18:54:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:54:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:54:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:54:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=166 reserved=2330MB allocated=2141MB
2025-10-09 18:54:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 166, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.6493707895279, 'train_avg_loss': 0.6887447565793992, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 18:54:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 166, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.12200927734375, 'train_avg_loss': 0.6544208526611328, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 18:54:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 166, 'Results_raw': {'train_total': 480, 'train_loss': 314.12200927734375, 'train_avg_loss': 0.6544208526611328, 'train_seen': 480, 'train_correct': 297, 'train_acc': 0.61875}}
2025-10-09 18:54:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:54:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:54:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #166, planning to set LR to 1.00e-05
2025-10-09 18:54:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-09 18:54:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:54:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:54:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:54:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:54:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:55:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:55:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=292.521362, avg_loss=0.609420, seen=480, correct=316, accuracy=0.658333
2025-10-09 18:55:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:55:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:55:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=166 reserved=2330MB allocated=2141MB
2025-10-09 18:55:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 166, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 69.45564439892769, 'train_avg_loss': 0.5787970366577307, 'train_seen': 120, 'train_correct': 80, 'train_acc': 0.6666666666666666}}
2025-10-09 18:55:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 166, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 292.5213623046875, 'train_avg_loss': 0.6094195048014323, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 18:55:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 166, 'Results_raw': {'train_total': 480, 'train_loss': 292.5213623046875, 'train_avg_loss': 0.6094195048014323, 'train_seen': 480, 'train_correct': 316, 'train_acc': 0.6583333333333333}}
2025-10-09 18:55:40 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #167) -------------
2025-10-09 18:55:41 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=167 aidx=2 | s=5 (candidates=9)
2025-10-09 18:55:41 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[41, 37, 5, 50, 31] (from 9)
2025-10-09 18:55:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:55:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:55:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #167, planning to set LR to 1.00e-05
2025-10-09 18:55:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 18:55:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:55:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:55:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:55:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:55:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:56:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:56:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=349.481903, avg_loss=0.728087, seen=480, correct=224, accuracy=0.466667
2025-10-09 18:56:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:56:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:56:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:56:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=167 reserved=2354MB allocated=2175MB
2025-10-09 18:56:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 167, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.37417936325073, 'train_avg_loss': 0.7114514946937561, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 18:56:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 167, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 349.4819030761719, 'train_avg_loss': 0.7280872980753581, 'train_seen': 480, 'train_correct': 224, 'train_acc': 0.4666666666666667}}
2025-10-09 18:56:23 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 167, 'Results_raw': {'train_total': 480, 'train_loss': 349.4819030761719, 'train_avg_loss': 0.7280872980753581, 'train_seen': 480, 'train_correct': 224, 'train_acc': 0.4666666666666667}}
2025-10-09 18:56:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:56:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:56:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #167, planning to set LR to 1.00e-05
2025-10-09 18:56:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 18:56:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:56:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:56:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:56:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:56:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:57:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:57:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=345.012573, avg_loss=0.718776, seen=480, correct=257, accuracy=0.535417
2025-10-09 18:57:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:57:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:57:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:57:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=167 reserved=2354MB allocated=2183MB
2025-10-09 18:57:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 167, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 93.60863310098648, 'train_avg_loss': 0.7800719425082207, 'train_seen': 120, 'train_correct': 54, 'train_acc': 0.45}}
2025-10-09 18:57:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 167, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 345.0125732421875, 'train_avg_loss': 0.7187761942545573, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 18:57:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 167, 'Results_raw': {'train_total': 480, 'train_loss': 345.0125732421875, 'train_avg_loss': 0.7187761942545573, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 18:57:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:57:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:57:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #167, planning to set LR to 1.00e-05
2025-10-09 18:57:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 18:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:57:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:57:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:57:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:57:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:57:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:57:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=351.902130, avg_loss=0.733129, seen=480, correct=231, accuracy=0.481250
2025-10-09 18:57:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:57:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:57:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:57:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=167 reserved=2410MB allocated=2192MB
2025-10-09 18:57:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 167, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.17375844717026, 'train_avg_loss': 0.7181146537264188, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 18:57:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 167, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 351.9021301269531, 'train_avg_loss': 0.7331294377644857, 'train_seen': 480, 'train_correct': 231, 'train_acc': 0.48125}}
2025-10-09 18:57:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 167, 'Results_raw': {'train_total': 480, 'train_loss': 351.9021301269531, 'train_avg_loss': 0.7331294377644857, 'train_seen': 480, 'train_correct': 231, 'train_acc': 0.48125}}
2025-10-09 18:57:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:57:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:57:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #167, planning to set LR to 1.00e-05
2025-10-09 18:57:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 18:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:57:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:57:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:57:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:57:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:58:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:58:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=343.320557, avg_loss=0.715251, seen=480, correct=251, accuracy=0.522917
2025-10-09 18:58:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:58:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:58:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:58:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=167 reserved=2356MB allocated=2200MB
2025-10-09 18:58:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 167, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.91468507051468, 'train_avg_loss': 0.7159557089209556, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 18:58:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 167, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 343.320556640625, 'train_avg_loss': 0.7152511596679687, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 18:58:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 167, 'Results_raw': {'train_total': 480, 'train_loss': 343.320556640625, 'train_avg_loss': 0.7152511596679687, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 18:58:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:58:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:58:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #167, planning to set LR to 1.00e-05
2025-10-09 18:58:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 18:58:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:58:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:58:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:58:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:58:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 18:59:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 18:59:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=342.352203, avg_loss=0.713234, seen=480, correct=237, accuracy=0.493750
2025-10-09 18:59:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 18:59:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:59:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 18:59:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=167 reserved=2354MB allocated=2209MB
2025-10-09 18:59:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 167, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.90562909841537, 'train_avg_loss': 0.6992135758201281, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 18:59:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 167, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 342.3522033691406, 'train_avg_loss': 0.713233757019043, 'train_seen': 480, 'train_correct': 237, 'train_acc': 0.49375}}
2025-10-09 18:59:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 167, 'Results_raw': {'train_total': 480, 'train_loss': 342.3522033691406, 'train_avg_loss': 0.713233757019043, 'train_seen': 480, 'train_correct': 237, 'train_acc': 0.49375}}
2025-10-09 18:59:23 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #168) -------------
2025-10-09 18:59:23 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=168 aidx=2 | s=5 (candidates=9)
2025-10-09 18:59:23 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[41, 37, 50, 31, 5] (from 9)
2025-10-09 18:59:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 18:59:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 18:59:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #168, planning to set LR to 1.00e-05
2025-10-09 18:59:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 18:59:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 18:59:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 18:59:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 18:59:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 18:59:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:00:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:00:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=340.237488, avg_loss=0.708828, seen=480, correct=242, accuracy=0.504167
2025-10-09 19:00:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:00:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:00:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:00:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=168 reserved=2356MB allocated=2209MB
2025-10-09 19:00:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 168, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.7221245765686, 'train_avg_loss': 0.6893510381380717, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 19:00:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 168, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 340.23748779296875, 'train_avg_loss': 0.7088280995686849, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 19:00:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 168, 'Results_raw': {'train_total': 480, 'train_loss': 340.23748779296875, 'train_avg_loss': 0.7088280995686849, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 19:00:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:00:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:00:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #168, planning to set LR to 1.00e-05
2025-10-09 19:00:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 19:00:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:00:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:00:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:00:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:00:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:00:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:00:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=338.865204, avg_loss=0.705969, seen=480, correct=250, accuracy=0.520833
2025-10-09 19:00:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:00:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:00:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:00:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=168 reserved=2356MB allocated=2209MB
2025-10-09 19:00:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 168, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 91.36847507953644, 'train_avg_loss': 0.761403958996137, 'train_seen': 120, 'train_correct': 56, 'train_acc': 0.4666666666666667}}
2025-10-09 19:00:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 168, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.8652038574219, 'train_avg_loss': 0.7059691747029623, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 19:00:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 168, 'Results_raw': {'train_total': 480, 'train_loss': 338.8652038574219, 'train_avg_loss': 0.7059691747029623, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 19:00:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:00:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:00:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #168, planning to set LR to 1.00e-05
2025-10-09 19:00:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:00:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:00:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:00:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:00:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:00:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:01:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:01:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=338.840088, avg_loss=0.705917, seen=480, correct=254, accuracy=0.529167
2025-10-09 19:01:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:01:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:01:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:01:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=168 reserved=2356MB allocated=2209MB
2025-10-09 19:01:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 168, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.8132992386818, 'train_avg_loss': 0.7234441603223483, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 19:01:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 168, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 338.840087890625, 'train_avg_loss': 0.7059168497721354, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 19:01:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 168, 'Results_raw': {'train_total': 480, 'train_loss': 338.840087890625, 'train_avg_loss': 0.7059168497721354, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 19:01:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:01:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:01:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #168, planning to set LR to 1.00e-05
2025-10-09 19:01:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 19:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:01:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:01:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:01:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:01:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:02:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:02:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=344.420532, avg_loss=0.717543, seen=480, correct=235, accuracy=0.489583
2025-10-09 19:02:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:02:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:02:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:02:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=168 reserved=2374MB allocated=2209MB
2025-10-09 19:02:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 168, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.32191705703735, 'train_avg_loss': 0.7110159754753113, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 19:02:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 168, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 344.4205322265625, 'train_avg_loss': 0.7175427754720052, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 19:02:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 168, 'Results_raw': {'train_total': 480, 'train_loss': 344.4205322265625, 'train_avg_loss': 0.7175427754720052, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 19:02:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:02:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:02:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #168, planning to set LR to 1.00e-05
2025-10-09 19:02:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 19:02:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:02:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:02:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:02:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:02:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:03:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:03:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=349.499207, avg_loss=0.728123, seen=480, correct=246, accuracy=0.512500
2025-10-09 19:03:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:03:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:03:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:03:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=168 reserved=2430MB allocated=2209MB
2025-10-09 19:03:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 168, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.60430371761322, 'train_avg_loss': 0.7050358643134434, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 19:03:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 168, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 349.49920654296875, 'train_avg_loss': 0.7281233469645182, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:03:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 168, 'Results_raw': {'train_total': 480, 'train_loss': 349.49920654296875, 'train_avg_loss': 0.7281233469645182, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:03:04 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #169) -------------
2025-10-09 19:03:04 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=169 aidx=2 | s=5 (candidates=9)
2025-10-09 19:03:04 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[36, 3, 37, 21, 32] (from 9)
2025-10-09 19:03:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:03:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:03:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #169, planning to set LR to 1.00e-05
2025-10-09 19:03:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:03:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:03:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:03:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:03:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:03:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:03:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:03:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.995392, avg_loss=0.687490, seen=480, correct=258, accuracy=0.537500
2025-10-09 19:03:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:03:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:03:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:03:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=169 reserved=2362MB allocated=2217MB
2025-10-09 19:03:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 169, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.42017644643784, 'train_avg_loss': 0.6868348037203152, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 19:03:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 169, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.9953918457031, 'train_avg_loss': 0.6874903996785482, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:03:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 169, 'Results_raw': {'train_total': 480, 'train_loss': 329.9953918457031, 'train_avg_loss': 0.6874903996785482, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:03:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:03:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:03:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #169, planning to set LR to 1.00e-05
2025-10-09 19:03:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 19:03:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:03:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:03:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:03:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:03:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:04:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:04:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.901398, avg_loss=0.703961, seen=480, correct=243, accuracy=0.506250
2025-10-09 19:04:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:04:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:04:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:04:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=169 reserved=2396MB allocated=2225MB
2025-10-09 19:04:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 169, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.10189867019653, 'train_avg_loss': 0.6925158222516378, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 19:04:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 169, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.9013977050781, 'train_avg_loss': 0.7039612452189128, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 19:04:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 169, 'Results_raw': {'train_total': 480, 'train_loss': 337.9013977050781, 'train_avg_loss': 0.7039612452189128, 'train_seen': 480, 'train_correct': 243, 'train_acc': 0.50625}}
2025-10-09 19:04:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:04:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:04:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #169, planning to set LR to 1.00e-05
2025-10-09 19:04:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 19:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:04:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:04:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:04:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:04:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:05:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.566162, avg_loss=0.701180, seen=480, correct=255, accuracy=0.531250
2025-10-09 19:05:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:05:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:05:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=169 reserved=2330MB allocated=2200MB
2025-10-09 19:05:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 169, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 90.06437486410141, 'train_avg_loss': 0.750536457200845, 'train_seen': 120, 'train_correct': 53, 'train_acc': 0.44166666666666665}}
2025-10-09 19:05:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 169, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.566162109375, 'train_avg_loss': 0.7011795043945312, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 19:05:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 169, 'Results_raw': {'train_total': 480, 'train_loss': 336.566162109375, 'train_avg_loss': 0.7011795043945312, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 19:05:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:05:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:05:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #169, planning to set LR to 1.00e-05
2025-10-09 19:05:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 19:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:05:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:05:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:05:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:05:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:05:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:05:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.972412, avg_loss=0.695776, seen=480, correct=255, accuracy=0.531250
2025-10-09 19:05:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:06:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:06:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=169 reserved=2386MB allocated=2234MB
2025-10-09 19:06:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 169, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.05241459608078, 'train_avg_loss': 0.7087701216340065, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 19:06:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 169, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.972412109375, 'train_avg_loss': 0.695775858561198, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 19:06:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 169, 'Results_raw': {'train_total': 480, 'train_loss': 333.972412109375, 'train_avg_loss': 0.695775858561198, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 19:06:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:06:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:06:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #169, planning to set LR to 1.00e-05
2025-10-09 19:06:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:06:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:06:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:06:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:06:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:06:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:06:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:06:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.455200, avg_loss=0.703032, seen=480, correct=250, accuracy=0.520833
2025-10-09 19:06:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:06:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:06:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:06:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=169 reserved=2356MB allocated=2242MB
2025-10-09 19:06:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 169, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.08265328407288, 'train_avg_loss': 0.6923554440339407, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 19:06:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 169, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.4552001953125, 'train_avg_loss': 0.7030316670735677, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 19:06:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 169, 'Results_raw': {'train_total': 480, 'train_loss': 337.4552001953125, 'train_avg_loss': 0.7030316670735677, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 19:06:47 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #170) -------------
2025-10-09 19:06:48 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=170 aidx=2 | s=5 (candidates=9)
2025-10-09 19:06:48 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[50, 41, 32, 3, 21] (from 9)
2025-10-09 19:06:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:06:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:06:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #170, planning to set LR to 1.00e-05
2025-10-09 19:06:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:06:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:06:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:06:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:06:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:06:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:07:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:07:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.735474, avg_loss=0.695282, seen=480, correct=257, accuracy=0.535417
2025-10-09 19:07:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:07:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:07:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:07:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=170 reserved=2356MB allocated=2242MB
2025-10-09 19:07:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 170, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.18193101882935, 'train_avg_loss': 0.7098494251569112, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 19:07:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 170, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.7354736328125, 'train_avg_loss': 0.6952822367350261, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 19:07:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 170, 'Results_raw': {'train_total': 480, 'train_loss': 333.7354736328125, 'train_avg_loss': 0.6952822367350261, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 19:07:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:07:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:07:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #170, planning to set LR to 1.00e-05
2025-10-09 19:07:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 19:07:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:07:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:07:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:07:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:07:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:08:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:08:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.342438, avg_loss=0.702797, seen=480, correct=247, accuracy=0.514583
2025-10-09 19:08:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:08:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:08:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=170 reserved=2356MB allocated=2242MB
2025-10-09 19:08:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 170, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.30596750974655, 'train_avg_loss': 0.6858830625812212, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 19:08:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 170, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.3424377441406, 'train_avg_loss': 0.702796745300293, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 19:08:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 170, 'Results_raw': {'train_total': 480, 'train_loss': 337.3424377441406, 'train_avg_loss': 0.702796745300293, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 19:08:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:08:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:08:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #170, planning to set LR to 1.00e-05
2025-10-09 19:08:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:08:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:08:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:08:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:08:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:08:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:08:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.885712, avg_loss=0.695595, seen=480, correct=251, accuracy=0.522917
2025-10-09 19:08:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:09:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:09:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=170 reserved=2356MB allocated=2242MB
2025-10-09 19:09:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 170, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.53589224815369, 'train_avg_loss': 0.6877991020679474, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 19:09:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 170, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.8857116699219, 'train_avg_loss': 0.6955952326456706, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 19:09:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 170, 'Results_raw': {'train_total': 480, 'train_loss': 333.8857116699219, 'train_avg_loss': 0.6955952326456706, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 19:09:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:09:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:09:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #170, planning to set LR to 1.00e-05
2025-10-09 19:09:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 19:09:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:09:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:09:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:09:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:09:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:09:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:09:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.856506, avg_loss=0.699701, seen=480, correct=247, accuracy=0.514583
2025-10-09 19:09:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:09:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:09:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:09:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=170 reserved=2392MB allocated=2242MB
2025-10-09 19:09:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 170, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.03468370437622, 'train_avg_loss': 0.6919556975364685, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 19:09:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 170, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.85650634765625, 'train_avg_loss': 0.6997010548909505, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 19:09:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 170, 'Results_raw': {'train_total': 480, 'train_loss': 335.85650634765625, 'train_avg_loss': 0.6997010548909505, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 19:09:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:09:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:09:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #170, planning to set LR to 1.00e-05
2025-10-09 19:09:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 19:09:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:09:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:09:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:09:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:09:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:10:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:10:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.151459, avg_loss=0.691982, seen=480, correct=259, accuracy=0.539583
2025-10-09 19:10:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:10:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:10:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:10:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=170 reserved=2400MB allocated=2242MB
2025-10-09 19:10:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 170, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.63548958301544, 'train_avg_loss': 0.7052957465251287, 'train_seen': 120, 'train_correct': 56, 'train_acc': 0.4666666666666667}}
2025-10-09 19:10:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 170, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.1514587402344, 'train_avg_loss': 0.6919822057088216, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 19:10:31 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 170, 'Results_raw': {'train_total': 480, 'train_loss': 332.1514587402344, 'train_avg_loss': 0.6919822057088216, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 19:10:32 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #171) -------------
2025-10-09 19:10:32 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=171 aidx=2 | s=5 (candidates=9)
2025-10-09 19:10:32 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[5, 3, 50, 32, 41] (from 9)
2025-10-09 19:10:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:10:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:10:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #171, planning to set LR to 1.00e-05
2025-10-09 19:10:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 19:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:10:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:10:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:10:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:10:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:11:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:11:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.281555, avg_loss=0.702670, seen=480, correct=263, accuracy=0.547917
2025-10-09 19:11:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:11:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:11:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=171 reserved=2398MB allocated=2217MB
2025-10-09 19:11:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 171, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.32031577825546, 'train_avg_loss': 0.6943359648187956, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 19:11:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 171, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.28155517578125, 'train_avg_loss': 0.702669906616211, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:11:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 171, 'Results_raw': {'train_total': 480, 'train_loss': 337.28155517578125, 'train_avg_loss': 0.702669906616211, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:11:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:11:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:11:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #171, planning to set LR to 1.00e-05
2025-10-09 19:11:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 19:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:11:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:11:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:11:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:11:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:11:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:11:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.639587, avg_loss=0.701332, seen=480, correct=248, accuracy=0.516667
2025-10-09 19:11:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:12:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:12:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=171 reserved=2370MB allocated=2217MB
2025-10-09 19:12:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 171, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.40745723247528, 'train_avg_loss': 0.6950621436039607, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 19:12:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 171, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.63958740234375, 'train_avg_loss': 0.7013324737548828, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 19:12:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 171, 'Results_raw': {'train_total': 480, 'train_loss': 336.63958740234375, 'train_avg_loss': 0.7013324737548828, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 19:12:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:12:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:12:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #171, planning to set LR to 1.00e-05
2025-10-09 19:12:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:12:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:12:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:12:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:12:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:12:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:12:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.924408, avg_loss=0.695676, seen=480, correct=246, accuracy=0.512500
2025-10-09 19:12:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:12:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:12:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:12:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=171 reserved=2330MB allocated=2217MB
2025-10-09 19:12:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 171, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.22995859384537, 'train_avg_loss': 0.7102496549487114, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 19:12:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 171, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.9244079589844, 'train_avg_loss': 0.6956758499145508, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:12:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 171, 'Results_raw': {'train_total': 480, 'train_loss': 333.9244079589844, 'train_avg_loss': 0.6956758499145508, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:12:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:12:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:12:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #171, planning to set LR to 1.00e-05
2025-10-09 19:12:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:12:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:12:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:12:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:12:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:13:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:13:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.565918, avg_loss=0.694929, seen=480, correct=249, accuracy=0.518750
2025-10-09 19:13:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:13:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:13:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=171 reserved=2330MB allocated=2217MB
2025-10-09 19:13:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 171, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.80577325820923, 'train_avg_loss': 0.6900481104850769, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 19:13:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 171, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.56591796875, 'train_avg_loss': 0.6949289957682292, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 19:13:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 171, 'Results_raw': {'train_total': 480, 'train_loss': 333.56591796875, 'train_avg_loss': 0.6949289957682292, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 19:13:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:13:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:13:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #171, planning to set LR to 1.00e-05
2025-10-09 19:13:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 19:13:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:13:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:13:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:13:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:13:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:14:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:14:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.625092, avg_loss=0.703386, seen=480, correct=240, accuracy=0.500000
2025-10-09 19:14:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:14:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:14:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=171 reserved=2330MB allocated=2217MB
2025-10-09 19:14:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 171, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.50613212585449, 'train_avg_loss': 0.6875511010487875, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 19:14:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 171, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.6250915527344, 'train_avg_loss': 0.70338560740153, 'train_seen': 480, 'train_correct': 240, 'train_acc': 0.5}}
2025-10-09 19:14:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 171, 'Results_raw': {'train_total': 480, 'train_loss': 337.6250915527344, 'train_avg_loss': 0.70338560740153, 'train_seen': 480, 'train_correct': 240, 'train_acc': 0.5}}
2025-10-09 19:14:14 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #172) -------------
2025-10-09 19:14:14 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=172 aidx=2 | s=5 (candidates=9)
2025-10-09 19:14:14 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[21, 37, 5, 32, 41] (from 9)
2025-10-09 19:14:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:14:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:14:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #172, planning to set LR to 1.00e-05
2025-10-09 19:14:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 19:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:14:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:14:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:14:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:14:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:14:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:14:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.811859, avg_loss=0.689191, seen=480, correct=265, accuracy=0.552083
2025-10-09 19:14:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:14:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:14:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:14:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=172 reserved=2374MB allocated=2217MB
2025-10-09 19:14:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 172, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.80640381574631, 'train_avg_loss': 0.6983866984645526, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 19:14:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 172, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.8118591308594, 'train_avg_loss': 0.6891913731892904, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 19:14:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 172, 'Results_raw': {'train_total': 480, 'train_loss': 330.8118591308594, 'train_avg_loss': 0.6891913731892904, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 19:14:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:14:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:14:58 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #172, planning to set LR to 1.00e-05
2025-10-09 19:14:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 19:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:14:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:14:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:14:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:14:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:15:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:15:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.423340, avg_loss=0.696715, seen=480, correct=264, accuracy=0.550000
2025-10-09 19:15:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:15:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:15:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:15:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=172 reserved=2330MB allocated=2217MB
2025-10-09 19:15:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 172, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 88.40950429439545, 'train_avg_loss': 0.7367458691199621, 'train_seen': 120, 'train_correct': 52, 'train_acc': 0.43333333333333335}}
2025-10-09 19:15:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 172, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.42333984375, 'train_avg_loss': 0.6967152913411458, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 19:15:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 172, 'Results_raw': {'train_total': 480, 'train_loss': 334.42333984375, 'train_avg_loss': 0.6967152913411458, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 19:15:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:15:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:15:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #172, planning to set LR to 1.00e-05
2025-10-09 19:15:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 19:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:15:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:15:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:15:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:15:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:16:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:16:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.366852, avg_loss=0.702848, seen=480, correct=247, accuracy=0.514583
2025-10-09 19:16:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:16:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:16:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:16:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=172 reserved=2396MB allocated=2217MB
2025-10-09 19:16:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 172, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.82881557941437, 'train_avg_loss': 0.6985734631617864, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 19:16:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 172, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.3668518066406, 'train_avg_loss': 0.7028476079305013, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 19:16:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 172, 'Results_raw': {'train_total': 480, 'train_loss': 337.3668518066406, 'train_avg_loss': 0.7028476079305013, 'train_seen': 480, 'train_correct': 247, 'train_acc': 0.5145833333333333}}
2025-10-09 19:16:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:16:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:16:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #172, planning to set LR to 1.00e-05
2025-10-09 19:16:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:16:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:16:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:16:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:16:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:17:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:17:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.129272, avg_loss=0.687769, seen=480, correct=268, accuracy=0.558333
2025-10-09 19:17:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:17:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:17:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=172 reserved=2330MB allocated=2217MB
2025-10-09 19:17:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 172, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.10423338413239, 'train_avg_loss': 0.6925352782011032, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 19:17:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 172, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.1292724609375, 'train_avg_loss': 0.6877693176269531, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 19:17:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 172, 'Results_raw': {'train_total': 480, 'train_loss': 330.1292724609375, 'train_avg_loss': 0.6877693176269531, 'train_seen': 480, 'train_correct': 268, 'train_acc': 0.5583333333333333}}
2025-10-09 19:17:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:17:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:17:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #172, planning to set LR to 1.00e-05
2025-10-09 19:17:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 19:17:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:17:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:17:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:17:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:17:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:17:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:17:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.145172, avg_loss=0.698219, seen=480, correct=246, accuracy=0.512500
2025-10-09 19:17:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:17:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:17:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:17:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=172 reserved=2330MB allocated=2217MB
2025-10-09 19:17:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 172, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.69548761844635, 'train_avg_loss': 0.6891290634870529, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 19:17:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 172, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.1451721191406, 'train_avg_loss': 0.698219108581543, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:17:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 172, 'Results_raw': {'train_total': 480, 'train_loss': 335.1451721191406, 'train_avg_loss': 0.698219108581543, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:17:54 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #173) -------------
2025-10-09 19:17:54 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=173 aidx=2 | s=5 (candidates=9)
2025-10-09 19:17:54 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[31, 37, 32, 50, 3] (from 9)
2025-10-09 19:17:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:17:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:17:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #173, planning to set LR to 1.00e-05
2025-10-09 19:17:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 19:17:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:17:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:17:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:17:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:17:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:18:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:18:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.791107, avg_loss=0.695398, seen=480, correct=234, accuracy=0.487500
2025-10-09 19:18:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:18:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:18:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:18:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=173 reserved=2348MB allocated=2217MB
2025-10-09 19:18:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 173, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.80331027507782, 'train_avg_loss': 0.6983609189589818, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 19:18:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 173, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.7911071777344, 'train_avg_loss': 0.6953981399536133, 'train_seen': 480, 'train_correct': 234, 'train_acc': 0.4875}}
2025-10-09 19:18:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 173, 'Results_raw': {'train_total': 480, 'train_loss': 333.7911071777344, 'train_avg_loss': 0.6953981399536133, 'train_seen': 480, 'train_correct': 234, 'train_acc': 0.4875}}
2025-10-09 19:18:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:18:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:18:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #173, planning to set LR to 1.00e-05
2025-10-09 19:18:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 19:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:18:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:18:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:18:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:18:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:19:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:19:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.183350, avg_loss=0.696215, seen=480, correct=267, accuracy=0.556250
2025-10-09 19:19:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:19:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:19:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=173 reserved=2330MB allocated=2217MB
2025-10-09 19:19:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 173, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.94777554273605, 'train_avg_loss': 0.7328981295228004, 'train_seen': 120, 'train_correct': 52, 'train_acc': 0.43333333333333335}}
2025-10-09 19:19:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 173, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.183349609375, 'train_avg_loss': 0.6962153116861979, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 19:19:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 173, 'Results_raw': {'train_total': 480, 'train_loss': 334.183349609375, 'train_avg_loss': 0.6962153116861979, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 19:19:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:19:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:19:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #173, planning to set LR to 1.00e-05
2025-10-09 19:19:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:19:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:19:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:19:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:19:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:19:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:20:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:20:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.983063, avg_loss=0.687465, seen=480, correct=267, accuracy=0.556250
2025-10-09 19:20:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:20:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:20:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=173 reserved=2330MB allocated=2217MB
2025-10-09 19:20:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 173, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.88048303127289, 'train_avg_loss': 0.690670691927274, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 19:20:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 173, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.9830627441406, 'train_avg_loss': 0.687464714050293, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 19:20:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 173, 'Results_raw': {'train_total': 480, 'train_loss': 329.9830627441406, 'train_avg_loss': 0.687464714050293, 'train_seen': 480, 'train_correct': 267, 'train_acc': 0.55625}}
2025-10-09 19:20:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:20:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:20:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #173, planning to set LR to 1.00e-05
2025-10-09 19:20:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:20:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:20:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:20:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:20:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:20:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:20:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.067780, avg_loss=0.691808, seen=480, correct=246, accuracy=0.512500
2025-10-09 19:20:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:20:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:20:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=173 reserved=2330MB allocated=2217MB
2025-10-09 19:20:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 173, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.5730391740799, 'train_avg_loss': 0.6964419931173325, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 19:20:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 173, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.0677795410156, 'train_avg_loss': 0.6918078740437825, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:20:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 173, 'Results_raw': {'train_total': 480, 'train_loss': 332.0677795410156, 'train_avg_loss': 0.6918078740437825, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:20:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:20:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:20:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #173, planning to set LR to 1.00e-05
2025-10-09 19:20:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 19:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:20:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:20:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:20:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:20:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:21:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:21:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.591614, avg_loss=0.699149, seen=480, correct=239, accuracy=0.497917
2025-10-09 19:21:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:21:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:21:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:21:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=173 reserved=2366MB allocated=2217MB
2025-10-09 19:21:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 173, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.29231429100037, 'train_avg_loss': 0.702435952425003, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 19:21:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 173, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.59161376953125, 'train_avg_loss': 0.6991491953531901, 'train_seen': 480, 'train_correct': 239, 'train_acc': 0.4979166666666667}}
2025-10-09 19:21:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 173, 'Results_raw': {'train_total': 480, 'train_loss': 335.59161376953125, 'train_avg_loss': 0.6991491953531901, 'train_seen': 480, 'train_correct': 239, 'train_acc': 0.4979166666666667}}
2025-10-09 19:21:30 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #174) -------------
2025-10-09 19:21:31 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=174 aidx=2 | s=5 (candidates=9)
2025-10-09 19:21:31 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[41, 5, 31, 37, 36] (from 9)
2025-10-09 19:21:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:21:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:21:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #174, planning to set LR to 1.00e-05
2025-10-09 19:21:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 19:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:21:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:21:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:21:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:21:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:22:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:22:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.964264, avg_loss=0.691592, seen=480, correct=255, accuracy=0.531250
2025-10-09 19:22:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:22:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:22:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:22:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=174 reserved=2330MB allocated=2217MB
2025-10-09 19:22:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 174, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.5009149312973, 'train_avg_loss': 0.6791742910941442, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 19:22:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 174, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.9642639160156, 'train_avg_loss': 0.6915922164916992, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 19:22:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 174, 'Results_raw': {'train_total': 480, 'train_loss': 331.9642639160156, 'train_avg_loss': 0.6915922164916992, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 19:22:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:22:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:22:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #174, planning to set LR to 1.00e-05
2025-10-09 19:22:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 19:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:22:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:22:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:22:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:22:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:22:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:22:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.470825, avg_loss=0.698898, seen=480, correct=262, accuracy=0.545833
2025-10-09 19:22:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:22:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:22:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:22:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=174 reserved=2396MB allocated=2217MB
2025-10-09 19:22:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 174, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.88726669549942, 'train_avg_loss': 0.6907272224624952, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 19:22:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 174, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.4708251953125, 'train_avg_loss': 0.6988975524902343, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 19:22:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 174, 'Results_raw': {'train_total': 480, 'train_loss': 335.4708251953125, 'train_avg_loss': 0.6988975524902343, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 19:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:22:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:22:58 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #174, planning to set LR to 1.00e-05
2025-10-09 19:22:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 19:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:22:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:22:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:22:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:22:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:23:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:23:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.508270, avg_loss=0.698976, seen=480, correct=234, accuracy=0.487500
2025-10-09 19:23:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:23:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:23:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=174 reserved=2348MB allocated=2217MB
2025-10-09 19:23:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 174, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.1342363357544, 'train_avg_loss': 0.7011186361312867, 'train_seen': 120, 'train_correct': 56, 'train_acc': 0.4666666666666667}}
2025-10-09 19:23:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 174, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.5082702636719, 'train_avg_loss': 0.6989755630493164, 'train_seen': 480, 'train_correct': 234, 'train_acc': 0.4875}}
2025-10-09 19:23:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 174, 'Results_raw': {'train_total': 480, 'train_loss': 335.5082702636719, 'train_avg_loss': 0.6989755630493164, 'train_seen': 480, 'train_correct': 234, 'train_acc': 0.4875}}
2025-10-09 19:23:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:23:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:23:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #174, planning to set LR to 1.00e-05
2025-10-09 19:23:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 19:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:23:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:23:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:23:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:23:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:24:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:24:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.842346, avg_loss=0.695505, seen=480, correct=261, accuracy=0.543750
2025-10-09 19:24:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:24:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:24:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=174 reserved=2330MB allocated=2217MB
2025-10-09 19:24:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 174, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 88.46924698352814, 'train_avg_loss': 0.7372437248627345, 'train_seen': 120, 'train_correct': 52, 'train_acc': 0.43333333333333335}}
2025-10-09 19:24:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 174, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.84234619140625, 'train_avg_loss': 0.695504887898763, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 19:24:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 174, 'Results_raw': {'train_total': 480, 'train_loss': 333.84234619140625, 'train_avg_loss': 0.695504887898763, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 19:24:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:24:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:24:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #174, planning to set LR to 1.00e-05
2025-10-09 19:24:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:24:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:24:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:24:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:24:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:24:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:25:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:25:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.903748, avg_loss=0.685216, seen=480, correct=258, accuracy=0.537500
2025-10-09 19:25:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:25:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:25:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=174 reserved=2348MB allocated=2217MB
2025-10-09 19:25:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 174, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.6250147819519, 'train_avg_loss': 0.6802084565162658, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 19:25:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 174, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.90374755859375, 'train_avg_loss': 0.6852161407470703, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:25:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 174, 'Results_raw': {'train_total': 480, 'train_loss': 328.90374755859375, 'train_avg_loss': 0.6852161407470703, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:25:07 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #175) -------------
2025-10-09 19:25:07 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=175 aidx=2 | s=5 (candidates=9)
2025-10-09 19:25:07 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[36, 5, 50, 21, 31] (from 9)
2025-10-09 19:25:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:25:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:25:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #175, planning to set LR to 1.00e-05
2025-10-09 19:25:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:25:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:25:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:25:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:25:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:25:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:25:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.320679, avg_loss=0.679835, seen=480, correct=264, accuracy=0.550000
2025-10-09 19:25:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:25:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:25:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:25:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=175 reserved=2348MB allocated=2217MB
2025-10-09 19:25:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 175, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.15496617555618, 'train_avg_loss': 0.6762913847963016, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 19:25:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 175, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.3206787109375, 'train_avg_loss': 0.6798347473144531, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 19:25:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 175, 'Results_raw': {'train_total': 480, 'train_loss': 326.3206787109375, 'train_avg_loss': 0.6798347473144531, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 19:25:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:25:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:25:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #175, planning to set LR to 1.00e-05
2025-10-09 19:25:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 19:25:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:25:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:25:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:25:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:25:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:26:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:26:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.405365, avg_loss=0.700845, seen=480, correct=264, accuracy=0.550000
2025-10-09 19:26:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:26:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:26:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:26:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=175 reserved=2396MB allocated=2217MB
2025-10-09 19:26:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 175, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.37635028362274, 'train_avg_loss': 0.6948029190301895, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 19:26:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 175, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.4053649902344, 'train_avg_loss': 0.7008445103963216, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 19:26:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 175, 'Results_raw': {'train_total': 480, 'train_loss': 336.4053649902344, 'train_avg_loss': 0.7008445103963216, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 19:26:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:26:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:26:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #175, planning to set LR to 1.00e-05
2025-10-09 19:26:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:26:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:26:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:26:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:26:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:26:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:27:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:27:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.590851, avg_loss=0.692898, seen=480, correct=241, accuracy=0.502083
2025-10-09 19:27:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:27:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:27:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:27:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=175 reserved=2330MB allocated=2217MB
2025-10-09 19:27:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 175, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.86371332406998, 'train_avg_loss': 0.7071976110339164, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 19:27:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 175, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.5908508300781, 'train_avg_loss': 0.6928976058959961, 'train_seen': 480, 'train_correct': 241, 'train_acc': 0.5020833333333333}}
2025-10-09 19:27:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 175, 'Results_raw': {'train_total': 480, 'train_loss': 332.5908508300781, 'train_avg_loss': 0.6928976058959961, 'train_seen': 480, 'train_correct': 241, 'train_acc': 0.5020833333333333}}
2025-10-09 19:27:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:27:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:27:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #175, planning to set LR to 1.00e-05
2025-10-09 19:27:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 19:27:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:27:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:27:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:27:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:27:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:28:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:28:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.624268, avg_loss=0.686717, seen=480, correct=262, accuracy=0.545833
2025-10-09 19:28:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:28:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:28:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:28:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=175 reserved=2374MB allocated=2217MB
2025-10-09 19:28:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 175, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.43316107988358, 'train_avg_loss': 0.6952763423323631, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 19:28:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 175, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.624267578125, 'train_avg_loss': 0.6867172241210937, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 19:28:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 175, 'Results_raw': {'train_total': 480, 'train_loss': 329.624267578125, 'train_avg_loss': 0.6867172241210937, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 19:28:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:28:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:28:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #175, planning to set LR to 1.00e-05
2025-10-09 19:28:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 19:28:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:28:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:28:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:28:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:28:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:28:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:28:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.184509, avg_loss=0.700384, seen=480, correct=245, accuracy=0.510417
2025-10-09 19:28:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:28:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:28:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:28:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=175 reserved=2348MB allocated=2217MB
2025-10-09 19:28:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 175, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.30883228778839, 'train_avg_loss': 0.7025736023982366, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 19:28:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 175, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.18450927734375, 'train_avg_loss': 0.7003843943277995, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-10-09 19:28:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 175, 'Results_raw': {'train_total': 480, 'train_loss': 336.18450927734375, 'train_avg_loss': 0.7003843943277995, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-10-09 19:28:46 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #176) -------------
2025-10-09 19:28:46 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=176 aidx=2 | s=5 (candidates=9)
2025-10-09 19:28:46 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[50, 21, 5, 31, 36] (from 9)
2025-10-09 19:28:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:28:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:28:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #176, planning to set LR to 1.00e-05
2025-10-09 19:28:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:28:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:28:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:28:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:28:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:28:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:29:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:29:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.608643, avg_loss=0.692935, seen=480, correct=242, accuracy=0.504167
2025-10-09 19:29:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:29:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:29:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=176 reserved=2330MB allocated=2217MB
2025-10-09 19:29:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 176, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.24370455741882, 'train_avg_loss': 0.7020308713118235, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 19:29:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 176, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.608642578125, 'train_avg_loss': 0.6929346720377604, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 19:29:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 176, 'Results_raw': {'train_total': 480, 'train_loss': 332.608642578125, 'train_avg_loss': 0.6929346720377604, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 19:29:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:29:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:29:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #176, planning to set LR to 1.00e-05
2025-10-09 19:29:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 19:29:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:29:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:29:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:29:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:29:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:30:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:30:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.167511, avg_loss=0.685766, seen=480, correct=263, accuracy=0.547917
2025-10-09 19:30:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:30:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:30:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:30:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=176 reserved=2374MB allocated=2217MB
2025-10-09 19:30:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 176, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.98395001888275, 'train_avg_loss': 0.691532916824023, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 19:30:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 176, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.1675109863281, 'train_avg_loss': 0.6857656478881836, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:30:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 176, 'Results_raw': {'train_total': 480, 'train_loss': 329.1675109863281, 'train_avg_loss': 0.6857656478881836, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:30:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:30:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:30:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #176, planning to set LR to 1.00e-05
2025-10-09 19:30:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 19:30:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:30:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:30:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:30:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:30:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:30:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:30:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.316223, avg_loss=0.698575, seen=480, correct=262, accuracy=0.545833
2025-10-09 19:30:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:30:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:30:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:30:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=176 reserved=2396MB allocated=2217MB
2025-10-09 19:30:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 176, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.8591457605362, 'train_avg_loss': 0.6904928813378016, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 19:30:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 176, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.31622314453125, 'train_avg_loss': 0.6985754648844401, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 19:30:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 176, 'Results_raw': {'train_total': 480, 'train_loss': 335.31622314453125, 'train_avg_loss': 0.6985754648844401, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 19:30:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:30:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:30:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #176, planning to set LR to 1.00e-05
2025-10-09 19:30:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 19:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:30:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:30:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:30:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:30:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:31:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:31:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.181366, avg_loss=0.694128, seen=480, correct=248, accuracy=0.516667
2025-10-09 19:31:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:31:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:31:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:31:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=176 reserved=2348MB allocated=2217MB
2025-10-09 19:31:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 176, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.40122723579407, 'train_avg_loss': 0.6950102269649505, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 19:31:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 176, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.1813659667969, 'train_avg_loss': 0.6941278457641602, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 19:31:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 176, 'Results_raw': {'train_total': 480, 'train_loss': 333.1813659667969, 'train_avg_loss': 0.6941278457641602, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 19:31:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:31:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:31:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #176, planning to set LR to 1.00e-05
2025-10-09 19:31:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:31:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:31:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:31:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:31:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:31:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:32:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:32:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.611420, avg_loss=0.680440, seen=480, correct=266, accuracy=0.554167
2025-10-09 19:32:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:32:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:32:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:32:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=176 reserved=2348MB allocated=2217MB
2025-10-09 19:32:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 176, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.238884806633, 'train_avg_loss': 0.6769907067219416, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 19:32:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 176, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.6114196777344, 'train_avg_loss': 0.6804404576619466, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 19:32:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 176, 'Results_raw': {'train_total': 480, 'train_loss': 326.6114196777344, 'train_avg_loss': 0.6804404576619466, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 19:32:27 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #177) -------------
2025-10-09 19:32:27 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=177 aidx=2 | s=5 (candidates=9)
2025-10-09 19:32:27 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[32, 50, 37, 21, 36] (from 9)
2025-10-09 19:32:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:32:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:32:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #177, planning to set LR to 1.00e-05
2025-10-09 19:32:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:32:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:32:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:32:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:32:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:32:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:33:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:33:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.464203, avg_loss=0.686384, seen=480, correct=258, accuracy=0.537500
2025-10-09 19:33:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:33:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:33:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:33:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=177 reserved=2330MB allocated=2217MB
2025-10-09 19:33:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 177, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.02504408359528, 'train_avg_loss': 0.6835420340299606, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 19:33:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 177, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.4642028808594, 'train_avg_loss': 0.6863837560017904, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:33:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 177, 'Results_raw': {'train_total': 480, 'train_loss': 329.4642028808594, 'train_avg_loss': 0.6863837560017904, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:33:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:33:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:33:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #177, planning to set LR to 1.00e-05
2025-10-09 19:33:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:33:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:33:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:33:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:33:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:33:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:33:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:33:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.189941, avg_loss=0.692062, seen=480, correct=257, accuracy=0.535417
2025-10-09 19:33:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:33:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:33:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:33:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=177 reserved=2330MB allocated=2217MB
2025-10-09 19:33:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 177, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.41480231285095, 'train_avg_loss': 0.7034566859404247, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 19:33:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 177, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.18994140625, 'train_avg_loss': 0.6920623779296875, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 19:33:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 177, 'Results_raw': {'train_total': 480, 'train_loss': 332.18994140625, 'train_avg_loss': 0.6920623779296875, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 19:33:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:33:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:33:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #177, planning to set LR to 1.00e-05
2025-10-09 19:33:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 19:33:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:33:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:33:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:33:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:33:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:34:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:34:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.498779, avg_loss=0.694789, seen=480, correct=258, accuracy=0.537500
2025-10-09 19:34:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:34:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:34:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:34:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=177 reserved=2330MB allocated=2217MB
2025-10-09 19:34:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 177, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.25109738111496, 'train_avg_loss': 0.727092478175958, 'train_seen': 120, 'train_correct': 54, 'train_acc': 0.45}}
2025-10-09 19:34:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 177, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.498779296875, 'train_avg_loss': 0.6947891235351562, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:34:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 177, 'Results_raw': {'train_total': 480, 'train_loss': 333.498779296875, 'train_avg_loss': 0.6947891235351562, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:34:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:34:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:34:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #177, planning to set LR to 1.00e-05
2025-10-09 19:34:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 19:34:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:34:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:34:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:34:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:34:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:35:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:35:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.625549, avg_loss=0.686720, seen=480, correct=254, accuracy=0.529167
2025-10-09 19:35:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:35:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:35:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:35:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=177 reserved=2374MB allocated=2217MB
2025-10-09 19:35:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 177, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.8641152381897, 'train_avg_loss': 0.6988676269849141, 'train_seen': 120, 'train_correct': 52, 'train_acc': 0.43333333333333335}}
2025-10-09 19:35:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 177, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.62554931640625, 'train_avg_loss': 0.6867198944091797, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 19:35:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 177, 'Results_raw': {'train_total': 480, 'train_loss': 329.62554931640625, 'train_avg_loss': 0.6867198944091797, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-10-09 19:35:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:35:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:35:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #177, planning to set LR to 1.00e-05
2025-10-09 19:35:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:35:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:35:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:35:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:35:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:35:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:35:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:35:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.561462, avg_loss=0.676170, seen=480, correct=273, accuracy=0.568750
2025-10-09 19:35:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:35:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:35:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:35:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=177 reserved=2348MB allocated=2217MB
2025-10-09 19:35:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 177, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.46508306264877, 'train_avg_loss': 0.6705423588554065, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 19:35:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 177, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.56146240234375, 'train_avg_loss': 0.6761697133382162, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 19:35:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 177, 'Results_raw': {'train_total': 480, 'train_loss': 324.56146240234375, 'train_avg_loss': 0.6761697133382162, 'train_seen': 480, 'train_correct': 273, 'train_acc': 0.56875}}
2025-10-09 19:36:00 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #178) -------------
2025-10-09 19:36:00 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=178 aidx=2 | s=5 (candidates=9)
2025-10-09 19:36:00 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[41, 37, 31, 36, 21] (from 9)
2025-10-09 19:36:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:36:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:36:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #178, planning to set LR to 1.00e-05
2025-10-09 19:36:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 19:36:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:36:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:36:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:36:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:36:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:36:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:36:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.288147, avg_loss=0.690184, seen=480, correct=260, accuracy=0.541667
2025-10-09 19:36:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:36:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:36:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:36:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=178 reserved=2330MB allocated=2217MB
2025-10-09 19:36:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 178, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.70968574285507, 'train_avg_loss': 0.6809140478571256, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 19:36:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 178, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.28814697265625, 'train_avg_loss': 0.6901836395263672, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 19:36:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 178, 'Results_raw': {'train_total': 480, 'train_loss': 331.28814697265625, 'train_avg_loss': 0.6901836395263672, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 19:36:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:36:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:36:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #178, planning to set LR to 1.00e-05
2025-10-09 19:36:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 19:36:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:36:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:36:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:36:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:36:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:37:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:37:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.210144, avg_loss=0.694188, seen=480, correct=263, accuracy=0.547917
2025-10-09 19:37:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:37:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:37:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:37:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=178 reserved=2330MB allocated=2217MB
2025-10-09 19:37:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 178, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 87.13130331039429, 'train_avg_loss': 0.7260941942532857, 'train_seen': 120, 'train_correct': 54, 'train_acc': 0.45}}
2025-10-09 19:37:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 178, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.21014404296875, 'train_avg_loss': 0.6941878000895182, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:37:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 178, 'Results_raw': {'train_total': 480, 'train_loss': 333.21014404296875, 'train_avg_loss': 0.6941878000895182, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:37:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:37:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:37:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #178, planning to set LR to 1.00e-05
2025-10-09 19:37:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 19:37:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:37:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:37:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:37:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:37:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:38:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:38:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.622070, avg_loss=0.695046, seen=480, correct=245, accuracy=0.510417
2025-10-09 19:38:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:38:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:38:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:38:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=178 reserved=2348MB allocated=2217MB
2025-10-09 19:38:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 178, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.37209892272949, 'train_avg_loss': 0.6947674910227458, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 19:38:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 178, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.6220703125, 'train_avg_loss': 0.6950459798177083, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-10-09 19:38:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 178, 'Results_raw': {'train_total': 480, 'train_loss': 333.6220703125, 'train_avg_loss': 0.6950459798177083, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-10-09 19:38:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:38:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:38:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #178, planning to set LR to 1.00e-05
2025-10-09 19:38:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:38:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:38:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:38:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:38:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:38:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:38:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:38:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.809113, avg_loss=0.670436, seen=480, correct=274, accuracy=0.570833
2025-10-09 19:38:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:38:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:38:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:38:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=178 reserved=2348MB allocated=2217MB
2025-10-09 19:38:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 178, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.81160825490952, 'train_avg_loss': 0.6650967354575793, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 19:38:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 178, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.8091125488281, 'train_avg_loss': 0.6704356511433919, 'train_seen': 480, 'train_correct': 274, 'train_acc': 0.5708333333333333}}
2025-10-09 19:38:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 178, 'Results_raw': {'train_total': 480, 'train_loss': 321.8091125488281, 'train_avg_loss': 0.6704356511433919, 'train_seen': 480, 'train_correct': 274, 'train_acc': 0.5708333333333333}}
2025-10-09 19:38:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:38:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:38:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #178, planning to set LR to 1.00e-05
2025-10-09 19:38:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 19:38:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:38:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:38:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:38:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:38:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:39:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:39:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.917847, avg_loss=0.685246, seen=480, correct=259, accuracy=0.539583
2025-10-09 19:39:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:39:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:39:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:39:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=178 reserved=2374MB allocated=2217MB
2025-10-09 19:39:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 178, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.28362941741943, 'train_avg_loss': 0.694030245145162, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 19:39:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 178, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.9178466796875, 'train_avg_loss': 0.6852455139160156, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 19:39:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 178, 'Results_raw': {'train_total': 480, 'train_loss': 328.9178466796875, 'train_avg_loss': 0.6852455139160156, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 19:39:38 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #179) -------------
2025-10-09 19:39:38 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=179 aidx=2 | s=5 (candidates=9)
2025-10-09 19:39:38 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[3, 5, 36, 32, 50] (from 9)
2025-10-09 19:39:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:39:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:39:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #179, planning to set LR to 1.00e-05
2025-10-09 19:39:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 19:39:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:39:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:39:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:39:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:39:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:40:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:40:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.295197, avg_loss=0.694365, seen=480, correct=242, accuracy=0.504167
2025-10-09 19:40:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:40:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:40:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:40:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=179 reserved=2370MB allocated=2217MB
2025-10-09 19:40:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 179, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.02176523208618, 'train_avg_loss': 0.7001813769340515, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 19:40:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 179, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.2951965332031, 'train_avg_loss': 0.6943649927775065, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 19:40:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 179, 'Results_raw': {'train_total': 480, 'train_loss': 333.2951965332031, 'train_avg_loss': 0.6943649927775065, 'train_seen': 480, 'train_correct': 242, 'train_acc': 0.5041666666666667}}
2025-10-09 19:40:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:40:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:40:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #179, planning to set LR to 1.00e-05
2025-10-09 19:40:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 19:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:40:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:40:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:40:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:40:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:41:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:41:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.257080, avg_loss=0.700536, seen=480, correct=263, accuracy=0.547917
2025-10-09 19:41:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:41:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:41:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:41:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=179 reserved=2398MB allocated=2217MB
2025-10-09 19:41:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 179, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.51185321807861, 'train_avg_loss': 0.6959321101506551, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 19:41:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 179, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.257080078125, 'train_avg_loss': 0.7005355834960938, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:41:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 179, 'Results_raw': {'train_total': 480, 'train_loss': 336.257080078125, 'train_avg_loss': 0.7005355834960938, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:41:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:41:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:41:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #179, planning to set LR to 1.00e-05
2025-10-09 19:41:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:41:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:41:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:41:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:41:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:41:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:41:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:41:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.039612, avg_loss=0.670916, seen=480, correct=270, accuracy=0.562500
2025-10-09 19:41:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:41:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:41:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:41:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=179 reserved=2348MB allocated=2217MB
2025-10-09 19:41:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 179, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.13738715648651, 'train_avg_loss': 0.6678115596373876, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 19:41:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 179, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.03961181640625, 'train_avg_loss': 0.6709158579508464, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 19:41:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 179, 'Results_raw': {'train_total': 480, 'train_loss': 322.03961181640625, 'train_avg_loss': 0.6709158579508464, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 19:41:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:41:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:41:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #179, planning to set LR to 1.00e-05
2025-10-09 19:41:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:41:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:41:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:41:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:41:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:42:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:42:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.783478, avg_loss=0.684966, seen=480, correct=266, accuracy=0.554167
2025-10-09 19:42:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:42:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:42:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:42:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=179 reserved=2330MB allocated=2217MB
2025-10-09 19:42:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 179, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.0884598493576, 'train_avg_loss': 0.6840704987446468, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 19:42:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 179, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.7834777832031, 'train_avg_loss': 0.6849655787150065, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 19:42:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 179, 'Results_raw': {'train_total': 480, 'train_loss': 328.7834777832031, 'train_avg_loss': 0.6849655787150065, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 19:42:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:42:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:42:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #179, planning to set LR to 1.00e-05
2025-10-09 19:42:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:42:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:42:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:42:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:42:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:43:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:43:16 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.535645, avg_loss=0.692783, seen=480, correct=244, accuracy=0.508333
2025-10-09 19:43:16 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:43:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:43:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:43:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=179 reserved=2330MB allocated=2217MB
2025-10-09 19:43:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 179, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.37042969465256, 'train_avg_loss': 0.7030869141221047, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 19:43:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 179, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.53564453125, 'train_avg_loss': 0.6927825927734375, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 19:43:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 179, 'Results_raw': {'train_total': 480, 'train_loss': 332.53564453125, 'train_avg_loss': 0.6927825927734375, 'train_seen': 480, 'train_correct': 244, 'train_acc': 0.5083333333333333}}
2025-10-09 19:43:19 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #180) -------------
2025-10-09 19:43:19 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=180 aidx=2 | s=5 (candidates=9)
2025-10-09 19:43:19 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[41, 50, 3, 5, 36] (from 9)
2025-10-09 19:43:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:43:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:43:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #180, planning to set LR to 1.00e-05
2025-10-09 19:43:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 19:43:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:43:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:43:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:43:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:43:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:44:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:44:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.140106, avg_loss=0.689875, seen=480, correct=255, accuracy=0.531250
2025-10-09 19:44:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:44:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:44:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=180 reserved=2330MB allocated=2217MB
2025-10-09 19:44:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 180, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.68947345018387, 'train_avg_loss': 0.6807456120848656, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 19:44:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 180, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.1401062011719, 'train_avg_loss': 0.6898752212524414, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 19:44:05 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 180, 'Results_raw': {'train_total': 480, 'train_loss': 331.1401062011719, 'train_avg_loss': 0.6898752212524414, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 19:44:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:44:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:44:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #180, planning to set LR to 1.00e-05
2025-10-09 19:44:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:44:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:44:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:44:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:44:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:44:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:44:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.828461, avg_loss=0.687143, seen=480, correct=261, accuracy=0.543750
2025-10-09 19:44:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:44:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:44:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=180 reserved=2330MB allocated=2217MB
2025-10-09 19:44:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 180, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.78830361366272, 'train_avg_loss': 0.6982358634471894, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 19:44:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 180, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.8284606933594, 'train_avg_loss': 0.6871426264444987, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 19:44:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 180, 'Results_raw': {'train_total': 480, 'train_loss': 329.8284606933594, 'train_avg_loss': 0.6871426264444987, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 19:44:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:44:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:44:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #180, planning to set LR to 1.00e-05
2025-10-09 19:44:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 19:44:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:44:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:44:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:44:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:44:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:45:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:45:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.258179, avg_loss=0.694288, seen=480, correct=248, accuracy=0.516667
2025-10-09 19:45:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:45:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:45:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:45:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=180 reserved=2370MB allocated=2217MB
2025-10-09 19:45:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 180, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.66497313976288, 'train_avg_loss': 0.697208109498024, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 19:45:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 180, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.2581787109375, 'train_avg_loss': 0.6942878723144531, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 19:45:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 180, 'Results_raw': {'train_total': 480, 'train_loss': 333.2581787109375, 'train_avg_loss': 0.6942878723144531, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 19:45:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:45:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:45:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #180, planning to set LR to 1.00e-05
2025-10-09 19:45:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 19:45:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:45:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:45:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:45:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:45:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:46:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:46:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=336.840027, avg_loss=0.701750, seen=480, correct=269, accuracy=0.560417
2025-10-09 19:46:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:46:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:46:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=180 reserved=2398MB allocated=2217MB
2025-10-09 19:46:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 180, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.76454669237137, 'train_avg_loss': 0.6980378891030947, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 19:46:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 180, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 336.84002685546875, 'train_avg_loss': 0.7017500559488933, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 19:46:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 180, 'Results_raw': {'train_total': 480, 'train_loss': 336.84002685546875, 'train_avg_loss': 0.7017500559488933, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 19:46:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:46:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:46:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #180, planning to set LR to 1.00e-05
2025-10-09 19:46:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:46:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:46:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:46:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:46:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:46:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:46:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:46:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.036621, avg_loss=0.670910, seen=480, correct=276, accuracy=0.575000
2025-10-09 19:46:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:46:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:46:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=180 reserved=2348MB allocated=2217MB
2025-10-09 19:46:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 180, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.63357597589493, 'train_avg_loss': 0.6636131331324577, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 19:46:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 180, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.03662109375, 'train_avg_loss': 0.6709096272786458, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 19:46:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 180, 'Results_raw': {'train_total': 480, 'train_loss': 322.03662109375, 'train_avg_loss': 0.6709096272786458, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 19:47:00 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #181) -------------
2025-10-09 19:47:00 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=181 aidx=2 | s=5 (candidates=9)
2025-10-09 19:47:00 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[21, 3, 31, 37, 41] (from 9)
2025-10-09 19:47:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:47:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:47:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #181, planning to set LR to 1.00e-05
2025-10-09 19:47:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 19:47:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:47:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:47:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:47:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:47:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:47:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:47:40 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.000549, avg_loss=0.681251, seen=480, correct=258, accuracy=0.537500
2025-10-09 19:47:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:47:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:47:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=181 reserved=2374MB allocated=2217MB
2025-10-09 19:47:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 181, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.89868688583374, 'train_avg_loss': 0.6908223907152812, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 19:47:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 181, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.00054931640625, 'train_avg_loss': 0.6812511444091797, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:47:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 181, 'Results_raw': {'train_total': 480, 'train_loss': 327.00054931640625, 'train_avg_loss': 0.6812511444091797, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 19:47:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:47:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:47:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #181, planning to set LR to 1.00e-05
2025-10-09 19:47:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 19:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:47:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:47:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:47:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:47:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:48:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:48:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.313843, avg_loss=0.694404, seen=480, correct=246, accuracy=0.512500
2025-10-09 19:48:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:48:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:48:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=181 reserved=2368MB allocated=2217MB
2025-10-09 19:48:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 181, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.03902316093445, 'train_avg_loss': 0.7003251930077871, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 19:48:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 181, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.3138427734375, 'train_avg_loss': 0.6944038391113281, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:48:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 181, 'Results_raw': {'train_total': 480, 'train_loss': 333.3138427734375, 'train_avg_loss': 0.6944038391113281, 'train_seen': 480, 'train_correct': 246, 'train_acc': 0.5125}}
2025-10-09 19:48:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:48:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:48:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #181, planning to set LR to 1.00e-05
2025-10-09 19:48:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 19:48:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:48:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:48:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:48:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:48:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:49:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:49:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.520844, avg_loss=0.694835, seen=480, correct=249, accuracy=0.518750
2025-10-09 19:49:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:49:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:49:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:49:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=181 reserved=2348MB allocated=2217MB
2025-10-09 19:49:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 181, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.77874398231506, 'train_avg_loss': 0.6981561998526256, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 19:49:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 181, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.5208435058594, 'train_avg_loss': 0.694835090637207, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 19:49:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 181, 'Results_raw': {'train_total': 480, 'train_loss': 333.5208435058594, 'train_avg_loss': 0.694835090637207, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 19:49:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:49:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:49:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #181, planning to set LR to 1.00e-05
2025-10-09 19:49:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 19:49:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:49:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:49:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:49:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:49:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:49:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:49:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.650696, avg_loss=0.690939, seen=480, correct=263, accuracy=0.547917
2025-10-09 19:49:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:49:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:49:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:49:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=181 reserved=2330MB allocated=2217MB
2025-10-09 19:49:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 181, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.21544694900513, 'train_avg_loss': 0.718462057908376, 'train_seen': 120, 'train_correct': 53, 'train_acc': 0.44166666666666665}}
2025-10-09 19:49:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 181, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.65069580078125, 'train_avg_loss': 0.6909389495849609, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:49:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 181, 'Results_raw': {'train_total': 480, 'train_loss': 331.65069580078125, 'train_avg_loss': 0.6909389495849609, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:49:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:49:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:49:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #181, planning to set LR to 1.00e-05
2025-10-09 19:49:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 19:49:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:49:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:49:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:49:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:49:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:50:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:50:30 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.744171, avg_loss=0.691134, seen=480, correct=259, accuracy=0.539583
2025-10-09 19:50:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:50:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:50:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:50:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=181 reserved=2330MB allocated=2217MB
2025-10-09 19:50:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 181, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.74200928211212, 'train_avg_loss': 0.6811834106842677, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 19:50:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 181, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.7441711425781, 'train_avg_loss': 0.691133689880371, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 19:50:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 181, 'Results_raw': {'train_total': 480, 'train_loss': 331.7441711425781, 'train_avg_loss': 0.691133689880371, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 19:50:33 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #182) -------------
2025-10-09 19:50:34 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=182 aidx=2 | s=5 (candidates=9)
2025-10-09 19:50:34 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[21, 50, 36, 31, 32] (from 9)
2025-10-09 19:50:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:50:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:50:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #182, planning to set LR to 1.00e-05
2025-10-09 19:50:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 19:50:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:50:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:50:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:50:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:50:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:51:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:51:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.248779, avg_loss=0.681768, seen=480, correct=263, accuracy=0.547917
2025-10-09 19:51:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:51:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:51:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:51:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=182 reserved=2374MB allocated=2217MB
2025-10-09 19:51:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 182, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.58034205436707, 'train_avg_loss': 0.6881695171197255, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 19:51:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 182, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.248779296875, 'train_avg_loss': 0.681768290201823, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:51:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 182, 'Results_raw': {'train_total': 480, 'train_loss': 327.248779296875, 'train_avg_loss': 0.681768290201823, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-10-09 19:51:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:51:16 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:51:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #182, planning to set LR to 1.00e-05
2025-10-09 19:51:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:51:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:51:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:51:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:51:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:51:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:51:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.552826, avg_loss=0.686568, seen=480, correct=249, accuracy=0.518750
2025-10-09 19:51:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:51:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:51:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=182 reserved=2330MB allocated=2217MB
2025-10-09 19:51:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 182, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.50917482376099, 'train_avg_loss': 0.6875764568646748, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 19:51:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 182, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.5528259277344, 'train_avg_loss': 0.6865683873494466, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 19:51:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 182, 'Results_raw': {'train_total': 480, 'train_loss': 329.5528259277344, 'train_avg_loss': 0.6865683873494466, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 19:51:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:51:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:51:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #182, planning to set LR to 1.00e-05
2025-10-09 19:51:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:51:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:51:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:51:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:51:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:51:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:52:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:52:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.062439, avg_loss=0.670963, seen=480, correct=280, accuracy=0.583333
2025-10-09 19:52:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:52:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:52:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:52:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=182 reserved=2348MB allocated=2217MB
2025-10-09 19:52:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 182, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.01711213588715, 'train_avg_loss': 0.6668092677990596, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 19:52:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 182, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.06243896484375, 'train_avg_loss': 0.6709634145100911, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 19:52:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 182, 'Results_raw': {'train_total': 480, 'train_loss': 322.06243896484375, 'train_avg_loss': 0.6709634145100911, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 19:52:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:52:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:52:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #182, planning to set LR to 1.00e-05
2025-10-09 19:52:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 19:52:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:52:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:52:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:52:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:52:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:53:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:53:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.636841, avg_loss=0.690910, seen=480, correct=249, accuracy=0.518750
2025-10-09 19:53:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:53:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:53:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:53:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=182 reserved=2348MB allocated=2217MB
2025-10-09 19:53:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 182, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.54635918140411, 'train_avg_loss': 0.6962196598450343, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 19:53:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 182, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.6368408203125, 'train_avg_loss': 0.6909100850423177, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 19:53:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 182, 'Results_raw': {'train_total': 480, 'train_loss': 331.6368408203125, 'train_avg_loss': 0.6909100850423177, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 19:53:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:53:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:53:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #182, planning to set LR to 1.00e-05
2025-10-09 19:53:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:53:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:53:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:53:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:53:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:53:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:54:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:54:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.516449, avg_loss=0.680243, seen=480, correct=276, accuracy=0.575000
2025-10-09 19:54:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:54:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:54:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=182 reserved=2330MB allocated=2217MB
2025-10-09 19:54:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 182, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.51033562421799, 'train_avg_loss': 0.6875861302018166, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 19:54:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 182, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.5164489746094, 'train_avg_loss': 0.6802426020304362, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 19:54:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 182, 'Results_raw': {'train_total': 480, 'train_loss': 326.5164489746094, 'train_avg_loss': 0.6802426020304362, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 19:54:08 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #183) -------------
2025-10-09 19:54:09 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=183 aidx=2 | s=5 (candidates=9)
2025-10-09 19:54:09 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[31, 32, 37, 50, 41] (from 9)
2025-10-09 19:54:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:54:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:54:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #183, planning to set LR to 1.00e-05
2025-10-09 19:54:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 19:54:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:54:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:54:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:54:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:54:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:54:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:54:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.526672, avg_loss=0.692764, seen=480, correct=250, accuracy=0.520833
2025-10-09 19:54:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:54:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:54:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:54:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=183 reserved=2348MB allocated=2217MB
2025-10-09 19:54:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 183, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.65590107440948, 'train_avg_loss': 0.6971325089534124, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 19:54:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 183, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.52667236328125, 'train_avg_loss': 0.6927639007568359, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 19:54:53 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 183, 'Results_raw': {'train_total': 480, 'train_loss': 332.52667236328125, 'train_avg_loss': 0.6927639007568359, 'train_seen': 480, 'train_correct': 250, 'train_acc': 0.5208333333333334}}
2025-10-09 19:54:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:54:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:54:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #183, planning to set LR to 1.00e-05
2025-10-09 19:54:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:54:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:54:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:54:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:54:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:54:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:55:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:55:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.987305, avg_loss=0.674974, seen=480, correct=280, accuracy=0.583333
2025-10-09 19:55:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:55:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:55:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=183 reserved=2330MB allocated=2217MB
2025-10-09 19:55:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 183, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.56470829248428, 'train_avg_loss': 0.6713725691040356, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 19:55:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 183, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.9873046875, 'train_avg_loss': 0.6749735514322917, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 19:55:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 183, 'Results_raw': {'train_total': 480, 'train_loss': 323.9873046875, 'train_avg_loss': 0.6749735514322917, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 19:55:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:55:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:55:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #183, planning to set LR to 1.00e-05
2025-10-09 19:55:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 19:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:55:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:55:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:55:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:55:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:56:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:56:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.744690, avg_loss=0.691135, seen=480, correct=264, accuracy=0.550000
2025-10-09 19:56:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:56:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:56:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:56:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=183 reserved=2330MB allocated=2217MB
2025-10-09 19:56:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 183, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.23486971855164, 'train_avg_loss': 0.7186239143212636, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 19:56:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 183, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.74468994140625, 'train_avg_loss': 0.6911347707112631, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 19:56:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 183, 'Results_raw': {'train_total': 480, 'train_loss': 331.74468994140625, 'train_avg_loss': 0.6911347707112631, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 19:56:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:56:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:56:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #183, planning to set LR to 1.00e-05
2025-10-09 19:56:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:56:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:56:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:56:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:56:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:56:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:57:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:57:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.490112, avg_loss=0.686438, seen=480, correct=256, accuracy=0.533333
2025-10-09 19:57:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:57:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:57:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:57:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=183 reserved=2330MB allocated=2217MB
2025-10-09 19:57:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 183, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.44434762001038, 'train_avg_loss': 0.6953695635000865, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 19:57:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 183, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.4901123046875, 'train_avg_loss': 0.6864377339680989, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 19:57:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 183, 'Results_raw': {'train_total': 480, 'train_loss': 329.4901123046875, 'train_avg_loss': 0.6864377339680989, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 19:57:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:57:07 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:57:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #183, planning to set LR to 1.00e-05
2025-10-09 19:57:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 19:57:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:57:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:57:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:57:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:57:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:57:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:57:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.055054, avg_loss=0.681365, seen=480, correct=259, accuracy=0.539583
2025-10-09 19:57:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:57:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:57:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:57:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=183 reserved=2330MB allocated=2217MB
2025-10-09 19:57:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 183, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.07606929540634, 'train_avg_loss': 0.6756339107950529, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 19:57:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 183, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.0550537109375, 'train_avg_loss': 0.6813646952311198, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 19:57:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 183, 'Results_raw': {'train_total': 480, 'train_loss': 327.0550537109375, 'train_avg_loss': 0.6813646952311198, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 19:57:48 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #184) -------------
2025-10-09 19:57:48 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=184 aidx=2 | s=5 (candidates=9)
2025-10-09 19:57:48 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[50, 36, 5, 32, 3] (from 9)
2025-10-09 19:57:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:57:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:57:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #184, planning to set LR to 1.00e-05
2025-10-09 19:57:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 19:57:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:57:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:57:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:57:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:57:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:58:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:58:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.764587, avg_loss=0.684926, seen=480, correct=257, accuracy=0.535417
2025-10-09 19:58:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:58:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:58:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:58:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=184 reserved=2330MB allocated=2217MB
2025-10-09 19:58:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 184, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.47015863656998, 'train_avg_loss': 0.6872513219714165, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 19:58:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 184, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.76458740234375, 'train_avg_loss': 0.6849262237548828, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 19:58:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 184, 'Results_raw': {'train_total': 480, 'train_loss': 328.76458740234375, 'train_avg_loss': 0.6849262237548828, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 19:58:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:58:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:58:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #184, planning to set LR to 1.00e-05
2025-10-09 19:58:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 19:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:58:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:58:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:58:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:58:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:59:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:59:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.000244, avg_loss=0.666667, seen=480, correct=279, accuracy=0.581250
2025-10-09 19:59:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:59:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:59:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:59:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=184 reserved=2348MB allocated=2217MB
2025-10-09 19:59:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 184, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.0201490521431, 'train_avg_loss': 0.6585012421011924, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 19:59:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 184, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.000244140625, 'train_avg_loss': 0.6666671752929687, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 19:59:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 184, 'Results_raw': {'train_total': 480, 'train_loss': 320.000244140625, 'train_avg_loss': 0.6666671752929687, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 19:59:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:59:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:59:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #184, planning to set LR to 1.00e-05
2025-10-09 19:59:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 19:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:59:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:59:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:59:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:59:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 19:59:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 19:59:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=337.499542, avg_loss=0.703124, seen=480, correct=248, accuracy=0.516667
2025-10-09 19:59:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 19:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:59:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 19:59:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=184 reserved=2396MB allocated=2217MB
2025-10-09 19:59:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 184, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.40435463190079, 'train_avg_loss': 0.7033696219325065, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 19:59:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 184, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 337.4995422363281, 'train_avg_loss': 0.7031240463256836, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 19:59:57 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 184, 'Results_raw': {'train_total': 480, 'train_loss': 337.4995422363281, 'train_avg_loss': 0.7031240463256836, 'train_seen': 480, 'train_correct': 248, 'train_acc': 0.5166666666666667}}
2025-10-09 19:59:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 19:59:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 19:59:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #184, planning to set LR to 1.00e-05
2025-10-09 19:59:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 19:59:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 19:59:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 19:59:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 19:59:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 19:59:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:00:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:00:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.031494, avg_loss=0.670899, seen=480, correct=290, accuracy=0.604167
2025-10-09 20:00:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:00:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:00:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:00:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=184 reserved=2330MB allocated=2217MB
2025-10-09 20:00:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 184, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.13648533821106, 'train_avg_loss': 0.6761373778184255, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 20:00:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 184, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.031494140625, 'train_avg_loss': 0.6708989461263021, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:00:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 184, 'Results_raw': {'train_total': 480, 'train_loss': 322.031494140625, 'train_avg_loss': 0.6708989461263021, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:00:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:00:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:00:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #184, planning to set LR to 1.00e-05
2025-10-09 20:00:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 20:00:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:00:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:00:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:00:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:00:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:01:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:01:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=333.864441, avg_loss=0.695551, seen=480, correct=235, accuracy=0.489583
2025-10-09 20:01:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:01:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:01:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:01:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=184 reserved=2368MB allocated=2217MB
2025-10-09 20:01:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 184, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.10014045238495, 'train_avg_loss': 0.7008345037698746, 'train_seen': 120, 'train_correct': 54, 'train_acc': 0.45}}
2025-10-09 20:01:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 184, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.86444091796875, 'train_avg_loss': 0.6955509185791016, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 20:01:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 184, 'Results_raw': {'train_total': 480, 'train_loss': 333.86444091796875, 'train_avg_loss': 0.6955509185791016, 'train_seen': 480, 'train_correct': 235, 'train_acc': 0.4895833333333333}}
2025-10-09 20:01:24 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #185) -------------
2025-10-09 20:01:25 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=185 aidx=2 | s=5 (candidates=9)
2025-10-09 20:01:25 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[50, 36, 41, 3, 31] (from 9)
2025-10-09 20:01:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:01:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:01:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #185, planning to set LR to 1.00e-05
2025-10-09 20:01:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:01:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:01:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:01:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:01:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:01:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:02:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:02:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.445435, avg_loss=0.686345, seen=480, correct=260, accuracy=0.541667
2025-10-09 20:02:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:02:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:02:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:02:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=185 reserved=2330MB allocated=2217MB
2025-10-09 20:02:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 185, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.67788845300674, 'train_avg_loss': 0.6973157371083896, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 20:02:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 185, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.4454345703125, 'train_avg_loss': 0.6863446553548177, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 20:02:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 185, 'Results_raw': {'train_total': 480, 'train_loss': 329.4454345703125, 'train_avg_loss': 0.6863446553548177, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 20:02:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:02:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:02:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #185, planning to set LR to 1.00e-05
2025-10-09 20:02:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 20:02:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:02:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:02:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:02:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:02:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:02:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:02:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.978241, avg_loss=0.666621, seen=480, correct=279, accuracy=0.581250
2025-10-09 20:02:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:02:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:02:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:02:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=185 reserved=2348MB allocated=2217MB
2025-10-09 20:02:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 185, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.95166039466858, 'train_avg_loss': 0.6579305032889048, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 20:02:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 185, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.9782409667969, 'train_avg_loss': 0.6666213353474935, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 20:02:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 185, 'Results_raw': {'train_total': 480, 'train_loss': 319.9782409667969, 'train_avg_loss': 0.6666213353474935, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 20:02:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:02:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:02:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #185, planning to set LR to 1.00e-05
2025-10-09 20:02:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 20:02:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:02:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:02:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:02:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:02:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:03:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:03:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.831238, avg_loss=0.678815, seen=480, correct=270, accuracy=0.562500
2025-10-09 20:03:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:03:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:03:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:03:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=185 reserved=2330MB allocated=2217MB
2025-10-09 20:03:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 185, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.23694866895676, 'train_avg_loss': 0.668641238907973, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 20:03:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 185, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.83123779296875, 'train_avg_loss': 0.6788150787353515, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 20:03:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 185, 'Results_raw': {'train_total': 480, 'train_loss': 325.83123779296875, 'train_avg_loss': 0.6788150787353515, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 20:03:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:03:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:03:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #185, planning to set LR to 1.00e-05
2025-10-09 20:03:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 20:03:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:03:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:03:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:03:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:03:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:04:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.002686, avg_loss=0.687506, seen=480, correct=249, accuracy=0.518750
2025-10-09 20:04:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:04:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:04:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=185 reserved=2368MB allocated=2217MB
2025-10-09 20:04:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 185, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.7489104270935, 'train_avg_loss': 0.6895742535591125, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 20:04:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 185, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.002685546875, 'train_avg_loss': 0.687505594889323, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 20:04:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 185, 'Results_raw': {'train_total': 480, 'train_loss': 330.002685546875, 'train_avg_loss': 0.687505594889323, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 20:04:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:04:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:04:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #185, planning to set LR to 1.00e-05
2025-10-09 20:04:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:04:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:04:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:04:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:04:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:04:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:04:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:04:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.187622, avg_loss=0.689974, seen=480, correct=251, accuracy=0.522917
2025-10-09 20:04:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:04:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:05:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:05:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=185 reserved=2348MB allocated=2217MB
2025-10-09 20:05:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 185, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.14217448234558, 'train_avg_loss': 0.6928514540195465, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 20:05:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 185, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.1876220703125, 'train_avg_loss': 0.6899742126464844, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 20:05:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 185, 'Results_raw': {'train_total': 480, 'train_loss': 331.1876220703125, 'train_avg_loss': 0.6899742126464844, 'train_seen': 480, 'train_correct': 251, 'train_acc': 0.5229166666666667}}
2025-10-09 20:05:04 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #186) -------------
2025-10-09 20:05:04 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=186 aidx=2 | s=5 (candidates=9)
2025-10-09 20:05:04 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[41, 32, 31, 21, 37] (from 9)
2025-10-09 20:05:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:05:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:05:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #186, planning to set LR to 1.00e-05
2025-10-09 20:05:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 20:05:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:05:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:05:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:05:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:05:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:05:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:05:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.826416, avg_loss=0.676722, seen=480, correct=276, accuracy=0.575000
2025-10-09 20:05:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:05:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:05:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:05:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=186 reserved=2330MB allocated=2217MB
2025-10-09 20:05:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 186, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.27578240633011, 'train_avg_loss': 0.6689648533860842, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 20:05:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 186, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.826416015625, 'train_avg_loss': 0.676721700032552, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 20:05:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 186, 'Results_raw': {'train_total': 480, 'train_loss': 324.826416015625, 'train_avg_loss': 0.676721700032552, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 20:05:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:05:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:05:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #186, planning to set LR to 1.00e-05
2025-10-09 20:05:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:05:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:05:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:05:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:05:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:05:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:06:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:06:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.180817, avg_loss=0.671210, seen=480, correct=282, accuracy=0.587500
2025-10-09 20:06:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:06:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:06:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:06:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=186 reserved=2330MB allocated=2217MB
2025-10-09 20:06:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 186, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.86870563030243, 'train_avg_loss': 0.6655725469191869, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 20:06:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 186, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.1808166503906, 'train_avg_loss': 0.6712100346883138, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 20:06:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 186, 'Results_raw': {'train_total': 480, 'train_loss': 322.1808166503906, 'train_avg_loss': 0.6712100346883138, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 20:06:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:06:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:06:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #186, planning to set LR to 1.00e-05
2025-10-09 20:06:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:06:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:06:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:06:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:06:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:06:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:07:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:07:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.123932, avg_loss=0.685675, seen=480, correct=256, accuracy=0.533333
2025-10-09 20:07:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:07:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:07:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:07:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=186 reserved=2348MB allocated=2217MB
2025-10-09 20:07:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 186, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.92063367366791, 'train_avg_loss': 0.6910052806138992, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 20:07:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 186, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.1239318847656, 'train_avg_loss': 0.6856748580932617, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 20:07:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 186, 'Results_raw': {'train_total': 480, 'train_loss': 329.1239318847656, 'train_avg_loss': 0.6856748580932617, 'train_seen': 480, 'train_correct': 256, 'train_acc': 0.5333333333333333}}
2025-10-09 20:07:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:07:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:07:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #186, planning to set LR to 1.00e-05
2025-10-09 20:07:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 20:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:07:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:07:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:07:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:07:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:07:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:07:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.215454, avg_loss=0.679616, seen=480, correct=289, accuracy=0.602083
2025-10-09 20:07:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:07:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:07:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=186 reserved=2374MB allocated=2217MB
2025-10-09 20:07:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 186, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.08872365951538, 'train_avg_loss': 0.6840726971626282, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 20:07:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 186, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.2154541015625, 'train_avg_loss': 0.6796155293782552, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 20:07:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 186, 'Results_raw': {'train_total': 480, 'train_loss': 326.2154541015625, 'train_avg_loss': 0.6796155293782552, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 20:07:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:07:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:07:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #186, planning to set LR to 1.00e-05
2025-10-09 20:07:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:07:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:07:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:07:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:07:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:08:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:08:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.471985, avg_loss=0.692650, seen=480, correct=260, accuracy=0.541667
2025-10-09 20:08:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:08:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:08:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=186 reserved=2330MB allocated=2217MB
2025-10-09 20:08:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 186, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.7860279083252, 'train_avg_loss': 0.7232168992360433, 'train_seen': 120, 'train_correct': 54, 'train_acc': 0.45}}
2025-10-09 20:08:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 186, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.47198486328125, 'train_avg_loss': 0.6926499684651692, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 20:08:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 186, 'Results_raw': {'train_total': 480, 'train_loss': 332.47198486328125, 'train_avg_loss': 0.6926499684651692, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 20:08:42 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #187) -------------
2025-10-09 20:08:43 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=187 aidx=2 | s=5 (candidates=9)
2025-10-09 20:08:43 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[31, 37, 5, 50, 36] (from 9)
2025-10-09 20:08:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:08:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:08:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #187, planning to set LR to 1.00e-05
2025-10-09 20:08:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:08:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:08:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:08:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:08:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:08:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:09:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:09:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.253937, avg_loss=0.692196, seen=480, correct=249, accuracy=0.518750
2025-10-09 20:09:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:09:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:09:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=187 reserved=2348MB allocated=2217MB
2025-10-09 20:09:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 187, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.53079903125763, 'train_avg_loss': 0.6960899919271469, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 20:09:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 187, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.2539367675781, 'train_avg_loss': 0.6921957015991211, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 20:09:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 187, 'Results_raw': {'train_total': 480, 'train_loss': 332.2539367675781, 'train_avg_loss': 0.6921957015991211, 'train_seen': 480, 'train_correct': 249, 'train_acc': 0.51875}}
2025-10-09 20:09:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:09:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:09:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #187, planning to set LR to 1.00e-05
2025-10-09 20:09:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:09:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:09:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:09:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:09:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:09:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:10:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:10:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.411621, avg_loss=0.686274, seen=480, correct=270, accuracy=0.562500
2025-10-09 20:10:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:10:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:10:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:10:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=187 reserved=2330MB allocated=2217MB
2025-10-09 20:10:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 187, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.15800833702087, 'train_avg_loss': 0.7179834028085073, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-10-09 20:10:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 187, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.41162109375, 'train_avg_loss': 0.6862742106119791, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 20:10:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 187, 'Results_raw': {'train_total': 480, 'train_loss': 329.41162109375, 'train_avg_loss': 0.6862742106119791, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 20:10:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:10:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:10:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #187, planning to set LR to 1.00e-05
2025-10-09 20:10:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 20:10:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:10:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:10:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:10:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:10:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:10:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:10:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.154297, avg_loss=0.698238, seen=480, correct=262, accuracy=0.545833
2025-10-09 20:10:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:10:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:10:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=187 reserved=2398MB allocated=2217MB
2025-10-09 20:10:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 187, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.21056389808655, 'train_avg_loss': 0.6850880324840546, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 20:10:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 187, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.154296875, 'train_avg_loss': 0.6982381184895833, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:10:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 187, 'Results_raw': {'train_total': 480, 'train_loss': 335.154296875, 'train_avg_loss': 0.6982381184895833, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:10:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:10:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:10:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #187, planning to set LR to 1.00e-05
2025-10-09 20:10:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:10:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:10:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:10:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:10:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:11:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:11:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.860748, avg_loss=0.685127, seen=480, correct=262, accuracy=0.545833
2025-10-09 20:11:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:11:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:11:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:11:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=187 reserved=2330MB allocated=2217MB
2025-10-09 20:11:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 187, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.01834881305695, 'train_avg_loss': 0.7001529067754746, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 20:11:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 187, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.8607482910156, 'train_avg_loss': 0.6851265589396159, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:11:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 187, 'Results_raw': {'train_total': 480, 'train_loss': 328.8607482910156, 'train_avg_loss': 0.6851265589396159, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:11:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:11:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:11:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #187, planning to set LR to 1.00e-05
2025-10-09 20:11:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 20:11:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:11:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:11:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:11:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:11:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:12:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:12:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.929382, avg_loss=0.662353, seen=480, correct=290, accuracy=0.604167
2025-10-09 20:12:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:12:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:12:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:12:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=187 reserved=2346MB allocated=2217MB
2025-10-09 20:12:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 187, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.08638799190521, 'train_avg_loss': 0.6507198999325434, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 20:12:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 187, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.92938232421875, 'train_avg_loss': 0.6623528798421224, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:12:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 187, 'Results_raw': {'train_total': 480, 'train_loss': 317.92938232421875, 'train_avg_loss': 0.6623528798421224, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:12:19 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #188) -------------
2025-10-09 20:12:20 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=188 aidx=2 | s=5 (candidates=9)
2025-10-09 20:12:20 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[50, 32, 5, 36, 37] (from 9)
2025-10-09 20:12:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:12:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:12:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #188, planning to set LR to 1.00e-05
2025-10-09 20:12:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:12:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:12:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:12:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:12:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:12:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:13:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:13:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.478607, avg_loss=0.686414, seen=480, correct=255, accuracy=0.531250
2025-10-09 20:13:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:13:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:13:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:13:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=188 reserved=2330MB allocated=2217MB
2025-10-09 20:13:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 188, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.72137975692749, 'train_avg_loss': 0.6976781646410625, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 20:13:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 188, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.4786071777344, 'train_avg_loss': 0.6864137649536133, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 20:13:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 188, 'Results_raw': {'train_total': 480, 'train_loss': 329.4786071777344, 'train_avg_loss': 0.6864137649536133, 'train_seen': 480, 'train_correct': 255, 'train_acc': 0.53125}}
2025-10-09 20:13:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:13:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:13:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #188, planning to set LR to 1.00e-05
2025-10-09 20:13:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:13:04 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:13:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:13:04 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:13:04 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:13:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:13:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.761932, avg_loss=0.672421, seen=480, correct=290, accuracy=0.604167
2025-10-09 20:13:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:13:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:13:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:13:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=188 reserved=2330MB allocated=2217MB
2025-10-09 20:13:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 188, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.09325736761093, 'train_avg_loss': 0.6674438113967578, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 20:13:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 188, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.7619323730469, 'train_avg_loss': 0.6724206924438476, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:13:46 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 188, 'Results_raw': {'train_total': 480, 'train_loss': 322.7619323730469, 'train_avg_loss': 0.6724206924438476, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:13:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:13:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:13:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #188, planning to set LR to 1.00e-05
2025-10-09 20:13:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 20:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:13:47 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:13:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:13:47 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:13:47 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:14:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:14:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=334.027466, avg_loss=0.695891, seen=480, correct=264, accuracy=0.550000
2025-10-09 20:14:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:14:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:14:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=188 reserved=2396MB allocated=2217MB
2025-10-09 20:14:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 188, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.14039498567581, 'train_avg_loss': 0.6845032915472984, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 20:14:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 188, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.0274658203125, 'train_avg_loss': 0.6958905537923177, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 20:14:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 188, 'Results_raw': {'train_total': 480, 'train_loss': 334.0274658203125, 'train_avg_loss': 0.6958905537923177, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 20:14:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:14:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:14:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #188, planning to set LR to 1.00e-05
2025-10-09 20:14:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 20:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:14:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:14:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:14:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:14:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:15:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:15:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.932312, avg_loss=0.656109, seen=480, correct=295, accuracy=0.614583
2025-10-09 20:15:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:15:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:15:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:15:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=188 reserved=2348MB allocated=2217MB
2025-10-09 20:15:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 188, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.16136580705643, 'train_avg_loss': 0.6430113817254702, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 20:15:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 188, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.93231201171875, 'train_avg_loss': 0.6561089833577474, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 20:15:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 188, 'Results_raw': {'train_total': 480, 'train_loss': 314.93231201171875, 'train_avg_loss': 0.6561089833577474, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 20:15:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:15:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:15:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #188, planning to set LR to 1.00e-05
2025-10-09 20:15:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:15:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:15:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:15:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:15:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:15:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:15:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.164124, avg_loss=0.685759, seen=480, correct=279, accuracy=0.581250
2025-10-09 20:15:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:15:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:15:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:15:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=188 reserved=2330MB allocated=2217MB
2025-10-09 20:15:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 188, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.97714614868164, 'train_avg_loss': 0.7164762179056804, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 20:15:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 188, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.16412353515625, 'train_avg_loss': 0.6857585906982422, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 20:15:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 188, 'Results_raw': {'train_total': 480, 'train_loss': 329.16412353515625, 'train_avg_loss': 0.6857585906982422, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 20:15:52 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #189) -------------
2025-10-09 20:15:53 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=189 aidx=2 | s=5 (candidates=9)
2025-10-09 20:15:53 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[3, 31, 50, 36, 37] (from 9)
2025-10-09 20:15:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:15:54 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:15:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #189, planning to set LR to 1.00e-05
2025-10-09 20:15:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 20:15:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:15:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:15:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:15:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:15:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:16:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.814423, avg_loss=0.685030, seen=480, correct=258, accuracy=0.537500
2025-10-09 20:16:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:16:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:16:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=189 reserved=2368MB allocated=2217MB
2025-10-09 20:16:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 189, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.6251630783081, 'train_avg_loss': 0.6885430256525675, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 20:16:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 189, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.8144226074219, 'train_avg_loss': 0.6850300470987956, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 20:16:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 189, 'Results_raw': {'train_total': 480, 'train_loss': 328.8144226074219, 'train_avg_loss': 0.6850300470987956, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 20:16:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:16:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:16:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #189, planning to set LR to 1.00e-05
2025-10-09 20:16:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:16:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:16:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:16:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:16:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:16:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:17:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:17:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.440918, avg_loss=0.690502, seen=480, correct=260, accuracy=0.541667
2025-10-09 20:17:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:17:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:17:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=189 reserved=2348MB allocated=2217MB
2025-10-09 20:17:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 189, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.22968459129333, 'train_avg_loss': 0.6935807049274445, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 20:17:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 189, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.44091796875, 'train_avg_loss': 0.6905019124348958, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 20:17:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 189, 'Results_raw': {'train_total': 480, 'train_loss': 331.44091796875, 'train_avg_loss': 0.6905019124348958, 'train_seen': 480, 'train_correct': 260, 'train_acc': 0.5416666666666666}}
2025-10-09 20:17:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:17:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:17:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #189, planning to set LR to 1.00e-05
2025-10-09 20:17:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:17:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:17:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:17:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:17:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:17:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:17:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:17:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.746307, avg_loss=0.686971, seen=480, correct=264, accuracy=0.550000
2025-10-09 20:17:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:17:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:17:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:17:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=189 reserved=2330MB allocated=2217MB
2025-10-09 20:17:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 189, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.60735493898392, 'train_avg_loss': 0.7050612911581993, 'train_seen': 120, 'train_correct': 57, 'train_acc': 0.475}}
2025-10-09 20:17:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 189, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.7463073730469, 'train_avg_loss': 0.6869714736938477, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 20:17:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 189, 'Results_raw': {'train_total': 480, 'train_loss': 329.7463073730469, 'train_avg_loss': 0.6869714736938477, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 20:17:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:18:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:18:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #189, planning to set LR to 1.00e-05
2025-10-09 20:18:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 20:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:18:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:18:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:18:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:18:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:18:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:18:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.231201, avg_loss=0.654648, seen=480, correct=293, accuracy=0.610417
2025-10-09 20:18:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:18:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:18:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:18:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=189 reserved=2348MB allocated=2217MB
2025-10-09 20:18:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 189, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.30376321077347, 'train_avg_loss': 0.6441980267564456, 'train_seen': 120, 'train_correct': 77, 'train_acc': 0.6416666666666667}}
2025-10-09 20:18:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 189, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.231201171875, 'train_avg_loss': 0.6546483357747396, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 20:18:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 189, 'Results_raw': {'train_total': 480, 'train_loss': 314.231201171875, 'train_avg_loss': 0.6546483357747396, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 20:18:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:18:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:18:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #189, planning to set LR to 1.00e-05
2025-10-09 20:18:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:18:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:18:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:18:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:18:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:19:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:19:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.739685, avg_loss=0.682791, seen=480, correct=275, accuracy=0.572917
2025-10-09 20:19:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:19:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:19:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:19:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=189 reserved=2330MB allocated=2217MB
2025-10-09 20:19:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 189, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.87625050544739, 'train_avg_loss': 0.7156354208787282, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 20:19:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 189, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.73968505859375, 'train_avg_loss': 0.682791010538737, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 20:19:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 189, 'Results_raw': {'train_total': 480, 'train_loss': 327.73968505859375, 'train_avg_loss': 0.682791010538737, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 20:19:27 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #190) -------------
2025-10-09 20:19:27 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=190 aidx=2 | s=5 (candidates=9)
2025-10-09 20:19:27 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[31, 37, 36, 21, 50] (from 9)
2025-10-09 20:19:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:19:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:19:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #190, planning to set LR to 1.00e-05
2025-10-09 20:19:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:19:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:19:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:19:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:19:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:19:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:20:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:20:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=332.514679, avg_loss=0.692739, seen=480, correct=258, accuracy=0.537500
2025-10-09 20:20:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:20:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:20:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:20:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=190 reserved=2348MB allocated=2217MB
2025-10-09 20:20:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 190, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.42670583724976, 'train_avg_loss': 0.695222548643748, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 20:20:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 190, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.5146789550781, 'train_avg_loss': 0.6927389144897461, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 20:20:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 190, 'Results_raw': {'train_total': 480, 'train_loss': 332.5146789550781, 'train_avg_loss': 0.6927389144897461, 'train_seen': 480, 'train_correct': 258, 'train_acc': 0.5375}}
2025-10-09 20:20:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:20:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:20:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #190, planning to set LR to 1.00e-05
2025-10-09 20:20:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:20:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:20:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:20:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:20:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:20:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:20:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.467499, avg_loss=0.673891, seen=480, correct=282, accuracy=0.587500
2025-10-09 20:20:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:20:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:20:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=190 reserved=2330MB allocated=2217MB
2025-10-09 20:20:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 190, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.63121628761292, 'train_avg_loss': 0.7052601357301076, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 20:20:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 190, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.4674987792969, 'train_avg_loss': 0.6738906224568685, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 20:20:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 190, 'Results_raw': {'train_total': 480, 'train_loss': 323.4674987792969, 'train_avg_loss': 0.6738906224568685, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 20:20:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:20:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:20:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #190, planning to set LR to 1.00e-05
2025-10-09 20:21:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 20:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:21:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:21:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:21:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:21:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:21:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:21:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.724609, avg_loss=0.653593, seen=480, correct=295, accuracy=0.614583
2025-10-09 20:21:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:21:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:21:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:21:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=190 reserved=2346MB allocated=2217MB
2025-10-09 20:21:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 190, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.08433657884598, 'train_avg_loss': 0.6423694714903831, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 20:21:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 190, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.724609375, 'train_avg_loss': 0.6535929361979167, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 20:21:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 190, 'Results_raw': {'train_total': 480, 'train_loss': 313.724609375, 'train_avg_loss': 0.6535929361979167, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 20:21:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:21:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:21:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #190, planning to set LR to 1.00e-05
2025-10-09 20:21:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 20:21:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:21:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:21:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:21:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:21:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:22:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:22:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.119537, avg_loss=0.677332, seen=480, correct=271, accuracy=0.564583
2025-10-09 20:22:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:22:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:22:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=190 reserved=2374MB allocated=2217MB
2025-10-09 20:22:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 190, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.89237040281296, 'train_avg_loss': 0.6824364200234413, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-10-09 20:22:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 190, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.1195373535156, 'train_avg_loss': 0.6773323694864909, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 20:22:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 190, 'Results_raw': {'train_total': 480, 'train_loss': 325.1195373535156, 'train_avg_loss': 0.6773323694864909, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 20:22:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:22:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:22:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #190, planning to set LR to 1.00e-05
2025-10-09 20:22:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:22:22 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:22:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:22:22 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:22:22 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:22:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:22:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.194061, avg_loss=0.687904, seen=480, correct=257, accuracy=0.535417
2025-10-09 20:22:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:22:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:23:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:23:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=190 reserved=2330MB allocated=2217MB
2025-10-09 20:23:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 190, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.91567236185074, 'train_avg_loss': 0.7076306030154228, 'train_seen': 120, 'train_correct': 54, 'train_acc': 0.45}}
2025-10-09 20:23:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 190, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.1940612792969, 'train_avg_loss': 0.6879042943318685, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 20:23:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 190, 'Results_raw': {'train_total': 480, 'train_loss': 330.1940612792969, 'train_avg_loss': 0.6879042943318685, 'train_seen': 480, 'train_correct': 257, 'train_acc': 0.5354166666666667}}
2025-10-09 20:23:02 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #191) -------------
2025-10-09 20:23:02 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=191 aidx=2 | s=5 (candidates=9)
2025-10-09 20:23:02 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[31, 32, 3, 21, 5] (from 9)
2025-10-09 20:23:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:23:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:23:03 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #191, planning to set LR to 1.00e-05
2025-10-09 20:23:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:23:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:23:03 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:23:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:23:03 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:23:03 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:23:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:23:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.961914, avg_loss=0.689504, seen=480, correct=262, accuracy=0.545833
2025-10-09 20:23:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:23:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:23:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=191 reserved=2348MB allocated=2217MB
2025-10-09 20:23:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 191, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.23889029026031, 'train_avg_loss': 0.6936574190855026, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 20:23:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 191, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.9619140625, 'train_avg_loss': 0.6895039876302084, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:23:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 191, 'Results_raw': {'train_total': 480, 'train_loss': 330.9619140625, 'train_avg_loss': 0.6895039876302084, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:23:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:23:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:23:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #191, planning to set LR to 1.00e-05
2025-10-09 20:23:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:23:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:23:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:23:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:23:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:24:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:24:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.305634, avg_loss=0.669387, seen=480, correct=284, accuracy=0.591667
2025-10-09 20:24:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:24:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:24:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:24:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=191 reserved=2330MB allocated=2217MB
2025-10-09 20:24:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 191, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.57240957021713, 'train_avg_loss': 0.6631034130851428, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 20:24:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 191, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.3056335449219, 'train_avg_loss': 0.6693867365519206, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 20:24:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 191, 'Results_raw': {'train_total': 480, 'train_loss': 321.3056335449219, 'train_avg_loss': 0.6693867365519206, 'train_seen': 480, 'train_correct': 284, 'train_acc': 0.5916666666666667}}
2025-10-09 20:24:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:24:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:24:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #191, planning to set LR to 1.00e-05
2025-10-09 20:24:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 20:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:24:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:24:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:24:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:24:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:25:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:25:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.965576, avg_loss=0.683262, seen=480, correct=269, accuracy=0.560417
2025-10-09 20:25:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:25:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:25:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=191 reserved=2368MB allocated=2217MB
2025-10-09 20:25:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 191, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.7496303319931, 'train_avg_loss': 0.6895802527666092, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 20:25:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 191, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.965576171875, 'train_avg_loss': 0.6832616170247395, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 20:25:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 191, 'Results_raw': {'train_total': 480, 'train_loss': 327.965576171875, 'train_avg_loss': 0.6832616170247395, 'train_seen': 480, 'train_correct': 269, 'train_acc': 0.5604166666666667}}
2025-10-09 20:25:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:25:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:25:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #191, planning to set LR to 1.00e-05
2025-10-09 20:25:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 20:25:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:25:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:25:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:25:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:25:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:25:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:25:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.697144, avg_loss=0.678536, seen=480, correct=266, accuracy=0.554167
2025-10-09 20:25:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:25:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:25:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:25:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=191 reserved=2374MB allocated=2217MB
2025-10-09 20:25:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 191, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.06129169464111, 'train_avg_loss': 0.6838440974553426, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 20:25:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 191, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.6971435546875, 'train_avg_loss': 0.6785357157389323, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 20:25:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 191, 'Results_raw': {'train_total': 480, 'train_loss': 325.6971435546875, 'train_avg_loss': 0.6785357157389323, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 20:25:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:25:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:25:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #191, planning to set LR to 1.00e-05
2025-10-09 20:25:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 20:25:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:25:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:25:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:25:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:25:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:26:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:26:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=335.566162, avg_loss=0.699096, seen=480, correct=262, accuracy=0.545833
2025-10-09 20:26:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:26:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:26:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:26:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=191 reserved=2396MB allocated=2217MB
2025-10-09 20:26:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 191, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.00266885757446, 'train_avg_loss': 0.6833555738131205, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 20:26:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 191, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.566162109375, 'train_avg_loss': 0.6990961710611979, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:26:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 191, 'Results_raw': {'train_total': 480, 'train_loss': 335.566162109375, 'train_avg_loss': 0.6990961710611979, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:26:38 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #192) -------------
2025-10-09 20:26:38 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=192 aidx=2 | s=5 (candidates=9)
2025-10-09 20:26:38 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[32, 41, 21, 37, 50] (from 9)
2025-10-09 20:26:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:26:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:26:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #192, planning to set LR to 1.00e-05
2025-10-09 20:26:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:26:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:26:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:26:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:26:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:26:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:27:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:27:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.139679, avg_loss=0.669041, seen=480, correct=290, accuracy=0.604167
2025-10-09 20:27:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:27:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:27:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=192 reserved=2330MB allocated=2217MB
2025-10-09 20:27:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 192, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.62165147066116, 'train_avg_loss': 0.671847095588843, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 20:27:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 192, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.1396789550781, 'train_avg_loss': 0.6690409978230795, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:27:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 192, 'Results_raw': {'train_total': 480, 'train_loss': 321.1396789550781, 'train_avg_loss': 0.6690409978230795, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:27:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:27:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:27:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #192, planning to set LR to 1.00e-05
2025-10-09 20:27:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 20:27:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:27:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:27:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:27:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:27:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:27:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:27:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.609894, avg_loss=0.676271, seen=480, correct=272, accuracy=0.566667
2025-10-09 20:27:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:27:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:27:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:27:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=192 reserved=2330MB allocated=2217MB
2025-10-09 20:27:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 192, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.06420302391052, 'train_avg_loss': 0.667201691865921, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 20:27:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 192, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.6098937988281, 'train_avg_loss': 0.6762706120808919, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 20:27:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 192, 'Results_raw': {'train_total': 480, 'train_loss': 324.6098937988281, 'train_avg_loss': 0.6762706120808919, 'train_seen': 480, 'train_correct': 272, 'train_acc': 0.5666666666666667}}
2025-10-09 20:27:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:28:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:28:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #192, planning to set LR to 1.00e-05
2025-10-09 20:28:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 20:28:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:28:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:28:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:28:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:28:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:28:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:28:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.115448, avg_loss=0.677324, seen=480, correct=275, accuracy=0.572917
2025-10-09 20:28:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:28:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:28:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:28:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=192 reserved=2374MB allocated=2217MB
2025-10-09 20:28:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 192, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.089883685112, 'train_avg_loss': 0.6840823640426, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 20:28:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 192, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.1154479980469, 'train_avg_loss': 0.677323849995931, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 20:28:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 192, 'Results_raw': {'train_total': 480, 'train_loss': 325.1154479980469, 'train_avg_loss': 0.677323849995931, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 20:28:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:28:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:28:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #192, planning to set LR to 1.00e-05
2025-10-09 20:28:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:28:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:28:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:28:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:28:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:28:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:29:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:29:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.591888, avg_loss=0.674150, seen=480, correct=286, accuracy=0.595833
2025-10-09 20:29:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:29:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:29:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:29:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=192 reserved=2330MB allocated=2217MB
2025-10-09 20:29:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 192, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.94194608926773, 'train_avg_loss': 0.7161828840772311, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 20:29:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 192, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.5918884277344, 'train_avg_loss': 0.67414976755778, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 20:29:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 192, 'Results_raw': {'train_total': 480, 'train_loss': 323.5918884277344, 'train_avg_loss': 0.67414976755778, 'train_seen': 480, 'train_correct': 286, 'train_acc': 0.5958333333333333}}
2025-10-09 20:29:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:29:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:29:22 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #192, planning to set LR to 1.00e-05
2025-10-09 20:29:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:29:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:29:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:29:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:29:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:29:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:30:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:30:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.474487, avg_loss=0.682239, seen=480, correct=259, accuracy=0.539583
2025-10-09 20:30:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:30:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:30:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:30:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=192 reserved=2330MB allocated=2217MB
2025-10-09 20:30:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 192, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.25354081392288, 'train_avg_loss': 0.6937795067826907, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 20:30:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 192, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.4744873046875, 'train_avg_loss': 0.682238515218099, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 20:30:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 192, 'Results_raw': {'train_total': 480, 'train_loss': 327.4744873046875, 'train_avg_loss': 0.682238515218099, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 20:30:05 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #193) -------------
2025-10-09 20:30:05 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=193 aidx=2 | s=5 (candidates=9)
2025-10-09 20:30:05 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[31, 21, 32, 3, 5] (from 9)
2025-10-09 20:30:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:30:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:30:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #193, planning to set LR to 1.00e-05
2025-10-09 20:30:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:30:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:30:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:30:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:30:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:30:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:30:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:30:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.693909, avg_loss=0.686862, seen=480, correct=261, accuracy=0.543750
2025-10-09 20:30:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:30:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:30:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:30:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=193 reserved=2348MB allocated=2217MB
2025-10-09 20:30:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 193, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.18081313371658, 'train_avg_loss': 0.6931734427809715, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 20:30:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 193, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.69390869140625, 'train_avg_loss': 0.6868623097737631, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 20:30:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 193, 'Results_raw': {'train_total': 480, 'train_loss': 329.69390869140625, 'train_avg_loss': 0.6868623097737631, 'train_seen': 480, 'train_correct': 261, 'train_acc': 0.54375}}
2025-10-09 20:30:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:30:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:30:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #193, planning to set LR to 1.00e-05
2025-10-09 20:30:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 20:30:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:30:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:30:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:30:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:30:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:31:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:31:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.900909, avg_loss=0.678960, seen=480, correct=262, accuracy=0.545833
2025-10-09 20:31:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:31:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:31:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:31:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=193 reserved=2374MB allocated=2217MB
2025-10-09 20:31:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 193, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.14928090572357, 'train_avg_loss': 0.6845773408810297, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-10-09 20:31:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 193, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.9009094238281, 'train_avg_loss': 0.6789602279663086, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:31:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 193, 'Results_raw': {'train_total': 480, 'train_loss': 325.9009094238281, 'train_avg_loss': 0.6789602279663086, 'train_seen': 480, 'train_correct': 262, 'train_acc': 0.5458333333333333}}
2025-10-09 20:31:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:31:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:31:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #193, planning to set LR to 1.00e-05
2025-10-09 20:31:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:31:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:31:28 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:31:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:31:28 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:31:28 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:32:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:32:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.320892, avg_loss=0.669419, seen=480, correct=288, accuracy=0.600000
2025-10-09 20:32:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:32:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:32:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:32:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=193 reserved=2330MB allocated=2217MB
2025-10-09 20:32:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 193, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.03117161989212, 'train_avg_loss': 0.6669264301657677, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 20:32:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 193, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.3208923339844, 'train_avg_loss': 0.6694185256958007, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 20:32:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 193, 'Results_raw': {'train_total': 480, 'train_loss': 321.3208923339844, 'train_avg_loss': 0.6694185256958007, 'train_seen': 480, 'train_correct': 288, 'train_acc': 0.6}}
2025-10-09 20:32:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:32:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:32:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #193, planning to set LR to 1.00e-05
2025-10-09 20:32:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-09 20:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:32:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:32:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:32:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:32:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:32:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:32:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=327.530884, avg_loss=0.682356, seen=480, correct=265, accuracy=0.552083
2025-10-09 20:32:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:32:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:32:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:32:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=193 reserved=2368MB allocated=2217MB
2025-10-09 20:32:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 193, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.4559296965599, 'train_avg_loss': 0.6871327474713326, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 20:32:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 193, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 327.5308837890625, 'train_avg_loss': 0.6823560078938802, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 20:32:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 193, 'Results_raw': {'train_total': 480, 'train_loss': 327.5308837890625, 'train_avg_loss': 0.6823560078938802, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 20:32:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:32:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:32:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #193, planning to set LR to 1.00e-05
2025-10-09 20:32:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 20:32:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:32:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:32:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:32:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:32:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:33:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:33:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=329.954407, avg_loss=0.687405, seen=480, correct=270, accuracy=0.562500
2025-10-09 20:33:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:33:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:33:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:33:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=193 reserved=2396MB allocated=2217MB
2025-10-09 20:33:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 193, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.85513192415237, 'train_avg_loss': 0.6737927660346031, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 20:33:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 193, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 329.95440673828125, 'train_avg_loss': 0.687405014038086, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 20:33:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 193, 'Results_raw': {'train_total': 480, 'train_loss': 329.95440673828125, 'train_avg_loss': 0.687405014038086, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 20:33:30 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #194) -------------
2025-10-09 20:33:31 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=194 aidx=2 | s=5 (candidates=9)
2025-10-09 20:33:31 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[21, 32, 31, 41, 37] (from 9)
2025-10-09 20:33:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:33:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:33:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #194, planning to set LR to 1.00e-05
2025-10-09 20:33:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 20:33:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:33:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:33:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:33:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:33:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:34:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:34:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.006836, avg_loss=0.677098, seen=480, correct=276, accuracy=0.575000
2025-10-09 20:34:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:34:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:34:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:34:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=194 reserved=2374MB allocated=2217MB
2025-10-09 20:34:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 194, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.8588439822197, 'train_avg_loss': 0.6821570331851642, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 20:34:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 194, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.0068359375, 'train_avg_loss': 0.6770975748697917, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 20:34:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 194, 'Results_raw': {'train_total': 480, 'train_loss': 325.0068359375, 'train_avg_loss': 0.6770975748697917, 'train_seen': 480, 'train_correct': 276, 'train_acc': 0.575}}
2025-10-09 20:34:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:34:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:34:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #194, planning to set LR to 1.00e-05
2025-10-09 20:34:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:34:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:34:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:34:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:34:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:34:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:34:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.834656, avg_loss=0.668406, seen=480, correct=302, accuracy=0.629167
2025-10-09 20:34:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:34:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:34:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:34:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=194 reserved=2330MB allocated=2217MB
2025-10-09 20:34:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 194, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.67325776815414, 'train_avg_loss': 0.6722771480679512, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 20:34:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 194, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.83465576171875, 'train_avg_loss': 0.6684055328369141, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 20:34:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 194, 'Results_raw': {'train_total': 480, 'train_loss': 320.83465576171875, 'train_avg_loss': 0.6684055328369141, 'train_seen': 480, 'train_correct': 302, 'train_acc': 0.6291666666666667}}
2025-10-09 20:34:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:34:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:34:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #194, planning to set LR to 1.00e-05
2025-10-09 20:34:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:34:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:34:57 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:34:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:34:57 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:34:57 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:35:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:35:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=328.620728, avg_loss=0.684627, seen=480, correct=271, accuracy=0.564583
2025-10-09 20:35:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:35:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:35:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:35:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=194 reserved=2348MB allocated=2217MB
2025-10-09 20:35:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 194, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.16770541667938, 'train_avg_loss': 0.6930642118056615, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 20:35:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 194, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 328.6207275390625, 'train_avg_loss': 0.6846265157063802, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 20:35:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 194, 'Results_raw': {'train_total': 480, 'train_loss': 328.6207275390625, 'train_avg_loss': 0.6846265157063802, 'train_seen': 480, 'train_correct': 271, 'train_acc': 0.5645833333333333}}
2025-10-09 20:35:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:35:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:35:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #194, planning to set LR to 1.00e-05
2025-10-09 20:35:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 20:35:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:35:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:35:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:35:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:35:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:36:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:36:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.068512, avg_loss=0.673059, seen=480, correct=275, accuracy=0.572917
2025-10-09 20:36:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:36:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:36:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:36:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=194 reserved=2330MB allocated=2217MB
2025-10-09 20:36:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 194, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.06384611129761, 'train_avg_loss': 0.6671987175941467, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 20:36:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 194, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.0685119628906, 'train_avg_loss': 0.6730593999226888, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 20:36:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 194, 'Results_raw': {'train_total': 480, 'train_loss': 323.0685119628906, 'train_avg_loss': 0.6730593999226888, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 20:36:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:36:20 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:36:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #194, planning to set LR to 1.00e-05
2025-10-09 20:36:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:36:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:36:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:36:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:36:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:36:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:36:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:36:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.097412, avg_loss=0.673120, seen=480, correct=292, accuracy=0.608333
2025-10-09 20:36:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:36:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:36:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=194 reserved=2330MB allocated=2217MB
2025-10-09 20:36:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 194, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.62515163421631, 'train_avg_loss': 0.7135429302851359, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 20:36:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 194, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.097412109375, 'train_avg_loss': 0.6731196085611979, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 20:36:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 194, 'Results_raw': {'train_total': 480, 'train_loss': 323.097412109375, 'train_avg_loss': 0.6731196085611979, 'train_seen': 480, 'train_correct': 292, 'train_acc': 0.6083333333333333}}
2025-10-09 20:37:00 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #195) -------------
2025-10-09 20:37:00 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=195 aidx=2 | s=5 (candidates=9)
2025-10-09 20:37:00 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[41, 37, 32, 31, 50] (from 9)
2025-10-09 20:37:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:37:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:37:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #195, planning to set LR to 1.00e-05
2025-10-09 20:37:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 20:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:37:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:37:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:37:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:37:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:37:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:37:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.979584, avg_loss=0.668707, seen=480, correct=279, accuracy=0.581250
2025-10-09 20:37:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:37:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:37:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:37:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=195 reserved=2330MB allocated=2217MB
2025-10-09 20:37:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 195, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.10595804452896, 'train_avg_loss': 0.6592163170377413, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-10-09 20:37:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 195, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.9795837402344, 'train_avg_loss': 0.6687074661254883, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 20:37:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 195, 'Results_raw': {'train_total': 480, 'train_loss': 320.9795837402344, 'train_avg_loss': 0.6687074661254883, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 20:37:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:37:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:37:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #195, planning to set LR to 1.00e-05
2025-10-09 20:37:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:37:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:37:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:37:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:37:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:37:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:38:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:38:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.296265, avg_loss=0.667284, seen=480, correct=294, accuracy=0.612500
2025-10-09 20:38:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:38:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:38:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:38:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=195 reserved=2330MB allocated=2217MB
2025-10-09 20:38:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 195, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.38509285449982, 'train_avg_loss': 0.7115424404541651, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 20:38:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 195, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.2962646484375, 'train_avg_loss': 0.6672838846842448, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 20:38:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 195, 'Results_raw': {'train_total': 480, 'train_loss': 320.2962646484375, 'train_avg_loss': 0.6672838846842448, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 20:38:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:38:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:38:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #195, planning to set LR to 1.00e-05
2025-10-09 20:38:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:38:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:38:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:38:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:38:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:39:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:39:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=322.535675, avg_loss=0.671949, seen=480, correct=278, accuracy=0.579167
2025-10-09 20:39:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:39:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:39:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:39:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=195 reserved=2330MB allocated=2217MB
2025-10-09 20:39:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 195, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.73975598812103, 'train_avg_loss': 0.6644979665676752, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 20:39:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 195, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 322.5356750488281, 'train_avg_loss': 0.6719493230183919, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 20:39:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 195, 'Results_raw': {'train_total': 480, 'train_loss': 322.5356750488281, 'train_avg_loss': 0.6719493230183919, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 20:39:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:39:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:39:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #195, planning to set LR to 1.00e-05
2025-10-09 20:39:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:39:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:39:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:39:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:39:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:39:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:39:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:39:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=331.292603, avg_loss=0.690193, seen=480, correct=265, accuracy=0.552083
2025-10-09 20:39:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:39:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:39:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=195 reserved=2348MB allocated=2217MB
2025-10-09 20:39:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 195, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.56440389156342, 'train_avg_loss': 0.6963700324296951, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-10-09 20:39:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 195, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 331.2926025390625, 'train_avg_loss': 0.6901929219563802, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 20:39:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 195, 'Results_raw': {'train_total': 480, 'train_loss': 331.2926025390625, 'train_avg_loss': 0.6901929219563802, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-10-09 20:39:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:39:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:39:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #195, planning to set LR to 1.00e-05
2025-10-09 20:39:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:39:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:39:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:39:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:39:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:39:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:40:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:40:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.271240, avg_loss=0.675565, seen=480, correct=275, accuracy=0.572917
2025-10-09 20:40:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:40:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:40:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:40:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=195 reserved=2330MB allocated=2217MB
2025-10-09 20:40:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 195, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.6647897362709, 'train_avg_loss': 0.6972065811355909, 'train_seen': 120, 'train_correct': 59, 'train_acc': 0.49166666666666664}}
2025-10-09 20:40:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 195, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.271240234375, 'train_avg_loss': 0.6755650838216146, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 20:40:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 195, 'Results_raw': {'train_total': 480, 'train_loss': 324.271240234375, 'train_avg_loss': 0.6755650838216146, 'train_seen': 480, 'train_correct': 275, 'train_acc': 0.5729166666666666}}
2025-10-09 20:40:34 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #196) -------------
2025-10-09 20:40:35 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=196 aidx=2 | s=5 (candidates=9)
2025-10-09 20:40:35 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[21, 32, 50, 5, 41] (from 9)
2025-10-09 20:40:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:40:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:40:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #196, planning to set LR to 1.00e-05
2025-10-09 20:40:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 20:40:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:40:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:40:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:40:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:40:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:41:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:41:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.490387, avg_loss=0.678105, seen=480, correct=266, accuracy=0.554167
2025-10-09 20:41:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:41:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:41:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:41:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=196 reserved=2374MB allocated=2217MB
2025-10-09 20:41:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 196, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.43134623765945, 'train_avg_loss': 0.6785945519804955, 'train_seen': 120, 'train_correct': 68, 'train_acc': 0.5666666666666667}}
2025-10-09 20:41:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 196, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.4903869628906, 'train_avg_loss': 0.6781049728393554, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 20:41:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 196, 'Results_raw': {'train_total': 480, 'train_loss': 325.4903869628906, 'train_avg_loss': 0.6781049728393554, 'train_seen': 480, 'train_correct': 266, 'train_acc': 0.5541666666666667}}
2025-10-09 20:41:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:41:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:41:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #196, planning to set LR to 1.00e-05
2025-10-09 20:41:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:41:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:41:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:41:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:41:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:41:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:41:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:41:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.671387, avg_loss=0.665982, seen=480, correct=294, accuracy=0.612500
2025-10-09 20:41:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:41:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:41:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:41:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=196 reserved=2330MB allocated=2217MB
2025-10-09 20:41:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 196, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 80.00678211450577, 'train_avg_loss': 0.6667231842875481, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 20:41:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 196, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.67138671875, 'train_avg_loss': 0.6659820556640625, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 20:41:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 196, 'Results_raw': {'train_total': 480, 'train_loss': 319.67138671875, 'train_avg_loss': 0.6659820556640625, 'train_seen': 480, 'train_correct': 294, 'train_acc': 0.6125}}
2025-10-09 20:41:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:42:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:42:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #196, planning to set LR to 1.00e-05
2025-10-09 20:42:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:42:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:42:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:42:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:42:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:42:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:42:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:42:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=321.117645, avg_loss=0.668995, seen=480, correct=279, accuracy=0.581250
2025-10-09 20:42:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:42:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:42:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:42:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=196 reserved=2330MB allocated=2217MB
2025-10-09 20:42:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 196, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.05190110206604, 'train_avg_loss': 0.6920991758505504, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 20:42:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 196, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 321.1176452636719, 'train_avg_loss': 0.6689950942993164, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 20:42:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 196, 'Results_raw': {'train_total': 480, 'train_loss': 321.1176452636719, 'train_avg_loss': 0.6689950942993164, 'train_seen': 480, 'train_correct': 279, 'train_acc': 0.58125}}
2025-10-09 20:42:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:42:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:42:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #196, planning to set LR to 1.00e-05
2025-10-09 20:42:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 20:42:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:42:43 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:42:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:42:43 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:42:43 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:43:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:43:21 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=323.659607, avg_loss=0.674291, seen=480, correct=278, accuracy=0.579167
2025-10-09 20:43:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:43:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:43:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:43:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=196 reserved=2396MB allocated=2217MB
2025-10-09 20:43:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 196, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.7682774066925, 'train_avg_loss': 0.6564023117224376, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 20:43:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 196, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 323.65960693359375, 'train_avg_loss': 0.6742908477783203, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 20:43:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 196, 'Results_raw': {'train_total': 480, 'train_loss': 323.65960693359375, 'train_avg_loss': 0.6742908477783203, 'train_seen': 480, 'train_correct': 278, 'train_acc': 0.5791666666666667}}
2025-10-09 20:43:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:43:24 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:43:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #196, planning to set LR to 1.00e-05
2025-10-09 20:43:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 20:43:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:43:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:43:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:43:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:43:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:44:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:44:04 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.678436, avg_loss=0.668080, seen=480, correct=283, accuracy=0.589583
2025-10-09 20:44:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:44:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:44:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:44:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=196 reserved=2330MB allocated=2217MB
2025-10-09 20:44:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 196, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.10399520397186, 'train_avg_loss': 0.6591999600330989, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 20:44:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 196, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.6784362792969, 'train_avg_loss': 0.6680800755818684, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 20:44:07 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 196, 'Results_raw': {'train_total': 480, 'train_loss': 320.6784362792969, 'train_avg_loss': 0.6680800755818684, 'train_seen': 480, 'train_correct': 283, 'train_acc': 0.5895833333333333}}
2025-10-09 20:44:07 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #197) -------------
2025-10-09 20:44:08 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=197 aidx=2 | s=5 (candidates=9)
2025-10-09 20:44:08 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[36, 21, 41, 5, 50] (from 9)
2025-10-09 20:44:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:44:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:44:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #197, planning to set LR to 1.00e-05
2025-10-09 20:44:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 20:44:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:44:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:44:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:44:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:44:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:44:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.256226, avg_loss=0.656784, seen=480, correct=295, accuracy=0.614583
2025-10-09 20:44:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:44:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:44:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=197 reserved=2348MB allocated=2217MB
2025-10-09 20:44:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 197, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.96533143520355, 'train_avg_loss': 0.6497110952933629, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 20:44:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 197, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.2562255859375, 'train_avg_loss': 0.6567838033040364, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 20:44:49 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 197, 'Results_raw': {'train_total': 480, 'train_loss': 315.2562255859375, 'train_avg_loss': 0.6567838033040364, 'train_seen': 480, 'train_correct': 295, 'train_acc': 0.6145833333333334}}
2025-10-09 20:44:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:44:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:44:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #197, planning to set LR to 1.00e-05
2025-10-09 20:44:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 20:44:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:44:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:44:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:44:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:44:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:45:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:45:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=326.601440, avg_loss=0.680420, seen=480, correct=264, accuracy=0.550000
2025-10-09 20:45:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:45:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:45:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=197 reserved=2374MB allocated=2217MB
2025-10-09 20:45:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 197, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.13303190469742, 'train_avg_loss': 0.6844419325391452, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-10-09 20:45:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 197, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 326.6014404296875, 'train_avg_loss': 0.6804196675618489, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 20:45:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 197, 'Results_raw': {'train_total': 480, 'train_loss': 326.6014404296875, 'train_avg_loss': 0.6804196675618489, 'train_seen': 480, 'train_correct': 264, 'train_acc': 0.55}}
2025-10-09 20:45:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:45:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:45:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #197, planning to set LR to 1.00e-05
2025-10-09 20:45:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 20:45:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:45:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:45:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:45:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:45:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:46:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:46:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.254303, avg_loss=0.665113, seen=480, correct=282, accuracy=0.587500
2025-10-09 20:46:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:46:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:46:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:46:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=197 reserved=2330MB allocated=2217MB
2025-10-09 20:46:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 197, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.1855439543724, 'train_avg_loss': 0.6598795329531034, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 20:46:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 197, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.2543029785156, 'train_avg_loss': 0.6651131312052408, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 20:46:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 197, 'Results_raw': {'train_total': 480, 'train_loss': 319.2543029785156, 'train_avg_loss': 0.6651131312052408, 'train_seen': 480, 'train_correct': 282, 'train_acc': 0.5875}}
2025-10-09 20:46:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:46:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:46:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #197, planning to set LR to 1.00e-05
2025-10-09 20:46:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 20:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:46:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:46:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:46:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:46:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:46:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:46:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.071777, avg_loss=0.675150, seen=480, correct=285, accuracy=0.593750
2025-10-09 20:46:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:46:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:46:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=197 reserved=2396MB allocated=2217MB
2025-10-09 20:46:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 197, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.37153685092926, 'train_avg_loss': 0.6614294737577439, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 20:46:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 197, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.07177734375, 'train_avg_loss': 0.6751495361328125, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 20:46:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 197, 'Results_raw': {'train_total': 480, 'train_loss': 324.07177734375, 'train_avg_loss': 0.6751495361328125, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 20:46:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:46:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:46:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #197, planning to set LR to 1.00e-05
2025-10-09 20:46:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:46:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:46:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:46:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:46:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:46:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:47:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:47:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=320.700897, avg_loss=0.668127, seen=480, correct=281, accuracy=0.585417
2025-10-09 20:47:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:47:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:47:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:47:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=197 reserved=2330MB allocated=2217MB
2025-10-09 20:47:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 197, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.32278251647949, 'train_avg_loss': 0.6860231876373291, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-10-09 20:47:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 197, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 320.7008972167969, 'train_avg_loss': 0.6681268692016602, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 20:47:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 197, 'Results_raw': {'train_total': 480, 'train_loss': 320.7008972167969, 'train_avg_loss': 0.6681268692016602, 'train_seen': 480, 'train_correct': 281, 'train_acc': 0.5854166666666667}}
2025-10-09 20:47:36 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #198) -------------
2025-10-09 20:47:37 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=198 aidx=2 | s=5 (candidates=9)
2025-10-09 20:47:37 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[21, 36, 50, 32, 31] (from 9)
2025-10-09 20:47:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:47:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:47:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #198, planning to set LR to 1.00e-05
2025-10-09 20:47:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-09 20:47:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:47:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:47:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:47:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:47:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:48:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:48:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=325.464752, avg_loss=0.678052, seen=480, correct=270, accuracy=0.562500
2025-10-09 20:48:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:48:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:48:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:48:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=198 reserved=2374MB allocated=2217MB
2025-10-09 20:48:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 198, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.00172120332718, 'train_avg_loss': 0.6833476766943931, 'train_seen': 120, 'train_correct': 69, 'train_acc': 0.575}}
2025-10-09 20:48:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 198, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 325.4647521972656, 'train_avg_loss': 0.6780515670776367, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 20:48:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 198, 'Results_raw': {'train_total': 480, 'train_loss': 325.4647521972656, 'train_avg_loss': 0.6780515670776367, 'train_seen': 480, 'train_correct': 270, 'train_acc': 0.5625}}
2025-10-09 20:48:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:48:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:48:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #198, planning to set LR to 1.00e-05
2025-10-09 20:48:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 20:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:48:18 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:48:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:48:18 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:48:18 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:48:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:48:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=312.795776, avg_loss=0.651658, seen=480, correct=305, accuracy=0.635417
2025-10-09 20:48:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:48:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:48:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=198 reserved=2348MB allocated=2217MB
2025-10-09 20:48:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 198, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.48088449239731, 'train_avg_loss': 0.6373407041033109, 'train_seen': 120, 'train_correct': 78, 'train_acc': 0.65}}
2025-10-09 20:48:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 198, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 312.7957763671875, 'train_avg_loss': 0.6516578674316407, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 20:48:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 198, 'Results_raw': {'train_total': 480, 'train_loss': 312.7957763671875, 'train_avg_loss': 0.6516578674316407, 'train_seen': 480, 'train_correct': 305, 'train_acc': 0.6354166666666666}}
2025-10-09 20:48:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:49:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:49:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #198, planning to set LR to 1.00e-05
2025-10-09 20:49:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-09 20:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:49:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:49:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:49:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:49:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:49:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:49:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.348602, avg_loss=0.659060, seen=480, correct=290, accuracy=0.604167
2025-10-09 20:49:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:49:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:49:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:49:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=198 reserved=2330MB allocated=2217MB
2025-10-09 20:49:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 198, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.38956546783447, 'train_avg_loss': 0.6865797122319539, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 20:49:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 198, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.3486022949219, 'train_avg_loss': 0.6590595881144206, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:49:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 198, 'Results_raw': {'train_total': 480, 'train_loss': 316.3486022949219, 'train_avg_loss': 0.6590595881144206, 'train_seen': 480, 'train_correct': 290, 'train_acc': 0.6041666666666666}}
2025-10-09 20:49:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:49:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:49:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #198, planning to set LR to 1.00e-05
2025-10-09 20:49:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:49:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:49:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:49:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:49:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:49:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:50:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:50:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=317.963531, avg_loss=0.662424, seen=480, correct=293, accuracy=0.610417
2025-10-09 20:50:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:50:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:50:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:50:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=198 reserved=2330MB allocated=2217MB
2025-10-09 20:50:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 198, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.62518233060837, 'train_avg_loss': 0.663543186088403, 'train_seen': 120, 'train_correct': 72, 'train_acc': 0.6}}
2025-10-09 20:50:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 198, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 317.9635314941406, 'train_avg_loss': 0.6624240239461263, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 20:50:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 198, 'Results_raw': {'train_total': 480, 'train_loss': 317.9635314941406, 'train_avg_loss': 0.6624240239461263, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 20:50:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:50:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:50:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #198, planning to set LR to 1.00e-05
2025-10-09 20:50:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:50:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:50:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:50:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:50:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:51:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:51:05 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=330.021759, avg_loss=0.687545, seen=480, correct=259, accuracy=0.539583
2025-10-09 20:51:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:51:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:51:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=198 reserved=2348MB allocated=2217MB
2025-10-09 20:51:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 198, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.1318951845169, 'train_avg_loss': 0.7010991265376408, 'train_seen': 120, 'train_correct': 60, 'train_acc': 0.5}}
2025-10-09 20:51:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 198, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 330.0217590332031, 'train_avg_loss': 0.6875453313191732, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 20:51:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 198, 'Results_raw': {'train_total': 480, 'train_loss': 330.0217590332031, 'train_avg_loss': 0.6875453313191732, 'train_seen': 480, 'train_correct': 259, 'train_acc': 0.5395833333333333}}
2025-10-09 20:51:08 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #199) -------------
2025-10-09 20:51:09 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=199 aidx=2 | s=5 (candidates=9)
2025-10-09 20:51:09 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[32, 41, 37, 36, 5] (from 9)
2025-10-09 20:51:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:51:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:51:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #199, planning to set LR to 1.00e-05
2025-10-09 20:51:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:51:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:51:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:51:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:51:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:51:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.490234, avg_loss=0.655188, seen=480, correct=301, accuracy=0.627083
2025-10-09 20:51:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:51:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:51:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=199 reserved=2330MB allocated=2217MB
2025-10-09 20:51:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 199, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.55676120519638, 'train_avg_loss': 0.6546396767099698, 'train_seen': 120, 'train_correct': 76, 'train_acc': 0.6333333333333333}}
2025-10-09 20:51:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 199, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.490234375, 'train_avg_loss': 0.65518798828125, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 20:51:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 199, 'Results_raw': {'train_total': 480, 'train_loss': 314.490234375, 'train_avg_loss': 0.65518798828125, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 20:51:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:51:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:51:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #199, planning to set LR to 1.00e-05
2025-10-09 20:51:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-09 20:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:51:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:51:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:51:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:51:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:52:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:52:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=316.934082, avg_loss=0.660279, seen=480, correct=285, accuracy=0.593750
2025-10-09 20:52:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:52:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:52:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:52:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=199 reserved=2330MB allocated=2217MB
2025-10-09 20:52:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 199, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 78.02451646327972, 'train_avg_loss': 0.6502043038606644, 'train_seen': 120, 'train_correct': 71, 'train_acc': 0.5916666666666667}}
2025-10-09 20:52:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 199, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 316.93408203125, 'train_avg_loss': 0.6602793375651042, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 20:52:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 199, 'Results_raw': {'train_total': 480, 'train_loss': 316.93408203125, 'train_avg_loss': 0.6602793375651042, 'train_seen': 480, 'train_correct': 285, 'train_acc': 0.59375}}
2025-10-09 20:52:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:52:35 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:52:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #199, planning to set LR to 1.00e-05
2025-10-09 20:52:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:52:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:52:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:52:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:52:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:52:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:53:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:53:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.691376, avg_loss=0.666024, seen=480, correct=291, accuracy=0.606250
2025-10-09 20:53:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:53:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:53:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=199 reserved=2330MB allocated=2217MB
2025-10-09 20:53:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 199, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.4034206867218, 'train_avg_loss': 0.703361839056015, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 20:53:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 199, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.6913757324219, 'train_avg_loss': 0.6660236994425456, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 20:53:18 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 199, 'Results_raw': {'train_total': 480, 'train_loss': 319.6913757324219, 'train_avg_loss': 0.6660236994425456, 'train_seen': 480, 'train_correct': 291, 'train_acc': 0.60625}}
2025-10-09 20:53:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:53:19 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:53:19 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #199, planning to set LR to 1.00e-05
2025-10-09 20:53:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 20:53:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:53:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:53:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:53:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:53:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:53:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:53:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.177246, avg_loss=0.652453, seen=480, correct=301, accuracy=0.627083
2025-10-09 20:53:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:53:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:54:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:54:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=199 reserved=2348MB allocated=2217MB
2025-10-09 20:54:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 199, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.1198166012764, 'train_avg_loss': 0.6343318050106367, 'train_seen': 120, 'train_correct': 79, 'train_acc': 0.6583333333333333}}
2025-10-09 20:54:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 199, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.17724609375, 'train_avg_loss': 0.6524525960286458, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 20:54:01 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 199, 'Results_raw': {'train_total': 480, 'train_loss': 313.17724609375, 'train_avg_loss': 0.6524525960286458, 'train_seen': 480, 'train_correct': 301, 'train_acc': 0.6270833333333333}}
2025-10-09 20:54:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:54:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:54:02 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #199, planning to set LR to 1.00e-05
2025-10-09 20:54:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 20:54:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:54:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:54:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:54:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:54:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:54:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:54:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.144348, avg_loss=0.675301, seen=480, correct=280, accuracy=0.583333
2025-10-09 20:54:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:54:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:54:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=199 reserved=2396MB allocated=2217MB
2025-10-09 20:54:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 199, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.04458087682724, 'train_avg_loss': 0.658704840640227, 'train_seen': 120, 'train_correct': 74, 'train_acc': 0.6166666666666667}}
2025-10-09 20:54:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 199, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.14434814453125, 'train_avg_loss': 0.6753007253011067, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 20:54:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 199, 'Results_raw': {'train_total': 480, 'train_loss': 324.14434814453125, 'train_avg_loss': 0.6753007253011067, 'train_seen': 480, 'train_correct': 280, 'train_acc': 0.5833333333333334}}
2025-10-09 20:54:44 (federatedscope.core.workers.server:466) INFO: ----------- Starting a new training round (Round #200) -------------
2025-10-09 20:54:44 (federatedscope.core.sampler:206) INFO: [ClusterSampler] round=200 aidx=2 | s=5 (candidates=9)
2025-10-09 20:54:44 (federatedscope.core.sampler:214) INFO: [ClusterSampler] picked=[36, 32, 37, 5, 31] (from 9)
2025-10-09 20:54:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:54:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:54:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #200, planning to set LR to 1.00e-05
2025-10-09 20:54:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-09 20:54:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:54:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:54:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:54:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:54:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:55:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:55:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=313.375458, avg_loss=0.652866, seen=480, correct=304, accuracy=0.633333
2025-10-09 20:55:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:55:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:55:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=200 reserved=2348MB allocated=2217MB
2025-10-09 20:55:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 200, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 77.19666355848312, 'train_avg_loss': 0.643305529654026, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 20:55:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 200, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 313.3754577636719, 'train_avg_loss': 0.6528655370076497, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 20:55:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 200, 'Results_raw': {'train_total': 480, 'train_loss': 313.3754577636719, 'train_avg_loss': 0.6528655370076497, 'train_seen': 480, 'train_correct': 304, 'train_acc': 0.6333333333333333}}
2025-10-09 20:55:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:55:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:55:29 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #200, planning to set LR to 1.00e-05
2025-10-09 20:55:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-09 20:55:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:55:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:55:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:55:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:55:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:56:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:56:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=315.998383, avg_loss=0.658330, seen=480, correct=300, accuracy=0.625000
2025-10-09 20:56:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:56:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:56:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:56:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=200 reserved=2330MB allocated=2217MB
2025-10-09 20:56:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 200, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 79.12570798397064, 'train_avg_loss': 0.659380899866422, 'train_seen': 120, 'train_correct': 75, 'train_acc': 0.625}}
2025-10-09 20:56:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 200, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 315.9983825683594, 'train_avg_loss': 0.658329963684082, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 20:56:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 200, 'Results_raw': {'train_total': 480, 'train_loss': 315.9983825683594, 'train_avg_loss': 0.658329963684082, 'train_seen': 480, 'train_correct': 300, 'train_acc': 0.625}}
2025-10-09 20:56:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:56:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:56:12 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #200, planning to set LR to 1.00e-05
2025-10-09 20:56:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-09 20:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:56:12 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:56:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:56:12 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:56:12 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:56:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:56:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=319.954651, avg_loss=0.666572, seen=480, correct=293, accuracy=0.610417
2025-10-09 20:56:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:56:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:56:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=200 reserved=2330MB allocated=2217MB
2025-10-09 20:56:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 200, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.38101917505264, 'train_avg_loss': 0.7115084931254387, 'train_seen': 120, 'train_correct': 61, 'train_acc': 0.5083333333333333}}
2025-10-09 20:56:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 200, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 319.95465087890625, 'train_avg_loss': 0.6665721893310547, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 20:56:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 200, 'Results_raw': {'train_total': 480, 'train_loss': 319.95465087890625, 'train_avg_loss': 0.6665721893310547, 'train_seen': 480, 'train_correct': 293, 'train_acc': 0.6104166666666667}}
2025-10-09 20:56:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:56:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:56:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #200, planning to set LR to 1.00e-05
2025-10-09 20:56:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-09 20:56:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:56:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:56:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:56:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:56:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:57:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:57:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=314.384613, avg_loss=0.654968, seen=480, correct=289, accuracy=0.602083
2025-10-09 20:57:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:57:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:57:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:57:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=200 reserved=2396MB allocated=2217MB
2025-10-09 20:57:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 200, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 76.33559954166412, 'train_avg_loss': 0.6361299961805343, 'train_seen': 120, 'train_correct': 73, 'train_acc': 0.6083333333333333}}
2025-10-09 20:57:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 200, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 314.3846130371094, 'train_avg_loss': 0.6549679438273112, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 20:57:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 200, 'Results_raw': {'train_total': 480, 'train_loss': 314.3846130371094, 'train_avg_loss': 0.6549679438273112, 'train_seen': 480, 'train_correct': 289, 'train_acc': 0.6020833333333333}}
2025-10-09 20:57:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:57:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=-1
2025-10-09 20:57:36 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #200, planning to set LR to 1.00e-05
2025-10-09 20:57:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-09 20:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:57:36 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-09 20:57:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=60, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-09 20:57:36 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-10-09 20:57:36 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-10-09 20:58:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-10-09 20:58:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=480, loss_sum=324.097687, avg_loss=0.675204, seen=480, correct=274, accuracy=0.570833
2025-10-09 20:58:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-09 20:58:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140130086027264 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-09 20:58:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-09 20:58:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=200 reserved=2348MB allocated=2217MB
2025-10-09 20:58:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 200, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.50517719984055, 'train_avg_loss': 0.6875431433320045, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-10-09 20:58:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 200, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 324.0976867675781, 'train_avg_loss': 0.675203514099121, 'train_seen': 480, 'train_correct': 274, 'train_acc': 0.5708333333333333}}
2025-10-09 20:58:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 200, 'Results_raw': {'train_total': 480, 'train_loss': 324.0976867675781, 'train_avg_loss': 0.675203514099121, 'train_seen': 480, 'train_correct': 274, 'train_acc': 0.5708333333333333}}
2025-10-09 20:58:17 (federatedscope.core.workers.server:488) INFO: Server: Training is finished! (skip final evaluation)
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 728.6316643666667, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 153456, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 1 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 728.6327576833334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 7749304, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 2 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 728.5739166666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8205152, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 3 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 728.5346865833333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 5470112, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 4 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 728.4933999666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9116832, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 5 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 728.45409895, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8205152, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 6 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 728.4148674166667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8205144, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 7 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 728.3745958666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 10940192, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 8 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 728.3340033999999, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9116832, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 9 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 728.2909874666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 10028512, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 10 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 728.2469917, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 6381784, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 11 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 728.2052262333333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8660992, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 12 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 728.16278425, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 7749312, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 13 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 728.1197015, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 7293464, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 14 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 728.0766282666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 10484352, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 15 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 728.0334178666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 5014264, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 16 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 727.9907420666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8205144, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 17 received finish message =================
2025-10-09 20:58:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:17 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 727.94798105, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 10484352, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:17 (federatedscope.core.workers.client:842) INFO: ================= client 18 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 727.9050960999999, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9116832, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 19 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 727.8605507666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 12763544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 20 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 727.7973518666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8205144, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 21 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 727.7569154666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8205152, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 22 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 727.7158941833334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 12763544, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 23 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 727.6747667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2735072, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 24 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 727.6341924833334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 6837632, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 25 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 727.59325615, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 10028512, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 26 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 727.5519275, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9572664, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 27 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 727.5110619999999, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8660992, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 28 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 727.4699239833334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8660984, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 29 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 727.4289451000001, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9572664, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 30 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 727.3837993333333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9572672, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 31 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 727.3417134833333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9572672, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 32 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 727.30019415, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9572672, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 33 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 727.2583858833333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 5925952, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 34 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 727.2161907333334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 7749304, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 35 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 727.17577925, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 7749312, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 36 received finish message =================
2025-10-09 20:58:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:18 (federatedscope.core.monitors.monitor:268) INFO: In worker #36, the system-related metrics are: {'id': 36, 'fl_end_time_minutes': 727.1346241166667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8660992, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:18 (federatedscope.core.workers.client:842) INFO: ================= client 37 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #37, the system-related metrics are: {'id': 37, 'fl_end_time_minutes': 727.0932725666667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9116832, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 38 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #38, the system-related metrics are: {'id': 38, 'fl_end_time_minutes': 727.0521908166667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 7749312, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 39 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #39, the system-related metrics are: {'id': 39, 'fl_end_time_minutes': 727.0105837833332, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 11851864, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 40 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #40, the system-related metrics are: {'id': 40, 'fl_end_time_minutes': 726.9507228166666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 7293464, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 41 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #41, the system-related metrics are: {'id': 41, 'fl_end_time_minutes': 726.9067502166668, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8205152, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 42 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #42, the system-related metrics are: {'id': 42, 'fl_end_time_minutes': 726.8651469333333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9572672, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 43 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #43, the system-related metrics are: {'id': 43, 'fl_end_time_minutes': 726.8232784333334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 5925944, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 44 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #44, the system-related metrics are: {'id': 44, 'fl_end_time_minutes': 726.7823188666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 10484352, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 45 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #45, the system-related metrics are: {'id': 45, 'fl_end_time_minutes': 726.7408981166667, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9572664, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 46 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #46, the system-related metrics are: {'id': 46, 'fl_end_time_minutes': 726.6992557, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 9572664, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 47 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #47, the system-related metrics are: {'id': 47, 'fl_end_time_minutes': 726.6581420833334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8660984, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 48 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #48, the system-related metrics are: {'id': 48, 'fl_end_time_minutes': 726.6170870000001, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 7293464, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 49 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #49, the system-related metrics are: {'id': 49, 'fl_end_time_minutes': 726.5763707666666, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8660992, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 50 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #50, the system-related metrics are: {'id': 50, 'fl_end_time_minutes': 726.53036825, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 10484352, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 51 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #51, the system-related metrics are: {'id': 51, 'fl_end_time_minutes': 726.4890214833334, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 10028504, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 52 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #52, the system-related metrics are: {'id': 52, 'fl_end_time_minutes': 726.4474802833333, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8660992, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.workers.client:842) INFO: ================= client 53 received finish message =================
2025-10-09 20:58:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #53, the system-related metrics are: {'id': 53, 'fl_end_time_minutes': 726.4053139, 'total_model_size': 511369344, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 7749312, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 727.5347633808643, 'sys_avg/total_model_size': '478.65M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '8.09M', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-10-09 20:58:19 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.666103140707573, 'sys_std/total_model_size': '65.75M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '2.01M', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
