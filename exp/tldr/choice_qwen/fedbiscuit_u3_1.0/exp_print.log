2025-11-06 11:39:38 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/fedbiscuit_u3_1.0/exp_print.log
2025-11-06 11:39:38 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/fedbiscuit_u3_1.0
2025-11-06 11:40:02 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/1] Train=92858, Val=33082, Test=50715, Total=176655
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/1] Train=2793, Val=146, Test=40, Total=2979
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/1] Train=214, Val=11, Test=40, Total=265
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/1] Train=691, Val=36, Test=40, Total=767
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/1] Train=213, Val=11, Test=40, Total=264
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/1] Train=285, Val=14, Test=40, Total=339
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/1] Train=2547, Val=134, Test=40, Total=2721
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/1] Train=1088, Val=57, Test=40, Total=1185
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/1] Train=1316, Val=69, Test=40, Total=1425
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/1] Train=3572, Val=188, Test=40, Total=3800
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/1] Train=1209, Val=63, Test=40, Total=1312
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/1] Train=621, Val=32, Test=40, Total=693
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/1] Train=2605, Val=137, Test=40, Total=2782
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/1] Train=1372, Val=72, Test=40, Total=1484
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/1] Train=3055, Val=160, Test=40, Total=3255
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/1] Train=14550, Val=200, Test=40, Total=14790
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/1] Train=2589, Val=136, Test=40, Total=2765
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/1] Train=5883, Val=200, Test=40, Total=6123
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/1] Train=2576, Val=135, Test=40, Total=2751
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/1] Train=2102, Val=110, Test=40, Total=2252
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/1] Train=2399, Val=126, Test=40, Total=2565
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/1] Train=2915, Val=153, Test=40, Total=3108
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/1] Train=224, Val=11, Test=40, Total=275
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/1] Train=583, Val=30, Test=40, Total=653
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/1] Train=4944, Val=200, Test=40, Total=5184
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/1] Train=4647, Val=200, Test=40, Total=4887
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/1] Train=3063, Val=161, Test=40, Total=3264
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/1] Train=2342, Val=123, Test=40, Total=2505
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/1] Train=1434, Val=75, Test=40, Total=1549
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/1] Train=6191, Val=200, Test=40, Total=6431
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/1] Train=3247, Val=170, Test=40, Total=3457
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/1] Train=3679, Val=193, Test=40, Total=3912
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/1] Train=2144, Val=112, Test=40, Total=2296
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/1] Train=1409, Val=74, Test=40, Total=1523
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/1] Train=4486, Val=200, Test=40, Total=4726
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/1] Train=4736, Val=200, Test=40, Total=4976
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/1] Train=1030, Val=54, Test=40, Total=1124
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/1] Train=4273, Val=200, Test=40, Total=4513
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/1] Train=6171, Val=200, Test=40, Total=6411
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/1] Train=1594, Val=83, Test=40, Total=1717
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/1] Train=4005, Val=200, Test=40, Total=4245
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/1] Train=2275, Val=119, Test=40, Total=2434
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/1] Train=5772, Val=200, Test=40, Total=6012
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/1] Train=1694, Val=89, Test=40, Total=1823
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/1] Train=7916, Val=200, Test=40, Total=8156
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/1] Train=1901, Val=100, Test=40, Total=2041
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/1] Train=2100, Val=110, Test=40, Total=2250
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/1] Train=2812, Val=147, Test=40, Total=2999
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/1] Train=880, Val=46, Test=40, Total=966
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/1] Train=2521, Val=132, Test=40, Total=2693
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/1] Train=2527, Val=133, Test=40, Total=2700
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/1] Train=1580, Val=83, Test=40, Total=1703
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/1] Train=3589, Val=188, Test=40, Total=3817
2025-11-06 11:40:44 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/1] Train=6791, Val=200, Test=40, Total=7031
2025-11-06 11:40:46 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-11-06 11:40:46 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-11-06 11:40:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=?] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140043029930048 | out_emb=(Linear) num=151646 ptr=140043029930048 | lora_ptr=None
2025-11-06 11:40:59 (federatedscope.llm.model.model_builder:187) INFO: [Warmup-Init] loaded from checkpoints_u10_warmup_1.0/final_tldr_choice_qwen_fedbiscuit_u10_warmup_1.0_round_250.ckpt (round=250) | missing=291 unexpected=2352
2025-11-06 11:40:59 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-11-06 11:41:00 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 11:41:02 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-11-06 11:41:02 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 11:41:05 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-11-06 11:41:05 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 11:41:07 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-11-06 11:41:07 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 11:41:09 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-11-06 11:41:10 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 11:41:12 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-11-06 11:41:12 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-11-06 11:41:12 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 1344.
2025-11-06 11:41:12 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 1634.
2025-11-06 11:41:12 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 1344. 
Preserved para names in local update: {'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight'}.
2025-11-06 11:41:12 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-11-06 11:41:12 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-11-06 11:41:12 (federatedscope.llm.llm_local.server:176) INFO: Waited all clients join, start now...
2025-11-06 11:41:12 (federatedscope.llm.llm_local.server:200) INFO: ----------- Starting training (Round #0) -------------
2025-11-06 11:41:12 (federatedscope.llm.llm_local.server:203) INFO: Server: Performing a grouping step...
2025-11-06 11:41:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 11:41:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=146, total=146)
2025-11-06 11:41:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140043029930048 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:14 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:41:24 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=146, loss_sum=101.476049, avg_loss=0.695041, seen=146, correct=79, accuracy=0.541096
2025-11-06 11:41:24 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:41:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:25 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:41:26 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:41:26 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 0 with val results: {'val_total': 146, 'val_loss': 101.47604882717133, 'val_avg_loss': 0.6950414303230913, 'val_seen': 146, 'val_correct': 79, 'val_acc': 0.541095890410959}
2025-11-06 11:41:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:26 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:41:28 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=26.962017, avg_loss=0.674050, seen=40, correct=23, accuracy=0.575000
2025-11-06 11:41:28 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:41:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:29 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:41:29 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:41:29 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.962017059326172, 'test_avg_loss': 0.6740504264831543, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-11-06 11:41:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=146, total=146)
2025-11-06 11:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:30 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:41:39 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=146, loss_sum=100.390806, avg_loss=0.687608, seen=146, correct=86, accuracy=0.589041
2025-11-06 11:41:39 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:41:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:40 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:41:40 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:41:40 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 1 with val results: {'val_total': 146, 'val_loss': 100.39080584049225, 'val_avg_loss': 0.6876082591814537, 'val_seen': 146, 'val_correct': 86, 'val_acc': 0.589041095890411}
2025-11-06 11:41:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:41:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:40 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:41:43 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=27.545370, avg_loss=0.688634, seen=40, correct=22, accuracy=0.550000
2025-11-06 11:41:43 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:43 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:41:44 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:41:44 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.545369505882263, 'test_avg_loss': 0.6886342376470566, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-11-06 11:41:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=146, total=146)
2025-11-06 11:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:44 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:41:53 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=146, loss_sum=100.388792, avg_loss=0.687594, seen=146, correct=72, accuracy=0.493151
2025-11-06 11:41:53 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:41:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:54 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:41:55 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:41:55 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 2 with val results: {'val_total': 146, 'val_loss': 100.38879209756851, 'val_avg_loss': 0.6875944664217022, 'val_seen': 146, 'val_correct': 72, 'val_acc': 0.4931506849315068}
2025-11-06 11:41:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:41:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:55 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:41:57 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=27.563477, avg_loss=0.689087, seen=40, correct=22, accuracy=0.550000
2025-11-06 11:41:57 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:41:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:58 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:41:58 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:41:58 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.563477218151093, 'test_avg_loss': 0.6890869304537773, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-11-06 11:41:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 11:41:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 11:41:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:41:59 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:00 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=11, loss_sum=7.607908, avg_loss=0.691628, seen=11, correct=6, accuracy=0.545455
2025-11-06 11:42:00 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:00 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:01 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:42:01 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.607907772064209, 'val_avg_loss': 0.6916279792785645, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-11-06 11:42:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:01 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:03 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=27.387657, avg_loss=0.684691, seen=40, correct=21, accuracy=0.525000
2025-11-06 11:42:03 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:04 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:04 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:42:04 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.387657046318054, 'test_avg_loss': 0.6846914261579513, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-11-06 11:42:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 11:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:05 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:06 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=11, loss_sum=7.861398, avg_loss=0.714673, seen=11, correct=5, accuracy=0.454545
2025-11-06 11:42:06 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:06 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:07 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:07 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.861398458480835, 'val_avg_loss': 0.7146725871346213, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-11-06 11:42:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:07 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:09 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=27.645732, avg_loss=0.691143, seen=40, correct=23, accuracy=0.575000
2025-11-06 11:42:09 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:09 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:10 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:10 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.645732402801514, 'test_avg_loss': 0.6911433100700378, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-11-06 11:42:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 11:42:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:11 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:11 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=11, loss_sum=7.284580, avg_loss=0.662235, seen=11, correct=6, accuracy=0.545455
2025-11-06 11:42:11 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:12 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:12 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:12 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.284580111503601, 'val_avg_loss': 0.6622345555912365, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-11-06 11:42:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:12 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:14 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=26.312472, avg_loss=0.657812, seen=40, correct=26, accuracy=0.650000
2025-11-06 11:42:14 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:15 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:15 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:15 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.312472462654114, 'test_avg_loss': 0.6578118115663528, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-11-06 11:42:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 11:42:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=36, total=36)
2025-11-06 11:42:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:16 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:18 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=36, loss_sum=25.751966, avg_loss=0.715332, seen=36, correct=16, accuracy=0.444444
2025-11-06 11:42:18 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:19 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:19 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:42:19 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 0 with val results: {'val_total': 36, 'val_loss': 25.75196635723114, 'val_avg_loss': 0.7153323988119761, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-11-06 11:42:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:19 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:22 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=26.556870, avg_loss=0.663922, seen=40, correct=22, accuracy=0.550000
2025-11-06 11:42:22 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:22 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:23 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:42:23 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.556870222091675, 'test_avg_loss': 0.6639217555522918, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-11-06 11:42:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=36, total=36)
2025-11-06 11:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:24 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:25 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=36, loss_sum=25.401295, avg_loss=0.705592, seen=36, correct=18, accuracy=0.500000
2025-11-06 11:42:25 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:26 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:26 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:26 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 1 with val results: {'val_total': 36, 'val_loss': 25.401294946670532, 'val_avg_loss': 0.7055915262964036, 'val_seen': 36, 'val_correct': 18, 'val_acc': 0.5}
2025-11-06 11:42:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:27 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:29 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=27.025985, avg_loss=0.675650, seen=40, correct=26, accuracy=0.650000
2025-11-06 11:42:29 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:29 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:30 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:30 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.02598464488983, 'test_avg_loss': 0.6756496161222458, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-11-06 11:42:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=36, total=36)
2025-11-06 11:42:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:30 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:32 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=36, loss_sum=25.993761, avg_loss=0.722049, seen=36, correct=16, accuracy=0.444444
2025-11-06 11:42:32 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:33 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:33 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:33 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 2 with val results: {'val_total': 36, 'val_loss': 25.99376106262207, 'val_avg_loss': 0.7220489184061686, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-11-06 11:42:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:33 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:35 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=28.302518, avg_loss=0.707563, seen=40, correct=19, accuracy=0.475000
2025-11-06 11:42:35 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:36 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:37 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:37 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.302517771720886, 'test_avg_loss': 0.7075629442930221, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-11-06 11:42:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 11:42:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 11:42:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:37 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:38 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=11, loss_sum=8.151049, avg_loss=0.741004, seen=11, correct=4, accuracy=0.363636
2025-11-06 11:42:38 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:39 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:39 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:42:39 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.151048839092255, 'val_avg_loss': 0.7410044399174777, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-11-06 11:42:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:39 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:41 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=25.970705, avg_loss=0.649268, seen=40, correct=26, accuracy=0.650000
2025-11-06 11:42:41 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:42 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:42 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:42:42 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.970704913139343, 'test_avg_loss': 0.6492676228284836, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-11-06 11:42:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 11:42:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:43 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:44 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=11, loss_sum=8.661100, avg_loss=0.787373, seen=11, correct=4, accuracy=0.363636
2025-11-06 11:42:44 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:44 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:45 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:45 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 1 with val results: {'val_total': 11, 'val_loss': 8.661100387573242, 'val_avg_loss': 0.7873727625066583, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-11-06 11:42:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:45 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:47 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=27.440605, avg_loss=0.686015, seen=40, correct=19, accuracy=0.475000
2025-11-06 11:42:47 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:48 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:48 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:48 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.440604746341705, 'test_avg_loss': 0.6860151186585426, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-11-06 11:42:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 11:42:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:49 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:49 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=11, loss_sum=8.760524, avg_loss=0.796411, seen=11, correct=4, accuracy=0.363636
2025-11-06 11:42:49 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:50 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:50 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:50 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.760523557662964, 'val_avg_loss': 0.7964112325148149, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-11-06 11:42:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:51 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:53 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=26.205442, avg_loss=0.655136, seen=40, correct=25, accuracy=0.625000
2025-11-06 11:42:53 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:53 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:54 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:42:54 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.205442190170288, 'test_avg_loss': 0.6551360547542572, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-11-06 11:42:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 11:42:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=14, total=14)
2025-11-06 11:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:55 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:55 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=14, loss_sum=9.776573, avg_loss=0.698327, seen=14, correct=6, accuracy=0.428571
2025-11-06 11:42:55 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:56 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:42:56 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:42:56 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 0 with val results: {'val_total': 14, 'val_loss': 9.776572704315186, 'val_avg_loss': 0.6983266217367989, 'val_seen': 14, 'val_correct': 6, 'val_acc': 0.42857142857142855}
2025-11-06 11:42:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:42:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:57 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:42:59 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=28.702360, avg_loss=0.717559, seen=40, correct=20, accuracy=0.500000
2025-11-06 11:42:59 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:42:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:42:59 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:43:00 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 11:43:00 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.70236027240753, 'test_avg_loss': 0.7175590068101882, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-11-06 11:43:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=14, total=14)
2025-11-06 11:43:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:43:00 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:43:01 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=14, loss_sum=9.291585, avg_loss=0.663685, seen=14, correct=8, accuracy=0.571429
2025-11-06 11:43:01 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:43:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:43:02 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:43:02 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:43:02 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 1 with val results: {'val_total': 14, 'val_loss': 9.291584968566895, 'val_avg_loss': 0.663684640611921, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-11-06 11:43:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:43:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:43:02 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:43:04 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=28.303123, avg_loss=0.707578, seen=40, correct=20, accuracy=0.500000
2025-11-06 11:43:04 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:43:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:43:05 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:43:05 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:43:05 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.303123354911804, 'test_avg_loss': 0.7075780838727951, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-11-06 11:43:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=14, total=14)
2025-11-06 11:43:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:43:06 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:43:07 (federatedscope.llm.trainer.trainer:1264) INFO: [val|final] total=14, loss_sum=9.833261, avg_loss=0.702376, seen=14, correct=7, accuracy=0.500000
2025-11-06 11:43:07 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:43:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:43:07 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:43:08 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:43:08 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 2 with val results: {'val_total': 14, 'val_loss': 9.833261489868164, 'val_avg_loss': 0.7023758207048688, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}
2025-11-06 11:43:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 11:43:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:43:08 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:43:10 (federatedscope.llm.trainer.trainer:1264) INFO: [test|final] total=40, loss_sum=29.585873, avg_loss=0.739647, seen=40, correct=14, accuracy=0.350000
2025-11-06 11:43:10 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:43:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:43:11 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:43:11 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 11:43:11 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.585872530937195, 'test_avg_loss': 0.7396468132734298, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-11-06 11:43:11 (federatedscope.llm.llm_local.server:393) INFO: Adapter 1 is done with the clients [3, 5]
2025-11-06 11:43:11 (federatedscope.llm.llm_local.server:393) INFO: Adapter 2 is done with the clients [2, 1]
2025-11-06 11:43:11 (federatedscope.llm.llm_local.server:393) INFO: Adapter 0 is done with the clients [4]
2025-11-06 11:43:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 11:43:12 (federatedscope.llm.llm_local.client:181) INFO: Activate the adapter 1 for training...
2025-11-06 11:43:12 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=-1
2025-11-06 11:43:12 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-06 11:43:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=691, total=691)
2025-11-06 11:43:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:43:46 (federatedscope.llm.trainer.trainer:817) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-06 11:43:46 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:43:46 (federatedscope.llm.trainer.trainer:859) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-11-06 11:43:47 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=346, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-11-06 11:44:51 (federatedscope.llm.trainer.trainer:1264) INFO: [train|final] total=120, loss_sum=85.452203, avg_loss=0.712102, seen=120, correct=58, accuracy=0.483333
2025-11-06 11:44:51 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:44:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:44:54 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:44:55 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1848MB allocated=1768MB
2025-11-06 11:45:19 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/1', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 85.45220255851746, 'train_avg_loss': 0.7121016879876455, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-11-06 11:45:19 (federatedscope.core.workers.client:469) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 120, 'train_loss': 85.45220255851746, 'train_avg_loss': 0.7121016879876455, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-11-06 11:45:19 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_total': 120, 'train_loss': 85.45220255851746, 'train_avg_loss': 0.7121016879876455, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-11-06 11:45:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 11:45:19 (federatedscope.llm.llm_local.client:181) INFO: Activate the adapter 2 for training...
2025-11-06 11:45:20 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=-1
2025-11-06 11:45:20 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-06 11:45:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=2793, total=2793)
2025-11-06 11:45:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:45:42 (federatedscope.llm.trainer.trainer:817) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-06 11:45:42 (federatedscope.llm.trainer.trainer:838) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 11:45:42 (federatedscope.llm.trainer.trainer:859) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-11-06 11:45:42 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1397, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-11-06 11:46:47 (federatedscope.llm.trainer.trainer:1264) INFO: [train|final] total=120, loss_sum=83.329639, avg_loss=0.694414, seen=120, correct=67, accuracy=0.558333
2025-11-06 11:46:47 (federatedscope.llm.trainer.trainer:1292) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 11:46:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 11:46:49 (federatedscope.llm.trainer.trainer:1316) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 11:46:50 (federatedscope.llm.trainer.trainer:1339) INFO: [VRAM] round=0 reserved=1864MB allocated=1768MB
2025-11-06 12:04:26 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/1', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.32963943481445, 'train_avg_loss': 0.6944136619567871, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-11-06 12:04:26 (federatedscope.core.workers.client:469) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 120, 'train_loss': 83.32963943481445, 'train_avg_loss': 0.6944136619567871, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-11-06 12:04:26 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_total': 120, 'train_loss': 83.32963943481445, 'train_avg_loss': 0.6944136619567871, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-11-06 12:04:27 (federatedscope.core.workers.server:412) INFO: ----------- Starting a new training round (Round #1) -------------
2025-11-06 12:04:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 12:04:28 (federatedscope.llm.llm_local.client:181) INFO: Activate the adapter 2 for training...
2025-11-06 12:04:28 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=-1
2025-11-06 12:04:28 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #1, planning to set LR to 1.00e-05
2025-11-06 12:04:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=2793, total=2793)
2025-11-06 12:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140038885081088 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:36:56 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/fedbiscuit_u3_1.0/exp_print.log
2025-11-06 21:36:56 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/fedbiscuit_u3_1.0
2025-11-06 21:37:20 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/1] Train=92858, Val=33082, Test=50715, Total=176655
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/1] Train=2793, Val=146, Test=40, Total=2979
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/1] Train=214, Val=11, Test=40, Total=265
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/1] Train=691, Val=36, Test=40, Total=767
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/1] Train=213, Val=11, Test=40, Total=264
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/1] Train=285, Val=14, Test=40, Total=339
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/1] Train=2547, Val=134, Test=40, Total=2721
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/1] Train=1088, Val=57, Test=40, Total=1185
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/1] Train=1316, Val=69, Test=40, Total=1425
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/1] Train=3572, Val=188, Test=40, Total=3800
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/1] Train=1209, Val=63, Test=40, Total=1312
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/1] Train=621, Val=32, Test=40, Total=693
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/1] Train=2605, Val=137, Test=40, Total=2782
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/1] Train=1372, Val=72, Test=40, Total=1484
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/1] Train=3055, Val=160, Test=40, Total=3255
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/1] Train=14550, Val=200, Test=40, Total=14790
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/1] Train=2589, Val=136, Test=40, Total=2765
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/1] Train=5883, Val=200, Test=40, Total=6123
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/1] Train=2576, Val=135, Test=40, Total=2751
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/1] Train=2102, Val=110, Test=40, Total=2252
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/1] Train=2399, Val=126, Test=40, Total=2565
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/1] Train=2915, Val=153, Test=40, Total=3108
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/1] Train=224, Val=11, Test=40, Total=275
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/1] Train=583, Val=30, Test=40, Total=653
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/1] Train=4944, Val=200, Test=40, Total=5184
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/1] Train=4647, Val=200, Test=40, Total=4887
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/1] Train=3063, Val=161, Test=40, Total=3264
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/1] Train=2342, Val=123, Test=40, Total=2505
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/1] Train=1434, Val=75, Test=40, Total=1549
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/1] Train=6191, Val=200, Test=40, Total=6431
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/1] Train=3247, Val=170, Test=40, Total=3457
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/1] Train=3679, Val=193, Test=40, Total=3912
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/1] Train=2144, Val=112, Test=40, Total=2296
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/1] Train=1409, Val=74, Test=40, Total=1523
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/1] Train=4486, Val=200, Test=40, Total=4726
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/1] Train=4736, Val=200, Test=40, Total=4976
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/1] Train=1030, Val=54, Test=40, Total=1124
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/1] Train=4273, Val=200, Test=40, Total=4513
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/1] Train=6171, Val=200, Test=40, Total=6411
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/1] Train=1594, Val=83, Test=40, Total=1717
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/1] Train=4005, Val=200, Test=40, Total=4245
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/1] Train=2275, Val=119, Test=40, Total=2434
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/1] Train=5772, Val=200, Test=40, Total=6012
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/1] Train=1694, Val=89, Test=40, Total=1823
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/1] Train=7916, Val=200, Test=40, Total=8156
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/1] Train=1901, Val=100, Test=40, Total=2041
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/1] Train=2100, Val=110, Test=40, Total=2250
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/1] Train=2812, Val=147, Test=40, Total=2999
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/1] Train=880, Val=46, Test=40, Total=966
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/1] Train=2521, Val=132, Test=40, Total=2693
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/1] Train=2527, Val=133, Test=40, Total=2700
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/1] Train=1580, Val=83, Test=40, Total=1703
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/1] Train=3589, Val=188, Test=40, Total=3817
2025-11-06 21:38:01 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/1] Train=6791, Val=200, Test=40, Total=7031
2025-11-06 21:38:03 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-11-06 21:38:03 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-11-06 21:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=?] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140079419711552 | out_emb=(Linear) num=151646 ptr=140079419711552 | lora_ptr=None
2025-11-06 21:38:16 (federatedscope.llm.model.model_builder:187) INFO: [Warmup-Init] loaded from checkpoints_u10_warmup_1.0/final_tldr_choice_qwen_fedbiscuit_u10_warmup_1.0_round_250.ckpt (round=250) | missing=291 unexpected=2352
2025-11-06 21:38:16 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-11-06 21:38:17 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 21:38:20 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-11-06 21:38:20 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 21:38:22 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-11-06 21:38:22 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 21:38:24 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-11-06 21:38:24 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 21:38:26 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-11-06 21:38:27 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-06 21:38:29 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-11-06 21:38:29 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-11-06 21:38:29 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 1344.
2025-11-06 21:38:29 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 1634.
2025-11-06 21:38:29 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 1344. 
Preserved para names in local update: {'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight'}.
2025-11-06 21:38:29 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-11-06 21:38:29 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-11-06 21:38:29 (federatedscope.llm.llm_local.server:176) INFO: Waited all clients join, start now...
2025-11-06 21:38:29 (federatedscope.llm.llm_local.server:200) INFO: ----------- Starting training (Round #0) -------------
2025-11-06 21:38:29 (federatedscope.llm.llm_local.server:203) INFO: Server: Performing a grouping step...
2025-11-06 21:38:29 (federatedscope.core.trainers.torch_trainer:181) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 21:38:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=146, total=146)
2025-11-06 21:38:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140079419711552 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:38:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:38:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=101.476049, avg_loss=0.695041, seen=146, correct=79, accuracy=0.541096
2025-11-06 21:38:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:38:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:38:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:38:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:38:43 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 0 with val results: {'val_total': 146, 'val_loss': 101.47604882717133, 'val_avg_loss': 0.6950414303230913, 'val_seen': 146, 'val_correct': 79, 'val_acc': 0.541095890410959}
2025-11-06 21:38:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:38:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:38:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:38:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.962017, avg_loss=0.674050, seen=40, correct=23, accuracy=0.575000
2025-11-06 21:38:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:38:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:38:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:38:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:38:46 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.962017059326172, 'test_avg_loss': 0.6740504264831543, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-11-06 21:38:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=146, total=146)
2025-11-06 21:38:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:38:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:38:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=100.390806, avg_loss=0.687608, seen=146, correct=86, accuracy=0.589041
2025-11-06 21:38:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:38:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:38:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:38:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:38:58 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 1 with val results: {'val_total': 146, 'val_loss': 100.39080584049225, 'val_avg_loss': 0.6876082591814537, 'val_seen': 146, 'val_correct': 86, 'val_acc': 0.589041095890411}
2025-11-06 21:38:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:38:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:38:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.545370, avg_loss=0.688634, seen=40, correct=22, accuracy=0.550000
2025-11-06 21:39:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:01 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.545369505882263, 'test_avg_loss': 0.6886342376470566, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-11-06 21:39:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=146, total=146)
2025-11-06 21:39:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=100.388792, avg_loss=0.687594, seen=146, correct=72, accuracy=0.493151
2025-11-06 21:39:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:12 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 2 with val results: {'val_total': 146, 'val_loss': 100.38879209756851, 'val_avg_loss': 0.6875944664217022, 'val_seen': 146, 'val_correct': 72, 'val_acc': 0.4931506849315068}
2025-11-06 21:39:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:39:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.563477, avg_loss=0.689087, seen=40, correct=22, accuracy=0.550000
2025-11-06 21:39:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:16 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.563477218151093, 'test_avg_loss': 0.6890869304537773, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-11-06 21:39:16 (federatedscope.core.trainers.torch_trainer:181) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 21:39:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 21:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=11, loss_sum=7.607908, avg_loss=0.691628, seen=11, correct=6, accuracy=0.545455
2025-11-06 21:39:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:39:18 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.607907772064209, 'val_avg_loss': 0.6916279792785645, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-11-06 21:39:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.387657, avg_loss=0.684691, seen=40, correct=21, accuracy=0.525000
2025-11-06 21:39:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:39:22 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.387657046318054, 'test_avg_loss': 0.6846914261579513, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-11-06 21:39:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 21:39:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=11, loss_sum=7.861398, avg_loss=0.714673, seen=11, correct=5, accuracy=0.454545
2025-11-06 21:39:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:24 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.861398458480835, 'val_avg_loss': 0.7146725871346213, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-11-06 21:39:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.645732, avg_loss=0.691143, seen=40, correct=23, accuracy=0.575000
2025-11-06 21:39:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:28 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.645732402801514, 'test_avg_loss': 0.6911433100700378, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-11-06 21:39:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 21:39:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=11, loss_sum=7.284580, avg_loss=0.662235, seen=11, correct=6, accuracy=0.545455
2025-11-06 21:39:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:30 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.284580111503601, 'val_avg_loss': 0.6622345555912365, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-11-06 21:39:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:39:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.312472, avg_loss=0.657812, seen=40, correct=26, accuracy=0.650000
2025-11-06 21:39:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:33 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.312472462654114, 'test_avg_loss': 0.6578118115663528, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-11-06 21:39:33 (federatedscope.core.trainers.torch_trainer:181) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 21:39:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=36, total=36)
2025-11-06 21:39:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=36, loss_sum=25.751966, avg_loss=0.715332, seen=36, correct=16, accuracy=0.444444
2025-11-06 21:39:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:39:37 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 0 with val results: {'val_total': 36, 'val_loss': 25.75196635723114, 'val_avg_loss': 0.7153323988119761, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-11-06 21:39:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:39:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.556870, avg_loss=0.663922, seen=40, correct=22, accuracy=0.550000
2025-11-06 21:39:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:39:41 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.556870222091675, 'test_avg_loss': 0.6639217555522918, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-11-06 21:39:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=36, total=36)
2025-11-06 21:39:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=36, loss_sum=25.401295, avg_loss=0.705592, seen=36, correct=18, accuracy=0.500000
2025-11-06 21:39:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:44 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 1 with val results: {'val_total': 36, 'val_loss': 25.401294946670532, 'val_avg_loss': 0.7055915262964036, 'val_seen': 36, 'val_correct': 18, 'val_acc': 0.5}
2025-11-06 21:39:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:39:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.025985, avg_loss=0.675650, seen=40, correct=26, accuracy=0.650000
2025-11-06 21:39:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:48 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.02598464488983, 'test_avg_loss': 0.6756496161222458, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-11-06 21:39:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=36, total=36)
2025-11-06 21:39:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:50 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=36, loss_sum=25.993761, avg_loss=0.722049, seen=36, correct=16, accuracy=0.444444
2025-11-06 21:39:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:51 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 2 with val results: {'val_total': 36, 'val_loss': 25.99376106262207, 'val_avg_loss': 0.7220489184061686, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-11-06 21:39:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:39:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.302518, avg_loss=0.707563, seen=40, correct=19, accuracy=0.475000
2025-11-06 21:39:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:39:55 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.302517771720886, 'test_avg_loss': 0.7075629442930221, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-11-06 21:39:55 (federatedscope.core.trainers.torch_trainer:181) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 21:39:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 21:39:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:39:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=11, loss_sum=8.151049, avg_loss=0.741004, seen=11, correct=4, accuracy=0.363636
2025-11-06 21:39:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:39:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:39:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:39:57 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.151048839092255, 'val_avg_loss': 0.7410044399174777, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-11-06 21:39:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:39:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:39:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=25.970705, avg_loss=0.649268, seen=40, correct=26, accuracy=0.650000
2025-11-06 21:40:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:40:01 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.970704913139343, 'test_avg_loss': 0.6492676228284836, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-11-06 21:40:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 21:40:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=11, loss_sum=8.661100, avg_loss=0.787373, seen=11, correct=4, accuracy=0.363636
2025-11-06 21:40:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:40:03 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 1 with val results: {'val_total': 11, 'val_loss': 8.661100387573242, 'val_avg_loss': 0.7873727625066583, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-11-06 21:40:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:40:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=27.440605, avg_loss=0.686015, seen=40, correct=19, accuracy=0.475000
2025-11-06 21:40:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:40:06 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.440604746341705, 'test_avg_loss': 0.6860151186585426, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-11-06 21:40:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=11, total=11)
2025-11-06 21:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=11, loss_sum=8.760524, avg_loss=0.796411, seen=11, correct=4, accuracy=0.363636
2025-11-06 21:40:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:40:09 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.760523557662964, 'val_avg_loss': 0.7964112325148149, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-11-06 21:40:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:11 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=26.205442, avg_loss=0.655136, seen=40, correct=25, accuracy=0.625000
2025-11-06 21:40:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:40:12 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.205442190170288, 'test_avg_loss': 0.6551360547542572, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-11-06 21:40:12 (federatedscope.core.trainers.torch_trainer:181) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 21:40:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=14, total=14)
2025-11-06 21:40:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=14, loss_sum=9.776573, avg_loss=0.698327, seen=14, correct=6, accuracy=0.428571
2025-11-06 21:40:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:40:15 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 0 with val results: {'val_total': 14, 'val_loss': 9.776572704315186, 'val_avg_loss': 0.6983266217367989, 'val_seen': 14, 'val_correct': 6, 'val_acc': 0.42857142857142855}
2025-11-06 21:40:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:40:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.702360, avg_loss=0.717559, seen=40, correct=20, accuracy=0.500000
2025-11-06 21:40:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1760MB
2025-11-06 21:40:19 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.70236027240753, 'test_avg_loss': 0.7175590068101882, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-11-06 21:40:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=14, total=14)
2025-11-06 21:40:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=14, loss_sum=9.291585, avg_loss=0.663685, seen=14, correct=8, accuracy=0.571429
2025-11-06 21:40:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:40:22 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 1 with val results: {'val_total': 14, 'val_loss': 9.291584968566895, 'val_avg_loss': 0.663684640611921, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-11-06 21:40:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:40:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=28.303123, avg_loss=0.707578, seen=40, correct=20, accuracy=0.500000
2025-11-06 21:40:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:40:26 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.303123354911804, 'test_avg_loss': 0.7075780838727951, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-11-06 21:40:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=14, total=14)
2025-11-06 21:40:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=14, loss_sum=9.833261, avg_loss=0.702376, seen=14, correct=7, accuracy=0.500000
2025-11-06 21:40:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:40:28 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 2 with val results: {'val_total': 14, 'val_loss': 9.833261489868164, 'val_avg_loss': 0.7023758207048688, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}
2025-11-06 21:40:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=40, total=40)
2025-11-06 21:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=29.585873, avg_loss=0.739647, seen=40, correct=14, accuracy=0.350000
2025-11-06 21:40:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1818MB allocated=1735MB
2025-11-06 21:40:32 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.585872530937195, 'test_avg_loss': 0.7396468132734298, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-11-06 21:40:32 (federatedscope.llm.llm_local.server:413) INFO: Adapter 1 is done with the clients [3, 5]
2025-11-06 21:40:32 (federatedscope.llm.llm_local.server:413) INFO: Adapter 2 is done with the clients [2, 1]
2025-11-06 21:40:32 (federatedscope.llm.llm_local.server:413) INFO: Adapter 0 is done with the clients [4]
2025-11-06 21:40:33 (federatedscope.core.trainers.torch_trainer:181) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-11-06 21:40:33 (federatedscope.llm.llm_local.client:181) INFO: Activate the adapter 1 for training...
2025-11-06 21:40:33 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=-1
2025-11-06 21:40:33 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-06 21:40:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=False, world_size=1, rank=0, local_count=691, total=691)
2025-11-06 21:40:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:34 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-06 21:40:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-06 21:40:34 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=30, grad_accum_step=2 (=> total micro-batches = 60)
2025-11-06 21:40:34 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=346, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-11-06 21:40:52 (federatedscope.llm.trainer.trainer:1265) INFO: [train|final] total=120, loss_sum=85.452203, avg_loss=0.712102, seen=120, correct=58, accuracy=0.483333
2025-11-06 21:40:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-06 21:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=?] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140075258085376 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-06 21:40:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-06 21:40:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1846MB allocated=1768MB
2025-11-06 21:40:54 (federatedscope.core.trainers.torch_trainer:49) INFO: [DEBUG] get_model_para: raw state_dict keys (partial) = ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight'] (total 1344)
2025-11-06 21:40:54 (federatedscope.core.trainers.torch_trainer:52) INFO: [DEBUG] get_model_para: filtered keys (partial) = ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight'] (total 1344)
